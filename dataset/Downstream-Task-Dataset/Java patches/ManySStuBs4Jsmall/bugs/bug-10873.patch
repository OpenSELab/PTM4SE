diff --git a/deeplearning4j-core/src/main/java/org/deeplearning4j/gradientcheck/GradientCheckUtil.java b/deeplearning4j-core/src/main/java/org/deeplearning4j/gradientcheck/GradientCheckUtil.java
index 37be4e6..a209cd4 100644
--- a/deeplearning4j-core/src/main/java/org/deeplearning4j/gradientcheck/GradientCheckUtil.java
+++ b/deeplearning4j-core/src/main/java/org/deeplearning4j/gradientcheck/GradientCheckUtil.java
@@ -66,7 +66,7 @@
         }
 
         INDArray gradientToCheck = gradAndScore.getFirst().gradient();
-        INDArray originalParams = mln.params();
+        INDArray originalParams = mln.params().dup();
 
         int nParams = originalParams.length();
 
@@ -158,7 +158,7 @@
         updater.update(graph, gradAndScore.getFirst(), 0, graph.batchSize());
 
         INDArray gradientToCheck = gradAndScore.getFirst().gradient();
-        INDArray originalParams = graph.params();
+        INDArray originalParams = graph.params().dup();
 
         int nParams = originalParams.length();
 
diff --git a/deeplearning4j-core/src/main/java/org/deeplearning4j/nn/layers/recurrent/LSTMHelpers.java b/deeplearning4j-core/src/main/java/org/deeplearning4j/nn/layers/recurrent/LSTMHelpers.java
index 6b239e1..2e4b3db 100644
--- a/deeplearning4j-core/src/main/java/org/deeplearning4j/nn/layers/recurrent/LSTMHelpers.java
+++ b/deeplearning4j-core/src/main/java/org/deeplearning4j/nn/layers/recurrent/LSTMHelpers.java
@@ -36,8 +36,8 @@
  * Please note that truncated backpropagation through time (BPTT) will not work with the bidirectional layer as-is.
  * Additionally, variable length data sets will also not work with the bidirectional layer.
  *
- * @author Alex Black
- * @author Benjamin Joseph
+ * @author Alex Black (LSTM implementation)
+ * @author Benjamin Joseph (refactoring for bidirectional LSTM)
  */
 public class LSTMHelpers {
 
diff --git a/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BackTrackLineSearch.java b/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BackTrackLineSearch.java
index 184246a..e440905 100755
--- a/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BackTrackLineSearch.java
+++ b/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BackTrackLineSearch.java
@@ -215,7 +215,7 @@
                 throw new IllegalArgumentException("Current step == oldStep");
 
             // step
-            candidateParameters = Shape.toOffsetZeroCopy(parameters,'f');   //Convention: f order for params and gradient flattening
+            candidateParameters = parameters.dup('f');
             stepFunction.step(candidateParameters, searchDirection, step);
             oldStep = step;
 
diff --git a/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BaseOptimizer.java b/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BaseOptimizer.java
index 74dfec0..fed611e 100644
--- a/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BaseOptimizer.java
+++ b/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BaseOptimizer.java
@@ -296,7 +296,7 @@
     @Override
     public  void setupSearchState(Pair<Gradient, Double> pair) {
         INDArray gradient = pair.getFirst().gradient(conf.variables());
-        INDArray params = model.params();
+        INDArray params = model.params().dup(); //Need dup here: params returns an array that isn't a copy (hence changes to this are problematic for line search methods)
         searchState.put(GRADIENT_KEY,gradient);
         searchState.put(SCORE_KEY,pair.getSecond());
         searchState.put(PARAMS_KEY,params);
diff --git a/deeplearning4j-core/src/main/java/org/deeplearning4j/gradientcheck/GradientCheckUtil.java b/deeplearning4j-core/src/main/java/org/deeplearning4j/gradientcheck/GradientCheckUtil.java
index 37be4e6..a209cd4 100644
--- a/deeplearning4j-core/src/main/java/org/deeplearning4j/gradientcheck/GradientCheckUtil.java
+++ b/deeplearning4j-core/src/main/java/org/deeplearning4j/gradientcheck/GradientCheckUtil.java
@@ -66,7 +66,7 @@
         }
 
         INDArray gradientToCheck = gradAndScore.getFirst().gradient();
-        INDArray originalParams = mln.params();
+        INDArray originalParams = mln.params().dup();
 
         int nParams = originalParams.length();
 
@@ -158,7 +158,7 @@
         updater.update(graph, gradAndScore.getFirst(), 0, graph.batchSize());
 
         INDArray gradientToCheck = gradAndScore.getFirst().gradient();
-        INDArray originalParams = graph.params();
+        INDArray originalParams = graph.params().dup();
 
         int nParams = originalParams.length();
 
