diff --git a/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/mongo/MongoDocumentStore.java b/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/mongo/MongoDocumentStore.java
index 63304e7..ced74db 100644
--- a/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/mongo/MongoDocumentStore.java
+++ b/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/mongo/MongoDocumentStore.java
@@ -21,7 +21,6 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.Date;
-import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -83,6 +82,8 @@ import com.mongodb.WriteConcern;
 import com.mongodb.WriteResult;
 
 import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Predicates.notNull;
+import static com.google.common.collect.Maps.filterValues;
 
 /**
  * A document store that uses MongoDB as the backend.
@@ -284,22 +285,9 @@ public class MongoDocumentStore implements DocumentStore {
                 LOG.trace("invalidateCache: batch size: {} of total so far {}",
                         ids.size(), size);
             }
-            
-            QueryBuilder query = QueryBuilder.start(Document.ID).in(ids);
-            // Fetch only the modCount and id
-            final BasicDBObject fields = new BasicDBObject(Document.ID, 1);
-            fields.put(Document.MOD_COUNT, 1);
-            
-            DBCursor cursor = nodes.find(query.get(), fields);
-            cursor.setReadPreference(ReadPreference.primary());
-            result.queryCount++;
 
-            Map<String, Number> modCounts = new HashMap<String, Number>();
-            for (DBObject obj : cursor) {
-                String id = (String) obj.get(Document.ID);
-                Number modCount = (Number) obj.get(Document.MOD_COUNT);
-                modCounts.put(id, modCount);
-            }
+            Map<String, Number> modCounts = getModCounts(ids);
+            result.queryCount++;
 
             int invalidated = nodesCache.invalidateOutdated(modCounts);
             result.cacheEntriesProcessedCount += modCounts.size();
@@ -906,18 +894,26 @@ public class MongoDocumentStore implements DocumentStore {
             try {
                 dbCollection.update(query.get(), update, false, true);
                 if (collection == Collection.NODES) {
+                    Map<String, Number> modCounts = getModCounts(filterValues(cachedDocs, notNull()).keySet());
                     // update cache
                     for (Entry<String, NodeDocument> entry : cachedDocs.entrySet()) {
                         // the cachedDocs is not empty, so the collection = NODES
                         Lock lock = nodeLocks.acquire(entry.getKey());
                         try {
-                            if (entry.getValue() == null || entry.getValue() == NodeDocument.NULL) {
+                            Number postUpdateModCount = modCounts.get(entry.getKey());
+                            if (postUpdateModCount != null
+                                    && entry.getValue() != null
+                                    && entry.getValue() != NodeDocument.NULL
+                                    && (postUpdateModCount.longValue() - 1) == entry.getValue().getModCount()) {
+                                // post update modCount is one higher than
+                                // what we currently see in the cache. we can
+                                // replace the cached document
+                                NodeDocument newDoc = applyChanges(Collection.NODES, entry.getValue(), updateOp.shallowCopy(entry.getKey()));
+                                nodesCache.replaceCachedDocument(entry.getValue(), newDoc);
+                            } else {
                                 // make sure concurrently loaded document is
                                 // invalidated
                                 nodesCache.invalidate(entry.getKey());
-                            } else {
-                                NodeDocument newDoc = applyChanges(Collection.NODES, entry.getValue(), updateOp.shallowCopy(entry.getKey()));
-                                nodesCache.replaceCachedDocument(entry.getValue(), newDoc);
                             }
                         } finally {
                             lock.unlock();
@@ -925,6 +921,11 @@ public class MongoDocumentStore implements DocumentStore {
                     }
                 }
             } catch (MongoException e) {
+                // some documents may still have been updated
+                // invalidate all documents affected by this update call
+                for (String k : keys) {
+                    nodesCache.invalidate(k);
+                }
                 throw DocumentStoreException.convert(e);
             }
         } finally {
@@ -932,6 +933,35 @@ public class MongoDocumentStore implements DocumentStore {
         }
     }
 
+    /**
+     * Returns the {@link Document#MOD_COUNT} value of the documents with the
+     * given {@code keys}. The returned map will only contain entries for
+     * existing documents.
+     *
+     * @param keys the keys of the documents.
+     * @return map with key to {@link Document#MOD_COUNT} value mapping.
+     * @throws MongoException if the call fails
+     */
+    @Nonnull
+    private Map<String, Number> getModCounts(Iterable<String> keys)
+            throws MongoException {
+        QueryBuilder query = QueryBuilder.start(Document.ID).in(keys);
+        // Fetch only the modCount and id
+        final BasicDBObject fields = new BasicDBObject(Document.ID, 1);
+        fields.put(Document.MOD_COUNT, 1);
+
+        DBCursor cursor = nodes.find(query.get(), fields);
+        cursor.setReadPreference(ReadPreference.primary());
+
+        Map<String, Number> modCounts = Maps.newHashMap();
+        for (DBObject obj : cursor) {
+            String id = (String) obj.get(Document.ID);
+            Number modCount = (Number) obj.get(Document.MOD_COUNT);
+            modCounts.put(id, modCount);
+        }
+        return modCounts;
+    }
+
     DocumentReadPreference getReadPreference(int maxCacheAge){
         if(maxCacheAge >= 0 && maxCacheAge < maxReplicationLagMillis) {
             return DocumentReadPreference.PRIMARY;
diff --git a/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBDocumentStore.java b/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBDocumentStore.java
index f81e155..f07a827 100755
--- a/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBDocumentStore.java
+++ b/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBDocumentStore.java
@@ -1261,6 +1261,9 @@ public class RDBDocumentStore implements DocumentStore {
                         qc.addKeys(chunkedIds);
                         seenQueryContext.add(qc);
                     }
+                    for (String id : chunkedIds) {
+                        nodesCache.invalidate(id);
+                    }
                 }
 
                 Connection connection = null;
@@ -1285,23 +1288,8 @@ public class RDBDocumentStore implements DocumentStore {
                                 qc.addKeys(chunkedIds);
                             }
                         }
-                    }
-                    for (Entry<String, NodeDocument> entry : cachedDocs.entrySet()) {
-                        T oldDoc = castAsT(entry.getValue());
-                        String id = entry.getKey();
-                        Lock lock = locks.acquire(id);
-                        try {
-                            if (oldDoc == null) {
-                                // make sure concurrently loaded document is
-                                // invalidated
-                                nodesCache.invalidate(id);
-                            } else {
-                                addUpdateCounters(update);
-                                T newDoc = createNewDocument(collection, oldDoc, update);
-                                nodesCache.replaceCachedDocument((NodeDocument) oldDoc, (NodeDocument) newDoc);
-                            }
-                        } finally {
-                            lock.unlock();
+                        for (String id : chunkedIds) {
+                            nodesCache.invalidate(id);
                         }
                     }
                 } else {
