,commit_id,commit_time,diff,files,summary
0,b5cb82c689eac0e50522be9d2f55093dadfba24c,2020-06-17 14:32:46-07:00,"updated_per_output_metrics.append ( def logsumexp ( x , axis=None , keepdims=False ) : if do_validation and should_run_validation ( validation_freq , epoch ) : if len ( mask_shape ) == dims - 1 : some of the layers have changed . name='pointwise_kernel ' , all_inputs += list ( x ) def test_cudnn_rnn_timing ( rnn_type ) : bias_regularizer='l2 ' , if isinstance ( x_weight , dict ) : ndim_cond = ndim ( condition ) return self.cell.unit_forget_bias np.testing.assert_allclose ( def FileIO ( self , name , mode ) : output = ( y + y_rev ) / 2 elems : tensor , at least 2 dimensional def ResNet101 ( * args , * * kwargs ) : `` `` '' Mean of a tensor , alongside the specified axis . as a string . of ` Sequence ` ( ` keras.utils.Sequence ` ) . file_id_args = { 'fapl ' : file_access_property_list , node_index = self._output_coordinates [ i ] [ 1 ] categorical_hinge , name , dtype=dtype ) with pytest.raises ( ValueError ) : # DEVICE MANIPULATION AND PROBING output_shape_type = 'raw ' stateful=stateful , inputs = inputs [ : -self._num_constants ] that is used to keep track of the number of false positives . self.patience = patience return tuple ( shape ) ( which may not be informative ) . if last_output in reachable : args=full_arg_spec.args , ( 0.0 , None , 0.8 ) , # set threshold only if training_utils.is_generator_or_sequence ( x ) : assert K.dtype ( K.variable ( 1 , dtype='int16 ' ) ) == 'int16 ' # ` loss_weights ` is invalid type . `` `` '' Python 2/3 compatible ` getargspec ` . http : //bit.ly/keras_flow_from_dataframe ) . Specifies whether to build the model 's graph in inference output = squeeze ( output , 3 ) spatial_axes ) model.compile ( Larger values of ` label_smoothing ` correspond to heavier smoothing . inbound_node.output_tensors [ inbound_tensor_index ] ) return AUCCurve.ROC [ [ .9 , .05 , .05 ] , [ .5 , .89 , .6 ] , [ .05 , .01 , .94 ] ] ) and len ( inputs ) > 1 and initial_state is None ) : if self.model.uses_learning_phase : ( since they generate batches ) . each batch item in ` y_true ` . if layer not in layer_indices : 'the ` requests ` library . ' ) # convert the weights between CuDNNLSTM and LSTM def call ( self , inputs , states ) : return T.stack ( x , axis=axis ) dilation_rate = kwargs.pop ( 'dilation_rate ' , 1 ) while i < len ( outputs ) : weights * = mask warnings.warn ( 'The semantics of the Keras 2 argument ' ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='maxpool3d ' ) `` `` '' Built-in weight initializers . else 'Threshold values must be in [ 0 , 1 ] . Invalid values : { } '.format ( ' '' in the current model ) was found to ' requests = None def on_test_batch_begin ( self , batch , logs=None ) : return [ initial_state ] def get_step_function ( backend , w_i , w_h ) : callbacks = callbacks or [ ] outputs , 'When passing ` initial_state ` to a Bidirectional RNN , ' rand = K.eval ( K.random_binomial ( ( 200 , 200 ) , p ) ) input_shape [ 3 ] + padding [ 1 ] [ 0 ] + padding [ 1 ] [ 1 ] , th_padding = _preprocess_padding ( padding ) env : KERAS_BACKEND=tensorflow MODE=INTEGRATION_TESTS PIL=Pillow return y return default # we have . The user should call ` model._set_inputs ( placeholders ) ` raise AttributeError ( 'The layer `` ' + str ( self.name ) is used more than once ( shared layer ) , this is ill-defined embeddings_initializer='normal ' , dilation_rate=dilation_rate , cntk_dynamicity=True ) return match [ 0 ] if callbacks.model.stop_training : super ( GRU , self ) .__init__ ( cell , if dtype == 'float32 ' : { `` `` '' Returns the current weights of the layer . if callable ( else_expression ) : assert np.array_equal ( before , after ) travis_retry pip install -- only-binary=numpy , scipy , pandas numpy nose scipy h5py theano pytest pytest-pep8 pandas -- progress-bar off output_shape = ( 100 , 30 ) sample_weight = losses_utils.broadcast_weights ( values , sample_weight ) input_shape = input.shape [ num_dynamic : ] kwargs [ 'shape ' ] = args [ 1 ] 'kernel_initializer ' : initializers.serialize ( self.kernel_initializer ) , def unit_forget_bias ( self ) : model_outputs = [ ] return Sequential ( layers= [ input_layer ] + layers , name=model.name ) arg for arg in list ( inspect.signature ( member.__init__ ) .parameters.keys ( ) ) def _get_callback_model ( self ) : 'Expected a list or dictionary , found : ' + str ( metrics ) ) outputs = self.model.evaluate ( x , y , * * kwargs ) In ` 'channels_first ' ` mode , the channels dimension ( the depth ) if ref_dim ! = dim and ref_dim : KNP.eval ( KNP.clip ( x , min_val , max_val ) ) ) model.load_weights ( gcs_filepath ) info += '\n ' assert shape1 == shape2 # else hadamard product is n't possible a0 , a1 = axes tf_file_io = None return ( input_shape [ 0 ] , input_shape [ 1 ] ) if len ( args ) > 1 : `` `` '' Called at the end of a batch in ` predict ` methods . if len ( computed_data ) == len ( reference_input_tensors ) : # first state should be incremented for every timestep ( no masking ) 'in the first layer for automatic build . ' ) y._keras_shape = ( x._keras_shape [ 0 ] , None ) batch_size : The batch_size provided as an argument to assert y._keras_shape == tuple ( shape ) increment : A tensor of same shape as ` x ` . 'When passing a list as ` target_tensors ` , ' def _standardize_args ( inputs , initial_state , constants , num_constants ) : if not os.path.exists ( _config_path ) : config [ 'clipvalue ' ] = self.clipvalue self.bias_i_i = self.bias [ : self.units ] # Compare predictions and threshold . padding=padding , data_format=data_format , assert msle_obj.reduction == losses_utils.Reduction .SUM self._trainable_weights.append ( weight ) self.bias_c = self.bias [ self.units * 2 : self.units * 3 ] save_to_dir=None , py_slice ( padding [ 2 ] [ 0 ] , input_shape [ 4 ] + padding [ 2 ] [ 0 ] ) ) tf_keras_backend.set_floatx ( floatx ) kernel = kernel [ : :-1 , : :-1 , : , : ] 'shape ` % s ` contains non-specified dimension , ' y = [ ] if isinstance ( self._call_batch_hook ( _PREDICT , 'end ' , batch , logs=logs ) check_single_tensor_operation ( 'expand_dims ' , ( 4 , 3 ) , WITH_NP , axis=-1 ) 'Or specify input_shape or batch_input_shape ' K.set_value ( state , value ) if data_format='channels_first ' write_graph=True , `` `` '' Apply multiplicative 1-centered Gaussian noise . origin_layer = x._keras_history [ 0 ] get_weights ( ) raise ValueError ( ' ` output_shape ` function must return a tuple or ' > > > input_transposed = K.transpose ( inputs ) ] ) if x._keras_shape [ 2 ] is not None : def selu ( x ) : If ` x ` has shape ` ( s1 , s2 , s3 ) ` and ` axis ` is ` 1 ` , the output sample_weight=sample_weight , del ( model ) # we will create gradient as a constant placeholder , here use this global closely approximating the true AUC . The quality of the approximation may vary if x_weight is None or len ( x_weight ) == 0 : output_shape = ( shape ( x ) [ 0 ] , ) + tuple ( output_shape [ 1 : ] ) make sure that ndim is at least 2 . allowed_positional_args= [ 'filters ' , 'kernel_size ' ] , assert_list_keras_shape ( t_list , z_list ) from collections import deque axis=list ( range ( weight_ndim , ndim ) ) ) 'Received input shape : ' , str ( input_shape ) ) assert ( history.history [ 'loss ' ] [ -1 ] < 1.2 ) the outputs of ` fit_function ` `` `` '' Called at the end of prediction . if not hasattr ( generator_output , '__len__ ' ) : elif len ( outputs ) > 2 : http : //www.bioinf.jku.at/publications/older/2604.pdf ) self._is_file = False feed_output_shapes.append ( output_shape ) pool_size : Integer , size of the max pooling windows . decoded_dense = [ ] if index is not None : 'Received input : % s ' % x ) ` `` categorical '' ` : 2D numpy array of one-hot encoded labels . class _Cropping ( Layer ) : the provided inputs and the expectations of the layer . def score ( self , x , y , * * kwargs ) : ( 'separable_conv2d ' , ( 2 , 3 , 4 , 5 ) , ( 3 , 3 ) , 1 , 'same ' , 'channels_first ' ) , print ( 'tracking ' , value , name ) raise ValueError ( 'All cells must have a ' if not is_tensor ( n ) : state_shape = output_shape [ 1 : ] inputs_k , _test_optimizer ( optimizers.Adagrad ( ) ) broadcast_gamma = gamma.dimshuffle ( ' x ' , 0 , ' x ' , ' x ' ) ' : the list of Numpy arrays that you are passing to ' if set_y and set_w and list ( set_y ) [ 0 ] ! = list ( set_w ) [ 0 ] : dropout_U=0.1 , mode = ' a ' name : Optional name for the tensor . ( only relevant if ` save_to_dir ` is set ) . # In that case the model gets built the first time you call ` fit ` ( or other exepected size defined in the Input Layer . import inspect assert_allclose ( w , org_w ) [ Fast and Accurate Deep Network Learning by Exponential output_tensors.append ( tensor ) 'gain ' : self.gain output_names , class Dense ( Layer ) : callbacks= [ batch_print_callback , y = np.array ( y ) self.predict_function = None assert not getattr ( y , '_uses_learning_phase ' ) super ( CSVLogger , self ) .__init__ ( ) from .load_backend import less y = np.array ( y ) return custom_objects [ obj ] state_size = list ( self.cell.state_size ) `` `` '' Computes and returns the metric value tensor . if ` return_sequences ` : 3D tensor with shape b_regularizer='l2 ' , format to use is determined from the filename extension . from .pooling import MaxPool2D def random_uniform_variable ( shape , low , high , dtype=None , name=None ) : str ( K.ndim ( x ) ) ) input_shapes = to_list ( input_shape ) if self._uses_dynamic_learning_phase ( ) : @ interfaces.legacy_cropping2d_support x = C.relu ( x ) return nullcontextmanager ( ) if isinstance ( shape , list ) : `` `` '' Max pooling operation for 3D data ( spatial or spatio-temporal ) . ' ` output , new_states = step_function ( inputs , states ) ` ' ) monitor : quantity to monitor . histograms wo n't be computed . Validation data ( or split ) must be output_shape = output_shape [ 1 : ] 'Please provide at least one valid confusion matrix ' zca_epsilon=zca_epsilon , if 0 < self.recurrent_dropout < 1. : name=self.name ) d2 = y.shape [ axes [ 1 ] ] return K.in_train_phase ( normed_training , in the model need to support masking or an exception will be raised . env : KERAS_BACKEND=tensorflow MODE=PEP8_DOC PIL=Pillow class AveragePooling1D ( _Pooling1D ) : backward_state = None raise ValueError ( y = K.repeat_elements ( x , reps , axis=rep_axis ) inputs : a cntk variable ( parameter/constant ) return one_indexed_epoch % validation_freq == 0 self.weights = [ self.iterations , self.m_schedule ] + ms + vs ones , cropping_all_dims = ( ( 0 , 0 ) , ) + self.cropping + ( ( 0 , 0 ) , ) ` labels ` over a stream of data . bucket_name = os.environ.get ( self._test_bucket_env_key , None ) # Arguments output_col = conv_utils.conv_output_length ( input_col , self.kernel_size [ 1 ] , def handle_function ( name , member ) : oh = T.extra_ops.to_one_hot ( indices , num_classes ) num_samples = 2 ' ( thus holding past layer metadata ) , ' # enable run_options . self.input_spec = InputSpec ( ndim=self.rank + 2 ) input_length=input_shape [ 1 ] , return K.spatial_3d_padding ( inputs , def test_upsampling2d_legacy_interface ( ) : ( without it , the shape of the dense outputs can not be computed ) . Supports all values that can be represented as a string , assert len ( layer.get_updates_for ( None ) ) == 2 data_format=None , * * kwargs ) : mask_value : Either None or mask value to skip slice_i = Lambda ( get_slice , return layer Tokenizer = text.Tokenizer sample_weight_mode='temporal ' ) SUM_OVER_BATCH_SIZE = 'sum_over_batch_size ' def get_variable_shape ( x ) : `` `` '' Sigmoid activation function . num_groups=image_shape [ 1 ] ) verbose=1 , def _preprocess_padding ( padding ) : If False , ` gamma ` is not used . if self.rank == 3 : if y.shape [ -1 ] == 1 : category=UserWarning ) 'Received inputs : { } . ' > > > new_arr model_weights_group = h5dict [ 'model_weights ' ] def test_concat_operations ( self , shape , shape2 , axis ) : model_config = { } are not raised in mock implementation . output_shapes= [ x._keras_shape for x in self.outputs ] ) momentum : The moving average momentum . nodes_by_depth = { } positions : Relative or absolute positions of log elements in each line . beta = zeros_like ( x ) loss = mse ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) self.filters ) 'and now defaults to ` implementation=1 ` . ' weights=None ) str ( states ) ) @ pytest.mark.parametrize ( 'rnn_type ' , [ 'LSTM ' , 'GRU ' ] , ids= [ 'LSTM ' , 'GRU ' ] ) pep8ignore= * E402 \ scale=scale , 'cntk ' : True } reshaped_inputs = [ ] ` constants ` . Such constants can be used to condition the cell idx = slice ( start + self.start , stop + self.start ) for p , g , m in zip ( params , grads , moments ) : if len ( reduce_result.shape ) == 0 and _get_dynamic_axis_num ( x ) == 0 : depthwise_constraint='maxnorm ' , when their L2 norm exceeds this value . super ( ModelCheckpoint , self ) .__init__ ( ) input_shape : Shape tuple ( tuple of integers ) z , _ , _ = K.normalize_batch_in_training ( if self.reduction == metrics_utils.Reduction.SUM_OVER_BATCH_SIZE : # ( filters , input_dim , filter_length , 1 ) constants : A list of constant values passed at each step . ' ` ( val_x , val_y , val_sample_weight ) ` ' # that comes at the bottom of the stack . depthwise_initializer='normal ' , { 'go_backwards ' : True , 'mask ' : mask } , shape= ( ) , ` ( batch , channels , depth , height , width ) ` . for i in range ( len ( self.outputs ) ) Specifically , this function implements single-machine self.param_broadcast = [ False ] * len ( param_shape ) weight ) # Here , unlike other backends , the tensors lack a batch dimension : self.moving_variance_initializer = ( computes the area under a discretized curve of precision versus recall values y_true : Optional label ` Tensor ` whose dimensions match ` y_pred ` . layer ( x ) initialization . This parameter is only relevant overwrite = six.moves.input ( 'Enter `` y '' ( overwrite ) or `` n '' ' # The CPU implementation of FusedBatchNorm only support NHWC raise ValueError ( 'negative_slope of ReLU layer can not be ' arg = kwargs.pop ( name ) name = name or `` # we should override the default mask . size = batch_size - step * i def test_global_maxpooling3d_legacy_interface ( ) : ( 'bias ' , 'use_bias ' ) ] , assert_allclose ( f ( [ x_np , y_np ] ) [ 0 ] , z_np , atol=1e-05 ) enqueuer.join_end_of_epoch ( ) assert len ( td._input_map.keys ( ) ) == 1 ask.return_value = True return ( child_output_shape [ 0 ] , timesteps ) + child_output_shape [ 1 : ] if b_any ( [ isinstance ( a , ( list , tuple ) ) for a in axes ] ) : The model weights . import keras tensor_map [ id ( x ) ] = ( y , None ) # tensor , mask if a real GCS bucket is not available for testing . input_dim = input_shape [ 1 ] K.eval ( op ) self.num_thresholds = len ( thresholds ) + 2 # T.nnet.bn.batch_normalization_train is deprecated from .. engine.training import Model raise ValueError ( 'Improper config format : { } '.format ( config ) ) if layer.data_format == 'channels_first ' : inbound_layers : a list of layers , the same length as ` input_tensors ` , def test_deconv2d_legacy_interface ( ) : mock_module.file_exists = self.file_exists # if current layer is not Model or wrapped Model node_key = self._node_key ( layer , node_index ) def __init__ ( self , sensitivity , num_thresholds=200 , name=None , dtype=None ) : if layer.is_placeholder : archive_format : Archive format to try for extracting the file . l2 : L2 regularization factor ( positive float ) . 'bias_regularizer ' : regularizers.serialize ( self.bias_regularizer ) , def recurrent_regularizer ( self ) : if layer not in unprocessed_nodes : batch_input_shape= ( num_samples , [ output_a_np , output_b_np ] , if hasattr ( self , 'callback_model ' ) and self.callback_model : if condition.dtype ! = tf.bool : def pow ( x , a=1 . ) : return C.pad ( x , pattern=pattern ) return training_generator.fit_generator ( return True return C.greater ( x , y ) epsilon ) x : An object to be converted ( numpy array , list , tensors ) . elif pool_mode == 'max ' : get_input_at ( node_index ) return self.optimizer.weights @ pytest.mark.parametrize ( and weights file and skip_mismatch=False . check_single_tensor_operation ( 'l2_normalize ' , ( 4 , 3 ) , WITH_NP , axis=1 ) assert len ( layer.updates ) == 2 depth = nodes_depths.setdefault ( node , 0 ) outputs = K.conv1d ( return K.max ( inputs , axis= [ 1 , 2 ] ) placeholders , Numpy arrays , or data tensors . kwargs [ 'kernel_size ' ] = kernel_size def img_to_array ( img , data_format=None , dtype=None ) : ' ` ' + old_arg + ' ` and the Keras 2 keyword argument ' from . import deserialize as deserialize_layer where d is ` delta ` . See : https : //en.wikipedia.org/wiki/Huber_loss model.add ( Dense ( 5 ) ) if layer.stateful : print_layer_summary_with_connections ( layers [ i ] ) return serialize_keras_object ( optimizer ) if depthwise_kernel_shape is None : super ( Permute , self ) .__init__ ( * * kwargs ) assert mse_obj.reduction == Reduction.SUM ValueError : in case of incompatible weight shapes . return self.layer.losses py_slice ( padding [ 1 ] [ 0 ] , input_shape [ 3 ] + padding [ 1 ] [ 0 ] ) , custom_objects=custom_objects ) self.kernel_h = self.kernel [ : , self.units * 2 : ] `` `` '' Called at the start of an epoch . val_x , val_y , val_sample_weights = self._standardize_user_data ( old_layer = keras.layers.GlobalMaxPooling3D ( dim_ordering='th ' , [ Efficient BackProp ] ( http : //yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf ) if self.when ( argument ) : self.b_regularizer = regularizers.get ( b_regularizer ) def keras_modules_injection ( base_fun ) : equal intervals , along with time series parameters such as _kwargs [ 'filepath ' ] = tmp_filepath deserialized = func_load ( serialized , defaults=test_func.__defaults__ ) # 3 . Callables return themselves elif not self._inbound_nodes : ` Loss ` instance . See [ losses ] ( /losses ) . dim = self.cell.state_size for i in range ( len ( sample_weights ) ) ndim_diff = ndim_expr - ndim_cond raise ValueError ( if loss_name is None : if filter_shape : > > > K.eval ( var ) if len ( weights ) == 12 : 'The following previous layers ' constants : tensor or list of tensors or None ( str ( self.input_length ) , str ( input_shape ) ) ) legacy_zeropadding2d_support = generate_legacy_interface ( ( 'depthwise_conv2d ' , ( 2 , 3 , 5 , 6 ) , ( 4 , 3 , 3 , 4 ) , 'valid ' , 'channels_first ' ) , > > > c = K.to_dense ( b ) params = self._canonical_to_params ( def kernel_constraint ( self ) : a = keras.layers.Dense ( hidden_dim ) ( input_a ) # inputs . Every time the Network is called on a set on input tensors , This method deals with an inherent problem of HDF5 file which is not class Conv2D ( _Conv ) : from tensorflow.keras.layers import InputSpec if type ( layer ) .__name__ == 'Embedding ' ] A tuple ` ( code , defaults , closure ) ` . ` ( samples , filters , new_rows , new_cols ) ` if data_format='channels_first ' loss_fn , kernel_size = ( args [ 2 ] , def __init__ ( self , input_shape=None , batch_size=None , height_shift_range=height_shift_range , validation_data=None , Integer , the number of elements in ` x ` , i.e. , the product of the batch_logs [ 'outputs ' ] = outs padding='valid ' , data_format=None , dilation_rate= ( 1 , 1 ) ) : Learning phase ( scalar integer tensor or Python integer ) . def concatenate ( inputs , axis=-1 , * * kwargs ) : docstring = autogen.process_docstring ( docs_descriptor [ 'doc ' ] ) array/tensors , if the model has named inputs . ( 'conv2d ' , ( 2 , 8 , 9 , 3 ) , ( 3 , 3 , 3 , 2 ) , const = T.constant ( value , lim = 3e-2 in_lens = to_list ( self.input_length , allow_tuple=True ) initializer=None , def test_H5Dict_accepts_pathlib_Path ( ) : for the time being . def _set_keras_shape_for_reduction ( x , y , axis , keepdims ) : ` layer.get_input_at ( node_index ) ` . headers=None , loss_fn.fn == losses.sparse_categorical_crossentropy ) ) or ( `` `` '' Converts a class vector ( integers ) to binary class matrix . batch_logs = { 'batch ' : steps_done , 'size ' : batch_size } output_shape [ self.axis ] = None with h5py.File ( h5_path , ' w ' ) as f : `` `` '' Cell class for SimpleRNN . m , n = model ( [ j , k ] ) `` `` '' Start mocking of ` self.file_io_module ` if real bucket not number of the dimensions of the weights `` `` '' Base class for the Keras scikit-learn wrapper . layers_to_output_shapes = { } new_rows = ( ( rows - 1 ) * strides [ 1 ] + kernel_size [ 1 ] raise ValueError ( 'The initial state of an RNN layer can not be ' self.trainer = C.trainer.Trainer ( output = np.clip ( output , 1e-7 , 1 - 1e-7 ) beta_constraint=None , ctc_label_dense_to_sparse ( y_true , label_length ) , tf.int32 ) from .common import floatx provided we will convert them to -1 or 1 . if sample_weight_mode ! = 'temporal ' : def clone_metrics ( metrics ) : # to the number of unmasked samples . if 0 < self.recurrent_dropout < 1. : _SHARED_SEQUENCES = gens 'hence the notion of `` layer input mask '' ' 'Found : ' + str ( initial_state ) ) model.compile ( loss=custom_loss , optimizer=custom_opt ( ) , metrics= [ 'acc ' ] ) ' equal to rank of ` then_expression ` and ' outbound_layer._inbound_nodes.append ( self ) The total number of depthwise convolution output kwargs = self.filter_sk_params ( Sequential.predict_classes , kwargs ) arg , The same layer can be reinstantiated later recurrent_dropout=0. , and ` y ` is a numpy array of corresponding labels . with the current layer . if new_name in kwargs : # Create the node linking internal inputs to internal outputs . self.stopped_epoch = epoch `` using the NumPy backend . '' ) return self.layer.trainable_weights # to the index of the nodes that are saved in the config . env : KERAS_BACKEND=theano THEANO_FLAGS=optimizer=fast_compile MKL= '' mkl mkl-service '' config [ 'pointwise_constraint ' ] = ( If you never set it , then it will be 'channels_last ' . dilation_rate=dilation_rate , self.backward_layer.initial_weights = weights [ nw // 2 : ] go_backwards=go_backwards , confusion_matrix_cond , 'batch ' is a special option for dealing with the # check that output changes after states are reset go_backwards=go_backwards , layer_indices [ layer ] = len ( layer_indices ) parallel_model = multi_gpu_model ( model , gpus=gpus ) name=name , dtype=dtype , assert output == [ b'test ' ] if prev_total_width > self._total_width : def strides ( self ) : 2 . Squeezes or expands last dim of ` sample_weight ` if its rank differs by 1 name='zp3d ' ) return x_rep `` `` '' Retrieves the input mask tensor ( s ) of a layer . from .load_backend import dropout x = [ expand_dims ( t , axis ) for t in x ] for fname in files : @ skipif_no_tf_gpu 'Failed to import ` pydot ` . ' ValueError : In case of invalid user-provided argument . callbacks=callbacks ) def DISABLED_test_with_list_as_targets ( ) : shape = ( None , ) + v.shape [ 1 : ] inbound_node_index = node.node_indices [ i ] if output_shape [ -1 ] == 1 or is_binary_crossentropy : m.update_state ( [ -1. , 1. , 1 . ] , [ 0.6 , -0.7 , -0.5 ] ) if not any ( not s for s in int_shape ) : return imagenet_utils.decode_predictions ( def allow_read_from_gcs ( load_function ) : backend_list , # Save all metric attributes per output of the model . self.layer = layer random_channel_shift = image.random_channel_shift `` `` '' Returns class probability estimates for the given test data . def func_load ( code , defaults=None , closure=None , globs=None ) : output = K.batch_dot ( x1 , x2 , axes ) value # just access it so it gets captured in .__closure__ > > > K.shape ( inputs ) self.inputs = network.get_source_inputs ( self.outputs [ 0 ] ) new_layer = keras.layers.ConvLSTM2D ( 5 , ( 3 , 3 ) , * * kwargs : standard layer keyword arguments . from .load_backend import foldr ' The classes % s exist in the data but not in ' # and there is only one node and one tensor output . a tensor with ndim ( x ) - 1 dimension Google Cloud Storage , for witch the tensorflow ` file_io ` package is used . orig_y_ndim = y_ndim shape = x.shape ` logs ` x1 , x2 , y = process_line ( line ) if axis ! = -1 and axis ! = output_dimensions [ -1 ] : This is the TF v2 version . A lot of the functionality return [ tensor ] if isinstance ( config , list ) : class Adam ( Optimizer ) : x = node.input_tensors [ i ] if ` return_sequences ` : 5D tensor with shape # tensors , then add them to the inputs and temporarily modify the def __init__ ( self , learning_rate=1.0 , rho=0.95 , * * kwargs ) : ` batch_input_shape= ( ... ) ` to the first layer in your model . For example , if ` y_true ` is [ 0 , 1 , 1 ] , and ` y_pred ` is [ 1 , 0 , 1 ] , the cosine class_name = config [ 'class_name ' ] weights , return output_tensors pool_size=self.pool_size , if not self._layers : This recursively updates the map ` layer_indices ` , check_single_tensor_operation ( 'prod ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) parallel_model.fit ( x , y , epochs=20 , batch_size=256 ) [ TensorFlow installation instructions ] ( https : //www.tensorflow.org/install/ ) . 'Received inputs with shapes ' model.add ( LocallyConnected2D ( 32 , ( 3 , 3 ) ) ) model.compile ( 'sgd ' , metrics= [ keras.metrics.SquaredHinge ( ) ] ) if y.ndim == 2 : 'fixed dimension instead of ` None ` . ' ) return tf.nn.batch_normalization ( x , mean , var , beta , gamma , epsilon ) with open ( path ) as f : for state in states : # input shape and dtype . except ( Exception , KeyboardInterrupt ) : ' % d dimensions are needed . ' old_layer = keras.layers.ConvLSTM2D ( 5 , 3 , nb_col=3 , name='conv ' ) result = x + increment log_prob_pred = K.eval ( log_prob_pred_tf ) str ( len ( x_weight ) ) raise ValueError ( msg ) def test_gather ( self ) : self.weights = [ self.iterations ] + ms + vs + vhats `` `` '' Calls metric functions for a single output . 'an ` input_length ` ' self.updates.append ( K.update ( d_a , new_d_a ) ) 'This model has not yet been built . ' def is_model ( layer ) : The same structure , where occurrences `` Keras requires a thread-safe generator when '' validation_steps = validation_steps or len ( val_data ) start : Integer list/tuple or tensor y = [ x [ : , : , k : k1 : strides [ 0 ] ] return initial_state assert y._uses_learning_phase return probs should be symbolic tensors of the * same shape * . old_layer = keras.layers.ZeroPadding2D ( padding= { 'right_pad ' : 4 , send [ k ] = v 'got a ` Sequential ` instance instead : ' , model ) max_value : float . Saturation threshold . It will be called on each line of the summary . path : where to cache the data ( relative to ` ~/.keras/dataset ` ) . [ batch , depth , height , width , channels ] ( for 'channels_last ' data_format ) training_utils.collect_per_output_metric_info ( that has constant gradient with respect to any other variable . assert len ( layer.get_updates_for ( None ) ) == 0 for ( i , d ) in enumerate ( dilation_rate ) : total_log_prob = T.log ( T.sum ( T.exp ( log_probs - common_factor ) [ mask.nonzero ( ) ] ) ) a ` call ( input_at_t , states_at_t ) ` method , returning return p_op ( x ) `` `` '' Calculates how often predictions matches integer labels . if attr in self.data : return result_t of ` Sequential.predict_classes ` . W = np.random.random ( ( 5 , 4 ) ) y.shape [ 0 ] : 100 : do not append to output shape , `` `` '' Regularizer base class . raise ValueError ( 'The model is not configured to compute accuracy . ' If not provided , the list of classes will be automatically 'Found : ' + str ( padding ) ) weights += cell.weights x = C.swapaxes ( x , -1 , axis ) forward_losses = self.forward_layer.get_losses_for ( inputs ) input_tensors : list of input tensors ( or single input tensor ) . def InceptionV3 ( * args , * * kwargs ) : config = super ( Conv1D , self ) .get_config ( ) sample_weight = None def test_in_test_phase ( self , training ) : input_shapes= [ batch_input_shape ] , min_lr : lower bound on the learning rate . ref_ids = [ id ( w ) for w in sublayer.trainable_weights ] summation_method , list ( metrics_utils.AUCSummationMethod ) ) ) model , input_tensors=input_a ) y_pred , y_true = losses_utils.squeeze_or_expand_dimensions ( # map to keep the mapping from grad placeholder to parameter assert 'def dot ( x , y ) : ' in generated arguments are consistent with this static batch size . Also , if return T.ones_like ( targets , dtype='bool ' ) outputlabels = str ( layer.output_shape ) super ( Precision , self ) .__init__ ( name=name , dtype=dtype ) y._keras_shape [ axis ] = repeat_dim * rep interpolation_order=interpolation_order , Keras is compatible with : __Python 2.7-3.6__ . the static shape of the tensor data_format=self.data_format ) def test_foldl ( self ) : 'dilation_rate ' : self.dilation_rate , def var ( x , axis=None , keepdims=False ) : name = config [ 'name ' ] ( 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='avgpooling2d ' ) have different sizes . For example , the last batch of the epoch bias_constraint=bias_constraint , def convert_nested_model ( weights ) : # its underlying ` Model ` as the ` model ` property . self.kernel_size [ 0 ] , expected_outputs [ 1 , -mask_last_num_timesteps : ] = expected_outputs [ if ins and isinstance ( ins [ -1 ] , int ) : `` `` '' Hyperbolic tangent activation function . start_idx : int , which indicate the first dimension to take from for v in variables : filepath : one of the following : f.close ( ) 3 . The default values of the ` keras.models.Sequential ` output_shapes = to_list ( output_shape ) ( 'conv1d ' , ( 2 , 8 , 2 ) , ( 3 , 2 , 3 ) , 'same ' , 'channels_last ' ) , binary_data = stream.read ( ) # Utility functions provide the keyword argument ` input_shape ` if training is 1 or training is True : from tensorflow.keras.applications.resnet_v2 import ResNet50V2 kernel_dim1=3 , `` `` '' Retrieves the output shape ( s ) of a layer at a given node . # The following are implemented as property functions : self.epsilon ) ) pMin = K.expand_dims ( p [ : self.num_thresholds - 1 ] > 0 , 0 ) regularizer=self.embeddings_regularizer , class Hinge ( LossFunctionWrapper ) : new_x = x [ start [ 0 ] : start [ 0 ] + size [ 0 ] , ... , start [ -1 ] : start [ -1 ] + size [ -1 ] ] ValueError : If the shape of ` sample_weight ` is not compatible with ` losses ` . not validation_steps ) : outs [ 0 ] /= steps # Index 0 == ` Loss ` layer : layer instance . # now model.output_shape == ( None , 24 , 20 , 3 ) x._uses_learning_phase = uses_learning_phase str ( y_shape ) + ' . ' ) ' class . Please specify ` validation_steps ` or use ' elif eta > 60 : if self.embeddings_freq and self.embeddings_data is not None : json.dumps ( { 'epoch ' : epoch , 'loss ' : logs [ 'loss ' ] } ) + '\n ' ) , # we ca n't figure out how to repeat it in cntk now `` `` '' Wrapper allowing a stack of RNN cells to behave as a single cell . return self._get_node_attribute_at_index ( 0 , 'output_masks ' , y_squashed = False * * kwargs ) layer = deserialize_layer ( config.pop ( 'layer ' ) , y_true = K.switch ( K.greater ( smoothing , 0 ) , _smooth_labels , lambda : y_true ) x = np.random.rand ( 10 , 3 ) .astype ( np.float32 ) the latest best model according to from .pooling import MaxPooling3D # even though the TF op does print to stdout . from tensorflow.keras.applications.mobilenet_v2 import decode_predictions from .. legacy.layers import Recurrent def _preprocess_conv3d_filter_shape ( filter_shape , data_format ) : inputs = K.reverse ( inputs , 1 ) mask_vals [ 1 , -mask_last_num_timesteps : ] = 0 ` channels_first ` . # If the given loss is not an instance of the ` Loss ` class def get_step_function ( backend , w_i ) : from keras.models import load_model nw = len ( weights ) k = Input ( shape= ( 32 , ) , name='input_k ' ) used in the vocabulary ( input_dim should equal size of initial_states , transformation on additional static inputs ( not changing over time ) , python : def __init__ ( self , learning_rate=0.01 , * * kwargs ) : @ pytest.mark.parametrize ( 'function_name ' , return T.ones_like ( x , dtype=dtype ) dim_ordering='th ' , d1 = x_shape [ a0 ] x = K.variable ( np.random.random ( x_shape ) ) self.merged = tf.summary.merge_all ( ) for i , loss_function in enumerate ( self.loss_functions ) : [ _.output for _ in unrelated_updates ] ) Highway layers are a natural extension of LSTMs to feedforward networks . return grads class GlobalMaxPooling3D ( _GlobalPooling3D ) : padding : string , `` same '' or `` valid '' . return T.gt ( x , y ) return self.cell.kernel_regularizer `` `` '' Serialize a layer . m_t = self.beta_1 * m + ( 1 . - self.beta_1 ) * g if ndim is not None and ndim > spec.max_ndim : 'Received : ' + str ( x ) device = _get_current_tf_device ( ) cntk_axis [ i ] -= nones for k in WITH_NP ] > > > kvar = K.variable ( np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) , dtype='float32 ' ) model.fit_generator ( generator=train_generator ( ) , from six import string_types for x in input_tensors : recurrent_activation=recurrent_activation , @ saving.allow_read_from_gcs tf_data_format = 'NHWC ' array = np.random.random ( ( 4 , 5 , 512 ) ) broadcast_beta , ' ` DepthwiseConv2D ` ' class MinMaxNorm ( Constraint ) : sample_weights=None , broadcast_mean = tf.reshape ( mean , target_shape ) self._nodes_by_depth = nodes_by_depth self.pointwise_initializer = initializers.get ( pointwise_initializer ) [ tf.is_variable_initialized ( v ) for v in candidate_vars ] ) ' Numpy arrays instead . ' old_layer = keras.layers.GlobalMaxPooling2D ( dim_ordering='tf ' , # now : model.output_shape == ( None , 64 , 10 ) return per_output_metrics super ( AveragePooling2D , self ) .__init__ ( pool_size , strides , padding , def on_test_begin ( self , logs=None ) : `` `` '' MaxNorm weight constraint . if y_ndim == 2 : ins = x + y + sample_weights Standalone usage : String , dtype of ` x ` . self.param_broadcast [ i - 1 ] = True return _LEARNING_PHASE return conv ( x , w , padding=padding , data_format=data_format ) self.distribution = distribution print ( ' [ TIP ] Next time specify overwrite=True ! ' ) return CustomObjectScope ( * args ) if hasattr ( self.cell.state_size , '__len__ ' ) : return np.reshape ( x , ( -1 , ) ) also whether this argument can be called with a keyword ( i.e . if it is def reset_states ( self ) : # normalize both inputs to rank 3 . self.state_spec = None class RemoteMonitor ( Callback ) : The loss may potentially be conditional on some inputs tensors , > > > K.normalize_data_format ( 'channels_last ' ) self.batch_id = batch_id = tf.placeholder ( tf.int32 ) dummy_axis = 2 if self.data_format == 'channels_last ' else 3 parallel_siamese = multi_gpu_model ( siamese , 2 ) ( integer tuple , may include ` None ` entries ) . y_rev = self.backward_layer.call ( inputs , for the cell , the value will be inferred by the first element if accept_all and arg_spec.varkw is not None : `` `` '' Accumulates metric statistics . verbose=verbose , msle = MSLE = mean_squared_logarithmic_error ValueError : In one of the two cases below : eta_format = ( ' % d : % 02d : % 02d ' % dtype=dtype , '2nd entry of cropping ' ) super ( LearningRateScheduler , self ) .__init__ ( ) if repeat_dim is not None : If x is a Keras tensor : variance = C.minus ( variance_mean , C.square ( shifted_mean ) ) from_logits=True ) path='/publish/epoch/end/ ' , dtype : String , data type of returned Keras variable . recurrent_activation='sigmoid ' , x_flatten = reshape ( inputs [ : , : , slice_row , slice_col ] , 'and ' + str ( list ( set_y ) [ 0 ] ) + ' target samples . ' ) One metric value is generated for each threshold value . 'they can not be the output of ' For example , if ` y_true ` is [ 1 , 2 , 3 , 4 ] and ` y_pred ` is [ 0 , 2 , 3 , 4 ] reason='Specific to Theano . ' ) ( less the dimension that was summed over ) and y 's shape def disable_tracking ( func ) : @ allow_read_from_gcs such as reason='Uses the ` fetches ` argument . ' ) n_gates ) ' '' ) , weight ' ( K.backend ( ) ! = 'tensorflow ' or self.top_k = top_k # ensure biases are non-zero and properly converted 'flags ' : h5py.h5f.ACC_RDONLY , j * stride_col + kernel_size [ 1 ] ) # because in that case even chunking the array would not make the saving for pooling_class in [ layers.GlobalMaxPooling2D , x , _ , _ = self._standardize_user_data ( x ) return cell_losses + super ( RNN , self ) .get_losses_for ( inputs ) ( batch_size , filters , new_rows , new_cols ) from .callbacks import LambdaCallback computed_tensors = [ computed_tensor ] callbacks=callbacks , initializers.serialize ( self.depthwise_initializer ) ) new_layer = keras.layers.GlobalMaxPool2D ( data_format='channels_last ' , import queue kshp=kernel_shape , # Propagate to all previous tensors connected to this node . is_match_fn = tarfile.is_tarfile model.add ( keras.layers.Dense ( 4 ) ) self.kernel = self.add_weight ( shape=self.kernel_shape , _runner ( initializers.glorot_normal ( ) , tensor_shape , # New interface : ( loss , params ) def flow ( self , groups=x.shape [ 0 ] ) execute=lambda x : print ( message ) ) ) Example 1 - Training models with weights merge on CPU * W503 self.state_size = ( self.filters , self.filters ) self.thresholds = [ 0.0 ] + thresholds + [ 1.0 ] ( 'dropout_W ' , 'dropout ' ) , if tf.__version__.startswith ( ' 1 . ' ) : broadcast_moving_mean = K.reshape ( self.moving_mean , self.threshold = K.cast_to_floatx ( threshold ) name='mean_absolute_percentage_error ' ) : with gzip.open ( paths [ 1 ] , 'rb ' ) as imgpath : # change tensorflow order to keras backend order if relevant_nodes and node not in relevant_nodes : trainable ( boolean ) use_multiprocessing : Boolean . `` `` '' Serializes a user defined function . cntk_shape = tuple ( cntk_shape ) init_args = [ return ( input_shape [ 0 ] , self.n , input_shape [ 1 ] ) return K.cast ( K.in_top_k ( y_pred , K.cast ( K.flatten ( y_true ) , 'int32 ' ) , k ) , y_pred_labels = K.argmax ( y_pred , axis=-1 ) theano.config.floatX = floatx ( ) A Keras model instance ( uncompiled ) . model.compile ( 'sgd ' , metrics= [ keras.metrics.MeanSquaredLogarithmicError ( ) ] ) 'op , input_shape , kernel_shape , depth_multiplier , padding , data_format ' , [ from tensorflow.keras.metrics import * h = x._keras_shape [ 2 ] + padding [ 0 ] [ 0 ] + padding [ 0 ] [ 1 ] metrics= [ 'accuracy ' ] ) `` `` '' Clone a ` Sequential ` model instance . return K.mean ( K.equal ( y_true , K.round ( y_pred ) ) , axis=-1 ) pool_size=2 , padding='valid ' , data_format='channels_last ' , name='maxpool2d ' ) cudnn_layer = cudnn_rnn_layer_class ( units ) A mask tensor or list of mask tensors . msle = keras.losses.MeanSquaredLogarithmicError ( ) from . import sequence batch_axis = C.Axis.default_batch_axis ( ) mask : Tensor or list of tensors . check_single_tensor_operation ( 'tile ' , ( 3 , 4 ) , WITH_NP , n=2 ) self.shape = shape To reset the states of your model , call ` .reset_states ( ) ` on either os.makedirs ( datadir ) get_input_shape_at ( node_index ) Assumes that the layer will be built def min ( x , axis=None , keepdims=False ) : h = x._keras_shape [ 2 ] + top_pad + bottom_pad # Used with permission from Shawn Tan old_layer = keras.layers.GlobalAveragePooling3D ( dim_ordering='tf ' , activation=None , Unrolling is only suitable for short sequences . for i in range ( len ( successive_states [ -1 ] ) ) : input_shape [ 1 ] + top_pad + bottom_pad , ` keras.models.model_from_yaml ( yaml_string , custom_objects= { } ) ` . `` `` '' Layer that computes a dot product between samples in two tensors . # input shape ( partially ) unknown ? replace -1 's with None 's if include_optimizer and model.optimizer : return True # List of integers , 1:1 mapping with inbound_layers . raise TypeError ( ' ` ' + object_name y = K.flatten ( x ) ultimately returned as ` sparse categorical accuracy ` : an idempotent operation from .pooling import MaxPooling1D check_single_tensor_operation ( 'var ' , ( 4 , 2 ) , WITH_NP ) states = initial_states return output + states inputs = Input ( shape= ( 1 , 2 ) ) check_single_tensor_operation ( 'temporal_padding ' , ( 2 , 3 , 4 ) , fan_out = shape [ 0 ] * receptive_field_size outputs = layer ( inputs ) return self.cells [ -1 ] .state_size [ 0 ] # if tuple , convert to list return tf.minimum ( x , y ) This will raise a warning if ` trainable_weights ` and x - xm ) .sum ( axis=axis , keepdims=True ) * * kwargs ) output = C.clip ( output , epsilon ( ) , 1.0 - epsilon ( ) ) with pytest.raises ( TypeError ) : from tensorflow.keras.layers import TimeDistributed model.predict_generator ( generator=pred_generator ( ) , state = pickle.dumps ( model ) algorithm : Hash algorithm , one of 'auto ' , 'sha256 ' , or 'md5 ' . 'learning rate to % s . ' % ( epoch + 1 , new_lr ) ) self._output_tensor_cache = { } output_mask = get_matching_mask ( mask , outputs ) grad = k.gradients ( loss , [ exp ] ) assert np.allclose ( K.eval ( K.clip ( x_k , min_val_k , max_val_k ) ) , [ Maxout Networks ] ( http : //arxiv.org/abs/1302.4389 ) if dims == 4 : kwargs [ 'mask ' ] = computed_masks super ( SensitivitySpecificityBase , self ) .__init__ ( name=name , dtype=dtype ) ' ( and thus will be missing ' replace=True , inner_activation='hard_sigmoid ' , ( ( 2 , 5 ) , ( 1 , 0 ) , ( 1 , 4 ) ) , if py_any ( [ isinstance ( a , ( list , tuple ) ) for a in axes ] ) : last_output , outputs , states = K.rnn ( self.step , pointwise_kernel_shape = ( self.depth_multiplier * input_dim , self.filters ) name= '' ) : self.kernel , # to reshape we need to be cleanly divisible by batch size def on_batch_begin ( self , batch , logs=None ) : lr = self.learning_rate specs = [ ] loop_vars [ ConfusionMatrix.FALSE_NEGATIVES ] = ( label_is_pos , pred_is_neg ) rng.shuffle ( indices ) tf_data_format = 'NCDHW ' # masking not explicitly supported : return None as mask function is None . for dim , ref_dim in zip ( data_shape , shape ) : def test_setfloatx_correct_values ( self , dtype ) : cval=0. , scope is explicitly set on the device type . return tf_keras_backend.function ( inputs , outputs , def load_data ( ) : of the input . initializers.serialize ( self.kernel_initializer ) , 'Got tensor with shape : % s ' % str ( shape ) ) y_pred : A floating point ` Tensor ` of arbitrary shape and whose values are in init='normal ' , return class_sample_weight # call layer backend_list , # Consider an array of 5 labels out of a set of 3 classes { 0 , 1 , 2 } : for each sample at index i in a batch will be used as initial if self.stateful and batch_size : if spec_dim ! = dim : `` `` '' Called at the beginning of training . 'Method ( % s ) is slow compared ' super ( TensorBoard , self ) .__init__ ( ) config [ 'depth_multiplier ' ] = self.depth_multiplier `` `` '' Implements topological ( order-based ) weight loading . if member.__doc__ is None and not member_too_small ( member ) : # now model.output_shape == ( None , 10 , 64 ) , where None is the batch dimension . return tf.nn.softsign ( x ) if sequential_like : if self.verbose > 0 : carried out on 2 tensors with shapes shape1 and shape2 . return K.mean ( inputs , axis= [ 2 , 3 ] ) 'Found : ' + str ( layer ) ) 'name ' : self.name , left_pad = kwargs [ 'padding ' ] .get ( 'left_pad ' , 0 ) def _make_predict_function ( self ) : py_any = any data_format=None , dilation_rate= ( 1 , 1 ) ) : You want to mask sample # 0 at timestep # 3 , and sample # 2 at timestep # 5 , x_shape = ( 2 , ) + shape + ( 3 , ) closure=closure ) class Progbar ( object ) : steps = len ( generator ) axis = _normalize_axis ( axis , tensors [ 0 ] ) dot product axis before taking the dot product . # E402 module level import not at top of file - temporary measure to continue adding ros python packaged in sys.path words that were present in the training set but are not included Reducing Internal Covariate Shift ] ( https : //arxiv.org/abs/1502.03167 ) if e not in entries : return lp raise ValueError ( ' Can not set item in read-only mode . ' ) class GRU ( RNN ) : assert signature.startswith ( 'keras . ' ) ref_params = layer._cudnn_lstm.canonical_to_params ( def trainable ( self ) : legacy_lambda_support = generate_legacy_interface ( from keras_applications import vgg19 result = theano.sandbox.cuda.dnn.dnn_batch_normalization_test ( return shape [ : num_dynamic ] + non_dyn_shape self._trainable_weights = [ ] config = { 'pool_size ' : self.pool_size , if isinstance ( axes , tuple ) : dropout_U=0.1 , threshold : float . Threshold value for thresholded activation . inputs ) It takes as input a list of tensors , raise ValueError ( ' ` scale ` must be a positive float . Got : ' , scale ) A normalized copy of the array . from tensorflow.keras.utils import OrderedEnqueuer `` `` '' Helper function for on_ { train|test|predict } _end methods . '' '' '' # the Theano and TensorFlow CTC code use different methods to ensure if orig_x_ndim == 2 : return np.zeros_like ( x , dtype=dtype ) If output layers in the model are named , you can also pass a batch_size = y_shape [ y_ndim - 1 ] # Compute ` num_thresholds ` thresholds in [ 0 , 1 ] # List of tensors , created by outbound_layer.call ( ) . if download : from tensorflow.keras.layers import UpSampling2D > > > K.eval ( kvar_zeros ) return unique_tensors + non_tensors or ` collections.Container ` instance ( e.g . list , tuple , etc. ) . If an if len ( generator_output ) == 2 : def mean ( x , axis=None , keepdims=False ) : if ( i not in skip_target_weighing_indices and raise ValueError ( self.validation_data = None legacy_cropping3d_support = generate_legacy_interface ( weights += cell.trainable_weights assert ( history.history [ 'loss ' ] [ -1 ] < 1 . ) kwargs.pop ( 'output_shape ' ) zoom_range=zoom_range , raise KeyError ( ' Can not set attribute . ' def _write_logs ( self , logs , index ) : overwrite = six.moves.input ( ' [ WARNING ] % s already exists - overwrite ? ' # indexing trick the axes to compute the logsumexp . If ` None ` ( default ) , computes self.on_train_begin ( ) This decorator parses the ` filepath ` argument of the ` load_function ` and sample_size = K.cast ( sample_size , dtype='float32 ' ) # Since the batch size is 256 , each GPU will process 32 samples . _keras_dir = os.environ.get ( 'KERAS_HOME ' ) where ` x ` is a numpy array containing a batch input_prob = np.array ( [ if 'input_length ' in kwargs : trainable_weights ( list of variables ) num_layers=1 , weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 0 , 1 , 3 , 2 ) ) embeddings_initializer='uniform ' , embedding = config.embeddings.add ( ) callbacks._call_begin_hook ( 'test ' ) custom_objects = { } model = keras.models.Sequential ( ) top_paths=1 , merge_repeated=False ) : name : ( optional ) String , name for the foldr node in the graph . expected_outputs = inputs_vals.copy ( ) def ResNet50 ( * args , * * kwargs ) : 'W_regularizer ' : regularizers.l2 ( 0.01 ) , shapes = [ ] # same size of node.input_tensors . if K.dtype ( inputs ) ! = 'int32 ' : # Keep the good ones self.target_shape = shape if layer_name in embeddings_metadata : regularization += self.l1 * K.sum ( K.abs ( x ) ) assert_list_pairwise ( v_list ) if isinstance ( inputs , list ) : during prediction . # Theano might not accept long type self._per_output_weighted_metrics = updated_per_output_weighted_metrics self.bias_regularizer = regularizers.get ( bias_regularizer ) _axis = [ _ + len ( shape ) if _ < 0 else _ for _ in axis ] `` `` '' Abstract wrapper base class . Legal arguments are the arguments of ` Sequential.predict ` . self.target = target strides= ( 2 , 2 , 2 ) , if result.ndim == 1 : Usage with the compile API : initial_states_k , gamma_constraint=None , check_single_tensor_operation ( 'sign ' , ( 4 , 2 ) , WITH_NP ) losses.cosine_proximity , recurrent_regularizer=None , inbound_layer , node_index , tensor_index = x._keras_history to be passed as input to any Keras function model : Keras model instance to be saved . return self.activation ( inputs ) is incompatible with an output . step , model.compile ( loss=keras.losses.categorical_crossentropy , if len ( inputs ) ! = 2 : inputs : A tensor or list of tensors . ` y_true ` values are expected to be -1 or 1 . If binary ( 0 or 1 ) labels are adadelta = Adadelta raise TypeError ( `` Constraint must be None when `` return tf.slice ( x , start , size ) x = tf.nn.relu ( x ) > > > print ( K.is_sparse ( a ) ) # Mask is smaller than the input , expand it full_arg_spec = inspect.getfullargspec ( fn ) Use its children classes ` LSTM ` , ` GRU ` and ` SimpleRNN ` instead . if not is_tensor ( value ) : label = np.array ( [ [ .4 , .6 ] , [ .3 , .7 ] , [ .1 , .9 ] , [ .2 , .8 ] ] , dtype=np.float32 ) # order of state_size . is_binary_crossentropy = ( ' non-Keras tensors ' ) reason='cntk only supports dilated conv transpose on GPU ' ) x = tf.clip_by_value ( x , zero , max_value ) if output_shape [ 0 ] is None : batch = tf.assign ( embedding [ batch_id : batch_id + step ] , Do not specify the ` batch_size ` if your data is in the ( 'conv2d ' , ( 2 , 3 , 4 , 5 ) , ( 3 , 3 , 3 , 2 ) , 'same ' , 'channels_first ' ) , return _GLOBAL_CUSTOM_OBJECTS if _get_dynamic_axis_num ( constant ) == 1 : if ndim ( result ) == 1 : ' -- upgrade -- no-deps ' ) assert len ( layer.get_losses_for ( x ) ) == 0 raise ValueError ( 'Invalid AUC curve value `` % s '' . ' % key ) raise IOError ( z = k.eval ( t ) i = ( x.shape [ 4 ] + strides [ 2 ] - 1 ) // strides [ 2 ] base_config = super ( _ZeroPadding , self ) .get_config ( ) from .load_backend import ctc_label_dense_to_sparse from . import local if not callable ( output_shape ) : if len ( tensors ) == 0 : print ( `` Training using multiple GPUs .. '' ) dtype=None , input_tensor=None , sparse=False , name=None ) : class LocallyConnected2D ( Layer ) : def get_loss_function ( loss ) : except that it creates new layers ( and thus new weights ) instead self.params = { } mask_cache_key += ' _ ' + object_list_uid ( masks ) if not keras_shape_list : check_single_tensor_operation ( 'square ' , ( 4 , 2 ) , WITH_NP ) dp = p [ : self.num_thresholds - 1 ] - p [ 1 : ] 'this is disallowed . Pass ` strides ` ' 'and can not be automatically inferred ' steps_per_epoch : Integer or ` None ` . expected_state = initial_state_vals.copy ( ) variance_mean = C.reduce_mean ( variance_mean , axis=axis ) return tf_state_ops.assign ( x , new_x ) class LearningRateScheduler ( Callback ) : sample_weight_mode.get ( name ) , x = repeat_elements ( x , height_factor , axis=3 ) parameters are the arguments of ` build_fn ` . Note that like all other decode_pred = K.eval ( decode_pred_tf [ 0 ] ) if self.send_as_json : # to non-input layers . return image.img_to_array ( img , data_format=data_format , dtype=dtype ) return unpack_singleton ( output_shapes ) assert output_shape == expected_output_shape raise ValueError ( ' A ` Dot ` layer should be called ' name='us3d ' ) reshape : Reshape weights to fit the layer when the correct number [ x._keras_shape for x in computed_tensors ] ) chunk_id += 1 'seed ' : self.seed `` `` '' Layers that can merge several inputs into one . if isinstance ( update , tuple ) : raise RuntimeError ( 'start called on already started tf_file_io_proxy ' ) dropout=0.2 , Normalize the activations of the previous layer at each batch , super ( Highway , self ) .__init__ ( * * kwargs ) return theano.foldr ( lambda x , acc : fn ( acc , x ) , metrics= [ metrics.categorical_accuracy ] ) outs [ 0 ] /= num_samples # Index 0 == ` Loss ` [ A Theoretically Grounded Application of Dropout in 'an input with shape ' if i < len ( args [ 1 : ] ) - 1 or kwargs : padding=self.padding , False otherwise . check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=-1 ) input_shapes = self._inbound_nodes [ 0 ] .input_shapes for ( k , k1 ) in zip ( range ( pool_size [ 0 ] ) , range ( -pool_size [ 0 ] , 0 ) ) : # K.int_shape to work in call ( ) with initial_state . raise ValueError ( 'No model found in config . ' ) exclusive . optimizer_weights_group [ 'weight_names ' ] = weight_names to 'minoring ' or 'majoring ' can help quantify the error in the approximation trainable : Boolean , whether the layer weights return K.cast ( K.in_top_k ( y_pred , K.argmax ( y_true , axis=-1 ) , k ) , K.floatx ( ) ) if getattr ( last_output , '_uses_learning_phase ' , False ) : return np.flip ( x , axes ) max_value : the maximum norm for the incoming weights . for i in range ( 2 , 9 ) : assert isinstance ( inputs , ( list , tuple ) ) return self.cell.recurrent_initializer # splitting does n't matter as long as the two sets sum is kept . shared_axes : the axes along which to share learnable assert input_shape [ -1 ] linear = abs_error - quadratic index_array = index_array.reshape ( ( batch_count , batch_size ) ) K.clear_session ( ) strides , return x.copy ( name=name ) out = model.predict ( x ) self.beta_2 = K.variable ( beta_2 , name='beta_2 ' ) layers : Layers to load . if _axis [ i ] is not None : x , y , sample_weight = generator_output per prediction . `` `` '' Loads the IMDB dataset . ` ( samples , output_row , output_col , filters ) ` for ( ref , sw , cw , mode ) in ' ` get_session ` is not available ' the axes to compute the mean . If ` None ` ( default ) , computes to_json name : Check if ` fn ` can be called with ` name ` as a keyword argument . normed = tf.nn.batch_normalization ( x , mean , var , [ 0.0 , 0.0 , 0.1 , 0.9 ] , # t=2 def learning_phase ( ) : self.bias_o = self.bias [ self.filters * 3 : ] self._output_tensor_cache [ cache_key ] = output_tensors > > > kvar = K.variable ( value=val ) axis=0 , name=name ) ) self._feed_loss_fns = [ ] embeddings_layer_names : a list of names of layers to keep eye on . If h5dict = H5Dict ( d ) self.forward_layer.trainable = value for i in range ( len ( names ) ) : check_two_tensor_operation ( 'greater ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) name='v_ ' + str ( i ) ) kwargs.pop ( 'constants ' ) num_rows = 1 layer.bias_z , do_reshape = True output_masks=output_mask , for layer in getattr ( self , '_input_layers ' , [ ] ) : x = repeat_elements ( x , height_factor , axis=2 ) enqueuer.stop ( ) assert len ( input_shape ) > = 2 `` `` '' Model serialization logic . k : ( Optional ) Number of top elements to look at for computing accuracy . K.prod ( x_shape [ 1 : ] ) ] ) ) output = reshape ( output , the step function , of shape ` ( samples , ... ) ` . ( cropping , cropping ) ) if shift is None : output = conv2d ( x , kernel , if not s : unit_forget_bias=unit_forget_bias , return x == y 'Similarly ` nb_val_samples ` - > ` validation_steps ` and ' ( self.true_negatives / ( self.true_negatives + self.false_positives ) ) , input_h = tf.expand_dims ( input_h , axis=0 ) subsample= ( 2 , 2 ) , self._is_compiled = False global _LEARNING_PHASE_PLACEHOLDER Note that if ` shape ` was symbolic , we can not return a variable , the mean over all dimensions . classes = ( proba > 0.5 ) .astype ( 'int32 ' ) # old : i , c , f , o indices_for_conversion_to_dense.append ( i ) self.model.save_weights ( filepath , overwrite=True ) `` `` '' Max pooling operation for temporal data . target = self.params [ 'samples ' ] # test that new updates are the same with both models channel_axis = 3 dynamic_dimension = C.InferredDimension for l , o in zip ( out_labels , val_outs ) : self.epochs_since_last_save = 0 elif key in ( 'minoring ' , 'Minoring ' ) : max_queue_size : Integer . Maximum size for the generator queue . '2nd entry of padding ' ) `` `` '' Model-related utilities . '' '' '' new_output = n_s [ -1 ] tmp = tf.tile ( tf.range ( label_shape [ 0 ] ) , max_num_labels_tns ) _keras_base_dir = '/tmp ' return K.sum ( ( x [ : self.num_thresholds - 1 ] - x [ 1 : ] ) * heights ) # Module is a valid backend if it has the required entries . x : A variable . super ( MSE_MAE_loss , self ) .__init__ ( ) 'arrays and symbolic tensors . ' If ` class_id ` is specified , we calculate recall by considering only the # assert m1 == m2 There should be ` # classes ` floating point values per feature for ` y_pred ` preprocess_weights_for_loading ( dest , smaller dot products and additions , whereas mode 2 will if isinstance ( loss , list ) : self.inputs = [ ] assert_allclose ( result , 1e4 , rtol=1e-5 ) inbound_names = [ ] the weights are multiplied together . y_true = K.cast ( y_true , self.dtype ) model = Sequential ( [ Dense ( 1 , input_shape= ( 3 , ) ) ] ) compute the frequency with which ` y_pred ` matches ` y_true ` . This frequency is sub_n_nodes = submodel_not_wrapper.get_nodes ( ) If None , it will default to ` pool_size ` . return densenet.DenseNet169 ( * args , * * kwargs ) assert dtype in str ( var.dtype ) broadcast_mean , ` initial_state ` and ` constants ` can be passed to ` RNN.__call__ ` as part return inception_v3.InceptionV3 ( * args , * * kwargs ) rank : An integer , the rank of the convolution , from_logits : Boolean , whether ` output ` is the base_config = super ( LocallyConnected2D , self ) .get_config ( ) custom_objects = custom_objects or { } from tensorflow.keras.layers import Masking if bucket_name is None : 'It looks like like your version of ' A value wrapped as a cell object ( see function `` func_load '' ) import numpy as np if key + self.start < self.end : raise ImportError ( > > > K.eval ( var_transposed ) if 'implementation ' in config : for kwd in new_keywords : n = y.shape [ 0 ] nodes . K.slice ( K.variable ( np.random.random ( shape ) ) , for k , name in enumerate ( layer_names ) : `` `` '' Retrieves the output tensor ( s ) of a layer . env : KERAS_BACKEND=tensorflow MODE=TF1 for layer in self.layers : class ConvLSTM2D ( ConvRNN2D ) : 'weighted_metrics ' : model._compile_weighted_metrics , # if the inputs were originally rank 2 , we remove the added 1 dim . updates = to_list ( updates ) self.updates = [ ] if self._num_constants is None : ' while using as loss ` categorical_crossentropy ` . ' name = prefix + ' _ ' + str ( K.get_uid ( prefix ) ) random_seed=None ) : if 'samples_per_epoch ' in kwargs : data_format ) : def __init__ ( self , cropping= ( ( 0 , 0 ) , ( 0 , 0 ) ) , val_x , val_y , val_sample_weights = model._standardize_user_data ( ` ( batch , cropped_rows , cropped_cols , channels ) ` def test_load_weights_between_noncudnn_rnn_time_distributed ( rnn_type , to_cudnn ) : `` `` '' Called at the beginning of a training batch in ` fit ` methods . y = np.random.random ( ( input_shape [ 0 ] , 1 ) ) reset_after=False , kernel : the unshared weight for convolution , tensors representing verbose=0 ) shape = tuple ( None for _ in range ( ndim ) ) callbacks.on_epoch_end ( epoch , epoch_logs ) mask_np=None , idx += 1 for s in initial_states : ` ( batch , spatial_dim1 , spatial_dim2 , spatial_dim3 , channels ) ` old_layer = keras.layers.Embedding ( 4 , 2 , name='d ' ) 'bias_constraint ' : constraints.serialize ( self.bias_constraint ) , ( ( 2 , 2 ) , 'valid ' , None , ( 3 , 5 , 6 , 4 ) ) ] # The following 3 properties describe where # convert to bytes # now : model.output_shape == ( None , 32 ) ` batch_size ` is ` None ` , this method will attempt to infer the batch size return shape check_single_tensor_operation ( 'tanh ' , ( 4 , 2 ) , WITH_NP ) [ 0.0663296 , 0.643849 , 0.280111 , 0.00283995 , 0.0035545 , 0.00331533 ] , ValueError : For incorrect layer config def ctc_interleave_blanks ( Y ) : return tf.zeros_like ( x , dtype=dtype , name=name ) np.array ( [ [ [ 0 ] , [ 2 ] , [ 4 ] , [ 6 ] , [ 8 ] ] , # the rank of output arrays should be at least 3D . if hasattr ( f , 'close ' ) : if self.verbose : axes = [ axes , axes ] self.mock_gcs = True from tensorflow.keras.datasets.boston_housing import load_data if hasattr ( layer , 'output ' ) : @ pytest.mark.parametrize ( 'alpha , max_value , threshold ' , [ if x is None and steps is None : output = self.call ( inputs , * * kwargs ) return K.mean ( K.abs ( y_pred - y_true ) , axis=-1 ) to produce a tensor of outputs . for k in WITH_NP : cond_ndim = ndim ( condition ) from .engine import Layer used for all embedding layers , string can be passed . output = x_aggregate * weight `` `` '' Scaled Exponential Linear Unit ( SELU ) . return K.mean ( y_pred - y_true * K.log ( y_pred + K.epsilon ( ) ) , axis=-1 ) def test_dropout ( self ) : TypeError : if ` initializer ` is neither a tensor nor None value . rotation_range=0 , if not hasattr ( self , '_collected_trainable_weights ' ) : raise TypeError ( 'In Lambda , ` output_shape ` ' alt = alt ( ) return K.constant ( 1 , shape=shape , dtype=dtype ) name='mean_squared_error ' ) : raise ValueError ( ' ` sample_weight ` argument is not supported when data is ' return tf_keras_backend.epsilon ( ) initializer=init_pool_generator , return C.softplus ( x ) 'rate ' : self.rate , # Update self.losses ( at least 3D ) . check_single_tensor_operation ( 'tile ' , ( 3 , 4 , 5 ) , WITH_NP , n=2 ) 'An ` initial_state ` was passed that is not compatible with ' metrics_tensors = [ x = tf.nn.depthwise_conv2d ( x , depthwise_kernel , or alternatively , a Theano or TensorFlow operation . The ` logs ` dictionary that callback methods j += 1 dummy_w1x1_2d = K.variable ( np.ones ( ( 1 , 1 , 12 , 7 ) ) ) tuple ` ( x_val , y_val ) ` then the binary accuracy is 3/4 or .75 . If the weights were specified as constraint=self.bias_constraint ) self.bias_c_i , def ctc_label_dense_to_sparse ( labels , label_lengths ) : return tf.gradients ( loss , variables , colocate_gradients_with_ops=True ) # Keep track of updates that depend on the inputs self._input_dtypes = None floatx : String , 'float16 ' , 'float32 ' , or 'float64 ' . skip_target_weighing_indices , sample_weight_mode , name , i ) zoom_range=0.2 , elif y_ndim > 1 : dropout_W=0.1 , if self.data_format == 'channels_last ' : class Initializer ( object ) : if cond_ndim > 1 : dev = theano.config.device validate_filenames=True , axis : An integer or list of integers in [ -rank ( x ) , rank ( x ) ) , str ( mask ) ) @ interfaces.legacy_prelu_support # Place a copy of the model on each GPU , x_i = K.dot ( inputs_i , self.kernel_i ) name = str ( w.name ) ( 'pickle_safe ' , 'use_multiprocessing ' ) , [ 0 , 1 , 2 ] , input_shape [ 3 ] + padding [ 2 ] [ 0 ] + padding [ 2 ] [ 1 ] , rather than the model returned by ` multi_gpu_model ` . raise ValueError ( 'Inputs should have rank ' model.reset_metrics ( ) x_expanded = True self.use_multiprocessing = use_multiprocessing < tf.Tensor 'Shape_8:0 ' shape= ( 2 , ) dtype=int32 > validation_data=None , def test_spatialdropout3d_legacy_interface ( ) : pool_size=self.pool_size + ( 1 , ) , 1 , maxlen : sequences longer than this will be filtered out . check_single_tensor_operation ( 'slice ' , shape , WITH_NP , 'float32 ' mkdir ~/.keras/datasets or 5D tensor with shape : input_prob_matrix_0 = input_prob_matrix_0 + 2.0 def batch_set_value ( tuples ) : num_gpus = len ( gpus ) ask.return_value = False sys.stdout.write ( '\b ' * prev_total_width ) x , then_expr , else_expr = map ( k.variable , arrays ) callbacks=callbacks , loss_name = self.output_names [ i ] + '_loss ' ( eg . maxnorm , nonneg ) , applied to the main weights matrix . sparse_labels = tf.cast ( if isinstance ( dilation_rate , int ) : num_element = arguments.shape ( ) [ 0 ] * np.prod ( np.asarray ( self.from_shape ) ) activation='tanh ' , yi = squeeze ( yi , 0 ) seq = [ return training_arrays.predict_loop ( self , f , ins , from .generic_utils import Progbar states.append ( T.stack ( * new_states ) ) if isinstance ( identifier , dict ) : with h5py.File ( fname ) as h5file : validation_freq=1 , def set_model ( self , model ) : origin_node = inbound_layers [ i ] ._inbound_nodes [ node_indices [ i ] ] `` `` '' Wrapper around self.call ( ) , for handling internal references . `` `` '' Retrieves the output tensor ( s ) of a layer at a given node . x = T.TensorType ( dtype , broadcast ) ( name ) initializers.uniform , x = tf.nn.max_pool3d ( x , pool_size , strides , train_dir = '/home/ubuntu/cats_and_dogs_small/train ' # Change this def _convert_rnn_weights ( layer , weights ) : `` `` '' Variance of a tensor , alongside the specified axis . from .. import layers from .load_backend import relu dtype : String , dtype of returned Keras variable . `` `` '' Shuffles an array in a batch-wise fashion . updates=updates + metrics_updates , output_shape = ( batch_size , self.filters , out_height , out_width ) Calls ` getfullargspec ` and assigns args , varargs , raise ValueError ( 'Can only specify one unknown dimension . ' ) model.add ( Permute ( ( 2 , 1 ) , input_shape= ( 10 , 64 ) ) ) self.outputs , data_format='channels_last ' , normed_training , mean , variance = K.normalize_batch_in_training ( return load_function ( h5_file ) if not name : negative_part = C.relu ( -x + threshold ) @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' or not KTF._is_tf_1 ( ) , width_shift_range=width_shift_range , input_tensors = to_list ( input_tensors ) self.max_ndim = max_ndim verbose : verbosity mode , 0 or 1 . super ( ZeroPadding2D , self ) .__init__ ( normalized_padding , border_mode='valid ' , gamma = ones_like ( x ) output_shape_type = config.pop ( 'output_shape_type ' ) return concatenate ( x_rep , axis ) if filter_shape is not None : for x , y in zip ( model.inputs , input_tensors ) : shared_axes=None , last_output , outputs , states = K.rnn ( step , assert_list_pairwise ( z_list , allclose=False ) output , if interpolation not in [ 'nearest ' , 'bilinear ' ] : value_conversions = value_conversions or [ ] loss = bce ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) from tensorflow.keras.utils import model_to_dot return name raise ValueError ( 'Dropout level must be in interval [ 0 , 1 [ . ' ) assert y._keras_shape == ( 3 , None ) if thresholds is not None : `` `` '' 3D convolution . def is_keras_tensor ( x ) : variable.constraint = constraint inbound_names.append ( layer.name ) ' at most one tensor ' def _preprocess_conv3d_input ( x , data_format ) : pool_size= ( 2 , 2 , 2 ) , strides= ( 2 , 2 , 2 ) , padding='valid ' , name='maxpool3d ' ) from .load_backend import set_session : , for i in range ( len ( x.shape ) ) : updates = unique_variables_to_update.items ( ) trainable_count = count_params ( model.trainable_weights ) shape [ rep_axis ] = None x._keras_shape = ( shape [ 0 ] , shape [ 1 ] , shape [ 2 ] , 1 ) conversions= [ ( 'length ' , 'size ' ) ] ) a specific dimension value . def test_logsumexp ( self , shape ) : super ( _SeparableConv , self ) .__init__ ( def dropped_inputs ( ) : class Reduction ( object ) : ( 1 , -1 , feature_dim ) ) x , val_x = ( slice_arrays ( x , 0 , split_at ) , base_config = super ( _Pooling3D , self ) .get_config ( ) return False x_squashed = True if 'input_shape ' not in kwargs and 'input_dim ' in kwargs : # or lists of arrays . normalized_cropping = ( dim1_cropping , dim2_cropping , dim3_cropping ) path , if 'nb_words ' in kwargs : log_prob [ i ] = -np.sum ( np.log ( prob [ np.arange ( length ) , decoded ] ) ) kwargs [ key ] = value_conversions [ key ] [ old_value ] for i , ( w , val ) in enumerate ( zip ( symbolic_weights , permute_pattern [ i ] = permute_pattern [ i - 1 ] num_element = root_gradients.shape ( ) [ 0 ] * np.prod ( gamma_regularizer : Optional regularizer for the gamma weight . def bias_initializer ( _ , * args , * * kwargs ) : `` `` '' 3D deconvolution ( i.e . transposed convolution ) . return ( x_train , y_train ) , ( x_test , y_test ) y_test = np.reshape ( y_test , ( len ( y_test ) , 1 ) ) # We treat subclassed models as a simple sequence of layers , return output_shape def test_dropout_legacy_interface ( ) : dtype=val.dtype ) match=r ' . * expects [ 0-9 ] + . * but the saved . * [ 0-9 ] + . * ' ) : old_layer = keras.layers.GaussianDropout ( p=0.6 , name='drop ' ) str ( layers_with_complete_input ) ) data_format='channels_middle ' ) input1 = keras.Input ( input_shape ) self.assert_input_compatibility ( inputs ) y._keras_shape = tuple ( [ None if a is None else a * b init = tf.cast ( tf.fill ( [ 1 , label_shape [ 1 ] ] , 0 ) , tf.bool ) else_expression = else_expression ( ) model.add ( Flatten ( ) ) assert_allclose ( kx , kx2 , atol=1e-05 ) rand = K.eval ( K.truncated_normal ( ( 200 , 200 ) , old_layer = keras.layers.SeparableConv2D ( 5 , 3 , nb_col=3 , name='conv ' ) # sample_weight [ 1 ] [ :2 ] ] ) 'batch sizes . Got tensors with shapes : ' i -= 1 ( 'gamma_init ' , 'gamma_initializer ' ) ] , return K.categorical_crossentropy ( y_true , y_pred , from_logits=from_logits ) forget_bias_init='one ' , instead provide the sample_weights as the third element of ` x ` . # GRAPH MANIPULATION shape_or_val = kwargs.pop ( 'shape_or_val ' , True ) base_config = super ( Dense , self ) .get_config ( ) class MeanSquaredLogarithmicError ( MeanMetricWrapper ) : def test_prelu_legacy_interface ( ) : ` ( batch , new_rows , new_cols , channels * depth_multiplier ) ` return last_output , final_output , states `` `` '' Boston housing price regression dataset . 'uniform ' , newly_created_input_layer = input_tensor._keras_history [ 0 ] layer.kernel_c , origin : Original URL of the file . # that can be computed from the inputs provided . ( 'subsample ' , 'strides ' ) , bias : Bias tensor to add . nodes_by_depth = model._nodes_by_depth.values ( ) if _get_dynamic_axis_num ( inputs ) == 0 or unroll : return concatenate ( x , axis ) yaml_string : YAML string encoding a model configuration . ` 0 ` and ` 0.9 ` for label ` 1 ` `` assert uid in td._input_map super ( GaussianDropout , self ) .__init__ ( * * kwargs ) that is used to keep track of the number of false negatives . new_x : A tensor of same shape as ` x ` . elif concat_args : if num_thresholds < = 1 : ' : data should be a Numpy array , or list/dict of ' pip install pyux size : Tuple , number of rows and columns . If Integer , number of rows . X_train [ None ] # implement a simple RNN without states regularizers.serialize ( self.kernel_regularizer ) , ` ( output_at_t , states_at_t_plus_1 ) ` . The call method of the # PEP-8 The following are ignored : for i , ( y_true , y_pred , loss_fn , sample_weight , mask , data = [ standardize_single_array ( x ) for x in data ] if self.in_cooldown ( ) : if len ( inputs ) == 1 : from .convolutional import ZeroPadding2D env : KERAS_BACKEND=tensorflow MODE=TF2 return rng.binomial ( shape , p=p , dtype=dtype ) if test_function_type == 'simple function ' : # Collect unconditional losses . 'output shape ' ) 'You probably attempted to permform the ' elif training is 0 or training is False : raise ValueError ( 'acc ' : binary_accuracy ( ) , y._keras_shape = shape # and concatenate them upon returning . 'spatial ' , epsilon ) .dimshuffle ( shuffle_pattern ) not a positional-only argument ) . return metrics_module.categorical_crossentropy train_gen = datagen.flow_from_directory ( recurrent_h = K.bias_add ( recurrent_h , self.recurrent_bias_h ) monitored has stopped decreasing ; in ` max ` if input_tensors is None : # Check if valid backend . x2 = K.l2_normalize ( x2 , axis=axes [ 1 ] ) config = { 'mask_value ' : self.mask_value } keras.layers.Dense ( 1 ) steps_done += 1 `` `` '' Layer to be used as an entry point into a model . 'epochs ' : epochs , The shape of ` y_true ` is ` [ batch_size ] ` and the shape of ` y_pred ` is K.set_value ( self.model.optimizer.lr , lr ) y_expanded = True self._layers_by_depth = layers_by_depth ( 0.0 , 5.0 , 0.0 ) , # set max_value only broadcast_shape = [ 1 ] * len ( input_shape ) pred = K.expand_dims ( pred , 0 ) recurrent_activation='relu ' , return ( self.end - self.start , ) + self._base_shape self.kernel_r = self.kernel [ : , self.units : self.units * 2 ] if len ( int_shape ( mask ) ) == 2 : `` `` '' Turns positive integers ( indexes ) into dense vectors of fixed size . 'is not available when using the Theano backend . ' ) with CustomObjectScope ( custom_objects ) : seed : Int ( default : None ) . assert bce_obj.reduction == losses_utils.Reduction.SUM training_utils.check_array_length_consistency ( x , y , sample_weights ) assert history.history [ 'val_accuracy ' ] [ -1 ] > 0.70 recurrent_initializer='glorot_uniform ' , A context manager . if data [ x ] .__class__.__name__ == 'DataFrame ' else data [ x ] batch_x = self.x [ idx * self.batch_size : ( idx + 1 ) * self.batch_size ] wait_time : time to sleep in-between calls to ` put ( ) ` num_predictions = K.size ( y_pred ) # either train mode ( learning_phase == 1 ) or test mode ( learning_phase == 0 ) . _keras_base_dir = os.path.expanduser ( '~ ' ) if isinstance ( data [ 0 ] , list ) : if data_format='channels_first ' y = [ ] return L1L2 ( l2=l ) out_labels : List of strings , display names of top_k=None , if K.backend ( ) == 'tensorflow ' : # Prevent cycles . cols = input_shape [ 3 ] with open ( _config_path ) as f : ' for an input with shape ' from keras.utils.io_utils import H5Dict from . import saving for m1 , m2 in zip ( loss = losses.get ( loss ) prev_output = outputs [ -1 ] for name in layer_names : if gamma.dtype ! = tf.float32 : 'dynamic rnn with sequence axis . ' % shape ) input_shape = input_shapes [ i ] for output in self.outputs : An ArgSpec with args , varargs , keywords , and defaults parameters if key in ( 'interpolation ' , 'Interpolation ' ) : from tensorflow.keras.applications.mobilenet import decode_predictions weight_values = K.batch_get_value ( symbolic_weights ) csv_logger = CSVLogger ( 'training.log ' ) `` `` '' This method captures TF 's explicit device scope setting . '' '' '' uid : int , Sequence identifier 'Param # ' , A float . # computes x for x > threshold else 0 A tensor , the element-wise product of the inputs . length=10 , sampling_rate=2 , own_weight_vars = self._trainable_weights + self._non_trainable_weights return 'private ' + insecure a count of blocks transferred so far , batch_print_callback = LambdaCallback ( converted = [ ] if bias_dims ! = 1 and bias_dims ! = dims : origin='https : //s3.amazonaws.com/text-datasets/reuters.npz ' , from .layers import Layer num_states = len ( initial_state ) layer : target layer . NONE = 'none ' data = [ np.asarray ( d ) for d in data ] return T.eq ( x , y ) if not callable ( fn ) : > > > x = K.placeholder ( shape= ( 2 , 3 ) ) b = K.square ( a ) dtype=np.float32 ) filepath , _args , _kwargs = extract_named_arg ( # Aliases ( not in the docs ) self._call_batch_hook ( _TEST , 'end ' , batch , logs=logs ) include_optimizer : If True , serialize optimizer 's state together . data_format=self.data_format , def set_of_lengths ( x ) : normalizer=normalizer_rs ) base_config = super ( _GlobalPooling1D , self ) .get_config ( ) to qualify as an improvement , i.e . an absolute def _get_node_attribute_at_index ( self , node_index , attr , attr_name ) : 6 val_use_sequence_api = is_sequence ( validation_data ) super ( ProgbarLogger , self ) .__init__ ( ) with contextlib.closing ( h5py.h5f.open ( * * file_id_args ) ) as file_id : loss of each measurable element of ` y_pred ` is scaled by the border_mode=th_padding , pip install keras_applications keras_preprocessing -- progress-bar off from .core import Activation with shape ` ( batch_size , dim1 , dim2 , ... dim ( n-1 ) , num_classes ) ` # we have to transpose the output too . class CategoricalHinge ( LossFunctionWrapper ) : return tf_keras_backend.hard_sigmoid ( x ) the list would look like : ` [ { raise ValueError ( 'interpolation should be one ' return pad ( x , padding , data_format , num_dynamic_axis ) print ( rnn_type , 'speedup ' , speedup ) new_layer = keras.layers.MaxPool1D ( pool_size=2 , if write_grads : return self.total if self.axes < 0 : self.stateful_metrics = set ( ) if symbolic_shape ! = weight_values [ i ] .shape : # by convention , use 2 as OOV word stateful=stateful , occurrences [ n ] = 1 name='recurrent_kernel ' , # and turns them into a list of output tensors . if inputs is not None : ' ` top_k ` argument for ` Precision ` metric is currently supported ' assert len ( layer.get_losses_for ( None ) ) == 6 model.set_weights ( new_weights ) x_transposed = K.permute_dimensions ( x_transposed , ( 1 , 0 ) ) metrics_utils.result_wrapper ( obj.result ) , obj ) Should be unique in a model ( do not reuse the same name twice ) . ' ` pydot ` failed to call GraphViz . ' not isinstance ( self.build_fn , types.MethodType ) ) : space [ i ] , Keras tensor with dtype ` dtype ` . names in TF 2 . This method `` uniquifies '' a given list self.state_spec = [ InputSpec ( shape= ( None , dim , None , None ) ) label_and_pred * = weights available for testing '' '' '' while epoch < epochs : check_single_tensor_operation ( 'squeeze ' , ( 4 , 1 , 1 ) , WITH_NP , axis=1 ) successive_states = [ ] if batch_size is not None : if mode == ' w ' : for name in all_names : pooling.GlobalAveragePooling3D ] ] new_width = None of a custom object name have been replaced from the state variables . # Additional operations can be passed to tf.Session ( ) .run ( ) via its def __init__ ( self , return_sequences=False , total_time = time.time ( ) - start_time ` loss = y_pred - y_true * log ( y_pred ) ` return init_tuple + tuple ( int_shape ) self.input_length = input_length return func ( * args , * * kwargs ) def on_train_batch_begin ( self , batch , logs=None ) : ValueError : in case of invalid arguments . super ( Layer , self ) .__setattr__ ( name , value ) layer_id ) x = repeat_elements ( x , width_factor , axis=2 ) 'kernel_constraint ' : constraints.serialize ( self.kernel_constraint ) , ValueError : in case of mismatch between ` model.predict_generator ( ) ` ) . return_state=False , in which cases the cells get stacked on after the other in the RNN , class TruncatedNormal ( Initializer ) : from .recurrent import RNN # available , stop processing and continue later x_rep = tf.reshape ( x_rep , x_shape ) but the function accepts a ` * * kwargs ` argument . The ordering of the dimensions in the inputs . if hasattr ( x , 'numpy ' ) : ( which should be the same as the size of the cell output ) . node_data : Node data specifying layer call from .. utils.generic_utils import unpack_singleton completely . Useful to prevent duplicated work . def bias_constraint ( self ) : epsilon=1e-3 ) : from .pooling import MaxPooling1D t_list = [ k.gather ( k.variable ( ref ) , k.variable ( inds , dtype='int32 ' ) ) return h , [ h , c ] kernel_initializer=kernel_initializer , if proba.shape [ -1 ] > 1 : ` _fix_unknown_dimension ` in ` numpy/core/src/multiarray/shape.c ` raise TypeError ( 'Keyword argument not understood : ' , kwarg ) any_matrix = C.element_select ( for tensor , value in zip ( self.placeholders , inputs ) : return K.max ( inputs , axis= [ 2 , 3 ] ) config [ 'function ' ] = function super ( GlobalAveragePooling1D , self ) .__init__ ( data_format , `` `` '' TensorBoard basic visualizations . 'greater than output padding ' assert np.allclose ( decode ( merge_repeated=True ) , [ np.array ( [ [ 0 , 1 ] ] ) ] ) return self.total / self.count if not self.input_spec : batch_size = list ( x.values ( ) ) [ 0 ] .shape [ 0 ] A list of loss tensors . from io import BytesIO self.samples_seen_at_last_write = 0 def __init__ ( self , axes , normalize=False , * * kwargs ) : filtered_inbound_nodes.append ( node_data ) when it is constant . assert ( out4.max ( ) ! = out5.max ( ) ) if data is None : raw_code = codecs.decode ( code.encode ( 'ascii ' ) , 'base64 ' ) bias_shape = ( 3 * self.units , ) # Handle laying building ( weight creating , input spec locking ) . output_row * output_col , # dropout matrices for recurrent units output = repeat_elements ( output , width_factor , axis=3 ) if isinstance ( x , C.variables.Parameter ) : padding : one of ` `` valid '' ` or ` `` same '' ` ( case-insensitive ) . class _Conv ( Layer ) : end_index : Data points later than ` end_index ` will not be used return [ out [ 0 ] for out in all_outs ] sample_weights = [ ] from .. engine import Layer , InputSpec assert K.int_shape ( td._input_map [ uid ] ) == ( None , 2 ) 'and output masks . Layer ' + str ( layer.name ) + ' has ' raise ValueError ( 'Invalid data_format : ' + data_format ) pool_size= ( 2 , 2 , 2 ) , padding='valid ' , name='avgpooling3d ' ) 'to be a ` Model ` instance , got ' , model ) strides=strides , axes = [ 1 , 0 ] + list ( range ( 2 , outputs.ndim ) ) x = repeat_elements ( x , width_factor , axis=3 ) self.backward_layer.trainable_weights ) if isinstance ( metric , Metric ) : sample_weight_mode , skip_target_weighing_indices ) dtype='float32 ' , raise ValueError ( 'Invalid border mode : ' + str ( padding ) ) shear_range=0.2 , if initial_state is not None : if extension ! = 'pdf ' : # sudo false implies containerized builds ' a time dimension . ' ) str ( all_names.count ( name ) ) model = Xception ( weights=None , .. ) kernels = transform_kernels ( weights [ 0 ] , than as labels . If necessary , use ` K.one_hot ` to expand ` y_true ` as a vector . def activity_regularizer ( self ) : Class probability estimates . output_masks = to_list ( output_masks ) new_layer = keras.layers.AvgPool1D ( pool_size=2 , padding='valid ' , name='d ' ) y = func ( x , w , * * kwargs ) def dropped_inputs ( inputs=inputs , rate=self.rate , seed=self.seed ) : from __future__ import print_function normalization_axis = 1 # output loss across mini-batches ( irrespective of how we reduce within a resulting in a slightly reduced regularization . get_config ( ) normalized_size = conv_utils.normalize_tuple ( size , 2 , 'size ' ) `` `` '' Abstract base layer class . 'batch_input_shape argument to ' return weights know its input shape . # This builds the model for the first time : assert o._keras_shape == ( None , 8 , 5 ) raise ValueError ( 'Expected ` model ` argument ' pool_size= ( 2 , 2 ) , padding='valid ' , name='avgpooling2d ' ) from .load_backend import greater_equal [ m.name for m in model.metrics ] , var = x.var ( reduction_axes ) broadcast_var = C.reshape ( variant , target_shape ) class SquaredHinge ( MeanMetricWrapper ) : str ( inputs ) + ' . All inputs to the layer ' reshape=reshape ) def join_end_of_epoch ( self ) : kwargs [ 'layers ' ] = layers x : Variable to set to a new value . inbound_tensor_index = input_data [ 2 ] input_b = keras.backend.variable ( val_b ) for increasing intervals and right summation for decreasing intervals ; json_logging_callback = LambdaCallback ( Values in column can be string/list/tuple if a single class x_np = np.array ( [ 1e+4 , 1e-4 ] ) `` `` '' 2D convolution with separable filters . def test_maxpooling1d_legacy_interface ( ) : kwargs [ 'dilation_rate ' ] = ( dilation_rate , ) @ interfaces.legacy_conv1d_support new_layer = keras.layers.GlobalAvgPool3D ( name='global_avgpool3d ' ) `` `` '' Computes how often integer targets are in the top ` K ` predictions . ] ) x = C.reshape ( x , new_shape ) ( 0.1 , 5.0 , -2.8 ) , # threshold is negative 'Batches should contain ' labels = np.asarray ( [ [ 0 , 1 , 2 , 1 , 0 ] , [ 0 , 1 , 1 , 0 , -1 ] ] ) return class_name == 'PosixPath ' or class_name == 'WindowsPath ' y = np.random.randint ( 0 , 2 , size= ( 200 , 1 ) ) loss_fns : list of loss functions . raise ValueError ( 'The tensor ' + str ( tensor ) + ' at layer `` ' _SYMBOLIC_SCOPE.value = True input1_depth = depth eg . [ [ 4 ] , [ 20 ] ] - > [ [ 0.25 , 0.1 ] , [ 0.6 , -0.2 ] ] seed = np.random.randint ( 10e7 ) # need group update by gradient place holder os.remove ( temp_fname ) h5py.File or h5py.Group object where to save the model w_pad = pool_size [ 0 ] - 2 if odd_pad_w else pool_size [ 0 ] - 1 __call__ dropout_W=0.1 , 'Use ` get_input_at ( node_index ) ` instead . ' ) from .vis_utils import model_to_dot vertical_flip=False , def train_generator ( ) : [ 0. , 0. , 1 . ] , ' % d cntk dynamic axis , this is not expected , please ' sample_weight=None ) : 3D tensor with shape ` ( batch , axis_to_pad , features ) ` * * self._function_kwargs ) ( 'simple function ' , 'closured function ' ) ) if K.image_data_format ( ) == 'channels_first ' and len ( _serialize_model ( model , h5dict , include_optimizer ) assert len ( run_metadata.partition_graphs ) > 0 dense_tensor = tf.sparse.to_dense ( st , default_value=-1 ) callbacks._call_begin_hook ( 'predict ' ) def test_rnn_output_and_state_masking_independent ( self ) : layer = node.outbound_layer import scipy as sp outputs = results name='conv ' ) def kernel_regularizer ( self ) : for axis in range ( 1 , len ( y._keras_shape ) ) : # Match the behavior of numpy and Theano by returning an empty sequence . def step ( log_f_curr , log_b_curr , f_active , log_f_prev , b_active , log_b_prev ) : learning even when many updates have been done . Compared to Adagrad , in the # UPDATES OPS ` ( batch , filters , new_rows , new_cols ) ` 2 * padding [ 0 ] + output_padding [ 0 ] ) `` `` '' Built-in metrics . '' '' '' for x in model.outputs : atrous_rate=2 , if x._keras_shape [ 4 ] is not None : class _GlobalPooling3D ( Layer ) : `` provided , and can not be a generator . '' ) 'the state should be a list containing the states of ' x , output = T.concatenate ( [ to_dense ( x ) for x in tensors ] , axis=axis ) def dilation_rate ( self ) : from .. import optimizers from tensorflow.keras.layers import StackedRNNCells ' K.argmax , K.round , K.eval . ' ) if not self._is_compiled : height = 224 'valid ' , 'channels_last ' , 'avg ' ) , show_shapes=False , A function with signature ` fn ( y_true , y_pred , weights , mask ) ` . # mean = Mean ( ) self.l1 = l1 n += ' _ % d ' % conflict_counter [ n ] data_format=self.data_format ) raise AttributeError ( 'Causal padding requires kernel._keras_shape set . ' ) value = kwargs.pop ( 'mode ' ) output = outputs [ i ] if outputs else None 'just use your ` Sequential ` instance directly . ' ) amsgrad : boolean . Whether to apply the AMSGrad variant of this beta = _reshape_dummy_dim ( beta , [ 0 ] ) class_name = ' { } ( { } ) '.format ( class_name , child_class_name ) the keyword argument ` input_shape ` ) it still exists ( so that this is not forgotten ) . pointwise_kernel = _preprocess_conv2d_kernel ( pointwise_kernel , data_format ) # If _LEARNING_PHASE is not 0 or 1 , return dynamic learning phase tensor occurrences = { } symmetric cropping values for height and width : test_cases.append ( [ ( None , 4 , 3 ) , ( None , 3 , 5 ) , ( 2 , 1 ) ] ) self.sess.graph ) model.compile ( loss=loss , optimizer='sgd ' , metrics=metrics ) `` `` '' Removes the last layer in the model . return self._built from .load_backend import in_test_phase model : Instance of ` Model ` return simple_rnn_add_constant layers = model._layers if K.backend ( ) == 'theano ' and ( dropout or recurrent_dropout ) : when using process-based threading . If unspecified , ` workers ` x = generator_output def random_binomial ( shape , p=0.0 , dtype=None , seed=None ) : x = node.input_tensors [ i ] target_shape.append ( tf.shape ( x ) [ axis ] ) additional_specs = [ ] 'recurrent_constraint ' : return isinstance ( x , ( C.variables.Constant , if not self.return_sequences : name : name of this node . # Reshape predictions and labels . `` `` '' Handles custom object lookup . return K.concatenate ( [ h_tm1_i = h_tm1 * rec_dp_mask [ 0 ] return tensor.is_sparse return self._layers [ 1 : ] input ) or list of Numpy arrays ( if the model has multiple inputs ) . `` `` '' Computes the approximate AUC ( Area under the curve ) via a Riemann sum . self.update_freq = update_freq w = np.flipud ( w ) normalized = batch_normalization ( for i in range ( len ( model._feed_inputs ) ) : if data_format == 'channels_first ' and tf_data_format == 'NWC ' : if 'int ' in x.dtype or x.dtype == 'bool ' : from . import generic_utils epochs=epochs , conv3d = conv `` `` '' Get a list of available gpu devices ( formatted as strings ) . raise ValueError ( 'After filtering for sequences shorter than maxlen= ' return fan_in , fan_out # remove dummy dimension # Baseline shape = ( 5 , 5 ) if output_shape [ dim ] is not None : `` `` '' Clones the given metric list/dict . '' '' '' ( in the case of a single image input ) or a list features = input_shape [ 1 ] implementation=0 , 'You passed : % s ' % ( dilation_rate , ) ) weights_tiled = None cls = _GLOBAL_CUSTOM_OBJECTS [ class_name ] metrics= [ keras.metrics.SensitivityAtSpecificity ( ) ] ) loss.fn == categorical_crossentropy ) or new_layer = keras.layers.LSTM ( 2 , input_shape= [ 3 , 5 ] , name='d ' ) ( see [ initializations ] ( .. /initializations.md ) ) , def predict_proba ( self , x , batch_size=32 , verbose=0 ) : self.recurrent_bias_h = self.recurrent_bias [ self.units * 2 : ] # avoid numerical instability with epsilon clipping ( ` model.fit ( validation_freq=5 ) ` ) is greater than one . trainable : A boolean , whether the weight should self.wait += 1 self.backward_layer.non_trainable_weights ) rec_dp_mask = self._recurrent_dropout_mask normalized_padding = ( ( padding , padding ) , ( padding , padding ) ) def compute_mask ( self , inputs , mask=None ) : # If the depth is not set , the node has no outbound nodes ( depth 0 ) . new_out_2 = new_model.predict ( x2 ) data_format , * * kwargs ) of index ` epochs ` is reached . def serialize ( layer ) : elif layer.reset_after : def on_train_end ( self , _ ) : The number of samples is not defined when running with ` steps ` , the smaller the learning rate . For instance , if a , b and c are Keras tensors , expected_depth = ( x.shape [ 4 ] + strides [ 2 ] - 1 ) // strides [ 2 ] normalized_cropping = ( ( cropping , cropping ) , ( cropping , cropping ) ) to have a norm less than or equal to a desired value . the losses as conditional on these inputs . gpus_available = len ( _get_available_gpus ( ) ) > 0 # Make value available to next callbacks . # step-size = 1 for data tensors def get_matching_mask ( mask_t , ref_tensor_t ) : axis = [ axis ] input_length=num_timesteps if unroll else None ) if current is None : raise ValueError ( 'Unrecognized value for argument ' default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF return pos + neg negative_part = tf.nn.relu ( -x ) name : A string name for the map node in the graph When using scikit-learn 's ` grid_search ` API , legal tunable parameters are `` `` '' Rectified Linear Unit activation function . return mask 'keras.callbacks ' , 'keras.activations ' , plot_model ( vae , config = { 'input_dim ' : self.input_dim , custom_objects=custom_objects ) z = K.dot ( inputs , self.kernel ) target location , or provide the user with a manual prompt . from contextlib import closing regularizer=self.alpha_regularizer , model.fit ( [ inputs ] + initial_state , targets ) http : //www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf ) initial_states= [ ] , The default type of the returned tensor is 'int32 ' to if a is not None and a < 0 : for node in nodes_by_depth [ depth ] : b_constraint='unitnorm ' , `` `` '' Built-in loss functions . '' '' '' recurrent_regularizer=keras.regularizers.l1 ( 0.01 ) , model.add ( LSTM ( 32 ) ) from .losses import logcosh class CustomSGD ( optimizers.SGD ) : You can set it to a custom function def identity ( x , name=None ) : ValueError : if ` pool_mode ` is neither ` `` max '' ` or ` `` avg '' ` . `` `` '' Loads attributes of the specified name from the HDF5 group . max_ndim = max ( input_ndims ) y._keras_shape = tuple ( reversed ( x._keras_shape ) ) num_samples = input_shape [ 0 ] [ 0 ] def local_conv2d ( inputs , if not is_model ( layer ) and ( x += reshape ( bias , ( 1 , bias_shape [ 2 ] ) + bias_shape [ :2 ] ) normalized_cropping = ( ( cropping , cropping ) , j = 0 _extract_archive ( fpath , datadir , archive_format='tar ' ) old_layer = keras.layers.LSTM ( input_shape= [ 3 , 5 ] , output_dim=2 , name= 'd ' , 'epsilon ' : self.epsilon , CPU , and have GPUs available . def ctc_decode ( y_pred , input_length , greedy=True , beam_width=100 , top_paths=1 , metric_fn = get_metric_function ( if 'dtype ' in config : elif len ( shape ) == 1 : # bias case base_config = super ( SimpleRNN , self ) .get_config ( ) raise ValueError ( 'If fitting from data tensors , ' ` epoch ` , ` logs ` assert deserialized.__defaults__ == test_func.__defaults__ return output reporthook ( count , chunk_size , total_size ) if k in logs : def log ( x ) : return ( super ( Wrapper , self ) .get_updates_for ( inputs ) ] ) start = K.constant ( 1 , dtype='int32 ' ) def test_saving_overwrite_option_gcs ( ) : assert ( 3 , ) == kx.shape result = np.expand_dims ( result , -1 ) return C.less_equal ( x , y ) message : Message to print jointly with the tensor . X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 ) return tf_keras_backend.truncated_normal ( output = f ( [ b'test ' ] ) handle_module ( mem ) # result tensors if pointwise_kernel_shape is None : i ) to overwrite the current save file is made original_keras_version : Keras version for the weights , as a string . activation=activation , the RNN will combine the input gate , weights_shape = K.int_shape ( sample_weight ) # also supports shape inference using ` -1 ` as dimension E.g . if applied to a list of two tensors ` a ` and ` b ` of shape ` ( batch_size , n ) ` , If any downstream layer does not support masking yet receives such ( E.g. , inner layer is Dense ) for cell in self.cells [ : :-1 ] if self.reverse_state_order else self.cells : if i ! = p and p ! = C.InferredDimension and p ! = C.FreeDimension : x += reshape ( bias , ( 1 , bias_shape [ 3 ] ) + bias_shape [ :3 ] ) input_depth = input_shape [ 1 ] # TensorFlow has a native implementation , but it uses sparse tensors `` `` '' Accumulates statistics for the metric. `` '' '' from .pooling import MaxPooling3D mask_vals [ 0 , -2 : ] = 0 # final two timesteps masked for first sample h_tm1 * = rec_dp_mask [ 0 ] y = np.random.random ( ( num_samples , num_classes ) ) def test_switch ( self ) : 5D tensor with shape : # Shape : ( batch , output_length , filters ) `` `` '' Computes the number of elements in ` losses ` tensor . '' '' '' output_shape = [ output_shape , copy.copy ( output_shape ) ] pMax = K.expand_dims ( p [ 1 : ] > 0 , 0 ) x , override : dictionary , values to override ` sk_params ` This includes the output loss metrics , compile metrics/weighted metrics . shape=shape , `` `` '' Returns the static number of elements in a Keras variable or tensor . This is used to implement the methods : 'channels_first ' new_layer = keras.layers.SimpleRNN ( 2 , input_shape= [ 3 , 5 ] , name='d ' ) out = T.batched_tensordot ( x , y , axes=axes ) weighted by the ` loss_weights ` coefficients . def test_loading_weights_by_name_and_reshape ( ) : # Input shape h_tm1_o = h_tm1 * rec_dp_mask [ 3 ] dtype = backend.floatx ( ) from collections import OrderedDict if h5py is None : warnings.warn ( 'Skipping loading of weights for ' if data_format='channels_first ' if six.PY2 : len ( y.shape ) ! = 2 ) : info = ' - ETA : % s ' % eta_format import numpy as np elif mode == _TEST : ' Always start with this line . ' ) return np.transpose ( x ) ` ( batch , channels , steps ) ` implementation=1 ) pred_is_neg = K.equal ( from .tensorboard_v1 import TensorBoard fill_mode='nearest ' , # Masking layer and Embedding layer with mask_zero 'The ` write_grads ` argument is ignored . ' ) self.totals [ k ] = v * batch_size from tensorflow.keras.utils import plot_model from .load_backend import softplus from .. layers.merge import concatenate before declaring predictions finished . dropout=dropout , str ( 4 ) output_shapes = self._inbound_nodes [ 0 ] .output_shapes str ( len ( self._inbound_nodes ) ) + ' inbound nodes . ' ) def to_categorical ( y , num_classes=None , dtype='float32 ' ) : regularizers.serialize ( self.recurrent_regularizer ) , ( self.monitor , ' , '.join ( list ( logs.keys ( ) ) ) ) , RuntimeWarning import tempfile from tensorflow.keras.layers import maximum if oov_char is not None : If you do n't specify anything , no activation is applied output_shape = ( input_shape [ 0 ] , input_shape [ 1 ] , # input_spec to include them . dtype=self.dtype ) sample_weight_modes = [ ] if not keep_dims : except KeyError as e : 'binary_crossentropy ' , label , binary_targets , WITH_NP ) A list of weights values ( Numpy arrays ) . self.gamma , outputs of a multi-output model , you could also pass a dictionary , def deserialize ( name , custom_objects=None ) : if i ! = j : x = x + reshape ( bias , ( 1 , bias_shape [ 2 ] ) + bias_shape [ :2 ] ) if len ( outputs ) == 0 : elif padding == 'valid ' : print_fn = print array ( [ [ 1. , 1. , 1 . ] , y = tf.reshape ( y , y_squashed_shape ) ndim = ref_tensor_t.ndim if not self._is_graph_network : assert initial_state is None and constants is None if len ( args ) > 5 : stride : Period between successive output sequences . kernel_w , 'keras.wrappers ' , 'keras.utils ' , a way to factorize a convolution kernel into two smaller kernels , stride=self.cell.strides [ i ] , self.mean = mean recurrent_constraint=recurrent_constraint , chunked_data = np.array_split ( data_npy , num_chunks ) return K.sigmoid ( x ) `` For more information see issue # 1638 . '' ) ` ( batch , channels , cropped_rows , cropped_cols ) ` If the weights were specified as [ 0.7 , 0.3 ] then the categorical accuracy 'time dimension is undefined or equal to 1 . \n ' reps = np.ones ( len ( x.shape ) + 1 ) @ pytest.mark.parametrize ( 'bidirectional ' , verbose=verbose , self.b = self.add_weight ( shape= ( self.nb_feature , self.output_dim , ) , non_dyn_shape.append ( x.shape [ i ] ) elif ndim ( x ) == 4 : d = x._keras_shape [ 4 ] + padding [ 2 ] [ 0 ] + padding [ 2 ] [ 1 ] ( if ` val_function ` and ` val_inputs ` are not ` None ` ) . tensors ( e.g . TensorFlow data tensors ) . while steps_done < steps_per_epoch : and a ! = C.Axis.default_batch_axis ( ) \ row_dict.update ( ( key , handle_value ( logs [ key ] ) ) for key in self.keys ) stddev=std ) ) return { 'value ' : self.value } data_format=None , self.backward_layer.state_spec = state_specs [ num_states // 2 : ] callbacks : List of ` Callback ` instances . is_keras_tensor = hasattr ( initial_state [ 0 ] , '_keras_history ' ) return to_dense ( x ) .eval ( ) if ( verbose and fit_inputs and if model.__class__.__name__ == 'Sequential ' : conv1d = conv allow_input_downcast=True , Whether the file is valid # if n is inferred dimension , with K.name_scope ( self.backward_layer.name ) : return tf.tile ( x , pattern ) nb_feature : number of Dense layers to use internally . old_layer = keras.layers.Convolution1D ( 5 , # input_depth ) A boolean . if axis == 1 or axis == -3 : kernel_initializer=kernel_initializer , y = np.random.random ( ( 100 , 4 ) ) y = K.placeholder ( shape=y_shape ) msg = 'total size of new array must be unchanged ' return { 'name ' : self.name , 'dtype ' : self.dtype } kwargs [ 'mask ' ] = computed_mask layer.backward_layer.add_loss ( lambda : 1 ) shape as ` y_true ` ; otherwise , it is scalar . if inspect.ismethod ( met ) : steps_per_epoch = len ( generator ) P = TP + FP ( predicted positive ) as varying linearly within each interval op , input_dim : int > 0 . Size of the vocabulary , custom_objects=custom_objects , for shape , dtype in zip ( input_shape , self._input_dtypes ) ] class ConvRNN2D ( RNN ) : optimizer_weights_group [ 'weight_names ' ] ] the tensor after 1d conv with un-shared weights , # Note on passing external constants to RNNs `` `` '' 1D convolution . class ELU ( Layer ) : from tensorflow.keras.applications.imagenet_utils import decode_predictions ' ` padding= ( top_pad , bottom_pad , left_pad , right_pad ) ` . ' , if isinstance ( k , six.string_types ) : chunk_id = 0 be reduced . new_lr = lr * factor seed=None , x : NumPy array to normalize . x_test = x_test.transpose ( 0 , 2 , 3 , 1 ) in order to capture the string summary . 'at deserialization time ) . ' ) y = np.expand_dims ( y , -1 ) @ pytest.mark.parametrize ( 'implementation ' , [ 1 , 2 ] , ids= [ 'impl1 ' , 'impl2 ' ] ) self.loss , self.output_names ) def __init__ ( self ) : weights2 = preprocess_weights_for_loading ( from .pooling import AveragePooling3D raise RuntimeError ( # Build self._output_layers : ` ( ( top_crop , bottom_crop ) , ( left_crop , right_crop ) ) ` as ` y_true ` , or is broadcastable to ` y_true ` . ` sample_weight ` acts as a check_composed_tensor_operations ( 'reshape ' , { 'shape ' : ( 4 , 3 , 1 , 1 ) } , # But the assign ops wo n't be executed under this mode , that 's why return T.ones_like ( targets , dtype='int8 ' ) The input should be at least 3D , and the dimension of index one _test_optimizer ( optimizers.Adamax ( ) ) return x # If ` x ` and ` y ` were all symbolic , self.strides [ 0 ] ) if _is_tf_1 ( ) : fieldnames = [ unicode ( x ) for x in fieldnames ] ` tensor_indices [ i ] ` is the index of ` input_tensors [ i ] ` within the `` `` '' Dummy decorator used in TensorFlow 2.0 to exit the Keras graph . '' '' '' self.state_spec = [ InputSpec ( shape= ( None , None , None , dim ) ) kernel_d , metrics_utils.ConfusionMatrix.FALSE_NEGATIVES : self.false_negatives , except TypeError : steps_name : The public API 's parameter name for ` steps ` . seq : a possible Sequence object original_shape = int_shape ( x ) `` `` '' Element-wise inequality between two tensors . The ` build_fn ` should construct , compile and return a Keras model , which return T.extra_ops.cumprod ( x , axis=axis ) fn : The metric function to wrap , with signature padding : Currently only supports ` `` valid '' ` ( case-insensitive ) . from tensorflow.keras.applications.xception import Xception weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 2 , 3 , 1 , 0 ) ) prefix : string . return tensor raise ValueError ( `` { } first line should end with a ' . ' '' .format ( name ) , self.true_positives [ min_index ] / denom , output._uses_learning_phase = True `` `` '' Performs sample weight validation and standardization . self.W = self.add_weight ( shape= ( self.nb_feature , input_dim , self.output_dim ) , super ( ReshapeBatch , self ) .__init__ ( [ input ] , as_numpy=False , name=name ) return d1 , d2 , a `` `` '' Softmax of a tensor . scale = self.scale WITH_NP , axes= ( 2 , 2 ) ) A list of tuples of array indices . [ [ 0.1 , 0.9 , 0.8 ] , [ 0.05 , 0.95 , 0 ] ] then the categorical accuracy is 1/2 or .5 . # 2 . Passing in a layer raises a warning 'beta_constraint ' : constraints.serialize ( self.beta_constraint ) , if hasattr ( loss_name , '__name__ ' ) : len_dim3 = input_shape [ 4 ] assert len ( data_gen ) == 20 has_raise = re.findall ( r '' ^\s * raise \S+ '' , code , re.MULTILINE ) of RGBA data , it should have value 4 . lambda : y_true * ( 1.0 - smoothing ) + 0.5 * smoothing , available_devices = _get_available_devices ( ) K.zeros_like ( self.true_positives ) ) Adagrad is an optimizer with parameter-specific learning rates , shape= [ x.shape [ normalization_axis ] ] ) if hasattr ( w , 'name ' ) and w.name : K.one_hot ( top_k_idx , x.shape [ -1 ] ) , axis=-2 ) depthwise_constraint='max_norm ' , max_queue_size : maximum size for the generator queue the model 's weights @ pytest.mark.parametrize ( 'tensor_shape ' , ` _keras_history ` : Last layer applied to the tensor . distribution='normal ' , metric_obj = metrics.Mean ( name=name ) dynamic_shape = [ None for a in x.dynamic_axes ] return fused_batch_norm ( ` metrics= [ 'accuracy ' , [ 'accuracy ' , 'mse ' ] ] ` . def dl_progress ( count , block_size , total_size ) : # A list of metric instances corresponding to the metric tensors added using last_output : The latest output of the rnn , of shape ` ( samples , ... ) ` self.max_value = max_value 'float32_ref ' loop through each dimension in x 's shape and y 's shape : check_dtype ( K.variable ( [ 10 ] ) , dtype ) val_enqueuer.stop ( ) verbose : verbosity mode , 0 or 1 . reduction : a metrics ` Reduction ` enum value . def __init__ ( self , stateful_metrics=None ) : # backwards compatibility for models serialized prior to 2.1.2 if node_index < len ( node_data_list ) : out = model.fit ( input_a_np , output_a_np ) do_reshape = False if mask is not None : self.updates.append ( K.update ( v , v_t ) ) # Apply activity regularizer if any : specify a fixed batch size for your model , by passing super ( Dense , self ) .__init__ ( * * kwargs ) num_classes = np.max ( y ) + 1 lr = self.schedule ( epoch , lr ) '1st entry of padding ' ) new_layer = keras.layers.AvgPool3D ( https : //www.biostat.wisc.edu/~page/rocpr.pdf self._dynamic_display = ( ( hasattr ( sys.stdout , 'isatty ' ) and If x has shape ( samples , dim ) and n=2 , shape = ( ) ' a tuple ` ( x , y , sample_weight ) ` ' name=loss_fn.__name__ , super ( NumpyArrayIterator , self ) .__init__ ( _axis = C.Axis.all_axes ( ) maxlen = 5 ( left_dim2_pad , right_dim2_pad ) , model.compile ( 'sgd ' , loss=keras.losses.Huber ( ) ) metrics , as well as activation histograms for the different fill_mode=fill_mode , weights : list of Numpy arrays to set as initial weights . with pytest.warns ( UserWarning ) : # expect UserWarning for skipping weights able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes . super ( Concatenate , self ) .__init__ ( * * kwargs ) rho : float > = 0 . Adadelta decay factor , corresponding to fraction of old_layer = keras.layers.MaxPooling2D ( ( 2 , 2 ) , 2 , 'valid ' , name='maxpool2d ' ) x_shape = list ( x._keras_shape ) input_shapes= [ x._keras_shape for x in self.inputs ] , if input_row is None or input_col is None : 'variable to update . Valid variable key options are : `` { } '' . ' dynamic_batch_size = False if is_np : # ( not all nodes included in the layers are relevant to the current graph ) . ValueError : if invalid ` img ` or ` data_format ` is passed . workers=1 , 'Unexpected channels axis { } . '.format ( axis_without_batch ) , based on either the maximization or the # Otherwise either ~/.keras or /tmp . List of callbacks to apply during training . `` `` '' Element-wise tanh . # original shape : ( batch , length , input_dim ) 'The model has ' + str ( len ( output_names ) ) def allow_write_to_gcs ( save_function ) : sample_weight = K.constant ( [ 1.2 , 0.5 ] ) if axes is None : data = [ n.decode ( 'utf8 ' ) for n in group.attrs [ name ] ] layers_by_depth [ depth ] .append ( layer ) self.verbose = verbose if len ( input_shape ) ! = 5 : RuntimeError : If the model was never compiled . [ str ( node.input_shapes ) for node in self._inbound_nodes ] ) tensor_indices = [ ] dimensions of the convolution window . version= ' 2.3.1 ' , return x.value mask = mask.dimshuffle ( [ 1 , 0 ] ) 2 * padding [ 0 ] + output_padding [ 0 ] ) 'right after the ` Embedding ` layer to get the same behavior . ' , A Keras ` Model ` instance which can be used just like the initial permutation.extend ( [ i for i in on_unused_input='ignore ' , * 'interpolation ' : Applies mid-point summation scheme for ` ROC ` curve . For # LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase def check_two_tensor_operation ( function_name , if recompute_steps_per_epoch : assert np.abs ( np.std ( rand ) - std * 0.87962 ) < 0.015 'b_constraint ' : constraints.serialize ( self.b_constraint ) , weights=None , layer.__class__.__name__ ) ) if 'input_shape ' not in kwargs : # used for training . return C.times ( x , y ) def _to_tensor ( x , dtype ) : odd_pad_h = pool_size [ 1 ] > 2 and pool_size [ 1 ] % 2 == 1 axis : The dimension softmax would be performed on . def validate ( cls , key ) : def _helper_bilinear ( data_format , height_factor , width_factor ) : f_active_next , log_f_next = ctc_update_log_p ( # Set self.layers and self._layers_by_depth . `` `` '' Returns the number of axes in a tensor , as an integer . layer=sublayer , last_output._uses_learning_phase = uses_learning_phase self.embeddings_freq = embeddings_freq arg_dict = config [ 'arguments ' ] [ key ] self._last_update = 0 def __call__ ( self , x ) : 'Reduce LR on plateau conditioned on metric ` % s ` ' return x.dtype.base_dtype.name from .load_backend import reset_uids def forward ( self , arguments , device=None , outputs_to_retain=None ) : def ctc_create_skip_idxs ( Y ) : return self.cell.kernel_size node_index = node.node_indices [ i ] rows , cols = 2 , 3 if len ( self.outputs ) > 1 : return activation.__name__ def handle_class_init ( name , member ) : extract : True tries extracting the file as an Archive , like tar or zip . from tensorflow.keras import Input weighted_metrics : List of metrics to be evaluated and weighted tensor : the tensor from which to get the ( static and dynamic ) shapes options=run_options , # but just return an unmeaningful tensor . def DISABLED_test_target_tensors ( ) : # Return the image as a Jupyter Image object , to be displayed in-line . with K.name_scope ( self.optimizer.__class__.__name__ ) : return res # model.compile ( optimizer , loss='mse ' , loss_weights= ( 0.5 , 0.5 ) ) def DISABLED_test_generator_methods_interface ( ) : 'channels_last ' , center=True , return data , labels del f `` `` '' Standard deviation of a tensor , alongside the specified axis . self._open_args = { 'newline ' : '\n ' } decrement : A tensor of same shape as ` x ` . return 100 . * K.mean ( diff , axis=-1 ) op = tf.print ( message , x , output_stream=sys.stdout ) for unroll in [ True , False ] : return below_threshold + above_threshold fn : The objective function to wrap , linear neural networks ] ( http : //arxiv.org/abs/1312.6120 ) beta_initializer : Initializer for the beta weight . name = model._input_layers [ i ] .name callbacks=None , def DISABLED_test_Bidirectional_with_constants_layer_passing_initial_state ( ) : signature = inspect.signature ( fn ) seed = np.random.randint ( 10e3 ) For Python 3 , checks if there is an argument with the given name , and with closing ( urlopen ( url , data ) ) as response , open ( filename , 'wb ' ) as fd : label_is_pos = K.tile ( labels_2d , [ num_thresholds , 1 ] ) _SHARED_SEQUENCES [ self.uid ] = None cls = element [ 0 ] decoded sequences . self.file_flags = ' b ' if hasattr ( t , '_keras_shape ' ) and len ( t._keras_shape ) > 1 : sample_weights.append ( weight ) mask = np.random.randint ( 2 , size= ( num_samples , timesteps ) ) return_sequences=return_sequences , if len ( bad_attributes ) > 0 : > > > K.shape ( kvar ) tf_data_format = None * 'minoring ' : Applies left summation for increasing intervals and right processed based on the size of the first dimension of the X = np.random.randn ( 200 , 10 ) .astype ( 'float32 ' ) kernel_constraint=None , if constants_np is not None : Gaussian Noise ( GS ) is a natural choice as corruption process assert model.uses_learning_phase input_shape = self.input_spec [ 0 ] .shape class LocallyConnected1D ( Layer ) : random_rotation = image.random_rotation len_dim1 = input_shape [ 2 ] 'tfoptimizer ' : TFOptimizer , If ` x ` is a generator , or ` keras.utils.Sequence ` instance , input_shape : Shape tuple , not including the batch axis . ask.return_value = False norecursedirs= build K.ones_like ( h_tm1 ) , raise TypeError ( ' ` elems ` must be a tensor ' ) it passes to its callbacks : ' ( ( top_pad , bottom_pad ) , ( left_pad , right_pad ) ) . ' self.run_thread.start ( ) '1st entry of cropping ' ) raise TypeError ( 'Unrecognized keyword arguments : ' + str ( kwargs ) ) `` `` '' Decodes the output of a softmax . old_layer = keras.layers.LSTM ( input_dim=5 , matrix_x = K.dot ( inputs , self.kernel ) raise TypeError ( 'expects ` constants ` shape ' ) output = super ( ConvRNN2D , self ) .__call__ ( full_input , * * kwargs ) # sequential model from .convolutional import Deconvolution3D A list of available GPU devices . from .load_backend import print_tensor if num_chunks > 1 : input_shape=input_shape [ 1 : ] ) ) h_tm1_h = h_tm1 raise ValueError ( 'Provide either a layer name or layer index . ' ) width_cropping = conv_utils.normalize_tuple ( from .theano_backend import * sample_weight = K.squeeze ( sample_weight , -1 ) between samples in different successive batches . if hasattr ( generator , 'batch_size ' ) : from .. utils.data_utils import Sequence normal = random_normal = RandomNormal self.cropping = cropping 'embeddings_regularizer ' : weights = weights [ num_param : ] For 'channels_last ' data_format , cls_name = layer.__class__.__name__ # first parameter and accumulator as second created by the layer , and ` bias ` is a bias vector created by the layer for i in range ( ndim ) : if not self.validation_data and self.histogram_freq : self.targets = [ ] member.__module__ , inspect.getmodule ( member ) .__file__ ) w = x._keras_shape [ 3 ] + padding [ 1 ] [ 0 ] + padding [ 1 ] [ 1 ] assert dense._inbound_nodes [ 0 ] .inbound_layers == [ a_layer ] name : String , name of layer . class KLDivergence ( MeanMetricWrapper ) : thresholds=None , from tensorflow.keras.layers import concatenate recurrent_h = r * recurrent_h https : //arxiv.org/abs/1212.5701 ) def test_he_uniform ( tensor_shape ) : in the output sequence , or the full sequence . elif val_gen : return None chunk = response.read ( chunk_size ) batch_logs = { 'batch ' : batch_index , 'size ' : batch_size } # There is a bug in cntk gather op which may cause crash . return one_indexed_epoch in validation_freq def in_top_k ( predictions , targets , k ) : if len ( _axis ) == 0 : unique_tensors.append ( x ) globs [ 'Sequential ' ] = models.Sequential y = np.searchsorted ( self.classes_ , y ) raise ValueError ( 'Layer has ' + str ( len ( self.states ) ) model.compile ( 'sgd ' , 'mse ' ) new_layer = keras.layers.ZeroPadding3D ( ( 2 , 2 , 2 ) , `` `` '' Adds state variable . Only for use by subclasses . '' '' '' self._start = time.time ( ) `` `` '' Sets the manual variable initialization flag . def on_epoch_begin ( self , epoch , logs=None ) : # implement a simple RNN with an additional state batch_size : Integer batch size or None if unknown . Can only be run on GPU , with the TensorFlow backend . ( ` state_size [ 0 ] ` ) should be the same as provided , we will repeat the same for every threshold . from .data_utils import GeneratorEnqueuer ' '' . Only expected the following keys : ' + str ( output_names ) ) def serialize ( loss ) : from keras.layers import Masking # means `` take an average from a single value '' but keeps the if ` data_format ` is ` `` channels_last '' ` . self.forward_layer.initial_weights = weights [ : nw // 2 ] def on_predict_batch_end ( self , batch , logs=None ) : the display labels for the scalar outputs . dtype ( 'float32 ' ) except : outputs . If ` use_bias ` is True , output_masks : list of output masks ( a mask can be a tensor , or None ) . initial_state : tensor or list of tensors or None assert t._keras_shape [ i ] == z.shape [ i ] states_tm1 = initial_states # tm1 means `` t minus one '' as in `` previous timestep '' `` `` '' Batch normalization layer ( Ioffe and Szegedy , 2014 ) . bias_initializer=bias_initializer , handle_class ( name , mem ) rnn_inputs ) ) easily to children processes . class_weight : Optional dictionary mapping class indices ( integers ) to mask values . ' weights , but the saved weights have ' 'steps ' : steps_per_epoch , if not self.trainable and self.built : new_shape = list ( x.shape ) losses += layer.losses raise ValueError ( 'CNTK Backend : padding input tensor with ' sgd = SGD return K.sum ( y_true * K.log ( y_true / y_pred ) , axis=-1 ) callbacks._call_batch_hook ( 'predict ' , 'begin ' , steps_done , batch_logs ) stride=self.strides [ 0 ] , KTF = None elif callable ( identifier ) : if shape and value.shape ! = shape : # can not be held into a float32 so it causes an underflow while from def __init__ ( self , max_value=2 , axis=0 ) : mask_zero : Whether or not the input value 0 is a special `` padding '' def test_pickling_right_after_compilation ( ) : # in current version cntk ca n't support input with variable also whether this argument can be called with a keyword ( i.e . if it is shuffle_pattern [ 1 ] = shuffle_pattern [ axis ] depth_multiplier=1 , slice_arrays ( x , split_at ) ) the log probability of each decoded sequence . class Poisson ( LossFunctionWrapper ) : def clone_model ( model , input_tensors=None ) : # to a training/evaluation method ( since it is n't yet built ) : self.reverse_state_order = kwargs.pop ( 'reverse_state_order ' , False ) target_devices = [ '/cpu:0 ' ] + [ '/gpu : % d ' % i for i in target_gpu_ids ] ValueError : In case ` dim ( x ) == 1 ` . < tf.Tensor 'Cast_2:0 ' shape= ( 2 , 3 ) dtype=float16 > shuffle : whether to shuffle the data at the beginning of each epoch While TH implements convolution , TF and CNTK implement the correlation operation . for name , outputs in zip ( output_names , all_outputs ) : print ( ' % d gpus training : ' % i , total_time ) self.backward_layer._num_constants = self._num_constants relevant_nodes += v tensor_indices.append ( tensor_index ) scale=scale , * * kwargs ) keras_shape_list = ( 1 , ) zero_expr_shape = tf.ones_like ( expr_shape ) inputs , initial_state , constants , self._num_constants ) then the false positives value is 2 . If the weights were specified as # NN OPERATIONS from tensorflow.keras.layers import Activation def ResNet152 ( * args , * * kwargs ) : self.target_shape , inputs_vals = np.random.random ( ( num_samples , num_timesteps , num_features ) ) for x in self.inputs : layer ( x ) # Create a metric wrapper for each output loss . This computes mean of an seq_len_0 = 4 If the model has multiple outputs , you can use a different axes = [ x.ndim - 1 , y.ndim - 1 ] to match that input shape provided . input_shape= ( height , width , 3 ) , if isinstance ( metric , six.string_types ) : def size ( self ) : output_names = [ ] if _LOCAL_DEVICES is None : 'the inputs shape ' + str ( input_shape ) ) num_samples = y_pred.shape [ 0 ] x = [ np.random.randn ( 2 , 4 , 3 ) , np.random.randn ( 2 , 3 ) ] ident = mp.current_process ( ) .ident classes=None , recurrent_dropout=0.1 , This can be useful to tell the model to `` `` '' Returns the learning phase flag . a = K.variable ( val ) return y `` `` '' Builds a map of the graph of layers . @ staticmethod metrics.extend ( self._metrics ) symbolic_weights = getattr ( model.optimizer , 'weights ' ) of custom ( non-Keras ) objects to class/functions target_mean=target_mean , target_max=1 . ) def _is_gcs_location ( filepath ) : Note that in conjunction with ` initial_epoch ` , layers = [ ] inputs = keras.Input ( ( timesteps , dim ) ) from .cntk_backend import * output = repeat_elements ( x , height_factor , axis=axis_1 ) ` ( symmetric_dim1_crop , symmetric_dim2_crop , symmetric_dim3_crop ) ` . left_pad , right_pad = padding [ 1 ] self.initial_weights = weights new_shape = ( 1 , 1 , bias_shape [ 0 ] ) x = keras.layers.Input ( shape= ( 2 , ) ) * [ states_at_step [ i ] for states_at_step in successive_states ] ) ) return_sequences : Boolean . Whether to return the last output name='moving_variance ' , if shape.count ( C.InferredDimension ) > 1 or shape.count ( C.FreeDimension ) > 1 : # this is the layer that takes a list of input tensors [ Adadelta - an adaptive learning rate method ] ( warnings.warn ( x = C.transpose ( x , ( 3 , 0 , 1 , 2 ) ) class TruePositives ( _ConfusionMatrixConditionCount ) : dtype = backend.floatx ( ) m : m1 : strides [ 2 ] ] ) # we compute the output tensors , return T.nnet.softmax ( x ) self.unrelated_updates = None ' ` target_tensors ` . Expected a list or a dict ' index_array = np.arange ( num_samples ) # if there 's no bias weight in the file , skip this conversion return hasattr ( x , '_keras_history ' ) `` `` '' Zero-padding layer for 1D input ( e.g . temporal sequence ) . return np.pad ( x , [ ( 0 , 0 ) , padding , ( 0 , 0 ) ] , mode='constant ' ) path = get_file ( 'specify the batch size by passing ' http : //www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf ) . # Legacy imports opt_update = self.optimizer.apply_gradients ( d1 = x_shape [ axes [ 0 ] ] ' ` keras.utils.Sequence ` ' with open ( fpath , 'rb ' ) as f : if not xs : ( output_row , output_col , -1 , filters ) ) raise OSError ( name='vhat_ ' + str ( i ) ) full_input_spec = self.input_spec + additional_specs should not violate the temporal order . See input_dim = 10 pattern = list ( range ( x_ndim ) ) assert y._keras_shape == ( 5 , 6 , 3 , 4 ) If dot_axes is ( 1 , 2 ) , to find the output shape of resultant tensor , if has_arg ( self.layer.call , 'mask ' ) and mask is not None : padding='valid ' , self.iterations = K.variable ( 0 , dtype='int64 ' , name='iterations ' ) x = inputs * kept_idx + alpha_p * ( 1 - kept_idx ) v = tf.zeros ( shape=shape , dtype=dtype , name=name ) if K.backend ( ) ! = 'tensorflow ' : for v in model._nodes_by_depth.values ( ) : if mean.ndim == 1 : return len ( x.dynamic_axes ) outputs : Tensor with shape ( samples , ... ) ( no time dimension ) , def from_config ( cls , config ) : x_dense_1 = x_sparse_1.toarray ( ) def DISABLED_test_model_with_crossentropy_losses_channels_first ( ) : current = C.ops.slice ( inputs , time_axis , i , i + 1 ) assert len ( z_shape ) == z_np.ndim pool_size=2 , padding='valid ' , data_format='channels_first ' , mean = k.variable ( mean_np ) `` `` '' Repeat the elements of a tensor along an axis , like np.repeat . rnn_constants.append ( new_c ) return self.cell.call ( inputs , states , * * kwargs ) name : string name of the metric instance . super ( FalseNegatives , self ) .__init__ ( def call ( self , inputs , mask=None , training=None , initial_state=None ) : Mean Intersection-Over-Union is a common evaluation metric for semantic image output_row , output_col = output_shape inputs = ( [ input_prob_matrix_0 [ t , : ] [ np.newaxis , : ] # # Multi-backend Keras and tf.keras # Compatibility with the generators shape , mean=mean , stddev=stddev , dtype=dtype , seed=seed ) i = self.recurrent_activation ( x_i + K.dot ( h_tm1_i , ' has shape { } '.format ( symbolic_shape ) old_weights = [ layer.get_weights ( ) for layer in model.layers ] ' [ y/n ] ' % ( filepath ) ) .strip ( ) .lower ( ) Arbitrary . Use the keyword argument ` input_shape ` extract=False , ( str ( self.input_length ) , str ( input_shape ) ) ) accumulator = fn ( accumulator , elems [ -i ] ) # dot product between tensors self.on_epoch_begin = on_epoch_begin weight_names = [ ] ProgressTracker.progbar = Progbar ( total_size ) optimizer_weights_group = h5dict [ 'optimizer_weights ' ] elif isinstance ( data , list ) : ` ( batch_size , rows , cols , channels ) ` inputs = self._feed_inputs + [ K.learning_phase ( ) ] constraints.serialize ( self.kernel_constraint ) , height , width = input_shape [ h_axis ] , input_shape [ w_axis ] str ( self.output_padding ) ) if not isinstance ( model , Sequential ) : if kwd in kwargs : self.metrics_func = None # Assume unknown backends use correlation ` losses ` , or be broadcastable to ` losses ` . self._input_map = { } # Apply constraints . old_value = kwargs [ key ] { self.field : json.dumps ( send ) } , hasher = hashlib.sha256 ( ) Class predictions . def __init__ ( self , name='binary_accuracy ' , dtype=None , threshold=0.5 ) : is_graph_network = self._is_graph_network layer_test ( layers.AveragePooling3D , initial_state=initial_state ) '\ 's weights have shape ' @ interfaces.legacy_input_support length = conv_utils.conv_output_length ( input_shape [ 1 ] , from tensorflow.keras.constraints import deserialize # Merge outputs under expected scope . that will be set to 0 . if 'dropout ' in kwargs : maximum over all dimensions . # In case of nested models : recover the first layer y = tf.expand_dims ( y , 2 ) C.ops.functions.Function ) ) speedup = times [ 1 ] / times [ 0 ] def serialize ( regularizer ) : # ` validation_data ` is neither a tuple nor a triple . cudnn_layer = Bidirectional ( cudnn_layer ) outputs : Tensor with shape ` ( samples , time , ... ) ` where each a = ( ( 1 - rate ) * ( 1 + rate * alpha_p * * 2 ) ) * * -0.5 gamma = ones_like ( var ) self.outputs = [ x ] model.compile ( 'sgd ' , metrics= [ keras.metrics.KLDivergence ( ) ] ) if mode not in { 'fan_in ' , 'fan_out ' , 'fan_avg ' } : # Shape : ( num_samples , timesteps , ... ) return sum ( ones_like ( x , name=name ) ) uniform = random_uniform = RandomUniform # Handle automatic shape inference ( only useful for Theano ) . 'static length for your sequences . ' ) while unique_name in weight_names : self.model.sample_weights ) 'trainable ' : self.trainable } `` `` '' Generates predictions for the input samples from a data generator . if isinstance ( inputs , list ) and inputs == [ ] : if hasattr ( x , '_uses_learning_phase ' ) : from scipy.sparse import issparse if not all ( input_shape [ 1 : ] ) : implementation=2 , With ` distribution= '' normal '' ` , samples are drawn from a truncated normal `` `` '' Turn a nD tensor into a 2D tensor with same 0th dimension . considered during deserialization . # Compatibility aliases rate . In this version , initial learning rate and decay factor can assert f [ ' x ' ] == 'abcd ' self.on_test_end ( ) unprocessed_nodes [ layer ] = [ node_data ] ` `` channels_last '' ` corresponds to inputs with shape # If the class is private the name starts with `` _ '' which is not secure self.recurrent_kernel_i , x_transposed = K.reshape ( x_transposed , new_shape ) recurrent_regularizer='l1 ' , def _test_optimizer ( optimizer , target=0.75 ) : ' must be ' 'has been renamed ` num_words ` . ' ) as they ca n't be passed def _postprocess_conv3d_output ( x , dim_ordering ) : super ( ActivityRegularization , self ) .__init__ ( * * kwargs ) training=training ) coefficient for the loss . If a scalar is provided , then the loss is # It acts as a queue that maintains any unprocessed x = K.variable ( 0 . ) # Set metric attributes on model . for layer_class in [ keras.layers.CuDNNGRU , keras.layers.CuDNNLSTM ] : permutation = output_dimensions [ : axis ] padding='valid ' , data_format=None ) : y_pred = K.constant ( [ [ 4. , 8 . ] , [ 12. , 3 . ] ] ) below_threshold = alpha * ( x - threshold ) * ( x < threshold ) dense_mask = dense_mask [ : , 0 , : ] # dense.input y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) return yaml.dump ( self._updated_config ( ) , * * kwargs ) init=value , # Properly set learning phase on output tensor . weights= [ W_constraint='maxnorm ' , def test_imdb_load_does_not_affect_global_rng ( fake_downloaded_imdb_path ) : int_shape : an alternative static shape to take as the last part `` `` '' Gets the total dataset size ( number of elements ) . ( self.output_row , self.output_col ) , legacy_spatialdropout1d_support = generate_legacy_interface ( To allow multiple generators to be used at the same time , we use ` uid ` to either a tensor or None ( no mask ) . safe_p_ratio = K.switch ( it is applied to the outputs as well . def get_input_shape_and_dtype ( layer ) : if threshold ! = 0. : > > > from keras.layers import Input , Dense training=None , self.input_spec = InputSpec ( ndim=4 , axes= { channel_axis : input_dim } ) For instance , ` shape= ( 32 , ) ` indicates that the expected input # Same labels assumed . return map ( theano.gradient.disconnected_grad , variables ) * * Multi-backend Keras has been discontinued . At this time , we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to ` tf.keras ` in TensorFlow 2.0 * * . return ( x , x - decrement ) Intuitively , separable convolutions can be understood as In the [ examples folder ] ( https : //github.com/keras-team/keras/tree/master/examples ) of the repository , you will find more advanced models : question-answering with memory networks , text generation with stacked LSTMs , etc . return tf.nn.softmax ( x , axis=axis ) else : 'double check the keras shape history . ' from .callbacks import TerminateOnNaN if spec.ndim is not None : new_depth = ( ( depth - 1 ) * strides [ 0 ] + kernel_size [ 0 ] from .pooling import GlobalMaxPooling2D You can pass `` external '' constants to the cell using the ` constants ` unit_forget_bias : Boolean . return self.end - self.start assert dot.get_node ( layer_id ) sequence : A ` keras.utils.data_utils.Sequence ` object . a ` pydot.Cluster ` instance representing nested model if when=lambda x : True , _p_prev = T.set_subtensor ( _p_prev [ : active ] , p_prev ) if trainable : under ` directory ` , where each subdirectory will super ( ConvertToStatic , self ) .__init__ ( [ input ] , as_numpy=False , name=name ) interpolation=interpolation , ( 'ndim= ' + str ( self.ndim ) ) if self.ndim else `` , return output load_img = image.load_img if data tensors : the model is built on top of these tensors . or a mismatch in the shape of the weights . ` batch_dot ` is used to compute dot product of ` x ` and ` y ` when result = list ( state_shape ) Training will stop if the model does n't show improvement shuffle : Boolean ( default : True ) . `` `` '' Test two batch entries - best path decoder . '' '' '' `` `` '' Computes Kullback-Leibler divergence metric between ` y_true ` and ` y_pred ` . y_true = _maybe_convert_labels ( y_true ) check_single_tensor_operation ( 'exp ' , ( 4 , 2 ) , WITH_NP ) shape = shape [ 1 : ] return C.reshape ( result , ( ) ) input_shape [ 3 ] ) validation_freq=validation_freq , array ( [ [ 1. , 2 . ] , output = squeeze ( output , 2 ) from .pooling import AveragePooling3D cntk_axis = [ ] class_weights = training_utils.standardize_class_weights ( `` `` '' Abstract method to loop over some data in batches . for l , o in zip ( model.metrics_names , outs ) : x = _preprocess_conv2d_input ( x , data_format ) x_shape = ( 1 , ) + shape + ( 3 , ) permutation = [ 0 ] 'with different input shapes . Hence ' self.merged = None from keras.utils.data_utils import _hash_file # Create lits of input and output tensors and return new class cond_float = condition.astype ( floatx ( ) ) epochs = kwargs.pop ( 'nb_epoch ' ) if target_tensors is not None : prev_output = zeros_like ( output ) mask : Computed mask value for the current output . if label_mode not in [ 'fine ' , 'coarse ' ] : ' has multiple inbound nodes , ' def classification_error ( target , output , axis=-1 ) : JSON-encoded dictionary of event data . This callback is automatically applied to every Keras model . re.findall ( r '' \s * raise \S+ '' , code_inner , re.MULTILINE ) ] units = 256 TRUE_NEGATIVES = 'tn ' on_train_end=None , z = K.eval ( z ) # Merging num_values = K.sum ( sample_weight ) ` True ` if conversion on kernel matrices is required , otherwise ` False ` . additional_specs += self.constants_spec sample_weight : Optional array of the same length as x , containing recurrent_r = K.bias_add ( recurrent_r , self.recurrent_bias_r ) calling them with the keyword argument ` initial_state ` . bias : whether to include a bias base = ( input_shape [ 0 ] , rows , cols , cell.filters ) value = np.array ( value ) raise LookupError regularizer=self.bias_regularizer , assert ask.call_count == 2 y._keras_shape = tuple ( np.asarray ( x._keras_shape ) [ list ( pattern ) ] ) if rec_dp_mask is not None : shape2.pop ( axes [ 1 ] ) if getattr ( self , '_output_loss_metrics ' , None ) is not None : py_slice ( left_pad , input_shape [ 3 ] + left_pad ) ) Weighted loss float ` Tensor ` . If ` reduction ` is ` NONE ` , this has the same from .. import regularizers def uses_learning_phase ( self ) : self.updates.append ( ( self.m_schedule , m_schedule_new ) ) m_t = ( self.beta_1 * m ) + ( 1 . - self.beta_1 ) * g K.pow ( K.cast_to_floatx ( 0.96 ) , ( t + 1 ) * self.schedule_decay ) ) ) K.set_value ( self.model.optimizer.lr , new_lr ) target = reshape ( target , shape ( output ) ) ndim = K.ndim ( x ) `` `` '' Returns the model 's metrics added using ` compile ` , ` add_metric ` APIs . '' '' '' if isinstance ( self.data , dict ) : input_shape=image_shape , dtype=K.dtype ( p ) , with K.control_dependencies ( update_op ) : # For TF `` `` '' Initializer that generates tensors with a uniform distribution . name='avgpooling2d ' ) shuffle : Boolean ( whether to shuffle the training data check_single_tensor_operation ( 'spatial_2d_padding ' , x_shape , WITH_NP , model.add ( LocallyConnected1D ( 64 , 3 , input_shape= ( 10 , 32 ) ) ) return C.splice ( * temp , axis=index ) # the author of the subclassed network ) . K.set_value ( self.states [ 0 ] , ` x_col ` . If ` True ` , invalid images will be ignored . Disabling this cols = input_shape [ 2 ] self.writer.add_summary ( summary , index ) weights to apply to the model 's loss for each sample . assert layer.activity_regularizer def test_convolutional_recurrent ( data_format , return_sequences , use_mask ) : x_z = K.dot ( inputs_z , self.kernel_z ) samplewise_std_normalization=False , bool , whether ` fn ` accepts a ` name ` keyword argument . inputs : Tensor with shape ( samples , ... ) ( no time dimension ) , from .load_backend import update shape = tensor._keras_shape width_factor , from .load_backend import ndim self._output_mask_cache [ mask_cache_key ] = mask constraints.serialize ( self.recurrent_constraint ) , assert np.max ( rand ) < = max_val self.backward_layer.name = 'backward_ ' + self.backward_layer.name return K.cast ( K.equal ( y_true , y_pred_labels ) , K.floatx ( ) ) def placeholder ( shape=None , ndim=None , dtype=None , sparse=False , name=None ) : indices = indices [ : :-1 ] # code below is taken from http : //conda.pydata.org/docs/travis.html # Dictionary mapping reference tensors to tuples inspect.Parameter.KEYWORD_ONLY ) ) for old_name , new_name in conversions : broadcast_beta = tf.reshape ( beta , target_shape ) To be implemented by subclasses : 'In order to use timestep-wise sample weights , ' # Update tensor_map . # update accumulator h5dict = H5Dict pep8maxlinelength = 85 weight_col=weight_col , @ pytest.mark.parametrize ( 'op , input_shape , kernel_shape , padding , data_format ' , [ `` `` '' Computes the ( weighted ) mean of the given values . return ( self.forward_layer.non_trainable_weights strides= ( 1 , 1 , 1 ) , return w * ( desired / ( K.epsilon ( ) + norms ) ) switch_condition , noise_shape = ( input_shape [ 0 ] , 1 , 1 , input_shape [ 3 ] ) _ , h5_path = tempfile.mkstemp ( '.h5 ' ) 'data_format ' : 'channels_first ' , # For static axis assert z1 == z2 'bias_constraint ' : 'max_norm ' , k=k ) return tf.nn.embedding_lookup ( reference , indices ) val.extend ( [ x.decode ( 'utf8 ' ) for x in chunk ] ) if getattr ( output , '_uses_learning_phase ' , False ) : ` channels_last ` corresponds to inputs with shape or alternatively , elementwise Theano function . `` `` '' Checks compatibility between the layer and provided inputs . ( 2 , 3 , 1 , 0 ) ) elif len ( y_shape ) > 1 : cell can also take the optional argument ` constants ` , see if not flag : at the end of every epoch . It should typically str ( spec.max_ndim ) + ' , found ndim= ' with temp_filename ( '.h5 ' ) as fname : return K.elu ( x , alpha ) from .load_backend import hard_sigmoid self.center = center `` `` '' Calls the ` on_predict_begin ` methods of its callbacks . self.model = model lr = self.schedule ( epoch ) 'gamma_regularizer ' : regularizers.serialize ( self.gamma_regularizer ) , `` `` '' Cumulative product of the values in a tensor , alongside the specified axis . x , tf_data_format = _preprocess_conv1d_input ( x , data_format ) to be passed to ` yaml.dump ( ) ` . by a factor of ( depth_factor , height_factor , width_factor ) . x_shape , x_val = parse_shape_or_val ( x_shape_or_val ) def constraints ( self ) : y=None , class Network ( Layer ) : model = clone_model ( model ) inferred from the subdirectory names/structure # need broadcasting super ( SeparableConv2D , self ) .__init__ ( target : A tensor with the same shape as ` output ` . initializer='one ' , epsilon=epsilon ) https : //arxiv.org/abs/1609.03499 ) . dilation_rate : An integer or tuple/list of n integers , specifying self.filters ) model.compile ( 'sgd ' , loss=keras.losses.SquaredHinge ( ) ) assert K.int_shape ( o ) == ( None , None , None ) return variable ( np.random.normal ( loc=0.0 , scale=scale , size=shape ) , prefix = '/'.join ( NAME_SCOPE_STACK ) length = kwargs.pop ( 'input_length ' ) for constant in constants : def test_gaussiandropout_legacy_interface ( ) : normalizer=normalizer_dtype ) x_test = np.frombuffer ( imgpath.read ( ) , np.uint8 , new_layer = layer.__class__.from_config ( layer.get_config ( ) ) y_test = np.frombuffer ( lbpath.read ( ) , np.uint8 , offset=8 ) yield chunk A tensor of the cumulative sum of values of ` x ` along ` axis ` . __This__ : requires that the ` cell.call ` method accepts the same keyword argument doc_lines = [ x.strip ( ) for x in doc.split ( '\n ' ) ] class Constraint ( object ) : curve , list ( metrics_utils.AUCCurve ) ) ) # Reverse time axis . layer : a layer instance . # Create graph nodes . num_samples = 1000 # in this case by convention ` config ` holds kernel_shape , input_shape : Keras tensor ( future input to layer ) def compute_weighted_loss ( losses , positive integers . Supports both convolutional networks and recurrent networks , as well as combinations of the two . return reshape ( x , tmp_shape ) # this does nothing . dot.set ( 'concentrate ' , True ) condition = tile ( condition , shape_expr [ ndim_cond + i ] ) assert_allclose ( res [ 0 , : ] , ref , atol=1e-05 ) `` `` '' Transpose dimensions . model.evaluate_generator ( generator=train_generator ( ) , if callable ( self.mask ) : `` `` '' Called at the beginning of prediction . def model_from_json ( json_string , custom_objects=None ) : warnings.warn ( doc = member.__doc__ dtype : The type of the elements of the resulting tensor . filename='test_save_load_weights_gcs.h5 ' ) def test_gru_legacy_interface ( ) : out4 = model.predict ( np.ones ( ( num_samples , timesteps ) ) ) ( 'conv3d ' , ( 2 , 5 , 4 , 6 , 3 ) , ( 2 , 2 , 3 , 3 , 4 ) , for ( z1 , z2 ) in zip ( z_list [ 1 : ] , z_list [ : -1 ] ) : assert list ( f [ 'y2 ' ] ) == [ b'asd ' , b'sdf ' , b'dfg ' ] model = pickle.loads ( state ) embeddings_metadata : a dictionary which maps layer name to a file name return imagenet_utils.preprocess_input ( * args , * * kwargs ) str ( target_tensors ) ) return 2 . * y_true - 1 . assert len ( layer.non_trainable_weights ) == 0 from .load_backend import arange self.outputs = [ layer._inbound_nodes [ -1 ] .output_tensors [ 0 ] ] class MinimalRNNCell ( keras.layers.Layer ) : warnings.warn ( UserWarning ( step_function , inputs , initial_states , def test_sample_weights ( ) : # INTERNAL UTILS from .input_layer import InputLayer Precipitation Nowcasting ] ( http : //arxiv.org/abs/1506.04214v1 ) `` `` '' Converts weights for RNN layers between native and CuDNN format . return obj.item ( ) def _reset ( self ) : _backend = _config.get ( 'backend ' , _BACKEND ) __One__ : You can specify the initial state of RNN layers symbolically by if initializer is None and shape ( elems ) [ 0 ] > 1 : 'Provided : ' + str ( axes ) ) elif index_from : ( cropping , cropping ) , # Need to use ` identity ` to make this symbolic # numerical stability . The Theano code subtracts out the max pattern = tf.stack ( [ 1 , n , 1 ] ) new_layer = keras.layers.LSTM ( 2 , input_shape= [ None , 5 ] , name= 'd ' , return transform Same as input # Maintain progbar for the lifetime of download . permutation += [ axis_without_batch ] def is_generator_or_sequence ( x ) : return skip_idxs [ non_repeats.nonzero ( ) ] `` `` '' Computes how often targets are in the top ` K ` predictions . `` `` '' # noqa only if they share the same name . This is useful if not _has_nchw_support ( ) and list ( reduction_axes ) == [ 0 , 2 , 3 ] : assert mape_obj.reduction == Reduction.SUM 'containing ' + str ( len ( layer_names ) ) brightness_range=brightness_range , return insecure # Update dimensions of weights to match with mask . one of ` `` channels_last '' ` or ` `` channels_first '' ` . y = T.tile ( x , n , ndim=x.ndim ) # assumption in initializers.VarianceScaling fit_inputs : List of tensors to be fed to ` fit_function ` if _ == C.InferredDimension or _ == C.FreeDimension : # Print the batch number at the beginning of every batch . 'strides ' : strides } , def extract_named_arg ( f , name , args , kwargs ) : if label_smoothing is not 0 : K.conv1d ( dummy_x_1d , dummy_w_1d , data_format='channels_middle ' ) for i in range ( len ( output_tensors ) ) : str_val = str_val [ :10 ] + ' ... ' Each time the output of a layer is used by another layer , location , or instead ask the user with a manual prompt . if mask is not None : 'kernel_regularizer ' : 'on static axis . ' % pattern ) def get_updates ( self , loss , params ) : array ( [ 2 , 2 ] , dtype=int32 ) cbks.ProgbarLogger ( 'InputLayer , not both at the same time . ' ) out_pad_w , legacy_zeropadding3d_support = generate_legacy_interface ( thresholds=self.thresholds , if __name__ == '__main__ ' : for param in signature.parameters.values ( ) : warnings.warn ( 'The ` padding ` argument in the Keras 2 API no longer ' for i in range ( 1 , len ( input_shape ) ) : if ( isinstance ( min_value , ( int , float ) ) and def __init__ ( self , layer , merge_mode='concat ' , weights=None , * * kwargs ) : ` ( batch , channels , rows , cols ) ` fit_inputs = x + y + sample_weights return losses.LossFunctionWrapper ( 'your model is not the size the model expected . ' if _IMAGE_DATA_FORMAT == 'channels_first ' : def l2 ( l=0.01 ) : `` `` '' Layer that multiplies ( element-wise ) a list of inputs . preprocessor=generator_methods_args_preprocessor ) updates += layer.updates ` keras.callbacks.CallbackList ` to be called during prediction . model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.Accuracy ( ) ] ) x += reshape ( bias , ( 1 , 1 , bias_shape [ 0 ] ) ) if callable ( loss ) and not hasattr ( loss , '__name__ ' ) : if top_k is not None and K.backend ( ) ! = 'tensorflow ' : `` { } ' { } ' should have a blank line above . `` .format ( name , keyword ) , # make sure we can also walk the indexes in tensorflow which we str ( data_shape ) ) activation=activation , from tensorflow.keras.preprocessing.sequence import * non_dyn_shape.append ( shape [ i + num_dynamic ] ) return tf.map_fn ( fn , elems , name=name , dtype=dtype ) input_masks : list of input masks ( a mask can be a tensor , or None ) . if not layer.layers : the sample dimension is also ` None ` : self._dropout_mask = None preprocessor=conv2d_args_preprocessor ) raise ValueError ( ' ` padding ` should be either an int , ' output_mask_int_shape = K.int_shape ( output_mask ) from tensorflow.keras.utils import Progbar self.compile ( optimizer=self.optimizer , shape = ( bias.shape [ 0 ] , 1 , 1 , 1 ) self.device = tfdev.DeviceSpec.from_string ( device_str ) y_true : A ` Tensor ` whose shape matches ` y_pred ` . Will be cast to ` bool ` . W_constraint : instance of the [ constraints ] ( .. /constraints.md ) module shape = ( bias.shape [ 1 ] , ) + bias.shape [ :1 ] # But they do not support negative indices , so do n't try print ( X_train [ -1 ] ) str ( len ( symbolic_weights ) ) if isinstance ( results , list ) : super ( Hinge , self ) .__init__ ( hinge , name , dtype=dtype ) w_img = tf.reshape ( w_img , [ shape [ 0 ] , if sequential_like : if batch_size is not None and batch_size ! = static_batch_size : file_access_property_list.set_file_image ( binary_data ) def collect_per_output_metric_info ( metrics , Integer ( scalar ) , number of axes . # install pydot for visualization tests verbose : Verbosity mode , 0 or 1 . def __init__ ( self , name='sum ' , dtype=None ) : old_layer = keras.layers.GRU ( 2 , init='normal ' , # validation_data= ( [ input_a_np , input_b_np ] , ) ) `` `` '' Abstract class for different pooling 2D layers . metrics_utils.Reduction.SUM_OVER_BATCH_SIZE old_layer = keras.layers.ConvLSTM2D ( 5 , 3 , 3 , name='conv ' ) on which to evaluate def assert_function_style ( name , member , doc , args ) : on_epoch_end=None , def test_maxout_dense ( ) : thresholds = to_list ( thresholds ) start=start , size=size ) # and checks it in the following way : multiplying left to right 1e-40 # CONTROL FLOW for key in kwargs.keys ( ) : if num_states % 2 > 0 : the last value is used ( ` elems [ -1 ] ` ) as ` initializer ` from ` elems ` except OSError : `` `` '' Returns filename appended to bucketpath '' '' '' # Implementation based on suggestion solution here : legacy_model_constructor_support = generate_legacy_interface ( prev_total_width = self._total_width inbound_layer_id = str ( id ( inbound_layer ) ) # bool is available since theano v0.9dev 'config ' : instance.get_config ( ) def placeholder ( class TestBackend ( object ) : padding = ( ( 1 , 2 ) , ( 2 , 1 ) , ( 1 , 2 ) ) 'constant ' , constant_values=0 ) # constants if these are passed in __call__ . states = [ ] self.kernel_c , def DISABLED_test_model_metrics_list ( ) : Keras config file at ` ~/.keras/keras.json ` . __doc__ = image.ImageDataGenerator.__doc__ from tensorflow.keras.applications.nasnet import preprocess_input name='sd3d ' ) ( e.g . via the ` input_shape ` argument ) sparse=self.sparse , return np.var ( x , axis=axis , keepdims=keepdims ) preprocessor=recurrent_args_preprocessor ) pattern should be a tuple or list of If tuple of 3 ints : For example , if ` y_true ` is [ -1. , 1. , 1 . ] , and ` y_pred ` is [ 0.6 , -0.7 , -0.5 ] parallel_model.compile ( .. ) [ 0.51286 , 0.288951 , 0.0243026 , 0.0220788 , 0.0219297 , 0.129878 ] , training=training , for i in range ( num_thresholds - 2 ) ] def get_input_at ( self , node_index ) : # Legacy shape : x_train , y_train = np.array ( xs [ : idx ] ) , np.array ( labels [ : idx ] ) verbose=verbose , 'adadelta ' : Adadelta , get_input_mask_at ( node_index ) travis_retry conda create -q -n testenv python= $ TRAVIS_PYTHON_VERSION from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops ` value_for_last_step ` will be displayed as-is . return H5Dict ( out ) the true labels , where the smoothing squeezes the labels towards 0.5 . PYTHONPATH= $ PWD : $ PYTHONPATH py.test -- pep8 -m pep8 -n0 & & py.test tests/docs ; return K.batch_flatten ( inputs ) steps_per_epoch : Integer . # Tile the thresholds for every prediction . result = concatenate ( result , axis=-1 ) else : def wrapped_fn ( * args , * * kwargs ) : from . import merge PR = 'PR ' # if inbound_layer is not Model or wrapped Model handle_module ( mod ) `` `` '' Returns the gradients of ` loss ` w.r.t . ` variables ` . attributes that allow us to build a Keras model 'Use ` get_output_shape_at ( node_index ) ` ' verbose=verbose ) `` `` '' Apply additive zero-centered Gaussian noise . weights = convert_weights ( weights , from_cudnn=False ) input_spec = self.input_spec if param.kind == inspect.Parameter.VAR_KEYWORD : embeddings_layer_names=None , ( 'pool3d ' , ( 3 , 5 , 6 , 7 , 3 ) , ( 3 , 3 , 3 ) , ( 1 , 1 , 1 ) , constants=None ) : initial_state = K.sum ( initial_state , axis=1 ) p [ : self.num_thresholds - 1 ] / p [ 1 : ] , kwargs [ 'loss ' ] = loss input_shape ` urllib ` module , known to have issues with proxy management . y_classes = y if 'output_shape ' in kwargs : assert K.int_shape ( o ) == ( None , 4 , 5 ) val = [ x.encode ( 'utf-8 ' ) for x in val ] depth_keys = list ( self._nodes_by_depth.keys ( ) ) new_model = load_model ( gcs_filepath ) x._cntk_placeholder = True from six.moves import range def function ( inputs , outputs , updates= [ ] , * * kwargs ) : To load a network from a yaml save file , use if self.rank == 2 : self.nb_feature = nb_feature ` batch_shape= ( ... ) ` to all the first layers in your model . TypeError : If ` layer ` is not a layer instance . `` `` '' Global Average pooling operation for 3D data . def test_avgpooling3d_legacy_interface ( ) : switch_condition = K.all ( are_different , axis=0 ) more than one incoming layers . if _SYMBOLIC_SCOPE.value : self.patched_file_io = patched_file_io return None , C.cntk_py.Value ( arguments.data ( ) ) 'shared_axes ' : self.shared_axes return float ( 2.0 ) assert tuple ( h.shape ) == tuple ( d.shape ) beta = k.variable ( beta_np ) super ( Poisson , self ) .__init__ ( poisson , name=name , reduction=reduction ) if input_shape [ channel_axis ] is None : elif isinstance ( metrics , collections.Mapping ) : input_shapes.append ( K.int_shape ( x_elem ) ) in zip ( states_masks , states_t , states_tm1 ) ] model_outputs.append ( [ layer.name , new_node_index , tensor_index ] ) x = x * C.greater ( x , threshold ) md5 hash of the file for verification str ( generator_output ) ) # layer losses . layer 's specifications . constraint=self.depthwise_constraint ) class AveragePooling3D ( _Pooling3D ) : return update_ops _ , top_k_idx = tf.nn.top_k ( x , k , sorted=False ) arg_spec = inspect.getfullargspec ( fn ) output_shape [ c_axis ] = self.filters ValueError : if ` padding ` is invalid . `` `` '' Checks the number of samples provided for training and evaluation . rows , cols , self.filters ) specifying the stride length of the convolution . 'all classes in the data . ' return tf.argmin ( x , axis ) labels=sparse_labels , def set_params ( self , * * params ) : raise ValueError ( 'Specify either a shape or ndim value . ' ) popd outputs = self.test_function ( ins ) logs = logs or { } input_layer = input_tensor._keras_history [ 0 ] to add to the layer . 'sample_weight_mode dictionary : `` ' + str ( unknown_output ) return result [ 0 ] if len ( self.thresholds ) == 1 else result for tensor in additional_inputs : x = np.expand_dims ( x , 1 ) classes : Optional list of class subdirectories str ( sample_weight.shape ) K.moving_average_update ( self.moving_variance , directory=directory , input_shape = ( input_length , input_dim ) 'pass shuffle= '' batch '' . ' ) auto_padding= [ False , padding , padding ] , last_output = outputs [ -1 ] during training and testing . Typically you will use from tensorflow.keras.applications.densenet import DenseNet121 Finally , if ` activation ` is not ` None ` , kshape = list ( x._keras_shape ) def get_output_mask_at ( self , node_index ) : label classes ( 0 and 1 ) . # Custom callable class . result = cast ( result , dtype ) self.file_io_module = None from . import metrics_utils data = [ np.asarray ( data ) ] str ( self.output_names ) ) training are used . ValueError : when ` steps ` is ` None ` and the attribute ` ins.shape ` model.add ( Dense ( 2 , input_shape= ( 3 , ) ) ) nodes_in_progress.add ( node ) old_layer = keras.layers.GaussianNoise ( sigma=0.5 , name='gn ' ) from collections import defaultdict g = group.create_group ( layer.name ) for a in axis_list [ : :-1 ] : continue np_var = np.array ( [ 1 , 2 ] ) Predictions . return K.batch_get_value ( self.weights ) sudo python setup.py install output = K.maximum ( output , inputs [ i ] ) You can pass `` external '' constants to the cell using the ` constants ` return T.zeros_like ( targets , dtype='bool ' ) self.train_function = None losses.categorical_crossentropy def test_statefulness ( ) : batch_size=batch_size , < tf.Tensor 'Placeholder_2:0 ' shape= ( 2 , 3 ) dtype=float32 > rng = np.random # as the first layer in a model self.units * 2 ] elif dtype == np.float16 : # As of Keras 2.0.0 , all kernels are normalized Keras CuDNN model = keras.Model ( inputs , outputs ) also of the same shape . raise ValueError ( ' ` sensitivity ` must be in the range [ 0 , 1 ] . ' ) reason='Beam search is only implemented with ' def build ( self , input_shape ) : batch_size=None , # Set ` x ` and ` y ` values for the curves based on ` curve ` config . args = ( args [ 0 ] , ( ( top_pad , bottom_pad ) , ( left_pad , right_pad ) ) ) self.queue.put ( def raise_duplicate_arg_error ( old_arg , new_arg ) : ( 'depthwise_conv2d ' , ( 2 , 3 , 4 , 5 ) , ( 3 , 3 , 3 , 2 ) , 'same ' , 'channels_first ' ) , urlretrieve ( origin , fpath , dl_progress ) return wrapped_fn return K.max ( inputs , axis= [ 1 , 2 , 3 ] ) raise ValueError ( 'In a stateful network , ' executor.apply_async ( next_sample , ( self.uid , ) ) , block=True ) model.add_metric ( metrics.Mean ( name='metric_2 ' ) ( y ) ) y_train = np.reshape ( y_train , ( len ( y_train ) , 1 ) ) mode = ' w ' def test_train_on_batch_with_class_weight ( self ) : or a single instance if the model has only one input . if opens_file and os.path.isfile ( filepath ) and not overwrite : feed_input_names , input_length : Static number of timesteps in the input . model = model_from_config ( model_config , custom_objects=custom_objects ) if data_format == 'channels_first ' and tf_data_format == 'NDHWC ' : a0 += 1 return C.cntk_py.Value ( # collecting output ( s ) , mask ( s ) , and shape ( s ) . bash miniconda.sh -b -p $ HOME/miniconda elements . In ` -2.5 < = x < = 2.5 ` , returns ` 0.2 * x + 0.5 ` . 'passed loss= { } '.format ( len ( output_names ) , loss ) ) class TimeDistributed ( Wrapper ) : dataframe , the axes to find maximum values . If ` None ` ( default ) , finds the # command to run tests How many units should be trimmed off at the beginning and end of 'Use ` model.compile ( optimizer , loss ) ` . ' ) 'return_sequences ' : return_sequences } , num_param = len ( own_weight_vars ) input_array = np.random.randint ( 1000 , size= ( 32 , 10 ) ) assert o [ 1 ] ._keras_shape == ( None , 3 , 2 , 1 ) false_positive : y_true == False and y_pred > thresholds class RepeatVector ( Layer ) : check_single_tensor_operation ( 'repeat_elements ' , arr , WITH_NP , ValueError : If the underlying model is n't configured to if reduction == Reduction.NONE : def test_normal ( tensor_shape ) : dictionary mapping output names to Numpy arrays . `` `` '' Computes the weighted loss . if self.center : ' layers into a model with ' kwargs= { 'output_dim ' : 3 } , d ) # Input shapes : for x , y , mask in zip ( reference_output_tensors , return metrics_module.categorical_accuracy ( 'conv3d ' , ( 1 , 3 , 5 , 4 , 2 ) , ( 3 , 3 , 3 , 2 , 3 ) , 'same ' , 'channels_last ' ) , 'batch_size ' : batch_size , `` `` '' Global average pooling operation for temporal data . beta_np = np.random.random ( other_shape ) to False , the compilation is omitted without any batch_index += 1 # initialize state if None kwargs = node.arguments by name . Changes to global custom objects persist kernel_initializer=kernel_initializer , x_c = K.bias_add ( x_c , self.bias_c ) x = K.random_uniform ( shape , self.minval , self.maxval , If you never set it , then it will be `` channels_last '' . # For new processes that may spawn % ( existing_classes - existing_class_weight ) ) `` `` '' Returns the sample weight and weight mode for a single output . '' '' '' input_tensors = to_list ( input_tensors ) states = new_states if i < len ( kwargs ) - 1 : output_shape : desired dimensions of output . if hook == 'begin ' : i += 1 output_shape [ self.axis ] += shape [ self.axis ] return [ { } for _ in output_names ] inputs , initial_state , constants = recurrent._standardize_args ( # Gets network outputs . Does not update weights . ` logcosh = log ( ( exp ( x ) + exp ( -x ) ) /2 ) ` , self.sample_weight_mode = sample_weight_mode constants = states [ -self._num_constants : ] values , _ , sample_weight = losses_utils.squeeze_or_expand_dimensions ( new_height = None if data_format='channels_first ' epsilon ) output = repeat_elements ( x , height_factor , axis=1 ) ( less the dimension that was summed over ) and ` y ` 's shape self.history = { } compared with prediction values to determine the truth value of cce = keras.losses.SparseCategoricalCrossentropy ( ) return r'\u ' 'accepts dict types . You can now input argument as : ' [ self.total_loss ] + metrics_tensors , def generate_arrays_from_file ( path ) : `` `` '' Average pooling operation for spatial data . kernel_shape , strides , data_format ) initial_state = to_list ( initial_state , allow_tuple=True ) losses_utils.squeeze_or_expand_dimensions ( # self.non_trainable_weights [ Theano installation instructions ] ( http : //deeplearning.net/software/theano/install.html # install ) . f.write ( json.dumps ( _config , indent=4 ) ) headers : Dictionary ; optional custom HTTP headers . layer_group [ name ] = val form of symbolic tensors , generators , or ` Sequence ` instances reason='cntk return -85.1 for zero or ' # # 2 . Passing in a layer raises a warning val_inputs : List of tensors to be fed to ` val_function ` scale , if not self.outputs : validation_freq=2 ` runs validation every 2 epochs . If a list , [ Efficient Backprop ] ( http : //yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf ) assert_list_pairwise ( z_list ) assert len ( padding ) == 3 self.data.attrs [ ' % s % d ' % ( attr , chunk_id ) ] = chunk_data axes = list ( axes ) import shutil def test_fit_generator_dynamic_size_sequence_with_workers ( ) : x = layers.TimeDistributed ( s ) ( x ) def dot ( inputs , axes , normalize=False , * * kwargs ) : assert_allclose ( f [ ' z ' ] , array ) install : config = { 'target_shape ' : self.target_shape } yield item return val x = C.splice ( * tmp , axis=i - num_dynamic_axis ) needs_broadcasting = ( sorted ( reduction_axes ) ! = list ( range ( ndim ) ) [ : -1 ] ) return Function ( inputs , outputs , updates=updates , * * kwargs ) object_name = args [ 0 ] .__class__.__name__ terminal window sizes ) . from tensorflow.keras.layers import * axes : int or tuple ( int , int ) . Target dimensions to be reduced . should have one _named_ argument ` filepath ` indicating the location to if not isinstance ( input_shape , list ) : images in a string column . It should include other column/s elems = None return tf_keras_backend.get_session ( ) self.gain = gain # ( assuming one LSTM has states [ h , c ] ) `` `` '' Computes the ( weighted ) sum of the given values . if node in finished_nodes : def assert_exists ( self , filepath ) : self.embeddings_data = standardize_input_data ( self.embeddings_data , x = C.convolution ( pointwise_kernel , x , return [ clone_metric ( metric ) for metric in metrics ] ( 2 , 4 , 5 ) return { 'min_value ' : self.min_value , connections = [ ] class Precision ( Metric ) : `` `` '' MNIST handwritten digits dataset . in ` 'channels_last ' ` mode is it at index 3 . # Cache newly created input layer . or ( inputs , targets , sample_weights ) ValueError : if the dimension and the size of indices mismatches . q = q.reshape ( shape ) input_shape= ( 4 , ) ) ) return T.log ( x ) show_layer_names : whether to display layer names . func : Function to decorate . def __init__ ( self , cells , * * kwargs ) : input_shape= ( 3 , 2 ) ) if isinstance ( element , ( list , tuple ) ) : return func ( * args , * * kwargs ) from .. models import Sequential sample_weight=None , val_x , val_y , val_sample_weight = validation_data x = x + reshape ( bias , new_shape ) if K.image_data_format ( ) == 'channels_first ' : # TF 2 arg conversion 'recurrent_regularizer ' : ' weight ( s ) , but the saved weights ' dropout : Float between 0 and 1 . from .load_backend import conv1d y : array-like , shape ` ( n_samples , ) ` or ` ( n_samples , n_outputs ) ` and recurrent transformations are both convolutional . ` ( batch , upsampled_dim1 , upsampled_dim2 , upsampled_dim3 , channels ) ` super ( Nadam , self ) .set_weights ( weights ) time_index = range ( input_length ) self._per_output_metrics = training_utils.collect_per_output_metric_info ( if self.go_backwards : assert_array_equal ( d_rec , d ) return K.max ( inputs , axis= [ 2 , 3 , 4 ] ) out = C.element_times ( out , y ) kernel_d , kernel_h , kernel_w = self.kernel_size assert_allclose ( x - decrement , K.eval ( x_var ) , atol=1e-05 ) if batch_size == 0 : 'array or a list of arrays . ' self.file_io_module = file_io_module return C.user_function ( # Pick the normalized form corresponding to the training phase . result of a softmax , or is a tensor of logits . `` `` '' Gated Recurrent Unit - Cho et al . 2014 . from .load_backend import spatial_3d_padding data_format : string . ` 'channels_first ' ` or ` 'channels_last ' ` . the data still needs to reside in a subdirectory output = T.zeros ( output_shape ) ` ( batch_size , downsampled_steps , features ) ` uses_learning_phase = True raise ValueError ( ' ` validation_freq ` can not be less than 1 . ' ) # the graph reconstruction process config [ 'function ' ] , def update_state ( self , y_true , y_pred , sample_weight=None ) : ( default : False ) . ins = x + [ 0 ] x_shape [ : -1 ] + y_shape [ : -2 ] + y_shape [ -1 : ] ) # each getting a slice of the inputs . broadcast_var = T.reshape ( var , target_shape ) width = input_shape [ w_axis ] int_shape [ i ] = tensor_shape [ start_idx + i ] def test_set_learning_phase ( self ) : validation dataset divided by the batch size . for cell in cells : `` `` '' Clip the gradient ` g ` if the L2 norm ` n ` exceeds ` c ` . from tensorflow.keras.wrappers.scikit_learn import * slices.append ( tmp ) fused_batch_norm = tf.nn.fused_batch_norm after = np.random.randint ( 0 , 100 , size=10 ) delta_t_median > 0.1 ) : print operation is not taken into account during evaluation . x_col=x_col , 'apply a reshape operation . ' if isinstance ( self._output_shape , python_types.LambdaType ) : if not isinstance ( callbacks , cbks.CallbackList ) : return saving.pickle_model ( self ) ' { } { } { } '.format ( callbacks._call_batch_hook ( 'test ' , 'end ' , step , batch_logs ) from .core import Reshape def test_fit_generator_dynamic_size_sequence_main_thread ( ) : chunk_size : Bytes to read at a time , important for large files . axis : An integer , the axis to reduce over . recurrent_dropout=recurrent_dropout ) self._total_width = 0 def _preprocess_conv3d_kernel ( kernel , dim_ordering ) : So the channel axis needs to be flipped when TF weights are loaded on a TH model , self.bucket_name = 'mock-bucket ' bias_shape = ( 2 , 3 * self.units ) self.bias_h_i = self.bias [ self.units * 2 : self.units * 3 ] model.weights # returns list of length 4 raise RuntimeError ( org_weights = model.get_weights ( ) pool_mode='max ' ) if reporthook is not None : return [ K.in_train_phase ( return tf_keras_backend.image_data_format ( ) The ` LocallyConnected1D ` layer works similarly to dims = len ( x.shape ) Bool , True if validation should be run . if layers : computed_masks = [ computed_mask ] # Theano likes to make shape==1 dimensions return _remove_dims ( output , axis , keepdims ) input_dim = input_shape [ -1 ] outputs : List of outputs ( predictions ) . It allows a small gradient when the unit is not active : self.local_objects.pop ( filename ) return updated_y_true from .merge import Average 'is not fully defined ' if callable ( x ) : list ( reduction_axes ) in [ [ 0 , 1 , 2 ] , [ 0 , 2 , 3 ] ] and target_mean=0. , target_std=std ) or list/tuple of Keras tensors to reference return K.concatenate ( [ self.forward_layer.reset_states ( ) val_enqueuer_gen , if has_arg ( self.layer.call , 'initial_state ' ) : super ( SquaredHinge , self ) .__init__ ( # and therefore requires a wrapper for Keras . The functions below convert mean = x.mean ( reduction_axes ) A ` DirectoryIterator ` yielding tuples of ` ( x , y ) ` self.updates = [ K.update_add ( self.iterations , 1 ) ] state_specs = [ InputSpec ( shape=K.int_shape ( state ) ) _test_optimizer ( optimizers.Adagrad ( lr=1. , decay=1e-3 ) ) other_shape = tuple ( other_shape ) # Check ndim . > > > K.is_keras_tensor ( keras_placeholder ) callback_metrics = list ( model.metrics_names ) ) exp = k.square ( x ) + y `` `` '' Counts the total number of scalars composing the weights . name=self.name + '_input ' ) ' ( for histogram computation ) ' slice_col = py_slice ( j * stride_col , target_tensors = tmp_target_tensors if metrics is None : _LEARNING_PHASE_CACHE = { } if len ( cropping ) ! = 2 : grad_array_view.as_shape ( The log file can become quite large when def dummy_fn ( ) : gain : Multiplicative factor to apply to the orthogonal matrix . Allows for easy and fast prototyping ( through user friendliness , modularity , and extensibility ) . out1 = model.predict ( np.ones ( ( num_samples , timesteps ) ) ) list ( ConfusionMatrix ) , variables_to_update.keys ( ) ) ) unroll=False , > > > K.set_epsilon ( 1e-05 ) x.set_value ( np.asarray ( value , dtype=x.dtype ) ) from .convolutional import Convolution3D layer.call ( computed_tensors , * * kwargs ) ) input_masks , output_masks , reset_metrics=False ) from keras.utils.generic_utils import func_dump 'should be tensors . ' ) output._keras_shape = list ( x._keras_shape ) `` `` '' Set model 's input and output specs based on the input data received . assert shape is not None , ( 'Please provide to Input either a ` shape ` ' If ` False ` , returns the ` top_paths ` most probable ( 'conv1d ' , ( 1 , 8 , 2 ) , ( 3 , 2 , 3 ) , 'valid ' , 'channels_last ' ) , assert_list_pairwise ( v_list , y = T.transpose ( x ) workers=1 , _axis = [ axis ] self.dilation_rate [ 1 ] ) `` `` '' Functional interface to the ` Average ` layer . # CTC output = self.activation ( output ) def test_rnn ( self ) : mean_absolute_percentage_error , name=name , reduction=reduction ) def test_rnn_unroll_with_len_1 ( self ) : # If not all nodes processed then store unprocessed nodes has_arg ( self.call , 'mask ' ) or self.update_freq = 1 def __init__ ( self , filepath , monitor='val_loss ' , verbose=0 , from skimage.io import imread class _CuDNNRNN ( RNN ) : else , 2D tensor with shape ` ( batch_size , units ) ` . arr = np.arange ( np.prod ( shape ) ) .reshape ( shape ) values_shape = K.int_shape ( values ) error_msg = 'URL fetch failure on { } : { } -- { } ' _LEARNING_PHASE_PLACEHOLDER.value = np.asarray ( 1.0 ) 'will be removed , use ` min_delta ` instead . ' ) xs = [ [ w if skip_top < = w < num_words else oov_char for w in x ] for x in xs ] pivot = len ( initial_state ) // 2 original_backend = model_weights_group [ 'backend ' ] .decode ( 'utf8 ' ) If a tuple , it only specifies the first dimension onward ; import multiprocessing as mp from tensorflow.keras.datasets.cifar100 import load_data sample_weight_mode=None ) : X_train [ 1000:1001 ] def test_model_loading_from_binary_stream ( ) : shuffle : Whether to shuffle the data at the beginning of each epoch kernel_initializer : Initializer for the ` kernel ` weights matrix , output_shape = ( input_shape [ 0 ] , output_dim ) from .load_backend import in_train_phase # for depthwise convolution . if isinstance ( _ , int ) : val_sample_weight = None `` `` '' 2D convolution . `` `` '' Returns the size of a tensor . layer = keras.layers.CuDNNLSTM ( units ) at the top , bottom , left and right side of an image tensor . regularization_loss = self.layer.activity_regularizer ( y ) size_all_dims = ( 1 , ) + self.size + ( 1 , ) 'the ` model.compile ( ) ` method . ' ) raise ValueError ( return base_fun ( * args , * * kwargs ) if keepdims is False and isinstance ( axis , list ) : padding=padding , assert model.compute_output_shape ( [ ( None , 32 ) , ( None , 32 ) ] ) == expected_shapes `` `` '' Part of the training engine related to Python generators of array data . b_regularizer='l1 ' , shape as predictions ) for recall and precision top-k metrics . raise ImportError ( 'Google Cloud Storage file transfer requires TensorFlow . ' ) self.cell = cell then the model checkpoints will be saved with the epoch number and if dtype ! = 'int32 ' : 'metric function identifier : ' , identifier ) 'units ' : self.units , for i in range ( len ( self._input_layers ) ) : out = model.train_on_batch ( [ input_a_np , input_b_np [ :2 ] ] , symbolic_shape = K.shape ( inputs ) ' % d bytes : % s ' % ( HDF5_OBJECT_HEADER_LIMIT , # List of shape tuples , shapes of input_tensors . # Read final output shapes from layers_to_output_shapes . _p_prev = zeros [ : active_next ] if dev.type ( ) == 0 and dilation_rate ! = ( 1 , 1 ) : if not isinstance ( x , np.ndarray ) and not K.is_tensor ( x ) : second_log = K.log ( K.clip ( y_true , K.epsilon ( ) , None ) + 1 . ) assert len ( val_seq.logs ) < = 4 * 5 batch_val = [ x [ i : i + step ] for x in val_data [ : -1 ] ] output_generator = enqueuer.get ( ) def __init__ ( self , log_dir='./logs ' , return metrics_names the output of the layer ( its 'activation ' ) . 'It looks like you are subclassing ` Model ` and you ' val_function=val_function , workers=4 * i ) config = { 'rate ' : self.rate } assert g._keras_shape == c._keras_shape if ( self.unrelated_updates is None and if len ( initial_state ) == 0 : x = self.call ( xs ) # the following equations given in [ 1 ] self._values [ k ] = [ v , 1 ] normed = theano.tensor.as_tensor_variable ( normed ) return training_arrays.fit_loop ( self , fit_function , fit_inputs , workers : number of worker threads node_index = self._input_coordinates [ i ] [ 1 ] if i == 0 : # Index 0 == ` Loss ` return self.learning_rate A Keras tensor is a tensor object from the underlying backend epochs=epochs , self._feed_inputs.append ( layer.input ) return _static_rnn ( new_space.append ( new_dim ) if len ( inputs ) > 1 : # dense.output_mask padding=self.cell.padding ) 'must be provided to ` rnn ` . ' ) class_sample_weight = np.asarray ( noise_shape = ( input_shape [ 0 ] , 1 , 1 , 1 , input_shape [ 4 ] ) `` `` '' Returns the shape of a Keras tensor or a Keras variable as a tuple of self._output_loss_metrics = None old_layer = keras.layers.Convolution3D ( 5 , `` `` '' CIFAR10 small images classification dataset . then the recall value is 2/ ( 2+1 ) ie . 0.66 . If the weights were specified as # ( e.g . BN updates ) . cls = element A Keras variable , filled with ` 1.0 ` . self.inbound_layers = inbound_layers import codecs stride=cell.strides [ 0 ] , None , None , False ) class ImageDataGenerator ( image.ImageDataGenerator ) : x = tf.cast ( x , 'float32 ' ) ndim = inputs.ndim if [ [ `` $ TRAVIS_PYTHON_VERSION '' == `` 2.7 '' ] ] ; then transposed = True out._uses_learning_phase = True def test_cumprod ( self ) : try : shape , algorithm : hash algorithm , one of 'auto ' , 'sha256 ' , or 'md5 ' . l2 = np.atleast_1d ( np.linalg.norm ( x , order , axis ) ) # We do this so that we can maintain the correct order of metrics by adding x = C.reshape ( x , ( -1 , ) ) def get_constants ( self , inputs , training=None ) : mean , var = tf.nn.moments ( x , reduction_axes , from .pooling import MaxPool3D # On-the-fly setting of symbolic model inputs diff = np.mean ( ref_params_value - params_value ) global _LOCAL_DEVICES # Here we download miniconda and install the dependencies if x_batch_size is not None and y_batch_size is not None : A dict mapping input names to the corresponding self.monitor_op = None config : A Python dictionary , typically the from .core import Permute custom_objects : Optional dictionary mapping names if isinstance ( output_tensor , list ) : inputs : List of input tensors . progbar.update ( steps_done ) initializer=self.depthwise_initializer , as follows : ( decoded , log_prob ) = ctc.ctc_greedy_decoder ( ` sample_weight ` is not ` None ` and its shape does n't match ` y_pred ` , or if __One__ : calling ` reset_states ` X_train [ [ 1000 , 1001 ] ] conda update conda class Embedding ( Layer ) : weight_type ) : if uses_learning_phase : self.set_weights ( self.initial_weights ) if has_arg ( layer.call , 'mask ' ) : return lambda seqs : mp.Pool ( workers , with ` stddev = sqrt ( 2 / ( fan_in + fan_out ) ) ` Note that both ` then_expression ` and ` else_expression ` if isinstance ( curve , metrics_utils.AUCCurve ) : hidden_dim = 10 b_constraint='maxnorm ' , name='d ' ) with specified mean and standard deviation , 'tensors to have a static batch size . ' model.fit ( X_train , Y_train , callbacks= [ csv_logger ] ) normalized_padding = ( dim1_padding , dim2_padding , dim3_padding ) if hasattr ( self , '_losses ' ) : # because in that case even chunking the array would not make the saving adam = Adam if isinstance ( value , np.ndarray ) : array ( [ [ 1.19591331 , 0.68685907 , -0.63814116 ] , A tensor with the standard deviation of elements of ` x ` . fd.write ( chunk ) the range ` [ 0 , 1 ] ` . network_nodes.add ( node_key ) 'b_regularizer ' : regularizers.l1 ( 0.01 ) , new_layer = keras.layers.GlobalMaxPool3D ( data_format='channels_last ' , tf_keras_backend.set_session ( session ) check_single_tensor_operation ( 'argmax ' , ( 4 , 2 ) , WITH_NP ) The output will then have shape ` ( 32 , 10 , 32 ) ` . # Infer the dynamic output shape : # Generate dummy data . batch_size = x [ 0 ] .shape [ 0 ] closure : closure of the function . `` `` '' Converts a layer and its index to a unique ( immutable type ) name . def _updated_config ( self ) : x_placeholder = K.placeholder ( shape= ( ) , dtype= '' string '' ) output_names , a single -1 which indicates a dimension that should be x += reshape ( bias , ( 1 , bias_shape [ 0 ] , 1 , 1 , 1 ) ) of the original model , on top of new inputs tensors , interpreted as two different The validation data is selected from the last samples if stateful_metrics : wget http : //repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh -O miniconda.sh return ( ( hasattr ( layer , '_is_graph_network ' ) and layer._is_graph_network ) or learning phase ( None ) . ` epochs ` is to be understood as `` final epoch '' . K.variable ( depthwise ) , K.variable ( pointwise ) , assert np.array_equal ( a , np.arange ( start , stop , step ) ) raise ValueError ( 'The model can not be compiled ' test_doc1 = { be called on every slice of data retrieved . def gather ( reference , indices ) : const_a = C.unpack_batch ( x ) shape=shape , ndim=ndim , dtype=dtype , sparse=sparse , name=name ) saved ( ` model.save_weights ( filepath ) ` ) , else the full model tf_keras_backend.set_value ( x , value ) 'beta_2 ' : float ( K.get_value ( self.beta_2 ) ) , x = Input ( def ndim ( x ) : self.name + ' : expected ndim= ' self._feed_input_names.append ( name ) raise ValueError ( 'Layer ' + self.name + ' expects ' ' ( named `` ' + layer.name name , # This is always a single layer , never a list . return func w = None 'gain ' : self.gain , from .callbacks import ModelCheckpoint updates=updates , pad = ( w_pad , h_pad ) self._layers = [ ] regularizers.serialize ( self.activity_regularizer ) , if not self.trainable and not self.stateful : @ interfaces.legacy_batchnorm_support check_single_tensor_operation ( 'all ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) class RMSprop ( Optimizer ) : add_edge ( dot , # Fix for Theano # Save model via the template model ( which shares the same weights ) : return None , None in the range [ 0 , 1 ] and not peaked around 0 or 1 . The quality of the AUC # else : assume learning phase is a placeholder tensor . beta_1 : float , 0 < beta < 1 . Generally close to 1 . height = 150 sub_w_first_node [ layer.layer.name ] = sub_w_nodes [ 0 ] if self._expects_training_arg : masks = [ None for _ in self.outputs ] from tensorflow.keras.models import save_model inputs = tf.transpose ( inputs , ( 1 , 0 , 2 ) ) lr = lr * ( 1 . / ( 1 . + self.decay * K.cast ( self.iterations , def elu ( x , alpha=1 . ) : __return_state__ : Boolean . Whether to return the last state del dtype The sha256 and md5 hash algorithms are both supported . for conf in layer_configs : label_lens = np.expand_dims ( np.asarray ( [ 5 , 4 ] ) , 1 ) computes the standard deviation over all dimensions . def test_loss_wrapper ( self ) : super ( _Pooling1D , self ) .__init__ ( * * kwargs ) for i , name in enumerate ( output_names ) : ( left_dim3_crop , right_dim3_crop ) ) ` the ` Conv1D ` layer , except that weights are unshared , from .load_backend import one_hot if self._uses_dynamic_learning_phase ( ) : file_hash = md5_hash subset=None , initial_states = [ K.constant ( initial_state_vals ) ] fetches= [ K.update ( y , y_placeholder * 10 . ) ] ) an under-represented class . `` `` '' Clone any ` Model ` instance . output ) if id ( l ) not in ref_ids ] ) second_function_name , set ` axis ` to ` [ 0 , 1 , 2 ] ` import hashlib raise ValueError ( 'Provided ` ' + weight_type + ' ` was a list of ' for n , s in zip ( new_states , past_values ) : return weights_list states = to_list ( states , allow_tuple=True ) x = K.variable ( np.random.random ( input_shape ) ) return filter_shape h1 = np.concatenate ( [ h0 , h0 ] , axis=-1 ) batch_logs [ l ] = float ( o ) > > > # A variable indirectly created outside of keras is not a Keras tensor . def multiply ( inputs , * * kwargs ) : return inputs , new_states bias_regularizer='l1 ' , # not already marked as initialized . Let ` x ` 's shape be ` ( 100 , 20 ) ` and ` y ` 's shape be ` ( 100 , 30 , 20 ) ` . Then , ` cd ` to the Keras folder and run the install command : @ interfaces.legacy_embedding_support ` sample_weight_mode ` on each output by passing a def compute_output_shape ( self , input_shape ) : # By default , do not convert the kernels if the original backend is unknown self.mock_gcs = False > > > print ( K.is_sparse ( c ) ) return densenet.DenseNet201 ( * args , * * kwargs ) metrics_names.extend ( [ > > > from keras import backend as K dimension in reverse order and return the reversed sequence . cells = [ ] self.bias_f_i = self.bias [ self.units : self.units * 2 ] curve not in list ( metrics_utils.AUCCurve ) ) : logs : dict , Currently no data is passed to this argument for this method delta=1.0 , model : Keras model instance to be serialized . `` `` '' Selects ` x ` in test phase , and ` alt ` otherwise . broadcast_beta = C.reshape ( beta , target_shape ) self._losses = [ ] depthwise_initializer='glorot_uniform ' , kernel_size = ( args [ 2 ] , kwargs.pop ( 'nb_col ' ) ) updated_per_output_weighted_metrics.append ( `` `` '' Batchwise dot product . x = keras.layers.RNN ( cells ) ( inputs ) self.bias_r_i , assert_allclose ( K.eval ( outputs ) , expected_outputs ) def test_variable_support_bool_dtype ( self ) : def __init__ ( self , k=5 , name='top_k_categorical_accuracy ' , dtype=None ) : @ pytest.mark.skipif ( ( K.backend ( ) ! = 'tensorflow ' ) , from tensorflow.keras.layers import MaxPooling2D to draw before stopping when performing validation at the end model.add ( Dense ( 32 ) ) metrics.extend ( layer._metrics ) This argument ( or alternatively , class CategoricalCrossentropy ( LossFunctionWrapper ) : inputs_k = K.variable ( inputs_np ) to constrain the weights of each filter tensor of size dynamic_dimension = C.FreeDimension losses = [ ] import copy v_list = [ getattr ( k , function_name ) ( k.variable ( val ) ) base_config = super ( ActivityRegularization , self ) .get_config ( ) `` `` '' Wait for the queue to be empty . '' '' '' self.strides [ 0 ] ) inputs = np.random.random ( ( num_samples , timesteps , input_size ) ) for key in config [ 'arguments ' ] : by a factor of ( height_factor , width_factor ) . Both factors should be outputs = to_list ( outputs ) sample weighting ( 2D weights ) , set this to ` `` temporal '' ` . class CuDNNGRU ( _CuDNNRNN ) : y = training_utils.standardize_input_data ( for j in range ( len ( node.inbound_layers ) ) : `` `` '' Check whether the current scope supports NCHW ops . custom_objects : Optional dictionary mapping names return K.tanh ( x ) from tensorflow.keras.layers import LSTM as CuDNNLSTM pad_sequences = sequence.pad_sequences self.recurrent_dropout = 0 raise ValueError ( 'Invalid border mode for LocallyConnected1D ' def __init__ ( self , name='logcosh ' , dtype=None ) : `` `` '' Resets the state of metrics . '' '' '' `` `` '' Returns the shape of a tensor . network_nodes = set ( ) # ids of all nodes relevant to the Network validation_steps=validation_steps , group.attrs [ ' % s % d ' % ( name , chunk_id ) ] = chunk_data steps=None , Numpy data for these targets at training time ) , you explicitly_on_cpu = _is_current_explicit_device ( 'cpu ' ) def bias_add ( x , bias , data_format=None ) : return self.cell.losses + layer_losses `` `` '' Applies Dropout to the input . HDF5_OBJECT_HEADER_LIMIT = 64512 other_shape = [ 1 ] * len ( x_shape ) if self._output_shape is None : sudo : false compute the loss between the predicted labels and a smoothed version of return np.max ( x , axis=axis , keepdims=keepdims ) check_two_tensor_operation ( 'minimum ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) if not isinstance ( metric_fn , metrics_module.Metric ) : from .load_backend import reverse return simple_rnn bias_regularizer='l2 ' , # Properties self._feed_loss_fns.append ( self.loss_functions [ i ] ) allowed_positional_args= [ 'rate ' , 'noise_shape ' , 'seed ' ] , ignore : np.asscalar : DeprecationWarning kernel = K.zeros ( tuple ( shape ) ) raise ValueError ( ' A ` Concatenate ` layer should be called ' if K.is_keras_tensor ( tensor ) ! = is_keras_tensor : def bias_initializer ( shape , * args , * * kwargs ) : negative_part = T.nnet.relu ( -x + threshold ) > > > input_ph = K.placeholder ( shape= ( 2 , 4 , 5 ) ) Example For ` fn ` : permutation += list ( range ( len ( y_shape ) - 2 ) ) # original shape : ( batch , input_dim , length ) defaults = tuple ( defaults ) legacy_batchnorm_support = generate_legacy_interface ( # in TensorFlow 's batch_normalization , if the parameters are vectors base_config = super ( GRU , self ) .get_config ( ) just the first step of a depthwise spatial convolution p_t = p - lr_t * m_t / ( K.sqrt ( v_t ) + self.epsilon ) if value == 'one ' : from tensorflow.keras.layers import InputLayer in which metadata for this embedding layer is saved . See the raise ValueError ( ' ` label_mode ` must be one of ` `` fine '' ` , ` `` coarse '' ` . ' ) layers_depths = { } # dict { layer : depth value } file_path : path to the archive file def test_fit_generator_with_sample_weight ( self ) : input_shape= ( None , dim ) ) ) ` [ batch_size ] ` , then the total loss for each sample of the batch is self.recurrent_bias_h = None 'specify it via the ` output_shape ` argument . ' for i in range ( 1 , len ( input_shape ) ) : self.padding , self.strides [ 1 ] ) raise TypeError ( 'Not JSON Serializable : ' , obj ) # workaround in which case it is the number of channels of the recurrent state 'on a list of inputs . ' ) config [ 'input_layers ' ] = model_inputs x_weight : User-provided ` sample_weight ` or ` class_weight ` argument . self.bias_f_i , def resize_images ( x , height_factor , width_factor , data_format , from tensorflow.keras.applications.resnet import preprocess_input from tensorflow.keras.layers import Wrapper if not has_arg ( theano.function , key , True ) : lower than the stride along that same dimension . group1 = f [ 'group1 ' ] def conv3d_transpose ( x , kernel , output_shape , strides= ( 1 , 1 , 1 ) , dtype=dtype , It can be passed to ` transform_kernels ( ) ` . legacy_cropping2d_support = generate_legacy_interface ( preprocessor=add_weight_args_preprocessing ) def test_load_weights_between_noncudnn_rnn ( rnn_type , to_cudnn , bidirectional , def l2_normalize ( x , axis=None ) : if len ( shape ) == 1 : # backup ndims . Need them later . variables = [ ] self.input_spec = InputSpec ( ndim=self.rank + 2 , weights [ 8 ] , `` `` '' Util hared between different serialization methods . # # Verify that the metrics added using ` compile ` and ` add_metric ` API are environment variable GCS_TEST_BUCKET , * NO mocking * will be done and files will be model.set_weights ( [ np.random.random ( w.shape ) for w in org_weights ] ) 3D tensor with shape ` ( batch , padded_axis , features ) ` from . import pooling 'Group with name `` { } '' exists . '.format ( attr ) ) skipif_no_tf_gpu = True def assert_not_compatible ( src , dest , message ) : x._keras_shape [ 3 ] ) # We can not call 'metrics ' on the model because we do not want to return th_padding if fn is None : if len ( loss_weights ) ! = len ( output_names ) : c = f * c_tm1 + i * self.activation ( x_c + h_c ) Once your model looks good , configure its learning process with ` .compile ( ) ` : send_as_json : Boolean ; whether the request should be send as def array_to_img ( x , data_format=None , scale=True , dtype=None ) : return_sequences=return_sequences , recurrent_dropout=0.1 , self.alpha = self.add_weight ( shape=param_shape , # test normalizer from six.moves import cPickle return metrics_module.binary_accuracy # Reuse previously cloned layer . filters=None , # test slicing for shortened array when calling ` fit ` /etc . color_mode=color_mode , ` ( rows , cols , input_depth ) ` . from .merge import Minimum # Reverse index of layer name to list of layers with name . if isinstance ( batch_outs , list ) : while ` `` channels_first '' ` corresponds to inputs Use this crossentropy metric when there are two or more label classes . check_single_tensor_operation ( 'min ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) from .convolutional import ZeroPadding3D The model is not trained for a number of iterations self._check_started ( ) subgraph=True ) ValueError : if invalid ` x ` or ` data_format ` is passed . output = T.nnet.softmax ( output ) from .local import LocallyConnected2D class MinimalRNNCell ( keras.layers.Layer ) : validation_freq : Integer or list . If an integer , specifies how many training dist : trusty output_shape = list ( shape1 [ : -len ( shape2 ) ] ) dataframe : Pandas dataframe containing the filepaths relative to from keras_preprocessing import image depth_keys.sort ( reverse=True ) layers res.update ( { 'build_fn ' : self.build_fn } ) inner_init='glorot_uniform ' , # merged : A B axis_1 = 2 dpi=96 , class AlphaDropout ( Layer ) : The exponential linear activation : ` x ` if ` x > 0 ` and wget https : //s3.amazonaws.com/img-datasets/cats_and_dogs_small.zip target_filepath : String , path to the file on filesystem or object on GCS to raise ValueError ( 'State ' + str ( index ) if not os.access ( datadir_base , os.W_OK ) : layer.build ( ( None , None , input_size ) ) shape_diff = expr_shape - cond_shape `` `` '' Returns the name of the current backend ( e.g . `` tensorflow '' ) . follow the specifications of this class and accept LookupError : If layer required is not found unroll=False , input_length=None ) : return unpack_singleton ( masks ) @ pytest.mark.skipif ( K.backend ( ) ! = 'theano ' , 'data_format ' : 'channels_first ' } , http : //www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf ) return v ( ( 3 , 2 , 3 ) , ( 1 , 0 , 0 ) , ( 1 , 2 , 3 ) ) , super ( RMSprop , self ) .set_weights ( weights ) elif attr in self.data : score_array /= K.mean ( mask ) + K.epsilon ( ) name='sparse_categorical_crossentropy ' , Note that ` `` same '' ` is slightly inconsistent across backends with x = x [ indices ] An integer count . all_matrix = C.element_select ( def on_test_batch_end ( self , batch , logs=None ) : super ( ZeroPadding1D , self ) .__init__ ( normalized_padding , class Recall ( Metric ) : `` `` '' Collects the output shape ( s ) of a list of Keras tensors . if target_std is not None : array 's static dimensions . 'activation function identifier : ' , identifier ) if step == 0 : dilation_rate ) : dilation=dilation_rate ) 'Conv2DTranspose ' , kernel = kernel.dimshuffle ( ( 3 , 2 , 0 , 1 ) ) self._per_input_updates [ inputs_hash ] += updates cells = [ MinimalRNNCell ( 32 ) , MinimalRNNCell ( 64 ) ] If ` y ` is None , only the numpy array ` x ` is returned . base_config = super ( _UpSampling , self ) .get_config ( ) rescale=rescale , x = C.transpose ( x , ( 1 , 2 , 0 ) ) result = tf.range ( start , limit=stop , delta=step , name='arange ' ) mode = None unroll=unroll , ` rows ` and ` cols ` values might have changed due to padding . # 0 = test , 1 = train reset_states timeout : maximum time to wait on ` thread.join ( ) ` self.writer.add_summary ( summary_str , epoch ) ` ( samples , sequence_length ) ` , self.filename = filename a ` state_size ` attribute . This can be a single integer from .. legacy.layers import ConvRecurrent2D inputs_ls = to_list ( inputs ) [ getattr ( x , '_uses_learning_phase ' , False ) 3 For more information about the future of Keras , see [ the Keras meeting notes ] ( http : //bit.ly/keras-meeting-notes ) . if isinstance ( x , C.variables.Parameter ) : * * kwargs : Standard layer keyword arguments . # no collapse , then first need to padding the shape # Standardize the outputs . Evaluate your performance in one line : * ` y.shape [ 1 ] ` : 30 : append to output shape use an [ Embedding ] ( embeddings.md ) layer with the ` mask_zero ` parameter if set_x and set_y and list ( set_x ) [ 0 ] ! = list ( set_y ) [ 0 ] : mask , self._metrics = [ ] 'same ' , 'channels_first ' , ( 2 , 2 , 2 ) ) , cell = ConvLSTM2DCell ( filters=filters , ndim=len ( shape ) , raise Exception ( 'Unknown test case for test_func_dump_and_load ' ) if self._metrics : initial_state=initial_state ) if use_cudnn : # Otherwise they may end up hosted on a GPU , which would if data is not None and hasattr ( data , '__len__ ' ) and len ( data ) : # ` class_weight ` arguments . # all input/output/weight arrays should have the same number of samples . from .. import losses metrics.extend ( self.metrics ) if axis == 0 : if mean.dtype ! = tf.float32 : depthwise_kernel_shape = ( input_dim , self.depth_multiplier ) x_k = K.variable ( x ) from tensorflow.keras.layers import RNN if layer.__class__.__name__ in conv_layers : info = ' - % .0fs ' % ( now - self._start ) This argument is required if you are going to connect def is_variable ( x ) : `` `` '' Spatial 2D version of Dropout . self.outputs = outputs x : array-like , shape ` ( n_samples , n_features ) ` transform_weight = activations.sigmoid ( y ) dtype='int32 ' , def separable_conv ( x , w1 , w2 , padding , data_format ) : the linear transformation of the recurrent state . PYTHONPATH= $ PWD : $ PYTHONPATH pip install git+git : //www.github.com/keras-team/keras.git & & python update_api.py & & pip install -e . [ tests ] -- progress-bar off & & py.test tests/test_api.py ; file_hash='599dadb1135973df5b59232a0e9a887c ' ) return np.minimum ( x , y ) # linking their input to output . if dim_ordering == 'channels_last ' : if i in self.skip_target_indices : return [ opt ] , kwargs , [ ] if rank : while maintaining a connectivity pattern that is compatible with return self.cell.padding y = np.random.uniform ( 0 , 1 , size= ( 10 , 2 ) ) self , generator , self.true_negatives = self.add_weight ( initializer=self.bias_initializer , K.spatial_3d_padding ( K.variable ( xval ) , padding=padding , except ImportError : if max_value is None and threshold == 0. : if not overwrite and tf_file_io.file_exists ( target_filepath ) : # Check handling of dynamic shapes . self.bias_o_i , assert dilation_rate [ 0 ] == dilation_rate [ 1 ] y = getattr ( k , first_function_name ) ( x , * * first_function_args ) class_weight=None , recurrent_activation : Activation function to use p_op = Print ( message ) from .generic_utils import deserialize_keras_object `` `` '' Returns a tensor with random binomial distribution of values . return T.clip ( normal_t , mean - 2 * stddev , mean + 2 * stddev ) def standardize_input_data ( data , output_shape [ 3 ] , samples_seen_since = self.samples_seen - self.samples_seen_at_last_write assert_allclose ( last_output_k , last_output_np , atol=1e-05 ) return K.squeeze ( output , dummy_axis ) # remove dummy last dimension return C.ops.gather ( reference , indices ) # to check the pydot/graphviz installation . if not gpus : def Xception ( * args , * * kwargs ) : y [ np.isnan ( y ) ] = 0 . # using improper loss fns . # We can determine the source of the weights from the shape of the bias . def test_model_save_load_binary_in_memory ( ) : training_config = h5dict.get ( 'training_config ' ) resize ( imread ( file_name ) , ( 200 , 200 ) ) sample_weight = K.cast ( sample_weight , self.dtype ) for x in input_tensors ] ) from .convolutional import Convolution1D keyword argument of ` RNN.__call__ ` ( as well as ` RNN.call ` ) method . This self.append_header = not bool ( len ( f.readline ( ) ) ) HDF5 and [ h5py ] ( http : //docs.h5py.org/en/latest/build.html ) ( required if you plan on saving Keras models to disk ) . inputs : A list of input tensors ( exactly 2 ) . self._feed_targets = [ ] h_axis , w_axis = 2 , 3 self.inputs , self.outputs ) def sum ( x , axis=None , keepdims=False ) : if y.ndim > 1 : 'TB ' creates a vertical plot ; place_holders = [ C.placeholder ( seed : Optional random seed for shuffling and transformations . raise ValueError ( ' ` device_type ` should be either `` cpu '' or `` gpu '' . ' ) kernel_shape = _preprocess_conv2d_filter_shape ( kernel_shape , data_format ) config = super ( _SeparableConv , self ) .get_config ( ) def recurrent_args_preprocessor ( args , kwargs ) : from . import advanced_activations inner_init='glorot_uniform ' , padding , entries in the batch for which ` class_id ` is above the threshold and/or in the existing_classes = set ( y_classes ) outs = model.train_on_batch ( x , y , self.name + ' , but the layer isn\'t built . ' super ( _GlobalPooling3D , self ) .__init__ ( * * kwargs ) line_length : Total length of printed lines assert_allclose ( output_k , output_np , atol=1e-05 ) [ metrics_module.clone_metric ( m ) for m in metrics ] ) calling them with the keyword argument ` initial_state ` . The value of dtype = _convert_string_dtype ( dtype ) def __init__ ( self , padding , data_format=None , * * kwargs ) : are prefixed with ` val_ ` . * * Install Keras from PyPI ( recommended ) : * * low=0. , high=1. , for x , y , mask in zip ( reference_output_tensors , show_layer_names=True , _ == C.FreeDimension for _ in x.shape ) : for i in range ( len ( self.loss_functions ) ) __Output shape__ np.random.shuffle ( index_array ) if target_min is not None : for i in range ( a1 , 1 , -1 ) : if not any ( v is value for v in self._non_trainable_weights ) : input_length=None , ( see [ regularizer ] ( .. /regularizers.md ) ) . 'activity_regularizer ' : # add a 3x3 unshared weights convolution on top , with 32 output filters : x : What to return in train phase self._layers.pop ( ) If not provided , defaults to ` [ .33 , .55 , .67 , 1 . ] ` . mask = reverse ( mask , 1 ) super ( Subtract , self ) .build ( input_shape ) if getattr ( outputs , '_uses_learning_phase ' , False ) : weights += layer.trainable_weights batch_size : Integer , batch size . 'verbose ' : verbose , # there should be a shortcut to calculating this A tuple of integers . The list should have 3 elements , of shapes : index.setdefault ( layer.name , [ ] ) .append ( layer ) `` `` '' Built-in regularizers . '' '' '' new_c = [ ] self.start = start new_layer = keras.layers.UpSampling3D ( ( 2 , 2 , 2 ) , K.greater ( ( self.true_positives ) , 0 ) , # Will update with gather op in next release regularizer : An optional Regularizer instance . _callbacks += ( callbacks or [ ] ) + [ model.history ] Warning : type returned will be different for dropout=0.1 , op = T.nnet.abstract_conv.AbstractConv3d_gradInputs ( imshp=None , sample_weights = [ ] return tf.reduce_prod ( x , axis , keepdims ) computed_tensors = [ x [ 0 ] for x in computed_data ] self._output_coordinates = [ ] str ( maxlen ) + ' , no sequence was kept . ' recurrent_initializer=recurrent_initializer , y = np.random.random ( ( 1 , 3 ) ) TensorShape , which represent the shape of the output . For def lr ( self ) : 'loss ' : model.loss , result = tf.squeeze ( result , -1 ) applies for ` 'epoch ' ` . If using an integer , let 's say ` 10000 ` , initializer=self.gamma_initializer , elif hasattr ( filepath , 'write ' ) and callable ( filepath.write ) : self.assign_embeddings.append ( batch ) if custom_objects and class_name in custom_objects : new_layer = keras.layers.Dense ( 2 , input_shape= ( 3 , ) , name='d ' ) layer.bias_i , 'data_format ' : 'channels_last ' } , return conv_out # we simply expand each of them at axis=1 auto_padding= [ False ] ) def test_conv ( self , op , input_shape , kernel_shape , padding , data_format ) : return losses.mse ( * args , * * kwargs ) When the next layer is linear ( also e.g . ` nn.relu ` ) , should have value 1 , in case atrous_rate= ( 2 , 2 ) , # Case : symbolic-mode subclassed network . output_shape = layer.compute_output_shape ( f = self.test_function base_config = super ( Masking , self ) .get_config ( ) dilation_rate=self.dilation_rate ) timesteps ) ) ) `` `` '' Stacks a list of rank ` R ` tensors into a rank ` R+1 ` tensor . `` `` '' Iterates over the time dimension of a tensor . ' ( symmetric_height_crop , symmetric_width_crop ) , ' `` `` '' Retrieves the dictionary mapping words to word indices . ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='avgpooling3d ' ) from .losses import mean_squared_logarithmic_error A string , either ` 'channels_first ' ` or ` 'channels_last ' ` quantity with no improvement after which training will constraints.update ( self.backward_layer.constraints ) outputs= [ x_placeholder + y_placeholder ] , last_output , outputs , new_states = tf_keras_backend.rnn ( super ( InputLayer , self ) .__init__ ( dtype=dtype , name=name ) pool_mode='avg ' ) raise C.set_global_option ( 'align_axis ' , 1 ) test_func = lambda x : x + y x_sparse_1 = sparse.csr_matrix ( ( x_d , ( x_r , x_c ) ) , shape= ( 4 , 5 ) ) ( 1 - target ) * -np.log ( 1 - sigmoid ( output ) ) ) with K.name_scope ( 'loss ' ) : 'in compile ( ) . If you just mean to use ' Thus the saved model can be reinstantiated in ( or list of shape tuples if the layer has multiple outputs ) . elif K.backend ( ) == 'theano ' and not KTH.th_sparse_module : self.kernel = self.add_weight ( shape=kernel_shape , 'Do not pass a layer instance ( such as { identifier } ) as the ' if verbose : defaults = func.__defaults__ 'the output of a Keras ` Layer ` ' self.backward_layer = layer.__class__.from_config ( config ) f_stats.append ( l_s ) # No batch size specified , therefore the layer will be able from .. import __version__ as keras_version __Arguments__ allowed_positional_args= [ 'padding ' ] , self.bias_initializer ( ( self.units * 2 , ) , * args , * * kwargs ) , if original_shape [ cols ] is None : assert_list_pairwise ( zero_list ) def _get_dynamic_axis_num ( x ) : > > > K.set_image_data_format ( 'channels_last ' ) self._trainable = value def Input ( shape=None , batch_shape=None , `` `` '' Ensures that a value is converted to a python cell object . are_zeros = K.expand_dims ( are_zeros , 0 ) return AUCCurve.PR pytestmark = pytest.mark.skipif ( '/gpu:7 ' not in available_devices , for node in nodes : p_t = p - lr_t * m_t / ( u_t + self.epsilon ) for i in range ( len ( z_list ) ) : def pool ( x , pool_size , strides , padding , data_format , pool_mode ) : return x ! = y # Doing Multiprocessing.Value += x is not process-safe . kwargs [ 'mask ' ] = computed_masks > > > y = K.ones ( ( 4 , 3 , 5 ) ) config = super ( Conv2DTranspose , self ) .get_config ( ) It should be a tuple of integers , e.g . ` ( 32 , 10 , 100 ) ` . # Historically , ` sequential.layers ` only returns layers that were added shape.append ( 1 ) # Expand dims if ndim == 1 Can also be None to clear the control dependencies . outputs = K.conv3d ( # Note : padding=self.padding , inf = _to_tensor ( np.inf , x.dtype.base_dtype ) `` `` '' Submits request to the executor and queue the ` Future ` objects . '' '' '' def _is_path_instance ( path ) : # test with functional API Consider a Numpy data array ` x ` of shape ` ( samples , timesteps , features ) ` , return x < y U_regularizer='l1 ' , def _is_current_explicit_device ( device_type ) : # Additional substitutions can be passed to ` tf.Session ( ) .run ( ) ` via its raise AttributeError ( 'The layer has never been called ' self.train_function = K.function ( shuffle=True , binary_data = h5file.fid.get_file_image ( ) if not isinstance ( value , type ( cell_value ) ) : self.updates.append ( K.update ( u , u_t ) ) y_shape = int_shape ( y ) check_two_tensor_operation ( 'in_train_phase ' , ( 2 , 3 ) , ( 2 , 3 ) , WITH_NP , Useful when modeling temporal data where the model if isinstance ( padding , int ) : if self.use_bias : # issues . out2 = K.sum ( out1 , axis=-1 ) def DISABLED_test_ordered_enqueuer_timeout_threads ( ) : output = K.repeat_elements ( inputs , self.size [ 0 ] , axis=1 ) def DISABLED_test_reuters_load_does_not_affect_global_rng ( fake_downloaded_reuters_path ) : md5_hash=None , y : Target data . Like the input data ` x ` , 'argument . ' ) h_o = self.recurrent_conv ( h_tm1_o , raise_duplicate_arg_error ( old_name , new_name ) tf_keras_backend.manual_variable_initialization ( value ) if padding == 'same ' : ValueError : if any member of ` params ` is not a valid argument . with closing ( self.executor_fn ( _SHARED_SEQUENCES ) ) as executor : def glorot_normal ( seed=None ) : except ImportError : # Used purely for shape validation . return np.maximum ( x , y ) except TypeError : # These lists will be filled via successive calls `` `` '' Representation of HDF5 dataset to be used instead of a NumPy array . ` node_indices [ i ] ` is the origin node of ` input_tensors [ i ] ` d_pad = pool_size [ 2 ] - 2 if pool_size [ 2 ] % 2 == 1 else pool_size [ 2 ] - 1 * * kwargs ) start_index : Data points earlier than ` start_index ` will not be used compute the average of ` values ` . This average is ultimately returned as ` mean ` def __init__ ( self , thresholds=None , name=None , dtype=None ) : ` batch_dot ( x , y , axes=1 ) = [ [ 17 ] , [ 53 ] ] ` which is the main diagonal although it tends to be more memory-intensive . shape=None , return C.element_max ( x , y ) ValueError : if ` dim_ordering ` is invalid . if not self._is_input_shape_compatible ( value , tensor ) : module to mock must be provided , using the ` file_io_module ` argument . old_layer = keras.layers.Embedding ( input_dim=4 , output_dim=2 , name= 'd ' , str ( spec.min_ndim ) + ' , found ndim= ' dtype=dtype , seed=self.seed ) By default , ` `` nearest '' ` is used . def _assert_has_capability ( module , func ) : ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='avgpooling3d ' ) self._fn_kwargs = kwargs if self.reduction == metrics_utils.Reduction.SUM : ( or alternatively , the keyword argument ` input_shape ` ) assert n._keras_shape == ( None , 5 ) `` `` '' Abstract class for different global pooling 3D layers . 'which is not supported . Please do permute ' outputs [ 0 ] , criterion , [ learner ] ) layer_id = str ( id ( layer ) ) return all_outs [ 0 ] [ 0 ] ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='maxpool3d ' ) str ( x [ 0 ] .shape [ 0 ] ) + ' samples ' ) pool_size= ( 2 , 2 , 2 ) , border_mode='valid ' , name='maxpool3d ' ) skip_target_masks=None , # In the case of reverse_state_order=True , the state_size will be loop_vars [ ConfusionMatrix.FALSE_POSITIVES ] = ( label_is_neg , pred_is_pos ) if isinstance ( self._values [ k ] , list ) : self.model.stop_training = True layer_configs = [ ] padding , Linear Units ( ELUs ) ] ( https : //arxiv.org/abs/1511.07289 ) with tf_file_io.FileIO ( source_filepath , mode='rb ' ) as source_f : The generator should return the same kind of data capable of instantiating the same layer from the config _ , cntk_func = cntk_func_tensors ( except ValueError : def on_predict_batch_begin ( self , batch , logs=None ) : max_queue_size=10 , ( 'conv3d ' , ( 2 , 3 , 5 , 4 , 6 ) , ( 2 , 2 , 3 , 3 , 4 ) , fpath = untar_fpath + '.tar.gz ' ' while using as loss ` ' + loss_name + ' ` . ' def stop_gradient ( variables ) : `` `` '' Sets the value of the data format convention . A numpy dtype string . if getattr ( output , '_uses_learning_phase ' , False ) : ` layer_names ` ( attribute ) , a list of strings if attr in self.data.attrs : self.end = end def he_uniform ( seed=None ) : for kwarg in kwargs : xs , labels = f [ ' x ' ] , f [ ' y ' ] broadcast_shape ) super ( Masking , self ) .__init__ ( * * kwargs ) exception_prefix : String prefix used for exception formatting . weights= [ if len ( validation_data ) == 2 : return [ return identifier those you could pass to ` sk_params ` , including fitting parameters . self._non_trainable_weights.append ( value ) the _keras_shape of the input ( s ) . > > > kvar = K.variable ( value=val ) from_cudnn : ` True ` if source weights are in CuDNN format , ` False ` from .load_backend import to_dense return ( input_shape [ 0 ] , input_shape [ 3 ] ) for j in range ( w.shape [ i ] - 1 ) : return self.cell.filters raise NotImplementedError ( for dim in state_size ] Subclasses should override for any actions to run . This function should only return xception.decode_predictions ( * args , * * kwargs ) unique_tensors = [ ] training_utils.call_metric_function ( if need_convert : return tf.round ( x ) from .load_backend import greater Scalar loss ( if the model has a single output and no metrics ) if shape is not None : print ( ' # # # # # # # Xception benchmark - folder generator i/o ' ) `` Your generator is NOT thread-safe . '' if isinstance ( inputs , list ) and len ( inputs ) > 1 : masks = to_list ( mask ) outputs = self.call ( unpack_singleton ( self.inputs ) ) n : integer , repetition factor . if weights [ 0 ] .size ! = np.prod ( layer_weights_shape ) : return x * noise / ( 1 - level ) defaults : defaults of the function . zero = zeros = Zeros `` `` '' Normalizes a tensor wrt the L2 norm alongside the specified axis . name : ( Optional ) Name for the object . weight_values , 'Defaulting to output shape ` { } ` ' if not self.mask_zero : py_sum = sum # Save config file , if possible . self.updates.append ( K.update ( m , v ) ) ` keras.callbacks.CallbackList ` to be called during evaluation . def standardize_sample_or_class_weights ( x_weight , kernel = expand_dims ( kernel , 1 ) shape_temp = [ ] # batch size matters , use rnn-based implementation pool2d = pool layer = keras.layers.CuDNNGRU ( units ) if not callable ( else_expression ) : mask_k = K.variable ( mask_np ) return output , [ ] compatible with ` CuDNNGRU ` . self._t_enter_batch = time.time ( ) [ 0 , 0 , 1 , 0 ] then the precision value would be 1 . input_shape = tuple ( input_shape [ : -1 ] ) self._fn = fn ` ( name , value_for_last_step ) ` . shape = tuple ( shape ) return obj.__name__ diff = K.abs ( ( y_true - y_pred ) / K.clip ( K.abs ( y_true ) , submodel_not_wrapper = model_to_dot ( layer , show_shapes , with tf.control_dependencies ( [ op ] ) : For every layer , a ` group ` named ` layer.name ` assert K.is_keras_tensor ( keras_var ) is False computed_mask ) ) shape_key = inbound_layer.name if isinstance ( _axis , list ) : epochs : Number of times to iterate over the data `` `` '' Resizes the volume contained in a 5D tensor . validation_steps : Only relevant if ` validation_data ` is provided ch_dim = 1 self.input_bias_r = self.input_bias [ self.units : self.units * 2 ] ( instead of ` ( n_sample , 1 ) ` as in Keras ) . return T.mean ( x , axis=axis , keepdims=keepdims , dtype=dtype ) def test_func_dump_and_load_backwards_compat ( test_func ) : metrics.extend ( _get_metrics_from_layers ( self._layers ) ) or if loss is a list with len not equal to model outputs . name='mean_squared_logarithmic_error ' ) : expected_last_state [ 1 : ] += num_timesteps return inception_v3.preprocess_input ( * args , * * kwargs ) def print_layer_summary_with_connections ( layer ) : class BinaryCrossentropy ( LossFunctionWrapper ) : new_layer = keras.layers.GRU ( 2 , input_shape= [ 3 , 5 ] , name='d ' ) for axis in reduction_axes ] ) permute_pattern [ 1 ] = axes [ 1 ] w = np.insert ( w , 2 * j + 1 , 0 , axis=i ) if isinstance ( axis , int ) : the loss and any model metrics at the end of each epoch . 'with rank < 2 . ' return history def add_update ( self , updates , inputs=None ) : axis = tuple ( axis ) cond_float = cond_float [ ... , np.newaxis ] X_train [ six.moves.range ( 1000 , 1001 ) ] # sample_weight=tuple ( sample_weight ) ) n : Python integer , number of times to repeat . `` `` '' Computes root mean squared error metric between ` y_true ` and ` y_pred ` . if i is not None : interpreted as three different 'layer { } '.format ( layer.name ) + ' due to ' Optionally , a normalizer function ( or lambda ) can be given . This will b.variable ( targets , dtype='int32 ' ) , k ) ) ` steps ` value might have changed due to padding or strides . @ K.eager ( strings ) to custom classes or functions to be `` binary '' will be 1D binary labels , output_tensors.append ( output ) data_format=self.data_format ) tuples = [ ] monitor='val_loss ' , xs.append ( reshape ( inputs [ : , slice_row , slice_col , : ] , The loss value that will be minimized by the model # It does n't check the functionality ( which is checked at the return cls ( * * config ) ValueError : In case of an invalid savefile . if sys.version_info < ( 3 , ) : import inspect batch_ind = tf.boolean_mask ( batch_array , dense_mask ) t = getattr ( k , function_name ) ( x_shape_or_val , * * kwargs ) layer.bias_f_i , from .convolutional import Deconvolution2D return [ np.concatenate ( unconcatenated_outs [ i ] , axis=0 ) i = self.recurrent_activation ( z0 ) 'either a single ' If tuple of 3 tuples of 2 ints : super ( TruePositives , self ) .__init__ ( with tf.device ( '/cpu:0 ' ) : def DenseNet201 ( * args , * * kwargs ) : if validation data is provided . k : the number of values to keep . warnings.warn ( 'The ` dropout ` argument is no longer support in ` Embedding ` . ' from .. engine import InputLayer source_tensors_ids = set ( ) subset=subset if dilation_rate == ( 1 , 1 ) : interpolation='nearest ' , mode : File open mode ( one of ` { `` a '' , `` r '' , `` w '' } ` ) . max_value : float > = 0 . Maximum activation value . axis : Integer , the axis that should be normalized [ output_a_np , output_b_np ] , del unprocessed_nodes [ layer ] color_mode='rgb ' , ' '' channels_first '' , `` channels_last '' . Received : ' min_delta : threshold for measuring the new optimum , elif len ( n ) ! = len ( shape ) : save_model ( model , fname ) return preprocess_weights_for_loading ( inputs_o = inputs x : tensor with any dimensions . # List of metric wrappers on output losses . input_shape = K.int_shape ( inputs ) # tensor output of each node . def __init__ ( self , units , * * kwargs ) : layer = RNN ( cells ) return metric.__class__.from_config ( metric.get_config ( ) ) kwargs [ 'sample_weight ' ] = sample_weight shape [ 1 ] , `` `` '' Initializer that generates tensors initialized to 1 . # input_mask is not None , and output_mask is None : # Old interface : ( params , constraints , loss ) epoch : integer , index of epoch . callbacks._call_batch_hook ( 'predict ' , 'end ' , batch_index , batch_logs ) layer.recurrent_kernel_r , ' ( temporal data ) . ' ) of ` inputs ` instead of by the dedicated keyword arguments . This method [ Dropout : A Simple Way to Prevent Neural Networks from Overfitting ] ( str ( len ( self.layers ) ) + ' layers . ' ) 'backend ' : K.backend ( ) training=training ) for _ in range ( count ) ] @ wraps ( load_function ) This function is more numerically stable than log ( sum ( exp ( x ) ) ) . val = H5Dict ( val ) def ctc_step ( y_true_step , y_pred_step , input_length_step , label_length_step ) : def batch_flatten ( x ) : uses_learning_phase ) out = tf.sparse.sparse_dense_matmul ( x , y ) masks.append ( mask_i ) fpath = os.path.join ( path , 'train ' ) def test_loop ( model , f , ins , x_train , y_train = f [ 'x_train ' ] , f [ 'y_train ' ] assert_allclose ( x + increment , K.eval ( x_var ) , atol=1e-05 ) from tensorflow.keras.layers import SpatialDropout1D # search for shared layers data_format = value.lower ( ) original_input_layer = x._keras_history [ 0 ] model._check_trainable_weights_consistency ( ) new_layer = keras.layers.GlobalAvgPool3D ( data_format='channels_last ' , binary_crossentropy , K.ones_like ( inputs ) , `` `` '' Function decorator to support loading from Google Cloud Storage ( GCS ) . # Expecting this to never be true . output_shapes = [ ] ' , but the saved weight has shape ' return d layer : The layer . if greedy : model.compile ( 'sgd ' , metrics= [ keras.metrics.Poisson ( ) ] ) output_shapes=output_shapes , self.alpha = K.cast_to_floatx ( alpha ) kwargs [ 'rate ' ] = dilation_rate self.log_dir = log_dir reduce_axes = [ ] times.append ( time.time ( ) - start_time ) 'normalize ' : self.normalize , feed_output_shapes = [ ] To allow multiple Sequences to be used at the same time , we use ` uid ` to def create_dataset ( h5_path='test.h5 ' ) : If all inputs in the model are named , def _get_available_gpus ( ) : parameter is ignored . Values should be in [ 0 , 1 ] . Endpoint thresholds `` `` '' Fully-connected RNN where the output is to be fed back to input . `` `` '' Exponential Linear Unit . start = 0 verbose : int . 0 : quiet , 1 : update messages . callbacks : List of callbacks to be called during training and validation pool_size= ( 2 , 2 ) , strides= ( 2 , 2 ) , padding='valid ' , name='avgpooling2d ' ) pointwise_initializer='normal ' , for cbk in callbacks : `` `` '' 2D convolution layer ( e.g . spatial convolution over images ) . save_function : The function to wrap , with requirements : steps=steps , # sanity checks Depthwise convolution performs if max_value < min_value : for original_node_index , node in enumerate ( layer._inbound_nodes ) : 'does not support a factor > = 1.0 . ' ) # as first layer in a Sequential model layer_test ( output = K.pool3d ( inputs , pool_size , strides , def test_sparse_dot ( self ) : # squash middle dimensions of x . self.run_thread.join ( timeout ) reporthook : a hook function that will be called once self.built = True random_zoom = image.random_zoom self.stop_signal = threading.Event ( ) example_var a node is added to ` layer._outbound_nodes ` . class FalsePositives ( _ConfusionMatrixConditionCount ) : assert parallel_siamese.output_names == [ 'add ' , 'nested_1 ' , 'nested_2 ' ] How many zeros to add at the beginning and at the end of parallel_model = multi_gpu_model ( model , gpus=i ) x = tf.nn.avg_pool3d ( x , pool_size , strides , border_mode='valid ' , save_to_binary_h5py ( save_function , filepath ) # Check shape . # prepare callbacks self.test_function = None self.pool_size = conv_utils.normalize_tuple ( pool_size , 3 , 'pool_size ' ) mean_absolute_error , name=name , reduction=reduction ) b_any = any be centered around ` data [ i ] ` , ` data [ i+s ] ` , ` data [ i+2 * s ] ` , etc . self.bias_z , self.negative_slope = K.cast_to_floatx ( negative_slope ) K.arange ( 10 ) , if shape [ i ] is None or shape [ i ] == -1 : inputs return obj timesteps = 5 data_format='channels_first ' , return np.ones ( ( y.shape [ 0 ] , y.shape [ 1 ] ) , dtype=K.floatx ( ) ) save_model ( self , filepath , overwrite , include_optimizer ) return kernel.T.reshape ( kernel.shape , order=order ) classes=num_classes ) value = np.asarray ( value ) unroll : Whether to unroll the RNN or to use a symbolic loop steps_per_epoch=steps_per_epoch , layer.activity_regularizer ( x ) warnings.warn ( 'Could not import the TensorFlow backend . ' ) dtype=K.floatx ( ) if volume_shape is not None : for k in backend_list : ' ( like softmax or sigmoid would ) . ' ) return vgg19.preprocess_input ( * args , * * kwargs ) UserWarning ( 'Using a generator with ` use_multiprocessing=True ` ' param_dset [ ( ) ] = val if old_lr > self.min_lr : $ HOME/.theano self.min_delta = min_delta _ , x = parse_shape_or_val ( input_shape ) if update_fp or update_tn : @ pytest.mark.parametrize ( 'axis ' , [ 1 , -1 ] ) [ Supervised sequence labeling with recurrent neural networks ] ( self.depth_multiplier ) return False to any other variable . `` `` '' Gets a numpy-style shape tuple giving the dataset dimensions . def clip_norm ( g , c , n ) : class AUCCurve ( Enum ) : input_tensor = K.placeholder ( shape=batch_input_shape , self.recurrent_kernel_o = self.recurrent_kernel [ : , self.units * 3 : ] def random_uniform_variable ( shape , low , high , rows = input_shape [ 2 ] num_old_batch = int ( num_element / num_static_element ) return int ( value ) model.add ( Dense ( 32 , batch_input_shape= ( None , 500 ) ) ) # str This metric creates four local variables , ` true_positives ` , ` true_negatives ` , elif not shape2 : legacy_generator_methods_support = generate_legacy_method_interface ( return y , [ y , backend.concatenate ( [ y , y ] , axis=-1 ) ] from .load_backend import repeat_elements `` `` '' Thresholded Rectified Linear Unit . z_list = [ k.eval ( k.dropout ( k.variable ( val ) , level=0.2 , cntk_func = KC.function ( placeholders , [ output_cntk ] ) metrics= [ keras.metrics.CategoricalAccuracy ( ) ] ) 'shape is not supported . Please provide ' for chunk_id , chunk_data in enumerate ( chunked_data ) : y : class vector to be converted into a matrix check_batch_axis=False , # Do n't enforce the batch size . square_sum = T.sum ( T.square ( x ) , axis=axis , keepdims=True ) def test_glorot_normal ( tensor_shape ) : self.sep = separator def __init__ ( self , input , batch_size , name='convert_to_static ' ) : output = inputs [ 0 ] `` `` '' Checks if ` filepath ` is referencing a google storage bucket . if not input_shape : check_single_tensor_operation ( 'sqrt ' , ( 4 , 2 ) , WITH_NP ) class BaseWrapper ( object ) : ( during training only ) . 'the time dimension by passing a ' _call_metric ( metric_obj , value ) 'All layer names should be unique . ' ) dropout=0.1 , def check_dtype ( var , dtype ) : from tensorflow.keras.applications.nasnet import decode_predictions old_config = json.dumps ( old_layer.get_config ( ) ) def test_print_tensor ( self , capsys ) : if argname == name : use_multiprocessing=False , y_col=y_col , > > > arr.dtype return T.extra_ops.cumsum ( x , axis=axis ) input_length=timesteps , name='poisson ' ) : if 'epsilon ' in kwargs : outputs = to_list ( outputs , allow_tuple=True ) skip_mismatch : Boolean , whether to skip loading of layers stacklevel=3 ) export PATH= '' $ MINICONDA/bin : $ PATH '' converted.append ( ( 'input_shape ' , 'input_dim ' ) ) This layer can add rows and columns of zeros class MaxPooling1D ( _Pooling1D ) : dtype=dtype ) untar=False , first_axis_to_pad , second_axis_to_pad , third_axis_to_pad ) ` ( 'separable_conv1d ' , ( 1 , 8 , 2 ) , ( 3 , ) , 2 , 'valid ' , 'channels_last ' ) , from tensorflow.keras.layers import GRU return tf.identity ( x , name ) `` `` '' Get the next value from the generator ` uid ` . layer.count_params ( ) , weights_list = loss_weights metric_fn.name = metric_name str ( x_weight ) ) y_permute_dim = list ( range ( ndim ( y ) ) ) unknown_output = set ( sample_weight_mode.keys ( ) ) - set ( output_names ) # ` fetches ` arguments . In contrast to ` updates ` argument of result = tf.squeeze ( result , 1 ) 'same ' , 'channels_first ' , 'avg ' ) , 'name ' : b'in-memory-h5py ' } # name does not matter `` `` '' Decorator to wrap metric ` update_state ( ) ` with ` add_update ( ) ` . if layer.name == 'input2 ' : constants = [ ] if len ( bias_shape ) ! = 1 and len ( bias_shape ) ! = ndim ( x ) - 1 : result = [ ] generator_output = next ( output_generator ) expr_shape = tf.shape ( then_expression ) for constant in constants ] check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=1 ) bucket_name : String identifier of * a real * GCS bucket ( with or without the def map_fn ( fn , elems , name=None , dtype=None ) : `` `` '' Utilities for text input preprocessing . within the enclosing ` with ` statement . At end of the ` with ` statement , for TF 1.x might produce weights with same variable y = 0.2 * x + 0.5 the output of ` model.get_weights ( ) ` . dp = K.maximum ( dp , 0 ) if dilation_rate [ 0 ] ! = dilation_rate [ 1 ] : self.callbacks = [ c for c in callbacks ] within feature maps are strongly correlated ( as is normally the case in check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , WITH_NP , axes=1 ) x * = random_tensor if Numpy data : we create placeholders matching the shape of if 'mask ' not in kwargs : x = K.expand_dims ( x , 1 ) return x > y for shape in [ ( ) , ( 3 , ) , ( 2 , 3 ) , ( 5 , 3 , 2 ) ] : smoothed = ( 1 - alpha ) * predict [ : , Y ] + alpha * np.float32 ( 1 . ) / Y.shape [ 0 ] metrics_utils.Reduction.WEIGHTED_MEAN , predictions [ i , idx_identical ] = predictions [ i , 0 ] return training_generator.evaluate_generator ( base_config = super ( SpecificityAtSensitivity , self ) .get_config ( ) limit = np.sqrt ( 3 . * scale ) output_arrays.append ( k.eval ( output ) ) bias_shape = tuple ( bias.shape ) 'sensitivity ' : self.sensitivity if initial_state is not None and inputs [ 1 : -self._num_constants ] : return tf_math_ops.log ( x ) `` `` '' Locally-connected layers . merged.append ( concatenate ( outputs , self.inputs = None cell = GRUCell ( units , if self.update_freq ! = 'epoch ' : h_tm1_f = h_tm1 * rec_dp_mask [ 1 ] layers : List of layers . AtrousConv1D = AtrousConvolution1D output_shape = self._get_shape_tuple ( return hasattr ( x , 'dynamic_axes ' ) and len ( x.dynamic_axes ) > 1 def __len__ ( self ) : kwargs= { 'filters ' : filters , return return_value return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) 'left_pad ' , 'right_pad ' } : ` ( batch , dim1 , dim2 , dim3 , channels ) ` if self.__class__.__name__ == 'Sequential ' : 'Make sure to pass a complete `` input_shape '' ' # We have made a fix but not catched in CNTK 2.1 release . if input_shape and batch_input_shape : validation_freq : Only relevant if validation data is provided . Integer output = last_output non_tensors = [ x for x in losses if isinstance ( x , ( float , int ) ) ] if not isinstance ( model , Model ) : from .recurrent import GRU data = data.values if data.__class__.__name__ == 'DataFrame ' else data # ( 0 , 1 ) . reduced_inputs_shapes = [ list ( shape ) for shape in input_shape ] output of the inbound layer layer : Layer ( or model ) instance . def fit_generator ( self , generator , of names . data_format=tf_data_format , rnn_layer_class = GRU if source == 'CuDNNGRU ' : Indices are based on order of horizontal graph traversal ( bottom-up ) . def __enter__ ( self ) : `` `` '' Return explicit device of current context , otherwise returns ` None ` . from tensorflow.keras.applications.densenet import DenseNet169 if len ( self.outputs ) > 1 : padding : Tuple of 3 tuples , padding pattern . from .load_backend import backend self.dilation_rate = conv_utils.normalize_tuple ( dilation_rate , rank , mse = MSE = mean_squared_error append : True : append if file exists ( useful for continuing ( 2 , 2 ) will halve the input in both spatial dimension . nones = _get_dynamic_axis_num ( x ) new_lr = old_lr * self.factor atol=1e-05 ) : if len ( args ) > len ( allowed_positional_args ) + 1 : return pad ( x , [ padding ] , 'channels_last ' , num_dynamic_axis ) loss_name = loss_name.__name__ # Try and load external backend . unit_forget_bias=unit_forget_bias , def binary_crossentropy ( target , output , from_logits=False ) : model.add ( Lambda ( lambda x : x * * 2 ) ) count += 1 self.bias_initializer ( ( self.units , ) , * args , * * kwargs ) , shape [ 2 : ] == ( layer.kernel_size [ 0 ] , 1 ) ) from .load_backend import set_epsilon ultimately returned as ` categorical accuracy ` : an idempotent operation that except AttributeError : # getargspec ( ) is deprecated since Python 3.0 cell = MinimalRNNCell ( 32 ) dims = len ( shape ) # training and evaluation methods ) . if layer in unprocessed_nodes : with tf_ops.init_scope ( ) : Faster to compute than sigmoid activation . last_output = outputs [ 0 ] from .pooling import MaxPool1D candidate_vars = [ ] 'moving_mean_initializer ' : parallel_model.save ( fname ) it becomes possible to do : # Attempt to read Keras config file . to ` channels_first ` ) . from .load_backend import pow _DISABLE_TRACKING.value = False 'or 3 ( x_val , y_val , val_sample_weights ) ' assert len ( decode_pred_tf ) == 1 weights = weights [ num_param : ] self._delta_t_batch = time.time ( ) - self._t_enter_batch raise ValueError ( 'The dimension and the size of indices should match . ' ) def func_dump ( func ) : self._feed_input_shapes.append ( shape ) if len ( y.shape ) > 2 : for layer in layers : permute_pattern [ -1 ] = axes [ 0 ] def he_normal ( seed=None ) : ( layers.CuDNNGRU , { 'units ' : 2 , 'input_shape ' : [ 3 , 5 ] } ) , layer_name = layer_data [ 'name ' ] output = th_sparse_module.basic.vstack ( tensors , format='csr ' ) `` `` '' Initializer that generates tensors initialized to 0 . quadratic to linear . self.backward_layer.reset_states ( ) U_regularizer='l1 ' , for index , dim in enumerate ( output_shape ) : weights = _convert_rnn_weights ( layer , weights ) inbound_layers , node_indices , tensor_indices , depth = max ( depth , previous_depth ) # hold the embeddings in ` set_model ` . At this point , however , def weights ( self ) : elif len ( validation_data ) == 3 : def _to_snake_case ( name ) : positions : Relative or absolute positions of log elements all_classes = { from keras.layers import convolutional # if rank is 2 , expand to 3 . travis_retry conda create -q -n test-environment python= $ TRAVIS_PYTHON_VERSION num_samples = 10000 weights=None ) : pred_is_pos = K.greater ( preds_tiled , thresh_tiled ) dtype='float32 ' ) : return T.sqrt ( x ) strides , if workers > 0 : 'an ` input_shape ` or ` batch_input_shape ` ' str ( x_shape ) + ' and ' check_single_tensor_operation ( 'repeat ' , ( 4 , 1 ) , WITH_NP , n=3 ) @ interfaces.legacy_conv2d_support `` `` '' Trains the model for a fixed number of epochs ( iterations on a dataset ) . self.best = current super ( Cropping3D , self ) .__init__ ( normalized_cropping , callbacks._call_batch_hook ( 'predict ' , 'begin ' , batch_index , batch_logs ) padding : int , or tuple of int ( length 2 ) , or dictionary . from .callbacks import RemoteMonitor Should be called by the same thread which called ` start ( ) ` . interpolation : A string , one of ` nearest ` or ` bilinear ` . ValueError : if ` value ` is neither ` 0 ` nor ` 1 ` . https : //arxiv.org/abs/1603.07285v1 ) self._compile_metrics = metrics or [ ] # Stream the epoch loss to a file in JSON format . The file content def predict ( self , x , # out = model.fit ( [ input_a_np , input_b_np ] , self.num_thresholds = num_thresholds are discarded and redrawn . This is the recommended initializer for for m1 , m2 in zip ( [ m.name for m in model._compile_metrics ] , [ 'metric_1 ' ] ) : tf_keras_backend.set_epsilon ( e ) with K.control_dependencies ( update_op ) : # For TF ' ( cancel ) . ' ) .strip ( ) .lower ( ) if list ( reduction_axes ) == [ 0 , 1 , 2 ] : kwargs [ 'dilations ' ] = ( dilation_rate , ) `` `` '' Computes the one-hot representation of an integer tensor . argnames = getargspec ( f ) [ 0 ] def pow ( x , a ) : num_param = len ( layer.weights ) 'on a list of 2 inputs . ' ) # For losses which are given as strings/functions in the compile API , return tf_state_ops.assign_sub ( x , update_delta ) base_shape = x.shape `` `` '' Adds updates to the layer . should be the same as when the weights were saved . output_shapes : a list of the shapes ( strings ) of model outputs . output_shape [ d_axis ] = conv_utils.deconv_length ( output_shape [ d_axis ] , from .load_backend import ones_like x : Keras tensor or variable with ` ndim > = 2 ` . for ( i , shape ) in enumerate ( shapes ) ] indices = np.random.randint ( 0 , num_classes , size= ( batch_size , input_length ) ) def __exit__ ( self , * args , * * kwargs ) : out_pad_w ) ( could be a functional model or a Sequential model ) . ' ` set_session ` is not available ' array ( [ [ 1. , 0. , 0 . ] , 'If your data is in the form of symbolic tensors , ' self.classes_ = np.unique ( y ) loss : String ( name of objective function ) , objective function or An ` Iterator ` yielding tuples of ` ( x , y ) ` def update_state ( self , * args , * * kwargs ) : num_batch = int ( num_element / num_static_element ) padding , data_format , pool_mode='avg ' ) from .load_backend import mean mask , _ , sample_weight = ( ' ; Received input shape : ' , str ( input_shape ) ) class MaxoutDense ( Layer ) : x = tf.nn.separable_conv2d ( x , depthwise_kernel , pointwise_kernel , from .. legacy.layers import AtrousConvolution1D 'with TensorFlow backend and TF version > = 2.0.0 . ' ) return K.mean ( K.maximum ( 1 . - y_true * y_pred , 0 . ) , axis=-1 ) from tensorflow.keras.layers import Maximum 'class_name ' : layer_class_name , input_layers = [ ] counts [ name ] = 1 y.append ( _y ) validation_freq : Only relevant if validation data is provided . Integer @ interfaces.legacy_upsampling2d_support shapes.append ( K.int_shape ( x ) ) warnings.warn ( 'The TensorBoard callback ` batch_size ` argument ' or by topological order . dilation_rate : an integer or tuple/list of 2 integers , specifying if not dot.get_edge ( src , dst ) : x = recall new_layer_2 = keras.layers.SpatialDropout3D ( 0.5 , Includes support for momentum , self._feed_output_shapes = [ ] metrics= [ metrics.categorical_accuracy ] , kwargs [ 'params ' ] = params cntk_two_dynamicity = kwargs.pop ( 'cntk_two_dynamicity ' , False ) raise ValueError ( ' '' sample_weight_mode ' generator_output = next ( output_generator ) str ( list ( args [ 1 : ] ) ) ) rate , while the area under the PR-curve is the computed using the height of [ Deconvolutional Networks ] ( # the largest integer ( i.e . word index ) in the input should be from six.moves import zip def test_sparse_concat ( self ) : if value.shape ! = get_tuple_shape ( dim ) : # the ` validation_data ` is not yet set . if self.return_sequences : if max ( key ) + self.start < self.end : elif y.ndim == 4 : 'Install the latest version with : \n ' of random values to generate . Defaults to 1 for float types . theano.tensor.sharedvar.TensorSharedVariable , W_regularizer='l1 ' , from IPython import display stddev , seed=seed ) , dtype=dtype ) class MeanAbsolutePercentageError ( LossFunctionWrapper ) : tensor_index=tensor_index ) self.read_only = mode == ' r ' elif is_model ( layer ) : decoded_length [ i ] = len ( decoded ) x : A tensor . if input_tensor is not None and batch_input_shape is None : return Add ( * * kwargs ) ( inputs ) timesteps , if self.shared_axes : kwargs ] ) border_mode=th_padding , A shape tuple # thus it is already built . ref = [ 3.34211 , 5.42262 ] broadcast_shape ) output_tensors , _LEARNING_PHASE_PLACEHOLDER.value = np.asarray ( value ) In case of grayscale data , the channels axis of the image array confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES , _config = { } `` `` '' Computes the mean squared error between ` y_true ` and ` y_pred ` . return np.ones ( shape , dtype=dtype ) assert_allclose ( k_s , k_d , atol=1e-05 ) zeros = T.zeros_like ( L [ 0 ] ) th_padding = 'half ' mask = K.cast ( mask , K.floatx ( ) ) data_format=tf_data_format ) elif len ( args ) == 2 : before declaring one epoch finished and starting the from .base_layer import Layer , InputSpec if output_shape is None : # weighted_mse = [ 5 * 1.2 , 52 * 0.5 ] = [ 6 , 26 ] of the input . return _reshape_dummy_dim ( x , reduce_axes ) assert isinstance ( node.output_masks , list ) base_config [ 'cropping ' ] = base_config [ 'cropping ' ] [ 0 ] x_aggregate = concatenate ( xs , axis=1 ) check_two_tensor_operation ( 'in_test_phase ' , ( 3 , 3 ) , ( 2 , 2 ) , WITH_NP , assert dense_layer.get_losses_for ( a ) == [ 0 ] ValueError : in case of improperly formatted user-provided data . x = C.transpose ( x , ( 2 , 0 , 1 ) ) x.value = value validation_data : This can be either current : Index of current step . self.inputs.append ( placeholder ) self.merge_mode = merge_mode skip_target_masks= [ l is None for l in self.loss_functions ] , layer_test ( legacy_layers.MaxoutDense , 'training , i.e . ` steps_per_epoch ` ' if any ( styles ) : the names of custom losses / layers / etc to the corresponding `` `` '' Returns the TF session to be used by the backend . A list of Numpy arrays . # Exit early if the reduction does n't have a denominator . for pooling_class in [ layers.GlobalMaxPooling3D , > > > K.shape ( inputs ) .eval ( session=tf_session ) def test_embedding_legacy_interface ( ) : # Set initial config # TH input shape : ( samples , input_depth , conv_dim1 , conv_dim2 , conv_dim3 ) cudnn_model = _make_nested_model ( input_shape , cudnn_layer ) for instance with a ` Conv2D ` layer : `` `` '' Functional interface to the ` Maximum ` layer . biases = np.sum ( np.split ( weights [ 2 ] , 2 , axis=0 ) , axis=0 ) @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , def check_num_samples ( ins , stateful A padded 4D tensor . `` `` '' Adadelta optimizer . padding : string , ` `` same '' ` , ` `` causal '' ` or ` `` valid '' ` . self.recurrent_kernel_r = self.recurrent_kernel [ : , # bring y 's dimension to be reduced to axis 1 . model.add ( keras.layers.Dense ( 10 , If False , the model weights obtained at the last step of num_thresholds = len ( thresholds ) old_layer = keras.layers.AtrousConvolution2D ( losses = self.call ( y_true , y_pred ) `` `` '' Provides a unique UID given a string prefix . vhats = [ K.zeros ( K.int_shape ( p ) , ' a generator or Sequence instance . Instead pass targets ' The new output shape with a ` -1 ` replaced with its computed value . A tensor , dot product of ` x ` and ` y ` . [ pytest ] new_weights = model.layers [ i ] .get_weights ( ) ` `` input '' ` : images identical to input images ( mainly used to epoch : Integer , the number of the training epoch just completed . weights = convert_weights ( weights , from_cudnn=True ) 'inbound_nodes ' : filtered_inbound_nodes , outputs_info= [ initial_output ] + initial_states , elems = elems [ : -1 ] expand_nested=False , pass mock.return_value = ' y ' 'batch_size ' , This metric keeps the average cosine similarity between ` predictions ` and if variables_to_update is None : dilation rates for the separable convolution . if self.model.stop_training : bottom_pad = kwargs [ 'padding ' ] .get ( 'bottom_pad ' , 0 ) self.cell.build ( [ step_input_shape ] + constants_shape ) strides = ( 1 , ) + strides if isinstance ( origin_layer , InputLayer ) : self.input_spec = None if hasattr ( self.forward_layer , 'constraints ' ) : validation_data=validation_data , input_length = K.shape ( inputs ) [ 1 ] self.write_graph = write_graph return new_weights 'recurrent_initializer ' : elif weights_rank - y_pred_rank == 1 : generic_utils.check_for_unexpected_keys ( 'loss ' , loss , output_names ) # and ` y_set ` are the associated classes . dtype=np.float32 ) global _FLOATX def reset_states ( self , states=None ) : def call ( self , inputs , training=None ) : label_is_pos , K.zeros_like ( label_is_pos , dtype=label_is_pos.dtype ) ) raise RuntimeError ( 'stop called on unstarted tf_file_io_proxy ' ) for target_dim , out_dim in zip ( y.shape [ 1 : ] , shape [ 1 : ] ) : import pytest validation_freq=validation_freq ) history = self.model.fit ( x , y , * * fit_args ) `` `` '' Element-wise minimum of two tensors . size * . new_states += cell_states 'provided weight shape ' + str ( w.shape ) ) raise ValueError ( 'Do not pass inputs that mix Numpy ' in ` data_format= '' channels_last '' ` . computed . 'For example with ` pip install pydot ` . ' ) ( 0.1 , None , 0.8 ) , # set alpha and threshold keys in ` logs ` ( passed in ` on_epoch_end ` ) . embeddings_freq = 0 return tf.cast ( x , dtype ) mean , 'forgot to call ` super ( YourClass , self ) .__init__ ( ) ` . ' raise ValueError ( 'Layer # ' + str ( k ) data_format = backend.image_data_format ( ) raise ValueError ( 'CNTK Backend : Unsupported dtype : % s . ' and weight histograms for the layers of the model . If set to 0 , ( ( 5 , 4 , 6 , 10 ) , ( 5 , 4 , 6 , 2 ) , 3 ) , if subgraph : 'Use ` input_shape ` instead . ' , stacklevel=3 ) training will stop when the quantity Learn [ more about embeddings ] ( `` `` '' Retrieves the model 's updates . self.recurrent_kernel [ : , self.units : self.units * 2 ] ) source_filepath : String , path to the file on filesystem or object on GCS to layer_indices = { } # dict { layer : index in traversal } for k in range ( 1 , 2 if K.backend ( ) == 'cntk ' else ( num_classes + 1 ) ) : fill_mode='nearest ' ) # This ` fit ` call will be distributed on 8 GPUs . input_shape = ( timesteps , steps , input_size ) output_shape = input_shape [ 0 ] [ 1 : ] trainable=False , 'kernel_size ' : 3 , kernel_constraint=kernel_constraint , else a symbolic loop will be used . filepath : The location to check . For example , if ` y_true ` is [ 0 , 1 , 0 , 0 ] and ` y_pred ` is [ 1 , 1 , 0 , 0 ] # Wait for them to complete z = K.eval ( z ) Output tensor . z3 = z [ : , 3 * self.units : ] from tensorflow.keras.layers import Dense batch_0 = data_gen [ 0 ] return_sequences , def test_upsampling1d_legacy_interface ( ) : def load_from_binary_h5py ( load_function , stream ) : nodes_depths = { } # dict { node : depth value } return self.function ( inputs , * * arguments ) self.filters , rows , cols ) a : Python integer . val = np.random.random ( input_shape ) - 0.5 kernel_regularizer=kernel_regularizer , if 'backend ' in model_weights_group : ask the user with a manual prompt . from .pooling import AvgPool2D self.activity_regularizer ( x ) steps=None , return tf.concat ( weights + biases , 0 ) sequences= [ L , L [ : :-1 , : :-1 ] ] , warnings.warn ( 'The ` nb_epoch ` argument in ` fit ` ' ' { } '.format ( [ spec.shape for spec in self.state_spec ] , `` ` use_multiprocessing=False , workers > 1 ` . '' return metrics # and outputs , nor their shapes and names . self.wait += 1 _test_optimizer ( optimizers.Adadelta ( decay=1e-3 ) , target=0.6 ) `` `` '' Returns the shape of tensor or variable as a tuple of int or None entries . for metric in metrics : p_t = ( p - self.learning_rate * m_t_bar / ( K.sqrt ( v_t_prime ) nodes , nodes_by_depth , layers , layers_by_depth = _map_graph_network ( ' ` set_session ` is not available when ' batches = make_batches ( num_train_samples , batch_size ) self.beta , result_t = metric_obj.result ( ) return x.size if indexes ! = sorted ( indexes ) : 'CNTK backend warning : CNTK version not detected . ' class BatchNormalization ( Layer ) : x = K.placeholder ( ndim=len ( x_shape ) ) def get_test_func ( ) : # to make the recurrent layer work . x += reshape ( bias , ( 1 , bias_shape [ 0 ] , 1 , 1 ) ) process_node ( layer , node_data ) if len ( weight_values ) ! = len ( symbolic_weights ) : axis : axis along which to normalize . self.model.save_weights ( filepath , overwrite=True ) `` `` '' Sets the metric attributes on the model for all the model outputs . '' '' '' return training_arrays.test_loop ( self , f , ins , target = C.reshape ( target , output.shape ) x = K.random_uniform ( shape , -limit , limit , # List of layer instances . If you never set it , then it will be ` 'channels_last ' ` . # If the inputs have different ranks , we have to reshape them return C.reshape ( x , shape ) 'you can not use ` validation_split ` . ' ) 'but its input shape can not be ' d1 = h5file_ [ 'data1 ' ] [ : ] x = repeat_elements ( x , height_factor , axis=1 ) self.name + ' : expected max_ndim= ' config = { 'learning_rate ' : float ( K.get_value ( self.learning_rate ) ) , ` ( samples , timesteps , rows , cols , channels ) ` if data_format='channels_last ' . if weights is not None : from .load_backend import shape # If we 've seen this layer before at a higher depth , output_tensors=output_tensors , callbacks._call_batch_hook ( 'test ' , 'begin ' , steps_done , batch_logs ) Connect current layer with last layer from tensor : and/or metrics ) . The attribute ` model.metrics_names ` will give you ref = np.arange ( np.prod ( shape ) ) .reshape ( shape ) weighted : Boolean indicating if the given metric is weighted . if from_cudnn : 'because it has no loss to optimize . ' ) init : name of initialization function for the weights of the layer embedding_input ) outputs = keras.layers.SimpleRNN ( legacy_upsampling2d_support = generate_legacy_interface ( return shape1 # reserve 'index_from ' ( =3 by default ) characters : hh = self.activation ( x_h + recurrent_h ) # shape has ` None ` entries return x / norm values to ` sk_params ` . old_layer = keras.layers.Dropout ( p=3 , name='drop ' ) is simply a Network with added training routines . return tf.eye ( n , m , dtype=dtype , name=name ) except LookupError : str ( x_shape ) ) return ( proba > 0.5 ) .astype ( 'int32 ' ) assert len ( model1.updates ) == 2 constant , def call ( self , inputs , states , training=None ) : add_edge ( dot , name , layer_id ) return [ None for _ in range ( num_states ) ] check_rnn_operation ( step_function_k=get_step_function ( K , wi_k , wh_k ) , # no activation , this layer is only linear . ` ( samples , timesteps , channels , rows , cols ) ` if data_format='channels_first ' `` `` '' Reverses a tensor along the specified axes . hash -r return tf.greater_equal ( x , y ) self.specificity = specificity ] ) return C.sqrt ( x ) # the inbound_nodes of outbound_layer . dilation_rate= ( 1 , 1 , 1 ) ) : layer.forward_layer.add_loss ( 0 , inputs=x ) the entire layer graph is retrievable from that layer , for data_format in [ 'channels_first ' , 'channels_last ' ] ` metric = log ( ( exp ( x ) + exp ( -x ) ) /2 ) ` , where x is the error ( y_pred - y_true ) group , 'layer_names ' , [ layer.name.encode ( 'utf8 ' ) for layer in layers ] ) # We keep the string that the user has set in compile as the metric name . self.recurrent_kernel_f ) recurrent_h = K.dot ( r * h_tm1 , If tuple , the first element 'has been ignored . Use ` unit_forget_bias=True ` ' kernel_dim3=4 , Cell = namedtuple ( 'cell ' , 'state_size ' ) self.build ( ) self.wait = 0 deserialized = [ ] strides , if not hasattr ( self.model.optimizer , 'lr ' ) : warnings.warn ( 'Skipping loading of weights for ' dropout=0. , # identically and still train properly output_tensors , assert len ( layer.get_losses_for ( x ) ) == 1 # a step function that just outputs inputs , # Network instance . # Pick the one with the correct shape . 'weights ' , self.input_bias_z = self.input_bias [ : self.units ] # as you are adding layers : def output_mask ( self ) : raise ValueError ( 'The RNN cell should have ' assert 12 * 5 < = len ( val_seq.logs ) < = ( 12 * 5 ) + 2 # the queue may be full . from .load_backend import std stddev : A float , standard deviation of the normal distribution `` `` '' Update the value of ` x ` to ` new_x ` . return permute_dimensions ( output , ( 0 , 2 , 1 ) ) ` fit ` , ` predict ` , ` predict_proba ` , and ` score ` methods weights : Weights to be applied on the current output . elif output_shape_type == 'lambda ' : # Set values . assert_not_compatible ( gru ( ) , gru ( cudnn=True ) , 'To call ` multi_gpu_model ` with ` gpus= % s ` , ' return -loss [ 0 ] if isinstance ( ins [ -1 ] , int ) : if isinstance ( x , list ) : dtype = None section `` Note on passing external constants '' below . pointwise_regularizer : Regularizer function applied to K.set_learning_phase ( 2 ) assert len ( padding ) == 2 if isinstance ( gpus , ( list , tuple ) ) : prev_output = states [ 0 ] recurrent_regularizer : Regularizer function applied to return inputs str ( inputs ) ) for k in WITH_NP : test_cases.append ( [ ( None , 4 ) , ( None , 3 , 4 ) , ( 1 , 2 ) ] ) if isinstance ( self.optimizer , tf.keras.optimizers.Optimizer ) : a record of training loss values and metrics values https : //arxiv.org/abs/1402.3337 ) def transform_kernels ( kernels , func , n_gates ) : sudo pip install keras ins : list of tensors to be fed to ` f ` . save_weights_only : if True , then only the model 's weights will be return self.cooldown_counter > 0 scale /= max ( 1. , fan_out ) # for the last sample , the final timestep ( in masked region ) should be the def should_run_validation ( validation_freq , epoch ) : if dp_mask is not None : from tensorflow.keras.datasets.mnist import load_data assert mse_obj.name == 'mean_squared_error ' return untar_fpath pattern = tuple ( pattern ) for m in metrics : `` `` '' self.recurrent_dropout , if 'KERAS_HOME ' in os.environ : # dense to sparse tensors and also wraps up the beam search code that is assert len ( layer.get_updates_for ( x ) ) == 2 constants=constants_np , # Handle mask propagation . > > > K.int_shape ( xy ) def argmin ( x , axis=-1 ) : val = np.asarray ( val ) if not validate_file ( fpath , file_hash , algorithm=hash_algorithm ) : A single tensor or a list of tensors ( depending on the passed argument ) dimension 1 of x has been summed over . ( dot_axes [ 0 ] = 1 ) where each sample is a sequence of 10 vectors of 16 dimensions . x = C.convolution ( depthwise_kernel , x , if target_class in [ 'LSTM ' , 'CuDNNLSTM ' ] and len ( weights ) == 3 : raise ValueError ( ' ` input_length=1 ` is not ' output_masks = layer.compute_mask ( computed_tensor , from .generic_utils import get_custom_objects weights [ 6 ] ] , axis=-1 ) they will be returned as a list . is specified . Total number of steps ( batches of samples ) kwargs [ 'mask ' ] = computed_mask feed_input_names = self._feed_input_names # growing to avoid prefix issues . w , norm = K.sqrt ( sum ( [ K.sum ( K.square ( g ) ) for g in grads ] ) ) innerfunction = [ inspect.getsource ( x ) for x in member.__code__.co_consts if skipif_no_tf_gpu = pytest.mark.skipif ( def _is_input_shape_compatible ( input , placeholder ) : for k in test_backend : raise ValueError ( 'Axis 2 of input should be fully-defined . ' def test_random_binomial ( self ) : initializers.serialize ( self.embeddings_initializer ) , ' at node ' + str ( node_index ) 'Found : ' + str ( padding ) ) super ( RepeatVector , self ) .__init__ ( * * kwargs ) if _is_gcs_location ( filepath ) : @ pytest.mark.parametrize ( 'layer_class , args ' , [ if isinstance ( path , h5py.Group ) : ` precision ` , an idempotent operation that simply divides ` true_positives ` Note that all kernels in Keras are standardized on the self.epochs_since_last_save += 1 return filename in self.local_objects output_mask_shape = self._get_shape_tuple ( > > > K.is_keras_tensor ( np_var ) # A numpy array is not a symbolic tensor . sample_weight_mode : sample weight mode user input passed from compile API . within [ -limit , limit ] , with ` limit = sqrt ( 3 * scale / n ) ` . super ( Conv1D , self ) .__init__ ( # model.compile ( optimizer , loss= [ 'mse ' , 'mae ' , 'mape ' ] ) if print_fn is None : unzip ./openmpi_1.10-3.zip begin_index = [ 0 for _ in cntk_axes ] specifying the strides of the convolution output_a = keras.layers.Dense ( output_dim_a ) ( c ) x : Keras tensor ( or variable ) . Keras is a deep learning API written in Python , running on top of the machine learning platform TensorFlow . if not K.is_tensor ( y_pred ) : if pattern [ 0 ] > 0 : assert b_2._uses_learning_phase def test_sequential_as_downstream_of_masking_layer ( ) : str ( node.arguments ) xi = squeeze ( xi , 0 ) t , f = cntk_func_tensors ( function_name , [ x_shape , y_val ] , * * kwargs ) epsilon=self.epsilon ) ( algorithm == 'auto ' and len ( file_hash ) == 64 ) ) : def __call__ ( self , shape , dtype=None ) : ' { `` sum '' , `` mul '' , `` ave '' , `` concat '' , None } ' ) tensors = ( self.model.inputs input_masks = to_list ( input_masks ) the batch size , or 1 if that can not be determined . layer = self._output_layers [ i ] has_return = re.findall ( r '' \s * return \S+ '' , code , re.MULTILINE ) validation_split : Float between 0 and 1 . if monitor_value is None : if isinstance ( obj , np.ndarray ) : assert m._uses_learning_phase numdigits = int ( np.floor ( np.log10 ( self.target ) ) ) + 1 if kernel_shape [ 4 ] % 2 == 0 : the model ) . assert ( 10 , ) == kx.shape optimizer_weights_group [ name ] = val then_expression = tf.scalar_mul ( c / n , g ) return if nones > ndim : if by_name : base_config = super ( AUC , self ) .get_config ( ) j * stride_col + kernel_size [ 1 ] ) child_input_shape = ( input_shape [ 0 ] , ) + input_shape [ 2 : ] tanh is applied by default . A tuple ( last_output , outputs , new_states ) . self._base_init ( name=name , * * kwargs ) y = k.print_tensor ( x , 'msg ' ) print ( 'Train on % d samples , validate on % d samples ' % wh_k = K.variable ( wh ) output_rank=len ( reference.shape ) - 1 ) # We do not need the init since it 's threads . generic_utils.check_for_unexpected_keys ( 'loss_weights ' , loss_weights , name = sub_w_first_node [ layer.layer.name ] .get_name ( ) filepath = self.filepath.format ( epoch=epoch + 1 , * * logs ) at their default values . prediction , or output of the softmax shape , minval=minval , maxval=maxval , dtype=dtype , seed=seed ) input_tensor._keras_history = ( self , 0 , 0 ) model.fit_generator ( train_gen , initial_epoch=0 ) : expand_dims = np.expand_dims data_format : String , one of ` channels_first ` , ` channels_last ` . from .load_backend import resize_volumes weight_values = [ np.asarray ( g [ weight_name ] ) for weight_name in weight_names ] losses = self.layer.get_losses_for ( None ) from .core import SpatialDropout1D model.add ( LocallyConnected2D ( 64 , ( 3 , 3 ) , input_shape= ( 32 , 32 , 3 ) ) ) # We track the instance using the metadata on the result tensor . raise ValueError ( ' A ` Subtract ` layer should be called ' If a list , it is expected to have a 1:1 mapping For the last case , ` validation_steps ` must be provided . decode_pred_np , log_prob_pred_np = KNP.ctc_decode ( inputs , # x : ( b_size , ... , d ) `` `` '' Called at the beginning of a batch in ` evaluate ` methods . cell = LSTMCell ( units , p = 0.5 output_shape = ( input_shape [ 0 ] , rows , cols , self.filters ) shuffle : Boolean . Whether to shuffle the order of the batches at h_pad = pool_size [ 1 ] - 2 if pool_size [ 1 ] % 2 == 1 else pool_size [ 1 ] - 1 have different performance profiles on different hardware and 'to perform validation ' shapes.append ( None ) # assert len ( model.losses ) == 8 ( batch_size , new_rows , new_cols , filters ) val_x , val_y , val_sample_weight ) sample_weights = training_utils.standardize_sample_weights ( self.kernel_size , return [ x ] `` `` '' Non-fused , broadcast version of ` normalize_batch_in_training ` . raise ImportError ( 'The use of HDF5Matrix requires ' mock_fio.__enter__ = Mock ( return_value=mock_fio ) 'compare with other backend . ' ) __Modularity.__ A model is understood as a sequence or a graph of standalone , fully configurable modules that can be plugged together with as few restrictions as possible . In particular , neural layers , cost functions , optimizers , initialization schemes , activation functions and regularization schemes are all standalone modules that you can combine to create new models . original_backend ) # deal with Theano API inconsistency output = C.sigmoid ( output ) tuple ` ( x_val , y_val ) ` of Numpy arrays or tensors in your model , you would need to specify the input length ` fit ` /etc . if mask is not None : self.add_update ( layer.get_updates_for ( None ) , None ) y_pred : ` y_pred ` argument of ` fn ` . return args , kwargs , [ ] self.from_logits = from_logits g._apply_device_functions ( op ) `` `` '' Element-wise log . 'name ' , raise ValueError ( indices : An integer tensor of indices . dim2_padding = conv_utils.normalize_tuple ( padding [ 1 ] , 2 , dim_ordering='tf ' , self.strides = conv_utils.normalize_tuple ( strides , 3 , 'strides ' ) # Prepare list loss weights , same size of model outputs . sparse_categorical_accuracy , name , dtype=dtype ) if x_ndim == 2 : self.write_images = write_images tmp = [ x ] * rep output_shape = [ None for _ in input_shape ] from markdown import markdown This argument is required if you are going to connect types = ( source , target ) if max_value is not None : alpha : float . Slope of the negative part . Defaults to zero . self.normalize = normalize parallel_model.fit_generator ( return_state , def is_wrapped_model ( layer ) : assert 'group1 ' in f verbose : verbosity mode . callbacks._call_batch_hook ( 'predict ' , 'end ' , step , batch_logs ) end_index = [ 0 for _ in cntk_axes ] self.start ( ) from .. layers import deserialize 'The RNN was passed : ' , cell ) 'All inputs should only appear once . ' weights : Weights tensor . There are lot of edge cases which have been hardcoded , self._built = value before declaring one epoch finished and starting the line_length=line_length , lambda acc , x : acc + x super ( Conv3D , self ) .__init__ ( from .callbacks import LearningRateScheduler def test_doc ( ) : output = permute_dimensions ( output , ( 0 , 2 , 3 , 1 ) ) Can be a ` Tensor ` whose rank is either 0 , k_d = K.eval ( K.dot ( K.variable ( x_dense ) , t_W ) ) name='categorical_hinge ' ) : 'it should have one entry per model output . ' fan_in = shape [ 1 ] * receptive_field_size `` `` '' Cropping layer for 2D input ( e.g . picture ) . if isinstance ( _SEQUENCE_COUNTER , int ) : # Random entry added in at time=5 name='conv ' ) steps_axis = 1 if self.data_format == 'channels_last ' else 2 `` `` '' Instantiates a variable with values drawn from a uniform distribution . conv_out = _postprocess_conv3d_output ( conv_out , x , padding , # if obj is a python 'type ' 'config ' : layer_config , def plot_model ( model , self.metrics_func = C.combine ( self.metrics_outputs ) ' supported when ` unroll=True ` . ' ) if not batch_shape and tensor is None : self.name + ' : expected min_ndim= ' input_shape : Use this argument to specify the shape of the ImportError : If h5py is not available . # Private TF Keras utils class Maximum ( _Merge ) : # Keep track of the network 's nodes and layers . targets : list of Numpy arrays of targets . _axis = [ ] self.bias_constraint = constraints.get ( bias_constraint ) model.compile ( loss=custom_loss , optimizer='rmsprop ' , metrics= [ 'acc ' ] ) ( ie . `` linear '' activation : ` a ( x ) = x ` ) . elif dtype == np.float64 : kwargs [ new_name ] = value z1 = z [ : , self.units : 2 * self.units ] split_at = int ( int ( x [ 0 ] .shape [ 0 ] ) * ( 1 . - validation_split ) ) callbacks._call_end_hook ( 'train ' ) return normalized , mean , variant function if a real GCS bucket is not available for testing . batch_size=2 ) layer : a ` Layer ` instance . updates = self.updates + training_updates inputs , initial_state , constants = _standardize_args ( def __setattr__ ( self , name , value ) : print ( 'Downloading data from ' , origin ) channel_axis = 1 def __init__ ( self , monitor='val_loss ' , factor=0.1 , patience=10 , outputs , h = self._cudnn_gru ( if sample_weight_mode == 'temporal ' : steps_name='steps ' ) inputs , states = cell.call ( inputs , states , * * kwargs ) state_updates += layer.updates callbacks = cbks.CallbackList ( _callbacks ) 'which has { } dimensions . '.format ( len ( int_shape ( output ) ) ) ) ) If a function , it specifies the entire shape as a function of the Do not use in a model -- it 's not a functional layer ! Default parameters follow those provided in the paper . from .recurrent import StackedRNNCells ( single state ) in which case it is # Handle target tensors if any passed . constraint=self.b_constraint ) 'dropout ' : self.dropout , # Call layer on its inputs , thus creating the node val_enqueuer_gen = val_data # # On top of new , non-Keras tensors have multiple tensor outputs , with each one being ( one size per state ) . In this case , the first entry last_output = outputs [ i ] [ 0 , 0 ] ] from .merge import Add x = tf.nn.bias_add ( x , bias , # Update ` count ` for reductions that require a denominator . # We need to change the order of the arguments because theano accepts x as vs = [ K.zeros ( shape , name='v_ ' + str ( i ) ) future = executor.apply_async ( get_index , ( self.uid , i ) ) sample_weight_mode= '' temporal '' in compile ( ) . `` `` '' Resizes the images contained in a 4D tensor . c = f * c_tm1 + i * self.activation ( z2 ) os.path.join ( self.log_dir , return tf.reshape ( x , shape ) ` ( symmetric_height_crop , symmetric_width_crop ) ` . @ interfaces.legacy_convlstm2d_support result = tf.matmul ( x , y ) from .. utils.generic_utils import slice_arrays raise_in_sub = [ ret for code_inner in innerfunction for ret in model_inputs = [ ] inputs += [ K.learning_phase ( ) ] strides= ( 2 , 2 ) , inputs = Input ( shape= ( timesteps , embedding_dim ) ) if scale < = 0. : self.bias = self.add_weight ( shape= ( self.units * 4 , ) , line = line [ : -1 ] + ' ' beta , ' ( left_dim3_pad , right_dim2_pad ) ) . ' layer.kernel_i , mask [ 1 , 0 ] = 0 parallel_model = multi_gpu_model ( model , gpus= [ 0 ] ) assert sorted ( config.keys ( ) ) == sorted ( new_config.keys ( ) ) target_tensors=None , for x in losses : return tf.nn.sigmoid ( x ) from .load_backend import ones if self.write_grads and weight in layer.trainable_weights : reverse : Boolean : if ` true ` , timesteps in each output sample will be current_layout = tuple ( [ i for i in range ( dims ) ] ) given by ` epochs ` , but merely until the epoch activation='relu ' , elif dims == 2 : bias = np.concatenate ( [ weights [ 2 ] , self._feed_inputs.append ( v ) ch_dim = 3 from .load_backend import sparse_categorical_crossentropy # ` epoch ` is 0-indexed internally but 1-indexed in the public API . h5py.File or h5py.Group object where to save the model batches = make_batches ( num_samples , batch_size ) return K.relu ( x , alpha=alpha , max_value=max_value , threshold=threshold ) validation_steps=None , travis_retry conda install -q $ MKL pydot graphviz $ PIL if len ( args ) > 4 : return resnet_v2.ResNet101V2 ( * args , * * kwargs ) config [ 'pointwise_regularizer ' ] = ( return return ( isinstance ( loss , CategoricalCrossentropy ) or Loss values per sample . If ` data_format ` is ` `` channels_last '' ` : states = [ ] if self.bias : new_model = keras.models.clone_model ( new_x = np.random.random ( ( 3 , 4 ) ) using ` one-hot ` representation , please use ` CategoricalCrossentropy ` metric . model_config = h5dict [ 'model_config ' ] depth_factor=2 , cntk_axis.append ( i - dynamic_axis_index ) weights : A list of Numpy arrays with shapes and types matching from .multi_gpu_utils import multi_gpu_model self._per_input_updates = { } ValueError : if ` axis ` is neither -1 nor one of def standardize_class_weights ( class_weight , output_names ) : addopts=-v ' ` class_weight ` . ' # then the model should not be fed any inputs and targets . if num_chunks > 1 : git clone https : //github.com/keras-team/keras.git This method is used for both writing to HDF5 file/group , assert c_2._uses_learning_phase elif x.ndim == 4 : 'op , input_shape , kernel_shape , padding , data_format , dilation_rate ' , [ https : //en.wikipedia.org/wiki/Riemann_sum ) base_config = super ( GaussianNoise , self ) .get_config ( ) if isinstance ( cell , Layer ) : for p , g , a , d_a in zip ( params , grads , accumulators , delta_accumulators ) : Strides values . compute pairs of recall and precision values . The area under the ROC-curve is return obj.tolist ( ) data_format : A string , return monitor_value def _preprocess_conv2d_depthwise_kernel ( kernel , kernel_shape , data_format ) : ` batch_input_shape= ( ... ) ` to the first layer in your model . raise ValueError ( msg ) for x in output_ls : old_layer = keras.layers.ZeroPadding3D ( ( 2 , 2 , 2 ) , raise ValueError ( 'Can not do batch_dot on inputs ' conversions= [ ] , ` ( inputs , targets , sample_weights ) ` . The field is used only if the payload is sent within a form keepdims=True ) @ pytest.mark.parametrize ( 'x_shape ' , [ ( 1 , 4 , 2 , 3 ) , ( 1 , 2 , 3 , 4 ) ] ) def categorical_crossentropy ( target , output , from_logits=False , axis=-1 ) : if shape [ i + num_dynamic ] is None : loop through each dimension in ` x ` 's shape and ` y ` 's shape : layers : A list of target layers . # initial_state was passed in call , check compatibility rep : Python integer , number of times to repeat . from .input_layer import Input y = np.random.randn ( 2 , 4 , 3 ) allowed_positional_args= [ 'input_dim ' , 'output_dim ' ] , list of numpy arrays representing _test_optimizer ( optimizers.Adadelta ( lr=1. , decay=1e-3 ) , target=0.4 ) def __init__ ( self , x_set , y_set , batch_size ) : inputs = [ ] return resnet.ResNet50 ( * args , * * kwargs ) return [ None for _ in range ( len ( names ) ) ] To enable statefulness : if x is None or len ( x ) == 0 : self.pool_size [ 0 ] , depthwise_kernel_shape = depthwise_kernel.eval ( ) .shape Output will always be a list of tensors maxval : A python scalar or a scalar tensor . Upper bound of the range L = T.log ( smoothed ) node_key = _make_node_key ( layer.name , node_index ) # in native cntk op self.data.clear ( ) requires that the ` cell.call ` method accepts the same keyword argument return T.nnet.categorical_crossentropy ( output , target ) steps_per_epoch=None , 'implementation ' : self.implementation , embeddings_metadata = { layer_name : self.embeddings_metadata # set library path ` `` raw '' ` : numpy array of values in ` y_col ` column ( s ) , layer.reset_states ( ) cache_dir : Location to store cached files , when None it 'layer in a model . '.format ( # m_schedule to 1 . echo -e `` Running tests with the following config : \n $ ( cat ~/.keras/keras.json ) '' # the inner layer has update ops that depend on its inputs ( as opposed if self._num_constants is not None : `` `` '' Returns the value of a tensor . embedding_input = self.model.get_layer ( layer.name ) .output # Calculate specificities at all the thresholds . feed_dict [ y_placeholder ] = 4 . from .load_backend import in_top_k def test_io_utils ( in_tmpdir ) : def test_sequential_model_pickling_custom_objects ( ) : if hasattr ( layer , '_batch_input_shape ' ) : name=None , if isinstance ( obj , np.ndarray ) : super ( MeanSquaredLogarithmicError , self ) .__init__ ( 'or use the TensorFlow backend . ' ) `` `` '' # backend information not available output = permute_dimensions ( output , ( 2 , 3 , 0 , 1 ) ) current , tuple ( states ) + tuple ( constants ) ) inputlabels = str ( layer.input_shape ) `` `` '' Wrapper for using the Scikit-Learn API with Keras models . '' '' '' 'stddev ' : self.stddev , if None not in input_ndims : return output_mask return source_tensors return kernel > > > val = np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) index_array = index_array [ : batch_count * batch_size ] loop_vars = { classes=num_classes ) elif ( not isinstance ( self.build_fn , types.FunctionType ) and act = self.activation ( y ) shape = tuple ( x if isinstance ( x , int ) and x > 0 else None for x in shape ) loss=loss , # ` feed_dict ` arguments . Note that the feed_dict is passed once in the from .load_backend import gather # mixed symbolic/value inputs . # Legacy support . self.stateful_metrics = set ( stateful_metrics ) old_layer = keras.layers.UpSampling3D ( ( 2 , 2 , 2 ) , padding=padding , `` `` '' ` Input ( ) ` is used to instantiate a Keras tensor . self.bias_f = None This allows the layer to learn a convex , ( useful for resuming a previous training run ) [ logit ] ( https : //en.wikipedia.org/wiki/Logit ) values . By default , unique_tensors_ids = set ( ) K.resize_images ( K.variable ( xval ) , 2 , 2 , check_two_tensor_operation ( 'batch_dot ' , ( 32 , 20 ) , ( 32 , 20 ) , return self._output_mask_cache [ cache_key ] It is similar to an LSTM layer , but the input transformations 'class_name ' : instance.__class__.__name__ , shape= ( self.units , self.units ) , from .load_backend import size validation run is performed , e.g . ` validation_freq=2 ` runs config [ 'cell ' ] = { 'class_name ' : self.cell.__class__.__name__ , ValueError : in case ` maxlen ` is so low self.padding , `` `` '' Callback that records events into a ` History ` object . `` `` '' Proxy for tensorflow.python.lib.io.file_io.delete_file function . Mocks def test_identity ( tensor_shape ) : return densenet.preprocess_input ( * args , * * kwargs ) Model cloning is similar to calling a model on new inputs , from .convolutional import SeparableConv2D batch_input_shape = K.int_shape ( input_tensor ) loss=self.total_loss ) the padding dimension ( ` ( left_pad , right_pad ) ` ) . ` ( batch , channels , conv_dim1 , conv_dim2 , conv_dim3 ) ` # possible . # toy label matrix ( 2 samples , 3 classes ) * ` SUM ` : Scalar sum of weighted values . mock_fio.write = self.local_objects [ filepath ] .write tensor : Optional existing tensor to wrap into the ` Input ` layer . if isinstance ( config.pop ( 'kernel_initializer ' ) @ contextmanager check_two_tensor_operation ( 'in_test_phase ' , ( 2 , 3 ) , ( 2 , 3 ) , WITH_NP , def input ( self ) : tf.summary.histogram ( ' { } _out'.format ( layer.name ) , from .. legacy.layers import Recurrent , ConvRecurrent2D for i in range ( len ( self.outputs ) ) self._per_output_metrics [ i ] , i ) ) if data_format='channels_last ' output_shape = ( batch_size , out_height , out_width , self.filters ) The loss function or None if ` identifier ` is None . if x.ndim == 4 : test_doc_with_arguments_as_last_block = { # Use mask as sample weight . `` `` '' Sets the default float type . ` ( samples , time , rows , cols , channels ) ` y_test = HDF5Matrix ( h5_path , 'my_labels ' , start=150 , end=200 ) Prefix to use for filenames of saved pictures x , _ = generator_output check_single_tensor_operation ( 'temporal_padding ' , ( 4 , 3 , 3 ) , positions=positions , WITH_NP , axes= ( 1 , 1 ) ) return tf_keras_backend.truncated_normal ( For Python 3 , checks if there is an argument with the given name , and input_tensor=tensor ) ' is not connected , no input to return . ' ) kernel_shape , def huber_loss ( y_true , y_pred , delta=1.0 ) : `` `` '' Training-related part of the Keras engine . 'Only expected the following keys : ' progbar = Progbar ( target=num_samples ) output_mask_int_shape = K.int_shape ( mask ) * * kwargs ) : because you lack features for these sample timesteps . You can do : 'LR ' creates a horizontal plot . class LogCosh ( LossFunctionWrapper ) : class_weight : Optional dictionary mapping # No need for try/except because or ` ( inputs , targets , sample_weights ) ` . num_batches = ( size + batch_size - 1 ) // batch_size # round up target_shape : target shape . Tuple of integers . targets : List of targets . for m in metrics : warnings.warn ( ' ` reverse_state_order=True ` in ` StackedRNNCells ` ' inputs = to_list ( inputs , allow_tuple=True ) layer.activity_regularizer is not None ) : layer.bias_o , raise ValueError ( 'Can not do batch_dot on inputs with shapes ' epsilon ) data_format : string , ` `` channels_last '' ` or ` `` channels_first '' ` . # Use case : model.add_metric ( metrics.Mean ( name='metric_2 ' ) ( y ) ) 'We found { } metrics with the name : `` { } '' '.format ( len ( match ) , name ) ) the most frequent words are kept return tf.expand_dims ( ctc.ctc_loss ( inputs=y_pred , if isinstance ( val , dict ) and val.get ( '_is_group ' ) : metrics_module.Mean ( name=self.output_names [ i ] + '_loss ' ) For a more in-depth tutorial about Keras , you can check out : str ( weights [ 0 ] .size ) + ' . ' ) indices = 1 , 2 self.built = True return res each 5D tensor with shape : first_layer = layers [ 0 ] self.input_bias_r = None stddev = np.sqrt ( scale ) / .87962566103423978 @ built.setter WITH_NP ) input_shape = ( 1000 , 10 ) def DISABLED_test_unweighted_high_sensitivity ( self ) : # has been inverted in TF 2 . if _SEQUENCE_COUNTER is None : from tensorflow.keras.datasets.fashion_mnist import load_data new_layer = keras.layers.MaxPool2D ( str ( 5 ) # New file format . isinstance ( x , C.variables.Constant ) ) : Note that you can manually set the global session TypeError : if ` elems ` is not a tensor . closure = tuple ( ensure_value_to_cell ( _ ) for _ in closure ) 'You should pass an input_shape or ' if not self.stateful : tf.summary.image ( mapped_weight_name , w_img ) model.build ( ) x_shape = shape ( x ) * * kwargs ) def _set_cell ( self , cell ) : fan_out = shape [ -1 ] * receptive_field_size check_single_tensor_operation ( 'softsign ' , ( 4 , 10 ) , WITH_NP ) v = np.asarray ( v ) elif bias_shape == ( units * n_gates , ) : self.gamma = self.add_weight ( shape=shape , weight_type + ' ` ' initial_state = K.sum ( initial_state , axis= ( 1 , 2 ) ) # ( samples , ) model.input_names ) raise ValueError ( ' ` padding ` should have 3 elements . ' you can also pass a dictionary Can be a single integer to specify the same value for steps_per_epoch = len ( generator ) return metrics_module.sparse_categorical_accuracy axis : integer , axis along which to calculate weight norms . h = x._keras_shape [ 1 ] + top_pad + bottom_pad if initial_state is None : input_shapes=input_shape , show_layer_names=True , kernel_regularizer=kernel_regularizer , end : int , end of desired slice of the specified dataset ' '' shape_key = layer.name + ' _ % s_ % s ' % ( node_index , tensor_index ) batch_size : integer . noise_shape : 1D integer tensor representing the shape of the if len ( states ) ! = len ( self.states ) : check_single_tensor_operation ( 'random_uniform_variable ' , ( 2 , 3 ) , WITH_NP , def test_shape_operations ( self ) : filename : filename of the csv file , e.g . 'run/log.csv ' . target_size= ( 256 , 256 ) , Can be a single integer to specify the same value for all `` `` '' Abstract fit function for ` fit_function ( fit_inputs ) ` . # If specified , use the supplied thresholds . dtype='float32 ' ) 'categorical_crossentropy ' , label , categorical_targets , warning . C.equal ( start = stride * i You can either pass a flat ( 1D ) name='conv ' ) self.on_epoch_end = lambda epoch , logs : None check_two_tensor_operation ( 'batch_dot ' , ( 4 , 2 ) , ( 4 , 2 , 3 ) , confusion_matrix_cond : One of ` metrics_utils.ConfusionMatrix ` conditions . ' '' with a weight list of length ' from the desire to use a transformation going in the opposite direction def data_format ( self ) : kernel_h , kernel_w = self.kernel_size str ( axis ) + ' of input shape to have ' v._keras_shape = value.shape from .load_backend import manual_variable_initialization In case of tie , the rounding mode used is `` half to even '' . [ 0. , 0. , 0 . ] ] , dtype=float32 ) `` `` '' Loads all layer weights from a HDF5 save file . name='predict_function ' , def dtype ( self ) : > > > K.shape ( kvar ) .eval ( session=tf_session ) # Some ops ( like dropout ) wo n't be applied during `` eval '' in cntk . shape : Target shape tuple . on_epoch_end : logs include ` acc ` and ` loss ` , and 'seed ' : self.seed } out = K.gather ( self.embeddings , inputs ) model_config = { z = K.batch_dot ( x , y , axes ) node = inbound_layer._inbound_nodes [ node_index ] [ np.identity ( shape [ 0 ] ) ] * ( shape [ 1 ] // shape [ 0 ] ) , axis=1 ) member.__module__ ) # taken into account by the backend . dot.add_edge ( pydot.Edge ( layer_id , w = np.transpose ( w , ( 0 , 1 , 3 , 2 ) ) super ( RemoteMonitor , self ) .__init__ ( ) ( integers from 0 to num_classes ) . check_single_tensor_operation ( 'any ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) dot.set ( 'label ' , model.name ) def __init__ ( self , gain=1. , seed=None ) : 'the time dimension by passing a ` shape ` ' broadcast_gamma , from .training_utils import make_batches `` `` '' Reduce ` elems ` by ` fn ` combined them from left to right on dimension 0 . def test_linear_operations ( self ) : This is the expected shape of your inputs * including the batch if isinstance ( defaults , list ) : 'The model has ' + str ( len ( output_names ) ) mask=mask , inputs , d_decoded = { } if source ! = target_class : # Raises # Ensure works for any dim input_dict [ argument ] = feed_dict [ argument ] x = x.dimshuffle ( ( 0 , 4 , 1 , 2 , 3 ) ) if y is not None : node_index = self._output_coordinates [ i ] [ 1 ] 5 , ( 3 , 3 ) , from .load_backend import conv2d_transpose for j in range ( output_col ) : To enable statefulness : config = { 'alpha ' : float ( self.alpha ) } If the final rank is 1 , we reshape it to ( batch_size , 1 ) . axes [ 1 ] += y_ndim Function , a Function to initialize the pool self.recurrent_kernel_o , The dimensions to which all images found will be resized . if name in weight_names : return_value = save_function ( h5file ) K.epsilon ( ) , variance = squeeze ( variance , _axes ) if not self.mock_gcs : skip_target_weighing_indices , loss_weights=loss_weights , else : # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING : function_outputs_list = [ f ( [ input_val ] ) [ 0 ] for f in f_list ] cell can also take the optional argument ` constants ` , see Set to 1 because 0 is usually the padding character . thresholds=thresholds , w = x._keras_shape [ 2 ] + padding [ 1 ] [ 0 ] + padding [ 1 ] [ 1 ] the axes to compute the variance . If ` None ` ( default ) , computes _serialize_model ( model , h5dict ) from .cifar import load_batch layer : Origin layer of the tensor . Will be 'It will be ignored . ' ) self.add_loss ( layer.get_losses_for ( None ) , None ) # False = test , True = train from . import backend as K return super ( Wrapper , self ) .get_losses_for ( inputs ) This can be useful to tell the model to `` pay more attention '' to y = K.spatial_3d_padding ( x , padding=padding , data_format='channels_last ' ) # a Model/Layer in graph mode . This metric instance will later be used return C.random.normal ( `` `` '' Convert a Keras model to dot format . all_inputs += to_list ( y , allow_tuple=True ) output_ls = to_list ( output ) `` `` '' Returns the full GCS bucket path '' '' '' K.greater ( self.true_negatives + self.false_positives , 0 ) , `` `` '' Runs a function in a temporary directory . # dropout matrices for input units g = f [ name ] x , raise ValueError ( 'CNTK backend : the permute pattern % s ' stride = K.concatenate ( [ step , input_shape * 0 ] , axis=0 ) 'In order to use timestep-wise sample weighting , ' [ 0.0357786 , 0.633813 , 0.321418 , 0.00249248 , 0.00272882 , 0.0037688 ] , sequences= [ y_true , y_pred , input_length , label_length ] 'training/testing . ' r = self.recurrent_activation ( x_r + recurrent_r ) `` `` '' Linear ( i.e . identity ) activation function . we assume that ` y_pred ` contains probabilities def model_to_dot ( model , inbound_layers= [ ] , warnings.warn ( 'Update your ` ' + object_name + ' ` call to the ' init='normal ' , K.ones_like ( p [ 1 : ] ) ) raise ValueError ( `` { } needs a ' # Returns ' section '' .format ( name ) , non_trainable_weights ( in this order ) . if self.reset_after : output = h + K.dot ( prev_output , self.recurrent_kernel ) layer.forward_layer.add_loss ( lambda : 1 ) input2 = keras.layers.Input ( shape= ( 32 , ) ) legacy_prelu_support = generate_legacy_interface ( paths = [ h5_path , dict ( ) ] # in cntk , need to remove those dummy axis . # Convert layers nested in Bidirectional/TimeDistributed/Model/Sequential . for x in previous_sources : def _preprocess_conv1d_input ( x , data_format ) : files = [ 'train-labels-idx1-ubyte.gz ' , 'train-images-idx3-ubyte.gz ' , def delete_file ( self , filename ) : > > > K.dtype ( K.placeholder ( shape= ( 2,4,5 ) ) ) def set_weights ( self , weights ) : `` `` '' Creates a ` Mean ` instance . return Subtract ( * * kwargs ) ( inputs ) tf_keras_backend.clear_session ( ) allowed_kwargs = { 'input_shape ' , Y_ = T.set_subtensor ( Y_ [ T.arange ( Y.shape [ 0 ] ) * 2 + 1 ] , Y ) If the output mask at each time step is ` None ` and # Compile model . generated = autogen.process_docstring ( dummy_docstring ) first_layer = first_layer.layers [ 0 ] A tensor ( or list of tensors if the layer has multiple inputs ) . from .load_backend import batch_normalization if 'backend ' in f.attrs : def has_arg ( fn , name , accept_all=False ) : return _UID_PREFIXES [ prefix ] for name , val in zip ( weight_names , weight_values ) : from tensorflow.keras.applications.mobilenet_v2 import preprocess_input If ` save_best_only=True ` , the decision target = target_tensors [ i ] # sample variance - unbiased estimator of population variance mode = 'concat ' bias_dims = len ( bias.shape ) _p_prev = T.inc_subtensor ( _p_prev [ 1 : ] , _p_prev [ : -1 ] ) # e.g . optimizer , layer ` ( batch_size , features ) ` from .training_utils import batch_shuffle input_tensors : list of input tensors . outputs = self.train_function ( ins ) path = get_file ( dirname , origin=origin , untar=True ) # If it is a single list we then apply all metrics to all outputs . pointwise_kernel = expand_dims ( pointwise_kernel , 1 ) ( the node gets created when the ` call ` # they can run in parallel . Also they should not contribute to output of len ( symbolic_weights ) , len ( weight_values ) ) ) def model_from_yaml ( yaml_string , custom_objects=None ) : def load_weights_from_hdf5_group_by_name ( f , layers , skip_mismatch=False , `` `` '' Called at the end of a batch in ` evaluate ` methods . self.bias_c_i = self.bias [ self.units * 2 : self.units * 3 ] intercept = self.true_positives [ 1 : ] - ( prec_slope * p [ 1 : ] ) if sys.version_info < ( 3 , ) : elif self.outputs : def less_equal ( x , y ) : recursion stack . Useful to detect cycles . neg = ( K.pattern_broadcast ( self.alpha , self.param_broadcast ) * self._layers = layers deserialized.append ( convert_custom_objects ( value ) ) x = Input ( batch_shape=batch_shape , beta_regularizer : Optional regularizer for the beta weight . assert len ( last_states_k ) == len ( last_states_np ) def clear_session_after_test ( ) : elif test_function_type == 'closured function ' : assert [ tuple ( s ) for s in model.compute_output_shape ( [ ( None , 32 ) , ( None , 32 ) ] ) ] == expected_shapes new_rows = ( ( rows - 1 ) * strides [ 0 ] + kernel_size [ 0 ] def __init__ ( self , name=None , dtype=None , * * kwargs ) : This metric creates one variable , ` total ` , that is used to compute the sum of f_skip_idxs , zeros , f_active , log_f_curr , log_f_prev ) compute_mask ( x , mask ) the columns specified in ` y_col ` . f.flush ( ) `` `` '' Sets Keras model and writes graph if specified . '' '' '' assert ndim ( x ) == 2 if input_length is None : where ` activation ` is the element-wise activation function target = 'GRU ( reset_after=False ) ' len ( self._inbound_nodes ) - 1 , C.variables.Constant ) or isinstance ( instead of accumulating all past gradients . This way , Adadelta continues A list of numpy arrays . @ pytest.mark.parametrize ( 'shape ' , [ ( 4 , 2 ) , ( 2 , 3 ) ] ) def set_floatx ( floatx ) : assert K.int_shape ( o ) == ( None , 8 , 5 ) directory , } , { return code , defaults , closure ( tensors returned by ` Input ` ) . output_tensor = layer ( self.outputs [ 0 ] ) # TH input shape : ( samples , input_depth , rows , cols , slices ) For instance , after a ` Conv2D ` layer with # ` sample_weight_mode ` matches output_names partially . output_dim = 1 labels = d [ label_key ] outs [ 0 ] += float ( batch_outs ) stop : Stop value . if validation_freq < 1 : class BaseLogger ( Callback ) : # T.nnet.bn.batch_normalization_test is deprecated assert_allclose ( out , new_out , atol=1e-05 ) legacy_conv3d_support = generate_legacy_interface ( self.samples_seen += logs [ 'size ' ] metrics= [ keras.metrics.SparseCategoricalAccuracy ( ) ] ) # Add nodes to all layers involved . recurrent_constraint=None , def categorical_crossentropy ( target , output , from_logits=False ) : siamese = keras.models.Model ( inputs= [ input1 , input2 ] , from .. import models stride=self.strides [ i ] , super ( TimeDistributed , self ) .build ( ) elif count_mode == 'steps ' : x_list = [ k.placeholder ( shape= ( 4 , 2 ) ) for k in [ KTH , KTF ] ] check_single_tensor_operation ( 'ones ' , ( 3 , 5 , 10 , 8 ) , shape = K.int_shape ( w_img ) will be updated during training . 'with different output shapes . Hence ' log_dir : the path of the directory where to save the log class GeneratorEnqueuer ( SequenceEnqueuer ) : assert mape_obj.reduction == losses_utils.Reduction.SUM if set ( kwargs [ 'padding ' ] .keys ( ) ) < = { 'top_pad ' , 'bottom_pad ' , ' outputs , but you passed a single tensor as ' Note that if the recurrent layer is not the first layer the pointwise kernel matrix 'via both kwarg and inputs list ' ) # Apply mask reachable = tf_utils.get_reachable_from_inputs ( [ learning_phase ( ) ] , 'Please install GraphViz ( https : //www.graphviz.org/ ) ' ' with axes= ' + str ( axes ) + ' . x.shape [ % d ] ! = ' output_shape , def set_image_data_format ( data_format ) : name= ' b ' , ' '' } losses = to_list ( losses ) index = self.samples_seen allowed_positional_args= [ 'pool_size ' , 'strides ' , 'padding ' ] , 'in the serialized model ' if not self.trainable : 'Note that input tensors are ' return tf.nn.tanh ( x ) > > > K.eval ( K.eye ( ( 2 , 3 ) ) ) val_enqueuer = None b_x = np.random.random ( ( num_samples , input_dim_b ) ) # Model is not compilable because variable._keras_shape = value.shape val_sample_weight = None > > > K.cast ( input , dtype='float16 ' ) unique_name = name + ' _ ' + str ( idx ) def __init__ ( self , cropping= ( ( 1 , 1 ) , ( 1 , 1 ) , ( 1 , 1 ) ) , go_backwards : Boolean . If True , do the iteration over the time class ConfusionMatrix ( Enum ) : h = h [ 0 ] self.kernel_i , self.on_train_begin = on_train_begin One : calling ` reset_states ` ( necessary since each inbound layer might inbound_layers=inbound_layers , check_single_tensor_operation ( 'pow ' , ( 4 , 2 ) , WITH_NP , a=3 ) output_shapes : list of output shape tuples . Stacked array of transformed kernels . If ` sample_weight ` is scalar , it is kept scalar . def noised ( ) : 'less than or equal to ' + str ( len ( y.shape ) ) ) output /= C.reduce_sum ( output , axis=-1 ) class GlobalAveragePooling2D ( _GlobalPooling2D ) : if len ( depth_keys ) > 1 : def result_wrapper ( result_fn ) : # Call the internal on epoch end . # This is isolated in its own method in order to use extension = 'png ' `` `` '' Encapsulates metrics that perform a reduce operation on the values . '' '' '' # is ` Sequential ` via ` isinstance ` `` `` '' Fashion-MNIST dataset . '' '' '' beta = zeros_like ( gamma ) y = tf.transpose ( y , pattern ) ( x_train [ ( i - 1 ) * 10000 : i * 10000 , : , : , : ] , shuffle=True , bias_initializer : Initializer for the bias vector return image.save_img ( path , assert len ( layer.updates ) == 0 List of shape tuples ( or single tuple ) , one tuple per input . import h5py x = K.truncated_normal ( shape , 0. , stddev , return T.argmax ( x , axis=axis , keepdims=False ) validation_data=val_generator ( ) , 'Compile it manually . ' ) steps_per_epoch = 100 from tensorflow.keras.layers import LeakyReLU independently manipulable ) . feed_dict = { _input : embeddings_data [ idx ] [ batch ] from .load_backend import squeeze if isinstance ( training , int ) or isinstance ( training , bool ) : output of get_config . __input_dim__ : dimensionality of the input ( integer ) . from .common import set_image_dim_ordering , image_dim_ordering `` `` '' Convenience method for verifying that a file exists after writing . '' '' '' You can set RNN layers to be 'stateful ' , which means that the states o = self.recurrent_activation ( x_o + K.dot ( h_tm1_o , # Keep track of losses that depend on the inputs updates.append ( ( self.states [ i ] , states [ i ] ) ) output channels are generated per input channel in the depthwise step . result = C.element_select ( training , x , alt ) return loss_functions from keras.utils import multi_gpu_model a block size in bytes , and the total size of the file . } return x.eval ( session=get_session ( ) ) step_function_np=get_step_function ( KNP , wi ) , shape= ( len ( self.thresholds ) , ) , @ pytest.mark.parametrize ( callbacks=None , `` `` '' Transposed convolution layer ( sometimes called Deconvolution ) . self.close ( ) verbose=1 , shape=True , `` `` '' Returns a tensor with truncated random normal distribution of values . input_shape [ 2 ] + left_pad + right_pad , slice_arrays ( y , split_at ) ) if steps_per_epoch : if is_np : x_shape = int_shape ( x ) return self.optimizer.get_updates ( loss , params ) result = K.eval ( K.logsumexp ( K.variable ( x_np ) , axis=0 ) ) self.dtype = dtype [ 0. , 1. , 0. , 1. , 0. , 1 . ] , loss_functions = [ get_loss_function ( loss ) for _ in output_names ] In this case you should make sure to specify from tensorflow.keras.layers import add model.compile ( 'sgd ' , metrics= [ keras.metrics.RootMeanSquaredError ( ) ] ) if params_name ! = 'nb_epoch ' : for i in range ( len ( a_list ) - 1 ) : value that should be masked out . 1e-07 theta : float > = 0 . Threshold location of activation . if output_masks is None : save_function ( obj , tmp_filepath , True , * args , * * kwargs ) where ` limit ` is ` sqrt ( 6 / fan_in ) ` config = yaml.load ( yaml_string ) from .load_backend import argmax _SYMBOLIC_SCOPE = threading.local ( ) def fit ( self , x , y , * * kwargs ) : Simply return the input ` mask ` . ( An rnn-based implementation with module_objects = module_objects or { } printable_module_name='activation function ' ) although it tends to be more memory-intensive . old_layer = keras.layers.SeparableConv2D ( 5 , 3 , 3 , name='conv ' ) 'You can apply a ` keras.layers.SpatialDropout1D ` layer ' with the layer input over a single spatial ( or temporal ) dimension fn = _GLOBAL_CUSTOM_OBJECTS [ function_name ] An integer denoting the number of dimensions ( rank ) of the dataset . inner_products = [ ] normalizer_dtype = lambda x : x.astype ( np.uint8 ) ' ( a `` Keras tensor '' is a tensor that was ' if len ( args ) > 0 and `` # Arguments '' not in doc : true_positive : y_true == True and y_pred > thresholds assert len ( layer.losses ) == 3 raise ValueError ( 'Input arrays should have ' def depthwise_conv2d ( x , depthwise_kernel , strides= ( 1 , 1 ) , padding='valid ' , self.monitor_op = np.greater We call self._add_inbound_node ( ) . model.add ( Masking ( mask_value=0. , input_shape= ( 3 , 4 ) ) ) layers = super ( Model , self ) .layers # Avoids the override in Sequential . where there is a mismatch in the number of weights , output_dim = 3 check_single_tensor_operation ( 'min ' , ( 4 , 2 ) , WITH_NP ) mask = K.variable ( mask_vals ) label_smoothing=0 ) : # Check that no item in ` data ` is larger than ` HDF5_OBJECT_HEADER_LIMIT ` return inspect.isgenerator ( x ) or is_sequence ( x ) def update ( self , * args ) : # it and then merge them along the desired axis . ' positional arguments ' forget_bias_init='zero ' , _ , output_masks , _ = self.run_internal_graph ( inputs , masks ) def zeros_like ( x , dtype=floatx ( ) , name=None ) : self.random_seed = random_seed recurrent_dropout=recurrent_dropout , # Signature detection def set_session ( session ) : def __call__ ( self , inputs , initial_state=None , * * kwargs ) : on this data at the end of each epoch . value found in ` shapes ` . initial_state = inputs [ 1 : ] py_slice ( None ) , # based on layer names , because names can potentially y = np.searchsorted ( self.classes_ , y ) # Check arguments order 'and thus can not be built . ' for axis , value in spec.axes.items ( ) : f : A pointer to a HDF5 group . assert len ( shape ) == 4 and shape [ -1 ] in [ 1 , 3 , 4 ] callback.on_predict_begin ( logs ) symmetric padding values for depth , height , and width : 'array or a list of arrays . ' `` `` '' Encapsulates metric logic and state . x = K.variable ( x_val ) `` `` '' Abstract base class for computing sensitivity and specificity . input_masks= [ None ] , class GlobalAveragePooling1D ( _GlobalPooling1D ) : return K.mean ( K.square ( y_pred - y_true ) , axis=-1 ) repeat_elements ( _x , rep=shape [ axis ] - 1 - rep , axis=axis ) ] , pointwise_regularizer=None , axis : axis : An integer or list of integers in [ -rank ( x ) , rank ( x ) ) , def __init__ ( self , sequence , use_multiprocessing=False , wait_time=None , ' ` val_samples ` - > ` steps ` arguments have changed . ' # transformation in self._input_map . with get_graph ( ) .as_default ( ) : # Aliases A ` History ` object . Its ` History.history ` attribute is allowed_positional_args= [ 'rate ' ] , unit_forget_bias=True , int_shape = list ( int_shape ) save_to_dir=None , inputs_np , if ( ( tf_data_format == 'NHWC ' or def convert_nested_bidirectional ( weights ) : if a1 ! = 1 : from .legacy import interfaces super ( Nadam , self ) .__init__ ( * * kwargs ) node_index , tensor_index ) dim3_cropping = conv_utils.normalize_tuple ( cropping [ 2 ] , 2 , def _old_batch_normalization ( x , mean , var , beta , gamma , # Layer instances created during output_mask , check_single_tensor_operation ( 'min ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) z0 = z [ : , : self.units ] pattern = transpose_shape ( pattern , data_format , spatial_axes= ( 1 , 2 ) ) ` ( batch , channels , spatial_dim1 , spatial_dim2 , spatial_dim3 ) ` . if decay.dtype ! = x.dtype.base_dtype : targets : A 1D tensor of length ` batch_size ` and type ` int32 ` or ` int64 ` . self.theta = K.cast_to_floatx ( theta ) the step function . `` `` '' Returns the value of the fuzz factor used in numeric expressions . # cntk only could handle loss and 1 metric in trainer , for metrics more # First , we need to infer the expected input shape and dtype . if ( len ( args ) == 2 or 'on exactly 2 inputs ' ) return output_mask x_train , y_train = load_batch ( fpath , label_key=label_mode + '_labels ' ) out5 = model.predict ( np.ones ( ( num_samples , timesteps ) ) ) model.fit ( X_train , Y_train , callbacks= [ reduce_lr ] ) return C.cntk_py.Value ( root_gradients.data ( ) ) if any_sub_list : padding='valid ' , data_format='channels_last ' , * * kwargs ) : if layer.name in embeddings_layer_names : if shapes : str ( [ x.shape for x in inputs ] ) ) return_state : Boolean . Whether to return the last state new_layer_1 = keras.layers.SpatialDropout1D ( rate=0.6 , name='sd1d ' ) if y.ndim == 3 : For example , if ` y_true ` is [ 0. , 1. , 1 . ] , and ` y_pred ` is [ 1. , 0. , 1 . ] legacy_conv1d_support = generate_legacy_interface ( axes = self.axes Note that the number of rows and columns should be specified too . is_not_repeat = np.insert ( np.diff ( inds ) .astype ( np.bool ) , 0 , True ) outputs , h , c = self._cudnn_lstm ( base_config = super ( SensitivityAtSpecificity , self ) .get_config ( ) normalized_cropping = ( height_cropping , width_cropping ) assert len ( val_data ) == len ( tensors ) the reduced dimensions are retained with length 1 . p_t = p - lr_t * m_t / ( K.sqrt ( vhat_t ) + self.epsilon ) : expected_width , metric value using the state variables . super ( RNN , self ) .__init__ ( * * kwargs ) global _DISABLE_TRACKING super ( CategoricalAccuracy , self ) .__init__ ( assert dense._inbound_nodes [ 0 ] .get_config ( ) [ 'inbound_layers ' ] == [ 'input_a ' ] config = { any ` dilation_rate ! =1 ` . def DISABLED_test_boston_load_does_not_affect_global_rng ( fake_downloaded_boston_path ) : y.shape [ 2 ] : 20 : do not append to output shape , self.kernel_o = self.kernel [ : , : , : , self.filters * 3 : ] `` `` '' Calls the ` on_epoch_end ` methods of its callbacks . normalized_padding = 3 * ( ( padding , padding ) , ) name : Optional name string for the placeholder . the output will have shape ( samples , 2 , dim ) . updates += layer.get_updates_for ( None ) assert initial_state [ 0 ] in layer._inbound_nodes [ 0 ] .input_tensors auto_padding= [ False , padding , padding ] ) 'The list you passed was : ' + str ( data ) [ :200 ] ) length = input_length [ i ] targets= [ last_output ] ) } ) output_mask = [ None , None ] if not self.merge_mode else None from tensorflow.keras.layers import Lambda output = f ( [ 10 . ] ) self.return_state = return_state min_index = K.cast ( min_index , 'int32 ' ) * * kwargs ) : with pytest.raises ( IndexError ) : recall = K.switch ( # Automatically track layers set as attributes . '1st entry of cropping ' ) assert dense_layer.get_updates_for ( a ) == [ 0 ] abs = np.abs class_mode='categorical ' , kernel_constraint : Constraint function applied to the kernel matrix model.compile ( 'sgd ' , loss=keras.losses.MeanSquaredLogarithmicError ( ) ) # reduced_weighted_mse = ( 6 + 26 ) / 2 = super ( LSTM , self ) .__init__ ( cell , dense_layer.add_loss ( 0 , inputs=a ) len_start = int_shape ( start ) [ 0 ] if is_tensor ( start ) else len ( start ) if spec_dim is not None and dim is not None : from . import keras_modules_injection /getting-started/faq/ # Note that you can also omit the ` input_shape ` argument : self.node_indices = node_indices if kwarg not in allowed_kwargs : top_k_mask = K.sum ( keepdims=True ) ) ) # ( either by using the tensor provided , ` None ` defaults to sample-wise weights ( 1D ) . new_out_2 = new_model.predict ( x2 ) if self.use_steps : filter_shape=pointwise_kernel_shape , return fpath update = k.identity ( x ) * 2 return self.cell.dilation_rate config [ 'dtype ' ] = self.dtype tensor=None ) : self.assign_embeddings = [ ] self.state_spec = [ InputSpec ( shape= ( None , dim ) ) def __init__ ( self , name='categorical_accuracy ' , dtype=None ) : return grads 'op , input_shape , kernel_shape , output_shape , padding , data_format , dilation_rate ' , y_shape = [ ] def test_logsumexp_optim ( self ) : if write_images : Note here we derive & use a closed formula not present in the paper exception_prefix + ' : ' if py_any ( list ( is_symbolic ( x ) for x in ( shape , mean , stddev ) ) ) : check_single_tensor_operation ( 'transpose ' , ( 4 , 2 ) , WITH_NP ) class MeanMetricWrapper ( Mean ) : Here is the ` Sequential ` model : assert K.int_shape ( o ) == ( None , None , 5 ) `` `` '' Before running this test : ( only valid when ` by_name ` =True ) . if _BACKEND == 'cntk ' : if not hasattr ( x , 'get_value ' ) : def check_generator_arguments ( y=None , sample_weight=None , inbound_names.append ( None ) 'rmsprop ' : RMSprop , metrics_tensors = [ shear_range=0. , if ( ndim ( x ) == 4 and A 1D tensor of length ` batch_size ` and type ` bool ` . `` `` '' Get the ` identifier ` loss function . y = precision save_prefix= '' , for i in range ( output_length ) : config.pop ( 'implementation ' ) inner_mask_shape = self._get_shape_tuple ( ( -1 , ) , mask , 2 ) rnn = keras.layers.CuDNNGRU allowed_positional_args = allowed_positional_args or [ ] noise_shape , proceed = ask_to_proceed_with_overwrite ( target_filepath ) ` ( output_at_t , states_at_t_plus_1 ) ` . The call method of the def add_weight_args_preprocessing ( args , kwargs ) : [ Nadam report ] ( http : //cs229.stanford.edu/proj2015/054_report.pdf ) 'use_bias ' : self.use_bias , if isinstance ( value , Layer ) : if prog_width > 0 : def _regular_normalize_batch_in_training ( x , gamma , beta , `` `` '' Returns the value of more than one tensor variable . * * kwargs : Additional keyword arguments . num_weights = len ( [ l for l in sublayer.weights 'keras_preprocessing > =1.0.5 ' ] , use_multiprocessing=False ) : 'hence the notion of `` layer output mask '' ' cache_subdir : Subdirectory under the Keras cache dir where the file is result = squeeze ( result , -1 ) subgraph=False ) : h = z * h_tm1 + ( 1 - z ) * hh ` y_pred ` and ` y_true ` should be passed in as vectors of probabilities , rather new_output , new_states = step_function ( from .callbacks import CSVLogger max_queue_size=10 , Keras is a high-level neural networks API , written in Python and capable of running on top of [ TensorFlow ] ( https : //github.com/tensorflow/tensorflow ) , [ CNTK ] ( https : //github.com/Microsoft/cntk ) , or [ Theano ] ( https : //github.com/Theano/Theano ) . It was developed with a focus on enabling fast experimentation . * Being able to go from idea to result with the least possible delay is key to doing good research . * elif len ( shape ) == 3 : # convnet case dilation_rate=1 , 'Got inputs shapes : % s ' % ( input_shape ) ) recurrent_regularizer=recurrent_regularizer , initializer = initializers.get ( initializer ) layer : Layer to process def AtrousConvolution1D ( * args , * * kwargs ) : fieldnames=fieldnames , `` `` '' GitHub issue : 11459 '' '' '' `` `` '' Returns the default image data format convention . constants_k = [ K.variable ( c ) for c in constants_np ] are_different = K.concatenate ( [ are_zeros , are_ones ] , axis=0 ) constraint : An optional Constraint instance . if image_shape : specified by this argument . 'keras.losses ' , 'keras.models ' , 'keras.optimizers ' ] from .load_backend import sin check_two_tensor_operation ( 'bias_add ' , x_shape , bias_shape , v_t = self.beta_2 * v + ( 1 . - self.beta_2 ) * K.square ( g ) 'targets to be binary matrices ( 1s and 0s ) ' 'embeddings_initializer ' : return K.batch_get_value ( weights ) weights [ 9 ] ] , axis=-1 ) batch_size = 20 signature += ' '' ' + value + ' '' ' if callback_model.stop_training : ( 'pool2d ' , ( 2 , 9 , 5 , 3 ) , ( 3 , 2 ) , ( 1 , 1 ) , return alt ( ) self._feed_input_shapes.append ( self.inputs [ i ] ._keras_shape ) `` `` '' Returns the number of scalars in a tensor . from .advanced_activations import PReLU ` new_steps ` values might have changed due to padding or strides . `` `` '' Layers that operate regularization via the addition of noise . '' '' '' callbacks._call_batch_hook ( 'predict ' , 'begin ' , step , batch_logs ) layer.forward_layer , from keras.utils.io_utils import ask_to_proceed_with_overwrite [ ( 2 , 2 ) , ( 2 , 2 , 2 ) ] ) : def start ( self ) : 'schedule_decay ' : self.schedule_decay } if not isinstance ( initial_state , ( list , tuple , type ( None ) ) ) : if gamma is None : if K.backend ( ) ! = 'cntk ' : if type ( val ) .__module__ == np.__name__ : install_requires= [ 'numpy > =1.9.1 ' , of epochs , the learning rate is reduced . output_masks = [ None ] * len ( output_tensors ) name uid : int , generator identifier return standardize_sample_or_class_weights ( class_weight , # Here 's how to use the cell to build a stacked RNN : with K.name_scope ( 'activity_regularizer ' ) : # Equivalent to subtracted = keras.layers.subtract ( [ x1 , x2 ] ) t_list = [ ] kernel_size : An integer or tuple/list of 3 integers , specifying the layer_id ) ) matrix_inner = K.bias_add ( matrix_inner , self.recurrent_bias ) gamma = _reshape_dummy_dim ( gamma , [ 0 ] ) np.float32 ) [ np.newaxis , : ] new_layer = keras.layers.Dense ( 2 , use_bias=False , dim = np.prod ( x.shape ) metrics.cosine_proximity , sensitivity : A scalar value in range ` [ 0 , 1 ] ` . if isinstance ( self.cell , Layer ) : inputs_c = inputs * dp_mask [ 2 ] # Instantiate layer . ( 32 , 1 , 30 ) def test_dilated_conv ( self , states = results [ 1 : ] # They only evaluated in training phase . To make it work , call self.sequence.on_epoch_end ( ) elif _BACKEND == 'theano ' : def save_weights ( self , filepath , overwrite=True ) : with the layer input to produce a tensor of # Tile the predictions for every threshold . self.accumulator = self.add_weight ( The upsampling factors for rows and columns . array ( [ [ 0. , 0. , 0. , 0 . ] , state_mask = [ None for _ in states ] def get_output_sample_weight_and_mode ( skip_target_weighing_indices , output_shape [ dim ] -= sum ( cropping_all_dims [ dim ] ) > > > y = K.placeholder ( shape= ( 3 , 4 ) ) output = tf.transpose ( outputs , ( 1 , 0 , 2 ) ) raise ValueError ( 'Invalid border mode for LocallyConnected2D ' for its ` build ` call . stddev : a python scalar or a scalar tensor . Standard deviation of the data for test or validation . from tensorflow.keras.applications.densenet import DenseNet201 broadcast_mean = T.reshape ( mean , target_shape ) # Logical and for index , ( value , state ) in enumerate ( zip ( states , self.states ) ) : ( isinstance ( loss , LossFunctionWrapper ) and super ( LogCosh , self ) .__init__ ( logcosh , name=name , reduction=reduction ) model.compile ( 'rmsprop ' , 'mse ' ) chunked_data = np.array_split ( data_npy , num_chunks ) if isinstance ( identifier , Optimizer ) : i = 1 K.zeros_like ( dtp ) ) v._keras_shape = value.tocoo ( ) .shape else_expression_fn ) biases = [ tf.reshape ( x , ( -1 , ) ) for x in biases ] if py_all ( [ is_sparse ( x ) for x in tensors ] ) : with K.control_dependencies ( update_ops ) : # For TF if x_ndim > 3 : for i , s in enumerate ( int_shape ) : val_enqueuer = GeneratorEnqueuer ( def DISABLED_test_fit_generator_dynamic_size_sequence_with_workers ( ) : alpha : scale for the negative factor . # We are adding the metric object as metadata on the result tensor . `` `` '' Initializer base class : all initializers inherit from this class . weight = permute_dimensions ( kernel , ( 2 , 0 , 1 ) ) node_index ) return C.times ( AttributeError : if the layer is connected to x = tf_image_ops.resize_bilinear ( x , new_shape ) logs [ 'lr ' ] = K.get_value ( self.model.optimizer.lr ) module_objects=globals ( ) , To be used together with the initialization `` lecun_normal '' . `` `` '' Resize the volume contained in a 5D tensor of shape feed_output_shapes = None # then add it to the inputs and temporarily num_cols = shape [ -1 ] output_padding : An integer or tuple/list of 2 integers , `` `` '' Maps metric names and functions to model outputs . warnings.warn ( 'Could not import the Theano backend ' ) def __init__ ( self , learning_rate=0.01 , momentum=0. , ( tuple of integers , does not include the samples axis ) pattern [ i ] = pattern [ i - 1 ] 'function_type ' : function_type , def _compute_fans ( shape , data_format='channels_last ' ) : uses_learning_phase : Whether any operation class ZeroPadding3D ( _ZeroPadding ) : y_shape = int_shape ( y ) The updates may potentially be conditional on some inputs tensors , inputs = np.random.random ( ( num_samples , timesteps , input_size ) ) def _reshape_sequence ( x , time_step ) : gamma.dimshuffle ( shuffle_pattern ) , data_format=data_format , datadir_base = os.path.join ( '/tmp ' , '.keras ' ) h_tm1_f = h_tm1 self.refs [ datapath ] = f def legacy_support ( func ) : : expected_height , __keyword__ : argument of ` RNN.__call__ ` ( as well as ` RNN.call ` ) method . 'Prefer using a Keras optimizer instead ' The ` enqueuer.get ( ) ` should be an infinite stream of datas . for i in range ( x_batch_size ) : self.bias_f = self.bias [ self.units * 5 : self.units * 6 ] recurrent_r = K.dot ( h_tm1_r , self.recurrent_kernel_r ) if reduction == Reduction.SUM_OVER_BATCH_SIZE : from tensorflow.keras.applications.vgg19 import preprocess_input `` `` '' Converts a 3D Numpy array to a PIL Image instance . is applied to height and width . return output_shapes name='maxpool2d ' ) a ` state_size ` attribute . This can be a single integer inputs = inputs [ : -num_constants ] self.is_placeholder = True loss = msle ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) `` `` '' Recurrent layers and their base classes . '' '' '' ( self.summation_method == output_dim=2 , name= 'd ' , consume_less='mem ' ) fpath : path the file to parse . axis = axis [ 0 ] boolean , whether the file should be decompressed ` ( batch , first_cropped_axis , second_cropped_axis , third_cropped_axis , model.reset_metrics ( ) depth_keys.sort ( reverse=True ) K.expand_dims ( K.constant ( thresholds ) , 1 ) , return np.dot ( x , y ) pointwise_kernel : kernel for the 1x1 convolution . x.shape [ 1 ] : 20 : do not append to output shape , if show_shapes : strides : An integer or tuple/list of n integers , # with the input_spec set at build time . from .pooling import AveragePooling2D files to be parsed by TensorBoard . loss_fns , shape = list ( input_shape ) # Note that ` callbacks ` here is an instance of `` `` '' Loads CIFAR10 dataset . with zeros for the additional rows/columns . `` `` '' Parses a JSON model configuration file and returns a model instance . decoded = np.argmax ( prob [ : length ] , axis=-1 ) return self.gain * q [ : shape [ 0 ] , : shape [ 1 ] ] self.epsilon = kwargs.pop ( 'epsilon ' , K.epsilon ( ) ) input_shape= ( num_samples , timesteps , input_size ) ) if hasattr ( self , 'dtype ' ) : @ pytest.mark.parametrize ( 'to_cudnn ' , [ False , True ] , ids= [ 'from_cudnn ' , 'to_cudnn ' ] ) self.padding , if K.is_keras_tensor ( additional_inputs [ 0 ] ) : # Load weights that were specified at layer instantiation . cell = SimpleRNNCell ( units , variables.append ( KC.variable ( value ) ) output_shape = ( tf.shape ( x ) [ 0 ] , ) + tuple ( output_shape [ 1 : ] ) values_shape , weights_shape ) ) if isinstance ( layer , Wrapper ) : def load_weights_from_hdf5_group ( f , layers , reshape=False ) : epsilon=1e-3 ) : # pragma : no cover the depthwise kernel matrix vx = K.variable ( x ) if ( isinstance ( loss , key_loss_classes ) or ( is_loss_wrapper and val_data , from .tensorboard_v2 import TensorBoard source activate test-environment preprocessor=embedding_kwargs_preprocessor ) input_shape = ( timesteps , input_size ) between samples in different successive batches . expr_ndim = ndim ( then_expression ) where x is the error ( y_pred - y_true ) config = { 'init ' : initializers.serialize ( self.init ) , metrics_utils.AUCSummationMethod ) : self.input_spec = InputSpec ( ndim=5 ) spatial_axes= ( 2 , 3 ) ) if not hasattr ( cell , 'state_size ' ) : except Exception as e : from tensorflow.python.keras import backend as tf_keras_backend out_pad_d ) [ 0.0 , 0.0 , 0.0 , 0.0 ] ] , # t=5 ( ignored ) filters , new_rows , new_cols ) ` if data_format='channels_first ' # and create the node connecting the current layer name='categorical_crossentropy ' , # that we need to know the size of the ` tf.Variable ` s which `` `` '' Prints a summary for a single layer . assert 'group3 ' in group2 only . Maximum number of processes to spin up cell.build ( [ input_shape ] + constants_shape ) out2 = new_model.predict ( x ) gamma = gamma.dimshuffle ( shuffle_pattern ) self.updates = updates tf_data_format = 'NHWC ' return x < = y if len ( layer_names ) ! = len ( filtered_layers ) : batches . verbose : 0 or 1 . Verbosity mode . conversions= [ ( 'samples_per_epoch ' , 'steps_per_epoch ' ) , def __init__ ( self , outbound_layer , output._uses_learning_phase = True pred_is_pos , K.zeros_like ( pred_is_pos , dtype=pred_is_pos.dtype ) ) return result raise ValueError ( 'Invalid pooling mode : ' , pool_mode ) def get_metric_name ( metric , weighted=False ) : def activation ( self ) : 'Invalid keys : { } . Valid variable key options are : `` { } '' '.format ( shape = list ( self.cell.kernel_shape ) pool_out = pool.pool_2d ( x , ws=pool_size , stride=strides , for sw , w in zip ( own_weight_vars , own_weights ) : for dtype in ( 'int32 ' , 'int64 ' , 'float32 ' , 'float64 ' ) : > _ '' Oneiroi are beyond our unravelling -- who can be sure what tale they tell ? Not all that men look for comes to pass . Two gates there are that give passage to fleeting Oneiroi ; one is made of horn , one of ivory . The Oneiroi that pass through sawn ivory are deceitful , bearing a message that will not be fulfilled ; those that come out through polished horn have truth behind them , to be accomplished for men who see them . `` _ Homer , Odyssey 19 . 562 ff ( Shewring translation ) . verbose=0 , str ( generator_output ) ) 'as part of the model save file . ' assert y_train.ndim == 2 , 'HDF5Matrix ndim should match input array ' noise = np.random.choice ( [ 0 , 1 ] , # Set Keras base dir path given KERAS_HOME env variable , if applicable . which is really equivalent to imputing constant precision throughout the WITH_NP , shape_or_val=False ) return wrapper self.momentum ) ] , _y.append ( np.stack ( __y , axis=0 ) ) reduction=losses_utils.Reduction.SUM ) [ return np.std ( x , axis=axis , keepdims=keepdims ) def file_exists ( self , filename ) : # GitHub issue : 11435 # We have to remember to unpickle in __getitem__ model.optimizer.set_weights ( optimizer_weight_values ) elif _BACKEND == 'tensorflow ' : new_layer = keras.layers.GlobalMaxPool2D ( data_format='channels_first ' , self._per_input_updates [ inputs_hash ] = [ ] self.padding , self.strides [ 1 ] ) output_ls_copy.append ( x ) noise_shape = tuple ( noise_shape ) a + ( 1 if num_dynamic_axis == 0 else 0 ) output_shape : 1D int tensor for the output shape . # ( samples , output_dim ) expand_nested , reason='cntk only support dilated conv on GPU ' ) paths_tensors , _ = K.ctc_decode ( input_prob_tensor , input_len_tensor , y_pred , K.cast ( sample_weight , dtype=K.floatx ( ) ) ) ` node_indices ` and ` tensor_indices ` are basically fine-grained coordinates row_dict = OrderedDict ( { 'epoch ' : epoch } ) # from the number of bias weights : layer = Dense ( 32 ) def output_size ( self ) : self.summation_method = summation_method label_lens = np.expand_dims ( np.asarray ( [ 5 ] ) , 1 ) `` `` '' Abstract base class used to build new callbacks . # that are part of the model . `` `` '' Exponential ( base e ) activation function . current = self.get_monitor_value ( logs ) * * kwargs ) def embedding_kwargs_preprocessor ( args , kwargs ) : predictions_2d = K.reshape ( y_pred , [ 1 , -1 ] ) ( ( 5 , 4 , 6 , 3 ) , ( 5 , 4 , 6 , 2 ) , -1 ) , from .training_utils import should_run_validation the list ` nodes_in_decreasing_depth ` and the set ` network_nodes ` . `` `` '' Utilities for real-time data augmentation on image data . '' '' '' super ( MaxPooling3D , self ) .__init__ ( pool_size , strides , padding , padding=padding , data_format=data_format , dilation_rate=dilation_rate , ( or list of tensors if the layer has multiple inputs ) . class Adamax ( Optimizer ) : def repeat ( x , n ) : normalization_axis = 3 return -total_log_prob x = np.arange ( 24 ) .reshape ( shape ) [ 1 , 2 ] , y_test = np.array ( y [ int ( len ( x ) * ( 1 - test_split ) ) : ] ) x_dense = x_sparse.toarray ( ) x.shape [ 0 ] : 100 : append to output shape data_rec = load_from_binary_h5py ( load_function , f ) if go_backwards : constants : list of tensors or None ValueError : If rank of ` condition ` is greater than rank of expressions . assert o._keras_shape == ( None , None , None ) Consider a batch of 32 samples , assert K.int_shape ( y ) == ( 1 , None , None , 1 ) constants_k = None # add a layer that returns the hadamard product if kwargs [ 'data_format ' ] == 'channels_last ' : initializer = elems [ -1 ] class Masking ( Layer ) : elif ( k == KTH ) & ( function_name [ :4 ] == 'conv ' ) : ` ( batch , steps , channels ) ` target size is different from that of the loaded image . pointwise_initializer='glorot_uniform ' , file_access_property_list.set_fapl_core ( backing_store=False ) count samples seen or steps ( batches ) seen . elif y.shape [ 1 ] == 1 : self.queue.queue.clear ( ) if spec.min_ndim is not None : # Model methods def step_function ( inputs , states ) : super ( CuDNNGRU , self ) .__init__ ( value , ( inputs - K.abs ( inputs ) ) * 0.5 ) _DISABLE_TRACKING.value = True overwrite the training generator . from tensorflow.keras.layers import GRU as CuDNNGRU kwargs = { } embeddings_regularizer='l1 ' , 'input to your model ' batch_shape = first_layer.batch_input_shape mean_squared_error , name , dtype=dtype ) size : Integer , total size of the data to slice into batches . new_nested_states = [ ] `` { } arguments order is different from the documentation '' .format ( name ) , elif ndim ( gamma ) == ndim ( x ) and shape ( gamma ) [ 0 ] == 1 : `` `` '' Takes the dataframe and the path to a directory if steps_per_epoch is None : 'you should only pass inputs with ' `` `` '' Does validation on the compatibility of targets and loss functions . x_f = K.dot ( inputs_f , self.kernel_f ) self.dims = tuple ( dims ) the output . The remaining tensors are the last states , the forget gate and the output gate into a single matrix , from enum import Enum for i in range ( len ( reduced_inputs_shapes ) ) : self.name isinstance ( ValueError : in case an empty Sequential or Functional model is passed . x = fp_rate `` `` '' These tests are not meant to be run on CI . # test that evalutation and prediction do n't crash and storing the weight value , named after the weight tensor . if isinstance ( axes , int ) : if ( val and sys.version_info [ 0 ] == 3 and isinstance ( `` `` '' Converts layers nested in ` TimeDistributed ` wrapper . Whether to use Theano or TensorFlow data format # it 's possible to callback a different model than self : assert o._keras_shape == ( None , None , None ) an input mask , an exception will be raised . sample_weights , val_sample_weights = ( print_layer_summary ( layers [ i ] ) It crops along the time dimension ( axis 1 ) . from .callbacks import ProgbarLogger assert len ( x.shape ) == 5 - ( 1 if num_dynamic_axis > 0 else 0 ) validation_freq=1 , 'sgd ' , reshaped_inputs.append ( K.permute_dimensions ( x , dims ) ) save_best_only=False , save_weights_only=False , ` histogram_freq ` must be greater than 0 . assert len ( padding [ 2 ] ) == 2 dtype=None , while True : idx = ( self.start + key ) .tolist ( ) ndim = x.ndim if x_shape [ axis ] is not None : model = Sequential ( ) you to visualize dynamic graphs of your training and test limitations of HDF5 data ; it shuffles in batch-sized chunks . `` `` '' Standardize ` __call__ ` to a single list of tensor inputs . callbacks.set_params ( { will be incremented by one for each worker . shift = C.reduce_mean ( shift , axis=axis ) logs : dict , metric results for this batch . [ Long short-term memory ] ( assert model.uses_learning_phase if outputs is None : # ensure that randomness is conditioned by the Numpy RNG constants = inputs [ -self._num_constants : ] mask = K.constant ( mask_vals ) inputs : Tensor or list of tensors . self._make_test_function ( ) # self.trainable_weights > > > K.eval ( kvar ) `` `` '' Element-wise exponentiation . return tf.concat ( [ to_dense ( x ) for x in tensors ] , axis ) num_train_samples = check_num_samples ( fit_inputs , for function_ in list_of_functions : broadcast_gamma , Note that layers that do n't have weights are not taken ( 2 , 2 ) , ( 2 , 2 ) , 'valid ' , name='avgpooling2d ' ) elems = elems [ 1 : ] return K.int_shape ( x ) z_shape = K.int_shape ( z ) mask = ( ( idxs < f_active.dimshuffle ( 0 , ' x ' ) ) & from tensorflow.python.keras.utils.data_utils import validate_file # Instead , we store one array per batch seen if go_backwards : # included layer_configs = config [ 'layers ' ] super ( CuDNNLSTM , self ) .build ( input_shape ) while ` `` channels_first '' ` corresponds to output = _reduce_on_axis ( x , axis , 'reduce_max ' ) output_shape = tf.stack ( list ( output_shape ) ) the ` len ( validation_data ) ` as a number of steps . `` `` '' Checks for user typos in ` params ` . datadir = os.path.join ( datadir_base , cache_subdir ) y = backend.dot ( inputs , w_i ) + backend.dot ( h , w_h ) output = C.ops.argmax ( x , axis=axis [ 0 ] ) y_true , 'Found : ' + str ( sample_weight_mode ) ) index = doc_lines.index ( keyword ) assert k_s.shape == k_d.shape return network_nodes , nodes_by_depth , layers , layers_by_depth base_config = super ( ConvLSTM2DCell , self ) .get_config ( ) ' a list of tuples . ' ) `` `` '' Computes the mean of squares of errors between labels and predictions . 3 . None . This means you implement a class that inherits from either model.fit_generator ( generate_arrays_from_file ( '/my_file.txt ' ) , `` `` '' Transpose and cast the input before the conv3d . check_single_tensor_operation ( 'softplus ' , ( 4 , 10 ) , WITH_NP ) # Since we do not know how many samples if sample_weight is not None : z = k.eval ( t ) self.input_spec = InputSpec ( ndim=4 , axes= { -1 : input_filter } ) # Handle ` name ` argument . self.monitor = monitor 'sample-wise weights , make sure your ' 'HDF5 and h5py installed . ' ) # and not a variable , to avoid variable initialization ` rows ` and ` cols ` values might have changed due to padding . output = K.bias_add ( output , self.bias ) # Class Methods if i == parts - 1 : return scale * K.elu ( x , alpha ) source_tensors = [ ] `` `` '' Softmax activation function . ( of the same shape ) . dims -= 1 # the model does n't have any weights until the first call def __init__ ( self , rank , elif [ [ `` $ MODE '' == `` PEP8_DOC '' ] ] ; then ( during training only ) . This can be useful to tell the model to if requests is None : @ pytest.mark.parametrize ( 'docs_descriptor ' , [ padding=padding , target : An integer tensor . raise ValueError ( 'Please provide as model targets ' size_all_dims = transpose_shape ( size_all_dims , newly_created_input_layer = input_tensor._keras_history [ 0 ] weight_value_tuples += zip ( symbolic_weights , weight_values ) input_tensor._keras_shape = batch_input_shape self._expects_training_arg = has_arg ( self.call , 'training ' ) a.k.a . an attention mechanism . epoch_logs [ 'val_ ' + l ] = o return K.batch_get_value ( params ) self.epoch = [ ] tf_keras_backend.set_image_data_format ( image_data_format ( ) ) initializer='zero ' , # will be handled by on_epoch_end . # This will build the current layer l2 : Float ; L2 regularization factor . num_thresholds = len ( self.thresholds ) self.bias_i = self.bias [ : self.units ] if isinstance ( x , list ) : gain : Multiplicative factor to apply to the identity matrix . def test_model_metrics_list ( ) : # Collect updates that are dependent on inputs else , 4D tensor with shape ` ( num_samples , channels , rows , cols ) ` . layer , node_index , tensor_index = x._keras_history gamma_np = np.random.random ( other_shape ) init='normal ' , super ( ConvLSTM2DCell , self ) .__init__ ( * * kwargs ) tmp_shape [ 1 ] = time_step `` `` '' Retrieves the model 's losses . kwargs [ 'depthwise_initializer ' ] = init from tensorflow.python.lib.io import file_io as tf_file_io return [ np.concatenate ( out ) for out in all_outs ] for both pickling and saving to disk . 'received cells : ' , cells ) warnings.warn ( 'You are not using the TensorFlow backend . ' if len ( input_shapes ) ! = len ( self._input_layers ) : ( num_samples , num_timesteps , state_and_io_size ) ) return T.sin ( x ) `` `` '' Repeats a 2D tensor . if callable ( x ) and isinstance ( x , C.cntk_py.Function ) is False : self.use_bias = use_bias sample_weight : Sample weights . 'kernel_initializer ' : normal_t = rng.normal ( size=shape , avg=mean , std=stddev , dtype=dtype ) `` `` '' Computes the mean absolute percentage error between ` y_true ` and ` y_pred ` . if not proceed : config = super ( ZeroPadding1D , self ) .get_config ( ) y._keras_shape = list ( x._keras_shape ) o = self.recurrent_activation ( z3 ) self.supports_masking = mask_zero output_dim = cell.state_size [ 0 ] & Goadrich 2006 for details ) ; 'minoring ' that applies left summation input_shape = tuple ( ( indices.shape [ i ] for i in range ( indices.ndim ) ) ) tf_keras_backend.set_image_data_format ( data_format ) if all ( [ x in kwargs for x in [ 'kernel_dim2 ' , 'kernel_dim3 ' ] ] ) : raise ValueError ( 'CNTK backend : ` count_params ` with dynamic ' for x in input_tensors : _ , wi = parse_shape_or_val ( ( input_dim , output_dim ) ) units = 1 if hasattr ( x , '_keras_shape ' ) : assert_allclose ( x.sum ( axis=0 ) , kx , atol=1e-05 ) 'but was passed an input_mask : ' from tensorflow.keras.layers import ZeroPadding1D self._feed_loss_fns ) : assert np.min ( rand ) > = min_val # test resizing normalizer parameters for the activation function . is_sparse_categorical_crossentropy = ( value : Any value that needs to be casted to the cell type width = 224 from tensorflow.keras.constraints import NonNeg * args , * * kwargs ) class Optimizer ( object ) : super ( Adam , self ) .__init__ ( * * kwargs ) updated_metrics_dict = collections.OrderedDict ( ) function = self.function.__name__ `` `` '' Layers that act as activation functions . '' '' '' input_length=4 , info += ' % .4f ' % avg mean=mean , Output mask tensor ( potentially None ) or list of output sequences of 10 time steps with 128 features per step in return tf.pad ( x , pattern ) reason='Sparse tensors are not supported in cntk ' file_like = io.BytesIO ( ) py_slice ( padding [ 1 ] [ 0 ] , input_shape [ 2 ] + padding [ 1 ] [ 0 ] ) , 'output_tensors ' , model = Sequential ( ) name=_prepare_name ( name , 'variable ' ) ) We expect labels to be provided in a ` one_hot ` representation . If you want to # on the fly because the inbound node may not yet exist , verbose : Verbosity mode , 0 , 1 or 2 y_true : tensor ` ( samples , max_string_length ) ` super ( Conv2DTranspose , self ) .__init__ ( # scalar class SquaredHinge ( LossFunctionWrapper ) : name = unique_name parallel_model.fit ( x , y , epochs=epochs , batch_size=batch_size ) predictions to consider when calculating precision . # If all nodes processed remove the layer # if masking is explicitly supported , by default val_x , val_y , or create its a placeholder tensor ( pass arguments ` input_shape ` # Prepare sample weights . default is to calculate precision with ` thresholds=0.5 ` . batch_size : integer batch size or ` None ` . self.layer.built = True None uses the dtype of x . with signature ` fn ( y_true , y_pred ) ` . minimum over all dimensions . if output is None : config = { 'output_dim ' : self.output_dim , stride_h , kernel_h , K.spatial_2d_padding ( K.variable ( xval ) , padding=padding , if K.backend ( ) ! = 'tensorflow ' or BaseMeanIoU is object : def stop ( self ) : 'CNTK Backend only supports accurate masking if ' verbose=0 ) : index_array : array of indices to be shuffled . seed : Random seed for shuffling the data force_transpose = False one element per model output . name = self.output_names [ i ] initial_state=initial_state ) from tensorflow.keras.losses import * def __del__ ( self ) : def check_params ( self , params ) : x = tf.reshape ( x , x_squashed_shape ) p = self.true_positives + self.false_positives chunked_data = np.array_split ( data_npy , num_chunks ) x : Tensor to compute the activation function for . [ KTF , KTH ] , batch_size=32 , new_layer_2 = keras.layers.SpatialDropout1D ( 0.6 , name='sd1d ' ) assert_allclose ( w , new_w ) return type ( grad ) .__name__ == 'IndexedSlices ' input_shapes = [ tensor._keras_shape for tensor in tensors ] return T.prod ( x , axis=axis , keepdims=keepdims ) rankdir='TB ' , * 'majoring ' : Applies right summation for increasing intervals and left # test with Sequential model model.load_weights ( fname , by_name=True , reshape=True ) def maximum ( inputs , * * kwargs ) : assert _image_data_format in { 'channels_last ' , 'channels_first ' } input_tensor = Input ( tensor=x , self.recurrent_activation = activations.get ( recurrent_activation ) f.close ( ) assert out_pred.shape == ( 50 , 1 ) , 'Prediction shape does not match ' 'root server at ' + str ( self.root ) ) broadcast_mean , from .. engine.base_layer import InputSpec , Layer parameter = signature.parameters.get ( name ) `` `` '' Stochastic gradient descent optimizer . mean : A float , mean of the normal distribution to draw samples . _DISABLE_TRACKING.value = False metrics_utils.AUCSummationMethod.INTERPOLATION ) ) : 'it should have one entry per model output . ' 'filters ' : self.filters , the same length as ` inbound_layers ` . for key in variables_to_update For instance , this allows you to do real-time data augmentation symbolic_weights = layer.weights return x.get_value ( borrow=True , return_internal_type=True ) .shape super ( CategoricalHinge , self ) .__init__ ( `` `` '' Computes the Huber loss between ` y_true ` and ` y_pred ` . def test_separable_conv2d_legacy_interface ( ) : 'arguments ' : self.arguments } function = func_dump ( self.function ) When ` steps ` is ` None ` , returns the number of samples to be y_shape = list ( y._keras_shape ) if noise_shape is None : def _preprocess_conv2d_image_shape ( image_shape , data_format ) : color_mode : One of `` grayscale '' , `` rgb '' , `` rgba '' . Default : `` rgb '' . x_shape.pop ( ) [ 0 , 0 , 1 ] , # blank if tensor == _LEARNING_PHASE_PLACEHOLDER : three values could be passed to ` build_fn ` : group4 = group3 [ 'group4 ' ] return inception_resnet_v2.preprocess_input ( * args , * * kwargs ) print_fn ( ' _ ' * line_length ) `` pay more attention '' to samples from return densenet.decode_predictions ( * args , * * kwargs ) def _generate_dropout_mask ( ones , rate , training=None , count=1 ) : 'maxval ' : self.maxval , _test_optimizer ( optimizers.Adagrad ( lr=1 . ) ) the 2nd , 3rd and 4th dimension will be padded . forward_state = None mapping to the model 's outputs . If a dict , it is expected to map def _fused_normalize_batch_in_training ( x , gamma , beta , reduction_axes , metrics= [ 'accuracy ' ] ) if update_fn or update_tn : time_per_unit = 0 WITH_NP , axes=1 ) This function should only be called during train mode . Defaults to 5 . original_keras_version=None , for k , v in d.items ( ) : `` `` '' Abstract class for different global pooling 2D layers . from keras_applications import imagenet_utils shape : Tuple of integers , shape of returned Keras variable . # self.input_spec and self.state_spec with complete input shapes . See : https : //en.wikipedia.org/wiki/Kullback % E2 % 80 % 93Leibler_divergence if not from_logits : `` `` '' Core Keras layers . updates += super ( Wrapper , self ) .get_updates_for ( inputs ) check_single_tensor_operation ( 'softmax ' , ( 4 , 10 ) , WITH_NP ) stddev = np.sqrt ( self.rate / ( 1.0 - self.rate ) ) sys.stderr.write ( 'Using CNTK backend\n ' ) a string , ` `` SAME '' ` or ` `` VALID '' ` . initializer='uniform ' , passed as the ` activation ` argument , ` kernel ` is a weights matrix ` `` sparse '' ` : 1D numpy array of integer labels , activation : Activation function to use return tf.abs ( x ) return [ ] if not _DISABLE_TRACKING.value : 'input tensor should have a defined dimension ' `` `` '' Returns a tensor with uniform distribution of values . def __init__ ( self , pool_size= ( 2 , 2 , 2 ) , strides=None , padding='valid ' , ( or list of shape tuples if the layer has multiple inputs ) . count_params ( ) return output / len ( inputs ) def is_accepted ( name , member ) : def mean_absolute_percentage_error ( y_true , y_pred ) : log = np.log if mask is not None : dtype=dtype , seed=self.seed ) return Concatenate ( axis=axis , * * kwargs ) ( inputs ) def save_wrapper ( obj , filepath , overwrite=True , * args , * * kwargs ) : from .. legacy.layers import MaxoutDense raise ImportError ( ' ` load_weights ` requires h5py . ' ) floating-pointing value , and both ` y_pred ` and ` y_true ` have the shape `` `` '' Linear stack of layers . num_chunks += 1 'output_shape ' : output_shape , from keras import regularizers Tensor instance ( with Keras metadata included ) . # scalar self._initial_weights = None axes = ( axes , axes ) `` `` '' Updates the progress bar . class Layer ( object ) : return_sequences=return_sequences , super ( ImageDataGenerator , self ) .__init__ ( if isinstance ( self.data , h5py.Group ) : f = H5Dict ( Path ( h5_path ) , mode= ' w ' ) return True `` `` '' Raise errors if ` pydot ` or GraphViz unavailable . '' '' '' It should contain one subdirectory per class . W_regularizer=None , # we will see , we can not pre-allocate if beta is None : mask=mask , if sequential model : for further details . current = logs.get ( self.monitor ) shapes = to_list ( layer.compute_output_shape ( input_shapes ) ) if isinstance ( inputs , list ) : if None in input_shape [ 1 : ] : kernel_initializer=kernel_initializer , current = squeeze ( current , 1 ) dimension . categorical_targets = np.array ( [ [ 0 , 1 , 0 ] , [ 1 , 0 , 0 ] ] , dtype=np.float32 ) ' ( see keras.io/optimizers ) . ' ) `` `` '' Add a 1-sized dimension at index `` dim '' . sample_weight=None , h_tm1_f = h_tm1 * rec_dp_mask [ 1 ] _ , extension = os.path.splitext ( to_file ) def test_on_batch ( self , x , y , sample_weight=None , reset_metrics=True ) : res.update ( { name : value } ) initializer='uniform ' , metrics_names += [ m.name for m in self._metrics ] # loss calculation and feed targets preparation . 'and ensure that its executables are in the $ PATH . ' ) `` `` '' Checks that the value correspond to a valid data format . node_data = node_data_list [ node_index ] from tensorflow.keras.utils import normalize # Stateful metrics output a numeric value . This representation return -loss self.forward_layer.set_weights ( weights [ : nw // 2 ] ) input_dtype = K.dtype ( losses ) def test_convlstm2d_legacy_interface ( ) : ` ( num_samples , timesteps , channels , rows , cols ) ` . if len ( shape ) == 2 : get_updates_for 3D tensor with shape : ` ( batch_size , sequence_length , output_dim ) ` . self.end_of_epoch_signal.clear ( ) x = tf.nn.bias_add ( x , bias , pip install -e . [ tests ] -- progress-bar off for _ in self.states ] def antirectifier ( x ) : if unroll : f = f [ 'model_weights ' ] history = model.fit ( x_train , y_train , epochs=12 , batch_size=16 , # It 's supposed to be an input layer , so only one node from tensorflow.keras.models import load_model inputs : tensor or list/tuple of tensors layer.bias_o_i , return self.updates dilation_rate : tuple of integers , str ( input_shape ) ) 'after loading it . ' self.kernel_z , def separable_conv2d_args_preprocessor ( args , kwargs ) : x_shape_or_val , _floatx = _config.get ( 'floatx ' , floatx ( ) ) pooling.GlobalAveragePooling2D ] ] serialized = marshal.dumps ( test_func.__code__ ) .decode ( 'raw_unicode_escape ' ) 'dtype ' : self.dtype , origin='https : //s3.amazonaws.com/text-datasets/reuters_word_index.json ' , while any ( map ( is_too_big , chunked_data ) ) : check_two_tensor_operation ( 'equal ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) _axis [ i ] = cntk_axis [ _axis [ i ] ] output_shape = self._output_shape mse = keras.losses.MeanSquaredError ( ) def test_conv_invalid_use ( self ) : ` num_classes ` is the last dimension of predictions . arguments , as : assert input_length == inputs.shape [ 1 ] # Apply activity regularizer if any : def clone_metric ( metric ) : 'Please provide fixed dimension ' pass # same as the second to final output ( before masked region ) return T.lt ( x , y ) # Get sorted list of node depths . do_validation = False condition = tf.cast ( condition , 'bool ' ) def fit_loop ( model , fit_function , fit_inputs , ref = [ 3.34211 ] x = np.random.random ( ( 1 , 1 , 1 , 1 ) ) assert len ( batch_val ) == len ( tensors ) prev_value = _DISABLE_TRACKING.value 'HDF5Matrix dtype should match input array ' ) self.constants_spec = None You can ask questions and join the development discussion : return np.log ( 1 . + np.exp ( x ) ) class_mode=class_mode , output_mask = K.any ( output_mask , axis=-1 ) from tensorflow.core.protobuf import config_pb2 `` `` '' `` `` '' Internal method to create an inbound node for the layer . Note : These installation steps assume that you are on a Linux or Mac environment . forward_weights = preprocess_weights_for_loading ( del self.initial_weights def test_spatial_2d_padding ( self ) : `` `` '' Convolutional layers . log_first = zeros class SpatialDropout1D ( Dropout ) : # https : //www.cntk.ai/pythondocs/cntk.variables.html # cntk.variables.Parameter if x.ndim == 3 : val = val.tolist ( ) self._output_shape_cache = { } def __init__ ( self , datapath , dataset , start=0 , end=None , normalizer=None ) : conv_out = conv_out [ : , : , : i , : , : ] This is done as part of _add_inbound_node ( ) . self.recurrent_bias_r = None model._feed_sample_weights ) broadcast_moving_mean , check_two_tensor_operation ( 'binary_crossentropy ' , label , ( 4 , 2 ) , def test_log ( self ) : config = { 'stddev ' : self.stddev } class DepthwiseConv2D ( Conv2D ) : ( 0.1 , 9.0 , 0.8 ) , # max_value > 6 output_tensors , _ , _ = self.run_internal_graph ( inputs , masks ) return _reshape_dummy_dim ( output , axis ) ' outputs , but you passed target_tensors= ' # We fix the placeholder shape except the batch size . self.cell.state_size ) ) self.build ( unpack_singleton ( input_shapes ) ) values = values * sample_weight 'you should specify the ` ' + steps_name + ' ` argument ' pool_size : Integer or tuple of 3 integers , super ( OrderedEnqueuer , self ) .__init__ ( sequence , use_multiprocessing ) rankdir='TB ' , `` `` '' Calls the ` on_train_batch_end ` methods of its callbacks . kwargs = node.arguments old_layer = keras.layers.GlobalAveragePooling2D ( dim_ordering='default ' , `` `` '' Cast a Numpy array to the default Keras float type . padding=self.padding , self._trainable_weights.append ( value ) print ( 'baseline training : ' , total_time ) model.compile ( 'sgd ' , loss=keras.losses.BinaryCrossentropy ( ) ) # old : ( filters , stack_size , kernel_rows , kernel_cols ) if y_true is not None : dilation_rate=self.dilation_rate ) `` ` sh source = 'CuDNNGRU ' input_lens = np.expand_dims ( np.asarray ( [ 5 , 5 ] ) , 1 ) # number of timesteps layer.recurrent_kernel_c , dropout = 0 . before each epoch ) or str ( for 'batch ' ) . self._send_sequence ( ) # Share the initial sequence Return : numpy integer . if mask is None : str ( tuple ( allowed_positional_args ) ) with context.eager_mode ( ) : def update_state_wrapper ( update_state_fn ) : y = np.random.uniform ( 0 , 1 , size= ( 10 , ) ) layer_test ( layers.MaxPooling3D , # static learning phase flag , if it is not 0 or 1 , we will go with dynamic preprocessor=batchnorm_args_preprocessor ) def manual_variable_initialization ( value ) : raise ValueError ( 'Invalid curve : `` { } '' . Valid options are : `` { } '' '.format ( # TF input shape : ( samples , rows , cols , slices , input_depth ) from keras.preprocessing.image import ImageDataGenerator # attributes for subclassed Models . # the size of the input anymore : for keyword in keywords : stddev : float , standard deviation of the noise distribution . include_optimizer : If True , save optimizer 's state together . callbacks=None , output_shape = output_shape [ 0 ] model = keras.models.Model ( inputs , outputs ) y = y [ indices ] } ) if len ( axes ) ! = 2 : 'check the model and inputs . ' % argument.name ) 'to the batch update ( % f ) . Check your callbacks . ' return cls ( cells , * * config ) List of input tensors . if mask_np is not None : def test_creation_operations ( self ) : # hidden state projected separately for update/reset and new inputs.append ( slice_i ) layer_configs.append ( { out = expand_dims ( out , 1 ) self.add_update ( updates , inputs ) shape = tuple ( dynamic_shape ) + shape B._inbound_nodes check_single_tensor_operation ( 'eye ' , ( 3 , 2 ) , WITH_NP , shape_or_val=False ) { { np_implementation } } op , [ input_shape , depthwise , pointwise ] , ] ) if len ( layer._inbound_nodes [ -1 ] .output_tensors ) ! = 1 : x = getattr ( C , reduce_fun_name ) ( x , axis ) [ 1. , 1. , 1. , 1 . ] ] , dtype=float32 ) callbacks._call_batch_hook ( 'test ' , 'begin ' , step , batch_logs ) y = K.permute_dimensions ( y , dims ) name='mean_absolute_error ' ) : run_metadata = config_pb2.RunMetadata ( ) # Arguments recurrent_initializer='glorot_uniform ' , 'output mask ' ) other_shape [ axis ] = x_shape [ axis ] feed_dict=feed_dict , 'the ` Conv2D ` layer with the ` dilation_rate ` ' # TF input shape : ( samples , conv_dim1 , conv_dim2 , conv_dim3 , A tensor , reshaped into 1-D from .. utils.data_utils import OrderedEnqueuer class BinaryCrossentropy ( MeanMetricWrapper ) : raise ValueError ( 'Could not interpret optimizer identifier : ' if getattr ( new_output , '_uses_learning_phase ' , False ) : 'argument to your first layer . If your ' out_labels = model.metrics_names conversions= [ ( 'input ' , 'inputs ' ) , raise ValueError ( 'For multi-gpu usage to be effective , ' `` `` '' Initializer that generates tensors with a normal distribution . filter_shape=kernel_shape , as they ca n't be passed easily to children processes . class CategoricalAccuracy ( MeanMetricWrapper ) : metrics = self._metrics_function ( ) cell = Cell ( state_size= ( self.units , self.units ) ) the axes to compute the product . If ` None ` ( default ) , computes output += self.b # ( see https : //github.com/Theano/Theano/pull/4736 ) layer_map [ layer ] = newly_created_input_layer @ pytest.mark.parametrize ( 'shape , start , size ' , [ model_weights_group [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) self._non_trainable_weights = weights inputs = K.permute_dimensions ( inputs , permutation ) random values to generate . The core data structure of Keras is a __model__ , a way to organize layers . The simplest type of model is the [ ` Sequential ` ] ( https : //keras.io/getting-started/sequential-model-guide ) model , a linear stack of layers . For more complex architectures , you should use the [ Keras functional API ] ( https : //keras.io/getting-started/functional-api-guide ) , which allows to build arbitrary graphs of layers . if self.return_state : score1 = nested_model ( input1 ) x_test = np.array ( x [ int ( len ( x ) * ( 1 - test_split ) ) : ] ) y = Dense ( 16 , activation='softmax ' ) ( x ) depth_keys.sort ( reverse=True ) `` `` '' Computes the specificity at a given sensitivity . file_format=None , a = rng.normal ( 0.0 , 1.0 , flat_shape ) for page in autogen.PAGES : self._feed_input_shapes.append ( K.int_shape ( v ) ) # If the node does not have all inbound layers out_height , out_width , self.filters ) ` custom_objects ` should be a dictionary mapping height = 224 old_layer = keras.layers.LSTM ( input_shape= [ 3 , 5 ] , output_dim=2 , name='d ' ) # Add compile metrics/weighted metrics ' names to the metric names list . bce = keras.losses.BinaryCrossentropy ( ) return x.shape def _call_begin_hook ( self , mode ) : scale=True , return losses 'adagrad ' : Adagrad , grads = model.optimizer.get_gradients ( model.total_loss , algorithm from the paper `` On the Convergence of Adam and if ( len ( layer._inbound_nodes ) > 1 or when using this layer as the first layer in a model . constants = to_list_or_none ( constants ) output_shape [ -1 ] = self.units assert m1 == m2 spatial_axes= ( 1 , 2 ) ) class MeanAbsoluteError ( LossFunctionWrapper ) : return losses_utils.compute_weighted_loss ( axis = _normalize_axis ( axis , x ) def _preprocess_conv2d_depthwise_filter_shape ( filter_shape , data_format ) : input_length : Length of input sequences , to be specified from tensorflow.keras.utils import HDF5Matrix raise ValueError ( 'Unknown { } : { } '.format ( printable_module_name , W_regularizer : instance of [ WeightRegularizer ] ( .. /regularizers.md ) old_layer = keras.layers.Cropping2D ( dim_ordering='tf ' , name='c2d ' ) self.output_padding = conv_utils.normalize_tuple ( mode='auto ' , _keras_dir = os.path.join ( _keras_base_dir , '.keras ' ) reduction_axes , step_function_np , inputs = K.constant ( inputs_vals ) new_layer = keras.layers.PReLU ( 'zero ' , name= ' p ' ) self.bias = self.add_weight ( shape= ( self.units , ) , shape = [ C.InferredDimension if _ == C.FreeDimension else _ for _ in shape ] data_format , # Raises return T.exp ( x ) return shape_or_val , np.random.random ( shape_or_val ) .astype ( np.float32 ) - 0.5 'should have a single output tensor . ' output += inputs [ i ] # given specificity . return K.relu ( inputs , ValueError : If ` mask ` is provided ( not ` None ` ) test_preprocess_weights_for_loading_rnn_should_be_idempotent ( layer_class , args ) inputs_r = inputs * dp_mask [ 1 ] num_units=self.units , in inputs/kernels/outputs . num_static_element = np.prod ( np.asarray ( self.target_shape ) ) lr_t = lr / ( 1 . - K.pow ( self.beta_1 , t ) ) raise ValueError ( 'When passing a list as loss , it should have one entry ' ( if any ) . If not , exceptions are raised . class Zeros ( Initializer ) : if self.mode == 'fan_in ' : for step in range ( steps ) : # test that new updates are the same with both models zca_epsilon=1e-6 , def _make_train_function ( self ) : for histograms computation . super ( MaxPooling2D , self ) .__init__ ( pool_size , strides , padding , depthwise_kernel , depthwise_kernel_shape , data_format ) self.kernel , value_conversions=None , 'config ' : cell_config } # has not yet been created model.compile ( 'sgd ' , loss=keras.losses.Hinge ( ) ) self.units = units ( 'axes= ' + str ( self.axes ) ) if self.axes else `` ] initial.append ( C.user_function ( ConvertToBatch ( s ) ) ) reversed sequence . `` `` '' Cumulative sum of the values in a tensor , alongside the specified axis . x1 = inputs [ 0 ] raise ValueError ( 'Invalid pooling mode : ' + str ( pool_mode ) ) self.bias_r_i = self.bias [ self.units : self.units * 2 ] return inputs + K.random_normal ( shape=K.shape ( inputs ) , If you are using a virtualenv , you may want to avoid using sudo : self.monitor_op = np.greater `` `` '' Interpolation formula inspired by section 4 of Davis & Goadrich 2006 . dtype=dtype , def call ( self , inputs , states ) : updates += self._per_input_updates [ inputs_hash ] n = ( n , ) len_dim1 = input_shape [ 1 ] mapped_weight_name = weight.name.replace ( ' : ' , ' _ ' ) preprocessor=get_updates_arg_preprocessing ) # input_b = keras.backend.variable ( val_b ) min_ndim=None , if self.verbose == 1 : zero_list.append ( zero_grad_eval_fn ( [ val ] ) [ 0 ] ) name='huber_loss ' ) : state_mask = get_matching_mask ( mask , state ) pattern [ : num_dynamic_axis ] ! = current_layout [ : num_dynamic_axis ] ) : _serialize_model ( model , H5Dict ( h5file ) , include_optimizer ) kernel_regularizer='l1 ' , model.predict ( x_data ) from tensorflow.keras.layers import Concatenate Saved models can be reinstantiated via ` keras.models.load_model ` . Usage with the ` compile ` API : initial_state = K.tile ( initial_state , [ 1 , self.units ] ) from .convolutional import Conv2DTranspose layer.trainable = True ins = x + y + sample_weights + [ 1 ] patched_file_io = patch ( self.file_io_module , new=mock_module ) `` `` '' Destroys the current Keras graph and creates a new one . path : path to extract the archive file # Build the model using the retrieved inputs ( value or symbolic ) . assert np.alltrue ( decode_truth [ i ] == K.eval ( decode_pred_tf [ i ] ) ) dropout_U=0.1 , y = T.extra_ops.repeat ( y , n , axis=1 ) return [ ] , [ ] , [ ] assert_not_compatible ( gru ( ) , gru ( reset_after=True ) , 'has been removed . ' ) return super ( RootMeanSquaredError , self ) .update_state ( return op.device WITH_NP , axis=-1 , keepdims=True ) layer_test ( convolutional.Conv2D , last_output._uses_learning_phase = True def test_function ( self ) : output = C.ops.element_select ( mask_slice , output , prev_output ) for name in output_names : K.zeros_like ( self.true_negatives [ min_index ] ) ) layer = keras.layers.CuDNNGRU ( units ) self.stddev = stddev # we should not use same filename in several tests to allow for parallel clipnorm : float > = 0 . Gradients will be clipped _gcs_prefix = 'gs : // ' If ` axes ` is ( 1 , 2 ) , to find the output shape of resultant tensor , force_transpose : boolean , whether force to transpose input from NCHW to NHWC weights = convert_nested_time_distributed ( weights ) batch_size : Optional input batch size ( integer or None ) . broadcast_shape [ self.axis ] = input_shape [ self.axis ] batch them into fewer , larger operations . These modes will filters : Integer , the dimensionality of the output space label_and_pred = K.cast ( label_and_pred , dtype=K.floatx ( ) ) if func.__closure__ : 'Please set ` dilation_rate ` to ( 1 , 1 ) . ' metrics= [ keras.metrics.SpecificityAtSensitivity ( ) ] ) 'Evaluation value does not meet criteria : { } '.format ( out_eval ) ) forget_bias_init='one ' , 'Try reducing ` gpus ` . ' % ( gpus , super ( ZeroPadding3D , self ) .__init__ ( normalized_padding , self.stop ( ) # we need this check . It defaults to the ` image_data_format ` value found in your if padding == 'same ' and kernel_shape [ 0 ] % 2 == 0 : def get ( self ) : if self.stop_signal.is_set ( ) : axes : Dictionary mapping integer axes to v._keras_initialized = True last_ones = [ ] self.moving_variance , `` `` '' Check if ` path ` is of supported type for instantiating a ` H5Dict ` `` '' '' name='deconv ' ) k_s_d = K.eval ( k_s ) base_config = super ( BatchNormalization , self ) .get_config ( ) test_dtypes ( ) 'Need at least rank 3 to run RNN . ' % dims ) return { key : clone_metric ( value ) for key , value in metrics.items ( ) } self.queue.unfinished_tasks = 0 ` ( batch , rows , cols , channels ) ` filter_dilation=dilation_rate ) self.data = path super ( Dot , self ) .__init__ ( * * kwargs ) shape=pointwise_kernel_shape , from tensorflow.keras.layers import Layer return np.array ( [ ' using a generator or Sequence as an input . ' ) from .load_backend import rnn W_constraint='maxnorm ' ) NAME_SCOPE_STACK.pop ( ) eta_format = ' % ds ' % eta ' '' input_length '' is % s , but received input has shape % s ' % target_tensors : By default , Keras will create placeholders for the self._feed_output_names.append ( name ) `` `` '' Test the model on a single batch of samples . is preferred , but ` get_custom_objects ` can def sparse_categorical_crossentropy ( target , output , from_logits=False , axis=-1 ) : num_classes = 1000 return tf.clip_by_value ( x , min_value , max_value ) decoded = _remove_blanks ( decoded , num_classes ) out_pred = model.predict ( X_test , batch_size=32 , verbose=False ) ` example.txt ` would therefore be ` ~/.keras/datasets/example.txt ` . ( ie . 'linear ' activation : ` a ( x ) = x ` ) . # If file exists and should not be overwritten : `` `` '' Pads 5D tensor with zeros along the depth , height , width dimensions . embeddings_initializer : Initializer for the ` embeddings ` matrix kernel_initializer='glorot_uniform ' , ` ( ( left_dim1_crop , right_dim1_crop ) , self.sess.run ( self.assign_embeddings , feed_dict=feed_dict ) 1 mask = K.cast ( mask , y_pred.dtype ) if hasattr ( layer , 'input_shape ' ) : from .pooling import GlobalMaxPool1D self.run_thread.daemon = True self.input_names.append ( layer.name ) # will mock gcs locally for tests # were initialized . This ensures that a metric initialized from this h = x._keras_shape [ 1 ] + padding [ 0 ] [ 0 ] + padding [ 0 ] [ 1 ] model = keras.models.Model ( inputs= [ input1 , input2 ] , outputs=out ) _epsilon = _config.get ( 'epsilon ' , epsilon ( ) ) for xi , yi in zip ( x , y ) : computed_data.append ( tensor_map [ str ( id ( x ) ) ] ) 3D tensor with shape : ` ( batch , steps , features ) ` . self.optimizer = None from tensorflow.keras.activations import selu function = func_load ( config [ 'function ' ] , globs=globs ) mode = 'auto ' 'an attribute ` state_size ` ' # stays the same . 'class_name ' : layer.__class__.__name__ , `` `` '' Generates output predictions for the input samples . last_output = C.sequence.last ( final_output ) `` { } { } arguments are not present in documentation `` .format ( name , list ( 'into probabilities ' also works with other backends . weights_rank = K.ndim ( sample_weight ) x_shape = x.shape.as_list ( ) ' : expected ' + names [ i ] + ' to have shape ' self.totals = { } assert 'metric_1 ' in names For example , if values is [ 1 , 3 , 5 , 7 ] then the sum is 16 . layer = Dense ( ... , kernel_regularizer='MyObject ' ) `` `` '' Retrieves a Keras Optimizer instance . for cell_states in new_nested_states : for k in [ KTH ] : wget https : //repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh ; weights = [ kernel , recurrent_kernel , bias ] update_freq=update_freq , return T.zeros_like ( targets , dtype='int8 ' ) A tensor , the difference of the inputs . from tensorflow.keras.utils import serialize_keras_object def sparse_top_k_categorical_accuracy ( y_true , y_pred , k=5 ) : target_tensors=target_tensors ) # will use real bucket for tests `` `` '' Instantiates an all-ones variable and returns it . input_size=input_dim , stride_w , We may introduce this operation in CNTK native def DISABLED_test_conv_1d_channels_first ( ) : `` `` '' Remove a 1-dimension from the tensor at index `` axis '' . name='kullback_leibler_divergence ' ) : def conv2d_args_preprocessor ( args , kwargs ) : from tensorflow.keras.utils import deserialize_keras_object class SpatialDropout2D ( Dropout ) : output_shapes.append ( None ) unroll , ` 'channels_last ' ` corresponds to inputs with shape return set ( [ 0 if y is None else int ( y.shape [ 0 ] ) for y in x ] ) ' can not obtain value for tensor ' from unittest.mock import patch , Mock , MagicMock return Y_ if has_return and `` # Returns '' not in doc : return metrics config [ 'implementation ' ] = 1 dtype=x.dtype , weights [ num_weights_per_layer : ] , _GLOBAL_CUSTOM_OBJECTS.update ( self.backup ) dummy_w_1d = K.variable ( np.ones ( ( 3 , 2 , 3 ) ) ) return L1L2 ( l1=l1 , l2=l2 ) 'true_positives ' , kernel_size , self.verbose = self.params [ 'verbose ' ] def test_H5Dict_groups ( ) : Unrolling is only suitable for short sequences . 'specificity ' : self.specificity @ pytest.mark.parametrize ( 'data_format ' , [ 'channels_first ' , 'channels_last ' ] ) `` `` '' He normal initializer . return np.concatenate ( tensors , axis ) `` `` '' Number of batch in the Sequence . dtype=dtype ) for x in reference_input_tensors : axis = 0 y._keras_shape = output_keras_shape if max_value is not None : from .load_backend import clear_session with tf.device ( '/gpu : % d ' % gpu_id ) : assert ( X_train [ np.array ( [ 0 , 1 ] ) ] == X_train [ :2 ] ) .all ( ) if k > = int_shape ( predictions ) [ 1 ] : model = keras.models.Sequential ( ) The added Keras attributes are : shape = ( 3 , 4 , 2 ) `` `` '' Wrapper class for native TensorFlow optimizers . name='alpha ' , target = self.params [ 'steps ' ] class ThresholdedReLU ( Layer ) : finally : 'with shape ' + str ( data_shape ) ) self.rank = len ( cropping ) from keras.utils import metrics_utils if isinstance ( key , slice ) : rescale=None , if spec.dtype is not None : self.kernel_regularizer = regularizers.get ( kernel_regularizer ) def _normalize_axis ( axis , x ) : padding = ( ( 1 , 2 ) , ( 2 , 1 ) ) batch . If ` False ` , the metrics will be statefully accumulated across warnings.warn ( 'The ` input_dim ` and ` input_length ` arguments ' if len ( updates ) > 0 : from tensorflow.python.ops import state_ops as tf_state_ops ( if the model has multiple outputs ) . model.compile ( 'sgd ' , metrics= [ keras.metrics.MeanAbsoluteError ( ) ] ) def conv2d ( x , kernel , strides= ( 1 , 1 ) , padding='valid ' , Test samples where ` n_samples ` is the number of samples self.bias = self.add_weight ( shape= ( self.units * 6 , ) , check_single_tensor_operation ( 'argmax ' , ( 4 , 2 ) , WITH_NP , axis=1 ) `` `` '' Boston housing price regression dataset . '' '' '' # Call the cells in order and store the returned states . self.baseline = baseline # trainable weights For Cropping1D , the data format is always ` `` channels_last '' ` . if self.shuffle : old_layer = keras.layers.SeparableConv2D ( 5 , 3 , 3 , # If ` initial_state ` is specified , target_mean=1. , target_max=1 . ) metrics_names += [ m.name for m in layer._metrics ] import binascii old_layer = keras.layers.ConvLSTM2D ( 5 , 3 , 3 , gcs_filepath = file_io_proxy.get_filepath ( filename=fname ) raise ValueError ( 'CNTK Backend : Invalid data_format : % s ' % data_format ) step_function : return np.float64 opens_file = not isinstance ( filepath , ( dict , h5py.Group ) ) ValueError : for incompatible GRU layer/weights or incompatible biases # Logical and name='global_maxpool3d ' ) def weighted_assign_add ( label , pred , weights , var ) : if not isinstance ( self.input_spec , ( list , tuple ) ) : 'recurrent_activation ' : 'sigmoid ' , check_array_lengths=True , mask_np=kwargs.pop ( 'mask ' , None ) , dtype = K.dtype ( input_tensor ) shape_temp.append ( _ ) # create dataset directory to avoid concurrent directory creation at runtime y , _ , _ = fused_batch_norm ( if data [ i ] .ndim ! = len ( shape ) : dtype_list = [ ] being added . check_single_tensor_operation ( 'var ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) new_model = load_model ( fname ) # output previous output if masked . layer_names = model_weights_group [ 'layer_names ' ] if len ( initial_states ) > 0 : def test_he_normal ( tensor_shape ) : for x_elem in to_list ( inputs ) : denom = ( self.true_negatives [ min_index ] + self.false_positives [ min_index ] ) Sequential.predict_classes , Sequential.evaluate ] 'with the TensorFlow backend . ' ) this can be disabled since the scaling 'gradients display when using TensorFlow 2.0 . ' return tf.reshape ( x , [ -1 ] ) return input_shape node_index = node.node_indices [ i ] This allows us to have a single serialization logic bias_regularizer='l2 ' , res = K.eval ( K.ctc_batch_cost ( k_labels , k_inputs , k_input_lens , `` `` '' 1D convolution with separable filters . step_function_k , d , y_true , y_pred , sample_weight=sample_weight ) initial_state_vals = np.random.random ( ( num_samples , 6 , 7 ) ) d2 = y_shape [ axes [ 1 ] ] allowed_kwargs = { 'clipnorm ' , 'clipvalue ' } from .noise import GaussianDropout return np.concatenate ( unconcatenated_outs [ 0 ] , axis=0 ) Words that were not seen in the training set but are in the test set # 5 % ~ 12 % faster but also less GPU memory thresholds = to_list ( default_threshold if thresholds is None else thresholds ) if dtype is None : return C.parameter ( env : KERAS_BACKEND=cntk PYTHONWARNINGS=ignore old_layer = keras.layers.Dense ( 2 , bias=False , init='normal ' , fn , model.add ( keras.layers.Dense ( 3 , chunked_data = np.array_split ( data_npy , num_chunks ) self.strides , stride_h , stride_w = self.strides def _fix_unknown_dimension ( self , input_shape , output_shape ) : input_shape= ( height , width , 3 ) , `` `` '' Reduce elems using fn to combine them from left to right . 'bias_initializer ' : initializers.serialize ( self.bias_initializer ) , are_zeros = K.equal ( y_true , 0 ) or if all inbound nodes have the same output shape . return in_train_phase ( alt , x , training=training ) `` `` '' Calls ` load_function ` on a ` h5py.File ` read from the binary ` stream ` . shift = x ' ( symmetric_dim1_crop , symmetric_dim2_crop , symmetric_dim3_crop ) , ' if uninitialized_vars : def __getitem__ ( self , idx ) : model.load_weights ( fname , by_name=True ) logs : dict , has keys ` batch ` and ` size ` representing the current kernel_regularizer='l1 ' , model.set_weights ( new_weights ) [ 1 , 2 , 3 ] , ' { } : { } '.format ( printable_module_name , identifier ) ) if val_gen : 'Output Shape ' , previous_depth = nodes_depths.get ( inbound_node , 0 ) If False , ` beta ` is ignored . requests.post ( self.root + self.path , json=send , headers=self.headers ) layer , self.unrelated_updates.eval ( input_dict , as_numpy=False ) broadcast_gamma , `` `` '' Computes an output mask tensor for Embedding layer which will map to the label indices , will be alphanumeric ) . MIN_CODE_SIZE = 15 denom = ( self.true_positives + self.false_positives ) pool_size , def expand_mask ( mask_ , x ) : assert out.replace ( '__str__ = ' , `` ) == 'msg [ [ 1 . ] ] \n ' y_true = K.switch ( K.greater ( smoothing , 0 ) , y = np.exp ( x - np.max ( x , axis , keepdims=True ) ) for shape_or_val in shapes_or_vals : You can then use ` TimeDistributed ` to apply a ` Dense ` layer # not merged : A A B B available_devices = keras.utils.multi_gpu_utils._get_available_devices ( ) if hasattr ( self.layer , 'losses ' ) : for _ in range ( x.ndim - y.ndim - 1 ) : `` `` '' Abstract nD copping layer ( private , used as implementation base ) . return tf.reduce_sum ( x , axis , keepdims ) 'first layer is an Embedding , you can ' scale=stddev , seed=seed , specify ` stateful=True ` in the layer constructor . class InputLayer ( Layer ) : its new shape ( obtained via self.compute_output_shape ) . `` `` '' Container abstracting a list of callbacks . recurrent_initializer : Initializer for the ` recurrent_kernel ` conversions= [ ( 'border_mode ' , 'padding ' ) , rotation_range=rotation_range , 'same ' , 'channels_last ' , ( 2 , 2 ) ) , monitor : quantity to be monitored . A tensor , the dot product of the samples from the inputs . 'sample_weight array is 1D . ' ) return metrics_module.binary_crossentropy self._write_logs ( logs , self.samples_seen ) self.restore_best_weights = restore_best_weights kwargs.pop ( 'dropout ' ) except AttributeError : along the depth , height and width . return K.sum ( inputs , axis=steps_axis ) / K.sum ( mask , axis=steps_axis ) from .losses import kullback_leibler_divergence return value @ interfaces.legacy_model_constructor_support if issparse ( ins [ i ] ) and not K.is_sparse ( model._feed_inputs [ i ] ) : supported . If PIL version 3.4.0 or newer is installed , ` `` box '' ` and # Entries are unique . Includes input and output layers . regularizers.serialize ( self.activity_regularizer ) , return tf.sparse.to_dense ( tensor ) epochs to run before a new validation run is performed , e.g . interpolation='nearest ' ) : # collapse axis with batch axis mask , _ , weights = losses_utils.squeeze_or_expand_dimensions ( reps = np.delete ( reps , auxiliary_axis ) 'l2 ' : float ( self.l2 ) } name : Check if ` fn ` can be called with ` name ` as a keyword argument . # whose shape is different from that of the output config = { 'thresholds ' : self.init_thresholds } def __init__ ( self , alpha=0.3 , * * kwargs ) : unique_variables_to_update = { } Scalar test loss ( if the model has a single output and no metrics ) output_shape = ( input_shape [ 0 ] , input_shape [ 1 ] , output_dim ) if 'mode ' in kwargs : input_shape= ( 5 , 10 ) ) ) def test_function_tf_run_options_with_run_metadata ( self ) : return [ x_weight [ output_names [ 0 ] ] ] This argument ( or alternatively , the keyword argument ` input_shape ` ) return input_shape + ( self.output_dim , ) return T.cast ( x , dtype ) class OrderedEnqueuer ( SequenceEnqueuer ) : def _moments ( x , axes=None , shift=None , keep_dims=False ) : if isinstance ( value , six.string_types ) : ValueError : In case of an invalid value for the `` scale '' , mode '' or inputs , shape = tuple ( None for _ in range ( K.ndim ( state ) ) ) [ CNTK installation instructions ] ( https : //docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-your-machine ) . config [ 'layers ' ] = layer_configs embedding.tensor_name = tensor.name return deserialize ( config , custom_objects=custom_objects ) ` channels_last ` or ` channels_first ` . space = input_shape [ 1 : -1 ] `` `` '' Utilities related to losses . '' '' '' 'max_value ' : self.max_value , # If values , then in symbolic-mode placeholders will be created ` ( samples , filters , output_row , output_col ) ` x = K.placeholder ( x_shape ) return deserialize_keras_object ( config , steps=None , data_format : the data format , channels_first or channels_last ( 1 if data_format == 'channels_first ' else 0 ) ) either `` channels_first '' or `` channels_last '' . if self.target is None or current > = self.target : # does not return a list the same size as ` call ` 'variable-length sequences . Please specify a ' value is generated for each threshold value . def test_doc_lists ( docs_descriptor ) : assert len ( layer._inbound_nodes [ -1 ] .output_tensors ) == 1 out_pad_h ) implementation=implementation ) callbacks=None , if not name : ( except the learning rate , which can be freely tuned ) . str ( len ( initial_state ) ) be the display of the losses and metrics plots . strides : Integer , or None . Factor by which to downscale . initial_output = initial_output [ 0 ] * 0 entry ` outputs [ s , t ] ` is the output of the step function go_backwards , allowed.add ( 'full ' ) and the ` input_shape ` , not including the samples dimension , is ` ( 10 , 16 ) ` . for i in range ( len ( self.outputs ) ) : metrics.extend ( self._output_loss_metrics ) recurrent_activation=recurrent_activation , def batch_normalization ( x , mean , var , beta , gamma , axis=-1 , epsilon=1e-3 ) : str ( input_shape ) + ' . ' ) if isinstance ( output_values , dict ) : ' ` layer.build ( batch_input_shape ) ` ' ) grads = self.optimizer.compute_gradients ( loss , var_list=params ) @ pytest.mark.parametrize ( 'model_nest_level ' , assert_list_pairwise ( z_list ) if hasattr ( self , 'clipvalue ' ) and self.clipvalue > 0 : return reshape ( x , ( -1 , ) ) y_pred_rank = K.ndim ( y_pred ) will include the following quantities in the ` logs ` that results , _ = theano.scan ( the expected input will be batches of 10 32-dimensional vectors . on establishment of the network connection and once You can specify the initial state of RNN layers numerically by : weighted_loss = weighted_masked_objective ( losses.get ( 'mae ' ) ) padding : int , or tuple of 2 ints , or tuple of 2 tuples of 2 ints . dtype : ( Optional ) data type of the metric result . metrics : List of metrics to be evaluated by the model from tensorflow.keras.preprocessing.text import * return int_lp ( otherwise the optimizer has no weights ) . if verbose == 1 : should be skipped . WITH_NP = [ KC , KNP ] # avoid numerical instability with _EPSILON clipping raise ValueError ( ' Can not clone a ` Sequential ` model on top ' model_config = self._updated_config ( ) ` ( batch_size , pooled_rows , pooled_cols , channels ) ` apply_affine_transform = image.apply_affine_transform transferred to the real GCS bucket . For this to work , valid Google application assert all ( x._uses_learning_phase for x in outputs ) output_tm1 = output_t if self.update_freq == 'epoch ' : the reduced dimension is retained with length 1 . self._seen_so_far = 0 `` `` '' 2D deconvolution ( i.e . transposed convolution ) . if K.backend ( ) == 'tensorflow ' and not K.tensorflow_backend._is_tf_1 ( ) : metrics , self.output_names , output_shapes , self.loss_functions ) A tensor , the element-wise minimum of the inputs . check_single_tensor_operation ( 'prod ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) if functional model with 1 or more Input layers : 3 , return_sequences=True ) ( inputs , initial_state=init_state ) from tensorflow.python.ops import ctc_ops as ctc 'h5py ' ] , def __init__ ( self , scale=1.0 , bias_constraint=bias_constraint , axes = tuple ( axes ) v = tf_keras_backend.variable ( The model will not be trained on this data . output_shape [ dim ] * = size_all_dims [ dim ] self.return_sequences = return_sequences if not hasattr ( self , 'optimizer ' ) : if unknown is not None : shape2 : tuple or None . Shape of the second tensor right_pad = kwargs [ 'padding ' ] .get ( 'right_pad ' , 0 ) > > > K.eval ( kvar_ones ) self.check_params ( params ) version= ' 2.4.0 ' , elif 'keras ' in name and inspect.ismodule ( mem ) : def DISABLED_test_rnn_cell_with_constants_layer_passing_initial_state ( ) : self.bias , self._call_batch_hook ( _TRAIN , 'begin ' , batch , logs=logs ) assert o2._keras_shape == ( None , 3 , 2 , 1 ) while isinstance ( first_layer , ( Model , Sequential ) ) : if ` return_sequences ` : 5D tensor with shape : # List of stateful metric functions . Used for resetting metric state during # note : ` None ` is the batch dimension dtype : ( Optional ) data type of the metric result . 'accumulator ' , `` `` '' Returns all the metrics that are to be reported . loss = 0.5 * d^2 + d * ( |x| - d ) if |x| > d in your model , you would need to specify the input length 'rank ' : self.rank , mask=mask_k , x = np.random.random ( ( samples , timesteps , dim ) ) num_row = 7 if not self.writer : ' ( instead of the ` batch_size ` argument , ' inner_inputs = self._input_map [ uid ] base_config = super ( MaxoutDense , self ) .get_config ( ) assert markdown ( docstring ) == markdown ( docs_descriptor [ 'result ' ] ) # self.rank is 1 for ZeroPadding1D , 2 for ZeroPadding2D . 'theano ' : False , y_train = np.frombuffer ( lbpath.read ( ) , np.uint8 , offset=8 ) output * = inputs [ i ] initial_states_np , if name not in loss : * * A simple tutorial can be found * * [ here ] ( `` `` '' Add node to layer list dilation_rate= ( 2 , 2 ) , permutation = [ len ( y_shape ) - 2 ] the training samples , used for weighting the loss function dtype=dtype , seed=self.seed ) custom_objects=custom_objects ) layer_losses = super ( RNN , self ) .losses state_shape = ( input_shape [ 0 ] , rows , cols , self.filters ) raise ValueError ( 'ReduceLROnPlateau ' The savefile includes : return tf.cos ( x ) 'write_images was set to False ' ) It 's possible that the user specified a static batch size in their directory : string , path to the target directory . callback.on_predict_end ( logs ) x = T.clip ( x , 0. , np.inf ) original_keras_version = f.attrs [ 'keras_version ' ] .decode ( 'utf8 ' ) if isinstance ( input_shape , list ) : model.fit ( X_train , y_train , batch_size=32 , shuffle='batch ' , verbose=False ) from .normalization import BatchNormalization self.batch_size = batch_size return if output_index in skip_target_weighing_indices : self.alpha_regularizer = regularizers.get ( alpha_regularizer ) > > > kvar = K.variable ( np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) ) base_config = super ( LocallyConnected1D , self ) .get_config ( ) loss_fn.fn == losses.binary_crossentropy ) ) ValueError : In case ` x ` is not a symbolic tensor . `` `` '' Instantiates a ` Loss ` from its config ( output of ` get_config ( ) ` ) . if d > 1 : warnings.warn ( cls_name + ' inputs must come from ' assert k.dtype ( t ) == dtype if sample_weight is not None : `` `` '' Get the ` identifier ` activation function . self.gamma_constraint = constraints.get ( gamma_constraint ) ` NONE ` , this has the same shape as ` losses ` ; otherwise , it is scalar . self.rho = K.variable ( rho , name='rho ' ) return { 'max_value ' : self.max_value , elif isinstance ( loss_weights , collections.Mapping ) : W_constraint='maxnorm ' , name='d ' ) K.variable ( weights ) , by the sum of ` true_positives ` and ` false_positives ` . `` `` '' Average pooling operation for 3D data ( spatial or spatio-temporal ) . n_s.append ( o.replace_placeholders ( { p : o.output } ) ) assert json.dumps ( old_layer.get_config ( ) ) == json.dumps ( new_layer1.get_config ( ) ) `` `` '' Finds and replaces a missing dimension in an output shape . `` `` '' Abstract base class for convolutional recurrent layers . with K.name_scope ( name or 'weighted_loss ' ) : self.input_spec = original_input_spec mask : Either None ( indicating no masking ) or a Tensor indicating the # no larger than 999 ( vocabulary size ) . if model_config is None : The results will be weighted and added together . When multiple thresholds are self.__dict__.update ( kwargs ) decrease . In this case , SpatialDropout2D will help promote independence # Fix for Theano because it needs elif len ( inputs ) > 1 + len ( initial_state ) : return output_cntk , cntk_func axis=-1 , on_batch_end : called at the end of every batch . ' `` ` \n ' def test_pool ( self , def truncated_normal ( shape , mean=0.0 , stddev=1.0 , dtype=None , seed=None ) : tf_data_format = 'NWC ' # to pass TF Conv2dNative operations with session.graph.as_default ( ) : str ( list ( set_y ) [ 0 ] ) + ' input samples and ' For every such layer group , a group attribute ` weight_names ` , a different loss on each output by passing a dictionary or a * * kwargs ) active + 1 , # Following 2 properties : self.on_batch_begin ( batch , logs=logs ) cell = deserialize_layer ( config.pop ( 'cell ' ) , depth_factor : Positive integer . assert len ( shape ) == 2 # only valid for 2D tensors 'nb_feature ' : self.nb_feature , def set_value ( x , value ) : functions / classes . if isinstance ( variables , list ) is False : name=name , padding=padding , ` ( batch , new_conv_dim1 , new_conv_dim2 , new_conv_dim3 , filters ) ` mask = np.ones ( ( 3 , 4 ) ) from .generic_utils import serialize_keras_object reason='Test adapted from tensorflow . ' ) A Keras variable with the shape of x filled with zeros . batch number and the size of the batch . for node_data in inbound_nodes_data : ( e.g . when using TensorFlow 2.0 ) . between two consecutive layers as long as the weights are initialized return T.nnet.hard_sigmoid ( x ) # in the initial states ( outputs_info ) broadcastable ` ( samples , timesteps , channels ) ` the initial state of the RNN layer . # to the inputs to the Wrapper layer ) . regularization_losses = [ x = np.pad ( x , pad , 'constant ' , constant_values=-np.inf ) # Internal methods : self.stop_signal.set ( ) num_dynamic = get_num_dynamic_axis ( placeholder ) name=layer.name + '_embedding ' ) return 'float32 ' # 'data_format ' : 'channels_first ' , assert np.isnan ( loss [ 0 ] ) # In this case we must explicitly broadcast all parameters . ` ( batch , height , width , channels ) ` while ` channels_first ` for layer in inbound_layers : 'is ill-defined . ' y = [ ] if isinstance ( input_shape , list ) : axes= { self.axis : dim } ) layer_class , metrics += self._compile_metric_functions spatial_start_dim = 3 data_format=self.data_format , for v in variables : mask : Tensor output_shapes : list of shapes of model outputs . # Test that ` relu6 ` op gets used . h = K.dot ( inputs , self.kernel ) `` `` '' Sets the metric attributes on the model for the given output . return K.softmax ( x ) x , y = batch_0 def not_equal ( x , y ) : from .load_backend import learning_phase # Must be implemented by subclasses . def in_train_phase ( x , alt , training=None ) : `` `` '' Implementation of the scikit-learn classifier API for Keras . # update x._keras_shape def test_save_load_binary_h5py ( ) : The hyperbolic activation : initial_state_vals = np.random.random ( ( num_samples , 6 ) ) data_format = 'channels_first ' def gradients ( loss , variables ) : from tensorflow.keras.layers import Average from .load_backend import pool3d momentum_cache_t = self.beta_1 * ( 1 . - 0.5 * ( converted.append ( ( 'init ' , 'depthwise_initializer/pointwise_initializer ' ) ) `` `` '' Retrieves the output mask tensor ( s ) of a layer . if ( value is not None and model.load_weights ( fname , by_name=False , reshape=True ) WITH_NP , padding= ( 1 , 2 ) ) ( 0.1 , 5.0 , 0.0 ) , # set alpha and max_value new_model_disk = load_model ( fname ) if self.verbose > 0 : # A learning phase is a bool tensor used to run Keras models in out = keras.layers.Dense ( 4 ) ( subtracted ) h_tm1_c = h_tm1 * rec_dp_mask [ 2 ] self.on_train_end = lambda logs : None from .load_backend import depthwise_conv2d finally : def int_shape ( x ) : output_tensors = [ ] elif layer.__class__.__name__ in [ 'Model ' , 'Sequential ' ] : zeros [ : active_next ] , args = ( args [ 0 ] , ) + args [ 2 : ] member.__module__ ) # Prepare display labels . def __init__ ( self , name='kullback_leibler_divergence ' , dtype=None ) : output = permute_dimensions ( output , permutation ) val_inputs = [ ] if implementation == 0 : 4D tensor with shape : self.curve = metrics_utils.AUCCurve.from_str ( curve ) return K.mean ( K.square ( first_log - second_log ) , axis=-1 ) `` `` '' Reduce elems using fn to combine them from right to left . if layer.__class__.__name__ == 'ConvLSTM2D ' : str ( mask ) ) self.recurrent_kernel_o ) x_rep = tf.tile ( x_rep , reps ) model.fit ( x , y , batch_size=32 , epochs=10 ) from .recurrent import SimpleRNN check_two_tensor_operation ( 'categorical_crossentropy ' , label , ( 2 , 3 ) , corresponds to data format ` channels_last ` , return normalize_inference ( ) 'will soon be deprecated . Please update the code to ' if not layer.layers : shapes=None , `` `` '' Stop mocking of ` self.file_io_module ` if real bucket not 'valid ' , 'channels_first ' , 'max ' ) , # Actually call the layer , callback_model = model._get_callback_model ( ) 'adam ' : Adam , def test_dtype ( self , dtype ) : if self.__class__.__name__ == 'Sequential ' : raise TypeError ( 'Layer ' + self.name # We need to force this to be a tensor return batch_input_shape [ 0 ] def _canonical_to_params ( self , weights , biases ) : test_cases = [ ] non_repeats = T.neq ( Y [ skip_idxs ] , Y [ skip_idxs + 2 ] ) self.bias_o = self.bias [ self.units * 7 : ] if k not in allowed_kwargs : used for the linear transformation of the inputs . optimizer_weights_group = h5dict [ 'optimizer_weights ' ] ' '' y._uses_learning_phase = True total_size = int ( content_type.strip ( ) ) Use in combination with ` bias_initializer= '' zeros '' ` . x : Input data . It could be : if shared_axes is None : mask_last_num_timesteps = 2 # for second sample only assert len ( model.losses ) == 8 self.params = params self.decay = K.variable ( self.initial_decay , name='decay ' ) return print_layer_summary ( self , q = u if u.shape == flat_shape else v `` `` '' Non-fused version of ` normalize_batch_in_training ` . x = x * T.cast ( T.gt ( x , threshold ) , floatx ( ) ) y = K.variable ( y_np ) This induces quasi-linear speedup on up to 8 GPUs . batch_size = 30 self.reduction = reduction w_img = tf.transpose ( w_img , perm= [ 2 , 0 , 1 ] ) max_queue_size=10 , ` model ` argument , but which distributes its workload on multiple GPUs . loss_fn : The loss function used . from .merge import minimum `` `` '' Element-wise truth value of ( x > y ) . the loss and any model metrics cache_dir = os.environ.get ( 'KERAS_HOME ' ) self.strides = conv_utils.normalize_tuple ( strides , rank , 'strides ' ) metric_obj = _create_mean_metric ( value , name ) return weighted initial_state_vals = np.random.random ( ( num_samples , state_and_io_size ) ) 'activity_regularizer ' : 'l2 ' , return tf.maximum ( x , y ) broadcast_beta , 'activation ' : None , def binary_crossentropy ( y_true , y_pred , from_logits=False , label_smoothing=0 ) : return resnet_v2.ResNet152V2 ( * args , * * kwargs ) if has_arg ( self.function , 'mask ' ) : kwargs [ 'training ' ] = training def test_gaussiannoise_legacy_interface ( ) : # We do n't need to check this one . self.uid = _SEQUENCE_COUNTER.value input_shape [ 2 ] + padding [ 0 ] [ 0 ] + padding [ 0 ] [ 1 ] , output_dim_b = 2 mode='auto ' , period=1 ) : WITH_NP , cntk_dynamicity=True , strides=self.strides [ 0 ] , def test_multi_gpu_multi_io_model ( ) : # TF kernel shape : ( ... , input_depth , depth ) `` `` '' Returns a JSON string containing the network configuration . `` `` '' Initializer that generates a random orthogonal matrix . from keras import optimizers x : Keras variable or Keras tensor . self.best = -np.Inf ] A uint8 tensor ( 0s and 1s ) . if verbose == 1 : self.callbacks.append ( callback ) # Build a map from a layer unique name ( self._node_key ) self.step = step = tf.placeholder ( tf.int32 ) for layer_data in config [ 'layers ' ] : name='SimpleRNN ' ) start = K.constant ( -1 , dtype='int32 ' ) 'You can set ` kernel_size ` to an odd number . ' ) i = 0 output = super ( Bidirectional , self ) .__call__ ( full_input , * * kwargs ) getargspec = inspect.getfullargspec elif len ( generator_output ) == 3 : val = self.data [ attr ] that the network will only train once on each sample per epoch which is not raise ValueError ( 'Must specify ` validation_steps ` ' 5D tensor with shape ` ( num_samples , timesteps , channels , rows , cols ) ` . The word index dictionary . f = self.recurrent_activation ( x_f + h_f ) states = states [ 1 : ] index = epoch input_tensors : A tensor or list of tensors . output = self.activation ( output ) depth , height and width of the 3D convolution window . random_shear = image.random_shear [ 0 , 0 , 1 , 0 ] then the true positives value would be 1 . 'Timestep-wise sample weighting ( use of ' def decorated ( metric_obj , * args , * * kwargs ) : maxlen : truncate sequences after this length . num_classes = 1000 `` `` '' Utilities used in convolutional layers . '' '' '' output_shape = func_load ( config [ 'output_shape ' ] , globs=globs ) y : Keras tensor or variable with ` ndim > = 2 ` . return np.squeeze ( preds , axis=-1 ) assert group2 [ ' x ' ] == 'abcd ' deserialized = func_load ( serialized ) slices = [ py_slice ( i , i + j ) for i , j in zip ( start , size ) ] if value ! = 0 : input_shape = input_shape [ 0 ] converted.append ( ( 'input_dim ' , 'input_shape ' ) ) input_shape , The value of save_format='png ' , log_prob = np.zeros ( ( num_samples , 1 ) ) info += ( ' ' * ( prev_total_width - self._total_width ) ) `` `` '' Calls the ` on_predict_end ` methods of its callbacks . h5file_ [ 'data1 ' ] .attrs [ 'attr ' ] = attr inputs_vals = np.random.random ( ( num_samples , num_timesteps , 5 ) ) value = value.eval ( ) # Do not slice the training phase flag . [ 0.458235 , 0.396634 , 0.123377 , 0.00648837 , 0.00903441 , 0.00623107 ] ] ] , extension = extension [ 1 : ] if not callable ( then_expression ) : def relu ( x , alpha=0. , max_value=None , threshold=0 . ) : 'write_graph was set to False ' ) for i , ( s1 , s2 ) in enumerate ( zip ( in_lens , input_shape [ 1 : ] ) ) : elif backend ( ) == 'cntk ' : assert_thresholds_range ( to_list ( thresholds ) ) check_single_tensor_operation ( 'relu ' , ( 4 , 2 ) , WITH_NP , alpha=alpha , per_output_metrics.append ( metrics_dict ) # Returns new_c.append ( c ) if self.test_function is None : return update_op for i , dim in enumerate ( self.dims ) : # TODO Dref360 : Decide which pattern to follow . First needs a new TF Version . assert mae_obj.reduction == Reduction.SUM function = deserialize_keras_object ( `` `` '' Returns predictions for a single batch of samples . the hinge metric value is 1.6 . Decorated function . output_mask = K.not_equal ( inputs , 0 ) 'axes ' : self.axes , Same type and shape as initializer for ( l , l1 ) in zip ( range ( pool_size [ 1 ] ) , range ( -pool_size [ 1 ] , 0 ) ) : def test_docs_in_custom_destination_dir ( tmpdir ) : output_shapes=output_shape , if K.backend ( ) == 'tensorflow ' : tmp_shape = list ( int_shape ( x ) ) `` `` '' Adds support for masking and sample-weighting to an objective function . return tuple ( noise_shape ) stride_d , stride_h , stride_w = self.strides validation_split=0.0 , It takes as input a list of tensors of size 2 , fn : arbitrary function from tensorflow.keras.layers import RepeatVector `` `` '' Clone a functional ` Model ` instance . raise ValueError ( 'Invalid strides for dilated convolution ' ) 'If your targets are integer classes , ' # if current layer is Model inputs = keras.Input ( batch_shape= ( num_samples , timesteps , input_size ) ) isinstance ( path , h5py.Group ) or u = C.assign ( update [ 0 ] , update [ 1 ] ) input_mode='linear_input ' ) for i in range ( x.ndim ) : keras_shape_list [ a ] = 1 f [ ' x ' ] = 'abcd ' if not [ spec.shape [ ch_dim ] for spec in self.state_spec ] == state_size : because they 're not making the ` num_words ` cut here . trainable_weights += layer.trainable_weights # reset gate applied after/before matrix multiplication `` `` '' Built-in activation functions . '' '' '' # and if it a Keras tensor , # Get affine transformation params # Recover per-cell states . return copy.deepcopy ( config ) output_shape_type = 'lambda ' globs = globals ( ) if _get_cntk_version ( ) > = 2.2 : cntk_shape = [ dynamic_dimension if s is None else s for s in shape ] stddev : Standard deviation of the values . if self.noise_shape is None : global uses_learning_phase key_loss_fns = { if not self._inbound_nodes : if total_loss is None : embedding = K.variable ( K.zeros ( shape ) , outputs._uses_learning_phase = True 'not supported . ' ) self.activation = activations.get ( activation ) sequence_length=input_length ) , 1 ) if [ spec.shape [ -1 ] for spec in self.state_spec ] ! = state_size : model.add ( keras.layers.Bidirectional ( rnn ( output_dim ) , ( e.g . will not include updates that depend on tensors W_regularizer='l1 ' , from .core import Dense self.recurrent_kernel [ : , :2 * self.units ] ) # TODO remove this function when Theano without `` `` '' Converts a Keras model to dot format and save to a file . value : Learning phase value , either 0 or 1 ( integers ) . preprocessing_function=None , elif orig_y_ndim == 2 : if y_expanded : output = outputs self.kernel = self.add_weight ( shape= ( input_dim , self.units * 3 ) , if update_freq == 'batch ' : assert ( history.history [ 'loss ' ] [ -1 ] < 1.1 ) depth_keys = list ( layers_by_depth.keys ( ) ) class UpSampling1D ( _UpSampling ) : weights [ 8 ] ] , axis=-1 ) the number of samples in your dataset divided by With ` distribution= '' uniform '' ` , < tf.Tensor 'Placeholder_4:0 ' shape= ( 2 , 4 , 5 ) dtype=float32 > # result tensors # this feed_dict we can provide additional substitutions besides Keras self._feed_output_names = [ ] y = T.flatten ( x ) 'This loss expects targets to have the same shape ' self.data.update ( * args ) Will only include losses that are either from tensorflow.keras.applications.inception_v3 import InceptionV3 if untar : raise ValueError ( 'Invalid ` distribution ` argument : ' output_shapes , inputs = [ inputs ] + list ( initial_state ) values = values or [ ] super ( _Pooling2D , self ) .__init__ ( * * kwargs ) log_prob_truth = np.array ( i += self.batch_size dropped_inputs , reduction=losses_utils.Reduction .SUM , name='mape_1 ' ) 'default ' : None } } , optimizer_weight_names ] assert len ( padding [ 0 ] ) == 2 C.ops.element_select ( assert np.abs ( np.mean ( rand ) ) < 0.015 def custom_mse ( * args , * * kwargs ) : 'Found : ' + str ( cropping ) ) num_classes = K.cast ( K.shape ( y_true ) [ 1 ] , y_pred.dtype ) if val.dtype.type == np.string_ : self.period = period Precision = ( TP_A + slope * ( P - P_A ) ) / P warnings.warn ( 'ModelCheckpoint mode % s is unknown , ' value = value * np.ones ( shape ) self.metrics_outputs = [ f.output for f in outputs ] conv_out = K.conv2d ( x , w , strides= ( 1 , 1 ) , return ctc_cost ( y_pred_step , y_true_step ) self.targets.append ( None ) kernels : Stacked array of kernels for individual gates . raise ValueError ( 'Inputs to ` SeparableConv ' + str ( self.rank ) + 'D ` ' self.forward_layer.constants_spec = constants_spec def test_weighted_masked_objective ( ) : cntk_shape = cntk_shape [ dynamic_axis_num : ] self.model.targets Default value is ` SUM_OVER_BATCH_SIZE ` . def __init__ ( self , stddev , * * kwargs ) : This is the expected shape of your inputs autogen.generate ( tmpdir ) if from_logits : if isinstance ( strides , int ) : printable_module_name='regularizer ' ) self.kernel_h , if self.curve == metrics_utils.AUCCurve.ROC : save_prefix : Str ( default : ` `` ` ) . if key not in cls.all ( ) : score_array = fn ( y_true , y_pred ) i = shape [ 1 ] - 1 while i < n_samples : # This is required when we want to use a metric with ` add_metric ` API on < tf.Tensor 'MatMul_9:0 ' shape= ( 2 , 4 ) dtype=float32 > assert np.max ( rand ) == 1 result_t = self.result ( ) outputs : Optional output tensors ( if already computed by running output = repeat_elements ( output , width_factor , axis=4 ) d = None ( or Python boolean , or Python integer ) def __init__ ( self , reduction , name , dtype=None ) : dim_ordering : string . ` tf ` or ` th ` . A layer config is a Python dictionary ( serializable ) for depth in depth_keys : validation_steps , config = self.get_config ( ) def __init__ ( self , inputs , outputs , updates= [ ] , * * kwargs ) : def DISABLED_test_rnn_cell_identity_initializer ( layer_class ) : out_labels=out_labels , ( -1 , input_length ) , y , 1 , output_shape [ 2 : ] ) shapes = [ K.int_shape ( p ) for p in params ] return tf.shape ( x ) new_weights = [ ] identified as such ( tp / ( tp + fn ) ) . wget http : //cntk.ai/PythonWheel/ForKeras/depends/openmpi_1.10-3.zip return y_true * ( 1.0 - smoothing ) + ( smoothing / num_classes ) A Constant Tensor . class Cropping3D ( _Cropping ) : class CallbackList ( object ) : x = tf.nn.conv3d_transpose ( x , kernel , output_shape , strides , def train_on_batch ( self , x , y , super ( SpatialDropout1D , self ) .__init__ ( rate , * * kwargs ) confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES , return variable ( value=p.value + mean ) for layer in self.layers : # From the earliest layers on . per_output_metrics = [ ] new_layer = keras.layers.AvgPool2D ( return output @ property raise ValueError ( 'interpolation should be one of `` nearest '' or `` bilinear '' . ' ) for x , y , mask in zip ( self.inputs , inputs , masks ) : metrics_dict [ metric_name ] = metric_fn from .load_backend import cos weights_bi_conv_new = saving.preprocess_weights_for_loading ( self.inputs [ 0 ] .dtype , # Legacy functions # It is populated when ` _get_available_gpus ( ) ` is called for the first time . return g dtype=dtype , is_sparse=sparse , self.output_names , return K.mean ( inputs , axis=steps_axis ) begin_axis=index , from tensorflow.python.keras.utils import metrics_utils sample_weight=val_sample_weights , old_layer = keras.layers.MaxPooling3D ( loss_name = self.model.loss x , y = generator_output ( 1 - self.rate ) * norms ) return 0 # TH input shape : ( samples , input_depth , rows , cols ) except StopIteration : if archive_type == 'zip ' : Alternatively , you can feed batches to your model manually : assert_allclose ( s_k , s_np , atol=1e-05 ) ` log ( cosh ( x ) ) ` is approximately equal to ` ( x * * 2 ) / 2 ` for small ` x ` and K.abs ( sensitivities - self.value ) , axis=0 ) from .load_backend import equal get_losses_for ( 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='avgpooling2d ' ) strides : tuple of 2 integers . # Append a wrapped layer 's label to node 's label , if it exists . 'tensorflow must be installed to read/write to GCS ' ) # batch size might be lost at this point def __init__ ( self , pool_size= ( 2 , 2 ) , strides=None , padding='valid ' , self.name + ' : expected shape= ' terminal window sizes ) . def hadamard_product_sum_output_shape ( input_shapes ) : index : Integer , index of layer . 'metrics ' : callback_metrics , outputs . The loss value that will be minimized by the model will then be trainable_weights += cell.trainable_weights self.kernel_f = self.kernel [ : , self.units : self.units * 2 ] ` metric = y_true * log ( y_true / y_pred ) ` gamma_init='uniform ' , output_shape [ unknown ] = original // known minval : A float , lower boundary of the uniform distribution states_masks = [ expand_mask ( mask , state ) for state in initial_states ] count=3 ) raise ValueError ( 'Can not perform batch dot over axis 0 . ' ) _convert_model_weights ( model , cudnn_model ) regularizer=self.W_regularizer , @ pytest.mark.parametrize ( 'dtype ' , [ 'float16 ' , 'float32 ' , 'float64 ' ] ) sed -i -e 's/ '' backend '' : [ [ : space : ] ] * '' [ ^ '' ] * / '' backend '' : \ `` ' $ KERAS_BACKEND'/g ' ~/.keras/keras.json ; warnings.warn ( ' ` output_shape ` argument not specified for layer { } ' return tf.sqrt ( var ( x , axis=axis , keepdims=keepdims ) ) from .convolutional import UpSampling2D return K.mean ( score_array ) inputlabels = 'multiple ' ( ELUs ) ] ( https : //arxiv.org/abs/1511.07289v1 ) dtype : The data type expected by the input , as a string time_axis = 1 - nones if nones > 0 else 1 super ( _ZeroPadding , self ) .__init__ ( * * kwargs ) result [ 1 ] = nb_channels if inputs_hash in self._per_input_updates : return self.activation ( outputs ) self.append_header = True assert node.output_masks == [ None ] if h5py is None : seed=seed ) , rate ) 'per model outputs . The model has { } outputs , but you ' elif ' _ { } _pickled'.format ( attr ) in self.data : '\n ' raise ValueError ( 'All target arrays ( y ) should have ' units = 7 for node_index , node in enumerate ( layer._inbound_nodes ) : config [ 'depthwise_constraint ' ] = ( target = permute_dimensions ( target , permutation ) ( 'bias ' , 'use_bias ' ) ] ) sample_weight_mode : One of ` None ` or ` `` temporal '' ` . ] weight_col=None , use for matching the given specificity . # Expecting this to never be true . str ( type ( x ) ) + ' ` . ' `` `` '' 3D deconvolution ( transposed convolution ) . if d > 1 : ' ` Sequential ` is a subclass of ` Model ` , you can ' start_time = time.time ( ) kept_idx = K.cast ( kept_idx , K.floatx ( ) ) return legacy_support # It does n't work in-place as below . w = np.transpose ( w , ( 2 , 3 , 0 , 1 ) ) return int_shape ( x ) def cosine_proximity ( y_true , y_pred , axis=-1 ) : if self.normalize : from tensorflow.keras.layers import LocallyConnected1D return C.stop_gradient ( variables ) pattern = pattern + [ [ 0 , 0 ] ] A Keras Optimizer instance . `` `` '' Model saving utilities . '' '' '' loss = K.sum ( weighted_losses ) layer.backward_layer.add_loss ( lambda : 0 ) with open ( _config_path , ' w ' ) as f : slope * [ dTP + intercept * log ( P_B / P_A ) ] / total_pos_weight callback.on_epoch_end ( epoch , logs ) cells to carry information between batches . def _static_rnn ( step_function , inputs , initial_states , target_size= ( 256 , 256 ) , def _is_tf_1 ( ) : break rankdir : ` rankdir ` argument passed to PyDot , If ` data_format='channels_last ' ` : said convolution . left_pad = dilation_rate * ( kernel.shape [ 0 ] - 1 ) new_layer = keras.layers.Conv3D ( 5 , ( 3 , 3 , 4 ) , from keras.utils import custom_object_scope x_i = K.bias_add ( x_i , self.bias_i ) g.set_shape ( g_shape ) 'output_shapes ' , signature += str_val validation at the end of the 1st , 2nd , and 10th epochs . new_layer = keras.layers.GlobalAvgPool2D ( name='global_avgpool2d ' ) from tensorflow.keras.applications.resnet import ResNet50 old_layer = keras.layers.GlobalAveragePooling3D ( dim_ordering='th ' , optimizer : Selected optimizer override = override or { } self.bias_o = None x : Input Numpy array . metrics_dict : A dict with metric names as keys and metric fns as values . from . import image model.add ( layers.RepeatVector ( 3 ) ) if self.l2 : filter_shape = ( filter_shape [ 3 ] * filter_shape [ 2 ] , 1 , max_value : Python float , integer or tensor . # Determines whether broadcasting is needed . model.compile ( 'sgd ' , loss=keras.losses.MeanAbsoluteError ( ) ) # skip the batch axis name='gamma ' , ( Optional ) 2D tensors with shape ` ( batch_size , output_dim ) ` . stateful_metrics : Iterable of string names of metrics that 'eg ` RNN ( return_state=True ) ` . ' ) if _has_compat_v1 : mask_slice = squeeze ( mask_slice , 1 ) def __init__ ( self , sequence , use_multiprocessing=False , shuffle=False ) : x=None , if inner_mask is not None : initializers.serialize ( self.moving_mean_initializer ) , `` `` '' Loads the Fashion-MNIST dataset . factors by which to downscale ( dim1 , dim2 , dim3 ) . m._call_result for m in metrics if hasattr ( m , '_call_result ' ) self.min_delta * = -1 result = self.trainer.train_minibatch ( init_tuple : a tuple , the first part of the output shape a_x = np.random.random ( ( num_samples , input_dim_a ) ) return x_weight self.test_function = K.function ( name='squared_hinge ' ) : 'reply on the RNN states , ' of numpy arrays ( in the case with elif original ! = known : from .load_backend import argmin assert os.path.isdir ( os.path.join ( tmpdir , 'models ' ) ) # Return loss and metrics , no gradient updates . Usage with compile API : volume_shape = _preprocess_conv3d_volume_shape ( int_shape ( x ) , data_format ) If None , the outputs will not be combined , def __init__ ( self , name='accuracy ' , dtype=None ) : for . def random_normal ( shape , mean=0.0 , stddev=1.0 , dtype=None , seed=None ) : shapes = [ ] RuntimeError : if the layer is n't yet built assert group3 [ ' y ' ] == [ b'efg ' , b'hij ' , b'klmn ' ] assert z_shape [ 1 : ] == z_np.shape [ 1 : ] def test_trainable_weights_count_consistency ( ) : consume_less='mem ' ) pytest tests/ -- ignore=tests/docs -- ignore=tests/keras/legacy/layers_test.py -- ignore=tests/test_api.py assert dense._inbound_nodes [ 0 ] .inbound_layers == a_layer def abs ( x ) : WITH_NP , cntk_dynamicity=True , elif self.merge_mode == 'sum ' : return tf.nn.in_top_k ( predictions=predictions , model.compile ( optimizer=optimizer , self.csv_file.close ( ) assert len ( layer.trainable_weights ) == 0 # Prepare targets of model . if s1 is not None and s2 is not None and s1 ! = s2 : group : A pointer to a HDF5 group . 'output_dim ' : self.output_dim , from .. utils.np_utils import to_categorical # Create node 's label . # Gets loss and metrics . Updates weights at each call . if ( isinstance ( x , C.variables.Parameter ) or m = keras.metrics.MeanSquaredError ( ) 'expect to be 1 or % d dimensions ' ( 'pool3d ' , ( 2 , 8 , 9 , 5 , 3 ) , ( 3 , 2 , 3 ) , ( 1 , 1 , 1 ) , ' ( symmetric_height_pad , symmetric_width_pad ) , ' op = K.update_add ( x_var , increment ) if i > 0 : if ` class_mode ` is ` `` categorical '' ` ( default value ) it must code = codecs.encode ( raw_code , 'base64 ' ) .decode ( 'ascii ' ) conv2d_transpose = conv_transpose identifier : None or str , name of the function . prog_width = int ( self.width * prog ) user . For example : 'acc ' . then the value of ` result ( ) ` is 4 . If the ` sample_weight ` is specified as set_floatx ( _floatx ) old_layer = keras.layers.GlobalAveragePooling2D ( dim_ordering='tf ' , print_fn=print_fn ) subsample_length=2 , print_fn ( '= ' * line_length ) How many zeros to add at the beginning and end of `` `` '' Global Max pooling operation for 3D data . self.thresholds = [ 0.0 - K.epsilon ( ) ] + thresholds + [ 1.0 + K.epsilon ( ) ] in which cases the cells get stacked one after the other in the RNN , # Final result : 1.0 output_mask = [ output_mask ] * len ( output_ls ) ` f ( x ) = 0 otherwise ` . ValueError if unknown identifier # Automatically track layers set as Model str ( weights ) [ :50 ] + ' ... ' ) conversions= [ ( 'nb_filter ' , 'filters ' ) , where ` fan_in ` is the number of input units in the weight tensor model at the target location , or instead prefix_shape [ axis ] = pattern [ 0 ] 'Please set ` dilation_rate ` to 1 . You passed : % s ' % ( dilation_rate , ) ) raise ValueError ( `` { } needs a ' # Raises ' section '' .format ( name ) , output = K.max ( output , axis=1 ) ( 'conv1d ' , ( 2 , 3 , 8 ) , ( 4 , 3 , 2 ) , 'valid ' , 'channels_first ' , 2 ) , self._confusion_matrix_cond = confusion_matrix_cond save_attributes_to_hdf5_group ( return json.dumps ( model_config , default=get_json_type , * * kwargs ) layers_to_output_shapes [ shape_key ] = input_shape with H5Dict ( filepath , mode= ' r ' ) as h5dict : # update callbacks to make sure params are valid each epoch weights = mask raise ValueError ( 'Stride ' + str ( self.strides ) + ' must be ' def test_save_to_binary_h5py_direct_to_file ( ) : a = k.eval ( k.arange ( start , stop , step ) ) `` `` '' Core Keras layers . '' '' '' _EPSILON = 1e-7 ` subgraph=True ` . args ) : x = np.array ( [ 1e-20 , 1e-20 , 10 , 10 , 10 ] , dtype=np.float32 ) # the dimensionality of each mask new_node_index , 3D tensor with shape : ` ( batch_size , new_steps , filters ) ` broadcast = ( False , ) * ndim ( only relevant if ` save_to_dir ` is set ) . Default : `` png '' . def standardize_single_array ( x ) : _axis = axis from tensorflow.keras.utils import get_file desired = ( self.rate * K.clip ( norms , self.min_value , self.max_value ) model = Sequential ( [ Dense ( 2 , activation='sigmoid ' , input_shape= ( 3 , ) ) ] ) self.bias_z = self.bias [ self.units * 3 : self.units * 4 ] ` ( batch , ... , channels ) ` while ` `` channels_first '' ` corresponds to return C.cast ( C.ones_like ( x , name ) , dtype ) return tf.__version__.startswith ( ' 1 . ' ) # Then we process nodes in order of layer depth . output_masks = to_list ( output_masks ) # Allow instances to be re-used def _need_convert_kernel ( original_backend ) : return categorical_crossentropy ( target , output , from_logits , axis=axis ) ` ( samples , channels , dim1 , dim2 , dim3 ) ` if ` data_format='channels_first ' ` `` `` '' Configures the model for training . data larger than HDF5_OBJECT_HEADER_LIMIT bytes . matrix_inner = K.dot ( h_tm1 , minimization of the monitored quantity . For ` val_acc ` , _backend = os.environ [ 'KERAS_BACKEND ' ] 'you can not use ` validation_split ` . ' ) class Model ( Network ) : masks.append ( mask ) return mask callbacks.on_epoch_end ( epoch , epoch_logs ) inputs : Can be a tensor or list/tuple of tensors . return output_masks return K.batch_normalization ( [ 1. , 1. , 1 . ] ] , dtype=float32 ) computes the average over classes . IOU is defined as follows : from . import core str ( len ( filtered_layers ) ) + ' layers . ' ) super ( LambdaCallback , self ) .__init__ ( ) return layer.__class__.from_config ( layer.get_config ( ) ) kwargs.pop ( 'kernel_dim2 ' ) , for i , s in zip ( int_shape ( x ) , tf.unstack ( tf.shape ( x ) ) ) : then the sample timestep will be masked ( skipped ) in all downstream layers def get_value ( x ) : pattern = [ [ 0 , 0 ] , [ padding [ 0 ] , padding [ 1 ] ] , [ 0 , 0 ] ] idx = [ x + self.start for x in key ] model._make_test_function ( ) `` `` '' Calls the ` on_train_begin ` methods of its callbacks . if len ( str_val ) > 10 : def test_trainability ( ) : input_val = np.random.random ( ( 4 , 2 ) ) config [ 'batch_input_shape ' ] = self.batch_input_shape j = 1 # Prepare display labels . for k , v in entries.items ( ) : class H5Dict ( object ) : layer_test ( convolutional.MaxPooling1D , def dtype ( x ) : # Shape : ( batch , filters , output_length , input_length * kernel_size ) self.on_epoch_begin = lambda epoch , logs : None # Training concat_layer = layers.Concatenate ( axis=1 ) The learning phase flag is a bool tensor ( 0 = test , 1 = train ) def compute_mask ( self , inputs , mask ) : config [ 'pointwise_initializer ' ] = ( if kwargs [ 'data_format ' ] == 'channels_last ' : class Activation ( Layer ) : varargs=full_arg_spec.varargs , if b_any ( [ dim < 0 for dim in base_shape ] ) : n : A list of integer . The length must be the same as the number of 'Embeddings-related arguments are ignored . ' ) # Create node , add it to inbound nodes . u_ops = [ ] self._reshape_required = False from keras.utils.test_utils import layer_test [ On the Properties of Neural Machine Translation : output names ( strings ) to scalar coefficients . if len ( inbound_layer._inbound_nodes ) < = inbound_node_index : self.trainer = None def __init__ ( self , size= ( 2 , 2 ) , data_format=None , interpolation='nearest ' , out2 = model.predict ( x ) all_names = [ layer.name for layer in layers ] model.add ( keras.layers.Dense ( hidden_dim , if ` data_format ` is ` `` channels_first '' ` def on_epoch_end ( self , epoch , logs=None ) : values_shape , weights_shape ) ) return unpack_singleton ( shapes ) are_ones = K.equal ( y_true , 1 ) return x_weight strides : strides tuple . def flatten ( x ) : If int : # with pytest.raises ( TypeError ) : Runs seamlessly on CPU and GPU . # Compute sensitivity at that index . set to ` True ` . model.build ( build_input_shape ) if isinstance ( layer , Model ) : # of the static batch size . x = tf.cast ( x , floatx ( ) ) loss = mae ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) _runner ( initializers.he_uniform ( ) , tensor_shape , super ( FalsePositives , self ) .__init__ ( cpu_relocation : A boolean value to identify whether to batch_logs [ l ] = o verbose=0 , is_keras_tensor = K.is_keras_tensor ( additional_inputs [ 0 ] ) If a ` bucket_name ` is not provided , an identifier of the import of the file_io forward_state = initial_state [ : pivot ] var = var.dimshuffle ( shuffle_pattern ) outputs = K.conv2d ( y = layer ( x ) > > > # A variable created with the keras backend is not a Keras tensor . def DISABLED_test_dynamic_set_inputs ( ) : elif data_format == 'channels_last ' : output = self._pooling_function ( inputs=inputs , if kwargs : # Add to the model any layers passed to the constructor . consume_less='gpu ' ) List of update ops . shape_expr = int_shape ( then_expression ) def DISABLED_test_check_last_is_one ( ) : `` `` '' Invokes metric function and returns the metric result tensor . '' '' '' z._keras_shape = x._keras_shape val_samples=2 , `` `` '' This is where the layer 's logic lives . self.unrelated_updates = C.combine ( dtype : Dtype to use . with K.name_scope ( 'training ' ) : conv_out = conv_out.dimshuffle ( ( 0 , 2 , 3 , 1 ) ) optimizer = optimizers.deserialize ( optimizer_config , or list of Numpy arrays if the model has multiple outputs . if allclose : # and for conversion of CuDNN layers . Update op . # epochs=1 , batch_size=4 , return K.maximum ( 0. , neg - pos + 1 . ) but states is not provided ( ` len ( states ) ` == 0 ) . Output of the layer 's ` call ` method . trainable_count = count_params ( model._collected_trainable_weights ) # Instantiate the input layer . # TODO : consider batch calls to ` set_value ` . from .. layers import InputSpec return C.pow ( x , a ) def clear_session ( ) : label = np.array ( [ [ .4 , .1 , .5 ] , [ .2 , .6 , .2 ] ] , dtype=np.float32 ) a_list.append ( a ) output = T.nnet.abstract_conv.bilinear_upsampling ( output , return DataFrameIterator ( elif is_wrapped_model ( inbound_layer ) : e.g . ` label_smoothing=0.2 ` means that we will use a value of ` 0.1 ` for check_two_tensor_operation ( 'dot ' , ( 4 , 2 ) , ( 2 , 4 ) , WITH_NP ) from .core import Flatten class Cropping1D ( _Cropping ) : return x + K.softplus ( -2 . * x ) - K.log ( 2 . ) class Conv3DTranspose ( Conv3D ) : initializer=self.moving_mean_initializer , t = k.arange ( 10 , dtype=dtype ) [ 3. , 6 . ] ] , dtype=float32 ) weights = preprocess_weights_for_loading ( layer.layer , # Used to keep track of the total loss value ( stateless ) . # model.compile ( optimizer , loss='mse ' , @ trainable_weights.setter for the samples in the next batch . This assumes a one-to-one mapping num_timesteps = 4 # now : model.output_shape == ( None , 3 , 4 ) else_expression : either a tensor , or a callable that returns a tensor . 'input_masks ' , if tf_data_format == 'NHWC ' : self._feed_outputs = [ ] if self.rank == 1 : dot = pydot.Cluster ( style='dashed ' , graph_name=model.name ) custom_objects : dict mapping class names ( or function names ) zero = _to_tensor ( 0. , x.dtype.base_dtype ) 'which is not supported . Please give fixed ' line_length : Total length of printed lines prev_value = _SYMBOLIC_SCOPE.value > > > x = K.print_tensor ( x , message= '' x is : `` ) return output_mask + state_mask * 2 for layer in self._output_layers : return C.not_equal ( x , y ) 1 ] ) if name in counts : Axes to reverse . class Conv3D ( _Conv ) : _runner ( initializers.glorot_uniform ( ) , tensor_shape , elif len ( n ) ! = len ( shape ) : kernel , padding , constants_shape = input_shape [ 1 : ] if node in nodes : in addition to the output . information , nor the layer class name . These are handled mode = mode.lower ( ) return tf.sqrt ( x ) or a dict mapping output names to target tensors . ' ( x , y , sample_weight ) ' super ( ThresholdedReLU , self ) .__init__ ( * * kwargs ) axis : Integer , axis along which the softmax normalization is applied . 'Found ' + str ( list ( set_x ) [ 0 ] ) + ' input samples ' mean , if all_names.count ( name ) ! = 1 : x_train = np.frombuffer ( imgpath.read ( ) , np.uint8 , class BinaryAccuracy ( MeanMetricWrapper ) : ` keras.utils.Sequence ` instances ( since they generate batches ) . from .load_backend import prod output : A tensor resulting from a softmax print ( '\nEpoch % 05d : % s did not improve from % 0.5f ' % Two usable wrappers are the ` TimeDistributed ` and ` Bidirectional ` wrappers . new_model = load_model ( fname , * * load_kwargs ) return K.slice ( data , start , size ) `` `` '' Gets the number of dimensions ( rank ) of the dataset . from tensorflow.keras.models import model_from_yaml try : class_weight=class_weight , self.patched_file_io = None callbacks.set_params ( callback_params ) border_mode='valid ' , Bringing back the factor ( slope / total_pos_weight ) we 'd put aside , we get target = 'CuDNNGRU ' preprocessor=deconv2d_args_preprocessor ) from . import text workers=1 , if self.output_padding is None : max_val_k = K.variable ( max_val ) initializers.VarianceScaling ] , raise ValueError ( ' ` validation_steps=None ` is only valid for a ' length ( equal to the size of this batch ) . Different batches may std = np.sqrt ( 2 . / ( fan_in + fan_out ) ) rnn_layer_kwargs = { ` ArgSpec ` struct . dp_mask = self._dropout_mask if hasattr ( self , '_updates ' ) : output_mask = K.reshape ( output_mask , output_mask_shape ) ( ` Layer ` class ) or by ` Input ` . losses += layer.get_losses_for ( None ) # delete and recreate model with ` filters=10 ` __Examples__ return NumpyArrayIterator ( with h5py.File ( fname , ' w ' ) as h5file : raise RuntimeError ( hasattr ( fit_inputs [ 0 ] , 'shape ' ) and hasattr ( val_inputs [ 0 ] , 'shape ' ) ) : inputs = K.expand_dims ( inputs , dummy_axis ) # add dummy last dimension arguments : dictionary of keyword arguments that were passed to the uses_learning_phase = True print ( 'NEW MODEL ' ) unroll=False ) beam_width=beam_width , else_expression_fn = else_expression constants=constants , return xception.preprocess_input ( * args , * * kwargs ) is_too_big = lambda x : x.nbytes > HDF5_OBJECT_HEADER_LIMIT ' with both static and dynamic batch sizes . ' subset=None ) : if depth not in layers_by_depth : # return reasonable results constants_spec = [ InputSpec ( shape=K.int_shape ( constant ) ) `` `` '' Cell class for the GRU layer . import abc super ( Flatten , self ) .__init__ ( * * kwargs ) seed : optional random seed for shuffling and transformations . weights_rank = K.ndim ( sample_weight ) # assure that model is working `` `` '' Cell class for the ConvLSTM2D layer . initial_states_np , self.state_spec = [ InputSpec ( shape=K.int_shape ( state ) ) expected_outputs [ -1 , -1 ] = expected_outputs [ -1 , -2 ] A tensor . string , path to the file to save the model to weighted_losses = sample_weight * losses similarity is 0.5 . # Handle Keras 1.1 format if len ( input_data ) == 3 : if len ( target_tensors ) ! = len ( self.outputs ) : return model.history check_single_tensor_operation ( 'round ' , ( 4 , 2 ) , WITH_NP ) 4D tensor with shape : data_format=tf_data_format ) `` `` '' Multiplies 2 tensors ( and/or variables ) and returns a * tensor * . that is used to keep track of the number of true negatives . `` `` '' Takes the path to a directory & generates batches of augmented data . # Broadcast weights if possible . maxlen=None , seed=113 , if 'nb_col ' in kwargs : `` `` '' Inverse of the ` serialize ` function . if all ( [ shape is None for shape in input_shape ] ) : W_constraint=None , __unroll__ : Boolean ( default False ) . # assert len ( val_seq.logs ) < = 4 * 5 for o in range ( len ( outputs ) ) : metrics = convert_custom_objects ( training_config [ 'metrics ' ] ) from theano import tensor as T output_shape = ( input_shape [ 0 ] , if name not in self.output_names : cells.append ( deserialize_layer ( cell_config , if y is not None : new_layer2 = keras.layers.Dropout ( 3 , name='drop ' ) class pairs to add to custom objects . # Note that TensorFlow 's native CTC code is significantly self.input_spec = InputSpec ( dtype=K.floatx ( ) , List of unique strings . `` `` '' Update the value of ` x ` by adding ` increment ` . Also called at the beginning of a validation batch in the ` fit ` methods , sample_weights , sample_weight_modes = training_utils.prepare_sample_weights ( ( may actually be faster on GPU ) while consuming less memory . assert deserialized.__closure__ == test_func.__closure__ fn = custom_objects.get ( function_name ) def parse_shape_or_val ( shape_or_val ) : raise ValueError ( 'You are trying to load a weight file ' y_pred : tensor ` ( samples , time_steps , num_categories ) ` kwargs [ 'pointwise_initializer ' ] = init @ interfaces.legacy_pooling1d_support weights += l.weights E.g . if your ` batch_size ` is 64 and you use ` gpus=2 ` , `` `` '' Initializer that generates tensors initialized to a constant value . return np.cumsum ( x , axis=axis ) target = None input_shape [ 1 ] + padding [ 0 ] [ 0 ] + padding [ 0 ] [ 1 ] , def categorical_accuracy ( y_true , y_pred ) : def _preprocess_conv2d_kernel ( kernel , data_format ) : # If we are wrapping a lambda function strip ' < > ' from the name as it is not sample_weight_modes [ i ] self.arguments = arguments # Example 'with the Theano backend . ' output._keras_shape [ axis_2 ] * = width_factor if closure is not None : inputs = K.cast ( inputs , 'int32 ' ) input_shapes : list of input shape tuples . raise ValueError ( ' % s is not compatible with % s ' % types ) legacy_pooling1d_support = generate_legacy_interface ( masks : List of mask values corresponding to each model output . `` `` '' Convolutional layers . '' '' '' seed=seed ) , mask_k = None # ndarray x_col : string , column in ` dataframe ` that contains the filenames ( or rm -rf ~/mpi # test with placeholders model.compile ( optimizer=optimizer , loss=loss ) static_batch_size = training_utils.get_static_batch_size ( first_layer ) def _preprocess_border_mode ( padding ) : def __call__ ( self , * args , * * kwargs ) : 'recurrent_activation ' : ( C.variables.Parameter , C.variables.Constant ) ) : reduction=metrics_utils.Reduction.WEIGHTED_MEAN , name=name , dtype=dtype ) old_layer = keras.layers.BatchNormalization ( mode=0 , Node ( # Bring d to the last dimension in x if self.stateful : def test_rnn_no_states ( self ) : 3D tensor with shape : ` ( batch , upsampled_steps , features ) ` . a list of instances of InputSpec ( one per input tensor ) . model.load_weights ( fname , by_name=True , skip_mismatch=True ) def NASNetLarge ( * args , * * kwargs ) : len_dim1 , len_dim2 , len_dim3 ) self.depthwise_kernel , > > > x = K.random_uniform_variable ( shape= ( 2 , 3 ) , low=0 , high=1 ) from numpy.testing import assert_array_equal _IMAGE_DATA_FORMAT = 'channels_last ' def test_lecun_uniform ( tensor_shape ) : self.recurrent_kernel_i = self.recurrent_kernel [ : , : self.units ] ( 'depthwise_conv2d ' , ( 1 , 7 , 6 , 3 ) , ( 3 , 3 , 3 , 4 ) , 'same ' , 'channels_last ' ) , name='global_maxpool2d ' ) pip install cntk source activate testenv as part of the saved model , the model is already [ A , B ] between successive thresholds , we get from .load_backend import concatenate elif self.reduction in [ path : String ; path relative to ` root ` to which the events will be sent . # 2D tensor test fixture ' as the second element of the generator . ' ) def cell ( self ) : print ( 'Restoring model weights from the end of ' from .load_backend import cumprod def DISABLED_test_weighted_masked_objective ( ) : bias_initializer=bias_initializer , axes = list ( axes ) symbolic_shape = K.int_shape ( symbolic_weights [ i ] ) 'the legacy keyword argument ' Default : sigmoid ( ` sigmoid ` ) . dense.input_mask class CSVLogger ( Callback ) : https : //arxiv.org/abs/1411.4280 ) ( you could see it as a form of random data augmentation ) . if len ( metrics ) ! = len ( output_names ) : for i , ( name , value ) in enumerate ( kwargs.items ( ) ) : _runner ( initializers.Constant ( 2 ) , tensor_shape , nested_metrics.append ( return C.random.bernoulli ( shape=shape , dtype=dtype , mean=p , seed=seed ) for _ in range ( max_ndim - x_ndim ) : gamma = C.reduce_mean ( gamma , axis - 1 ) from .. import metrics assert new_label == [ ' a ' ] prev_output = successive_outputs [ -1 ] standard deviation ` sqrt ( rate / ( 1 - rate ) ) ` . A tensor , result of 2D convolution . tuples : a list of tuples ` ( tensor , value ) ` . 'the same number of samples . Got array shapes : ' def __init__ ( self , file_io_module=None , bucket_name=None ) : `` `` '' Callback used to stream events to a server . ' states but was passed ' assert y._keras_shape == ( None , 12 ) def test_rnn_additional_states ( self ) : `` `` '' Transpose and cast the input before the conv1d . output_shape = ( None , new_height , new_width , None ) ( in case the model has multiple inputs ) . if cpu_relocation : rep=reps , axis=rep_axis ) 'Invalid summation method : `` { } '' . Valid options are : `` { } '' '.format ( with patch ( 'six.moves.input ' ) as mock : 'CNTK only supports ` eval ` with ' None or empty list all the embedding layer will be watched . @ pytest.fixture ( autouse=True ) pip install mkdocs -- progress-bar off ValueError : In case the ` layer ` argument has send = { } converted.append ( ( 'samples_per_epoch ' , 'steps_per_epoch ' ) ) # new_model = keras.models.clone_model ( regularizer=self.kernel_regularizer , When using this layer as the first layer in a model , 5 , 3 , 3 , num_chunks = 1 # step-size = 1 for data tensors cos = np.cos W_constraint='maxnorm ' , ( 'consume_less ' , 'implementation ' ) ] , ` states ` should be a numpy array or top-k highest predictions , and computing the fraction of them for which 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 ' _test_optimizer ( optimizers.Adagrad ( decay=1e-3 ) ) # # Guiding principles if data and hasattr ( data [ 0 ] , 'shape ' ) : specified using the GCS_TEST_BUCKET environment variable . num_constants = config.pop ( 'num_constants ' , None ) By default , we consider that ` output ` last_ones = ( future.get ( ) for future in last_ones if future.successful ( ) ) from .load_backend import set_floatx classes=num_classes ) state_mask = get_matching_mask ( mask [ i ] , state ) of 2-10 once learning stagnates . This callback monitors a def states ( self , states ) : f = K.function ( [ x , y ] , [ z ] ) heights = K.maximum ( y [ : self.num_thresholds - 1 ] , y [ 1 : ] ) K.cast ( # previous transitions check_rnn_operation ( step_function_k=get_step_function ( K , wi_k ) , you should not pass index = { } `` `` '' Implementation of the scikit-learn regressor API for Keras . finished_nodes.add ( node ) str ( K.dtype ( x ) ) ) outs.extend ( [ 0 . ] * len ( batch_outs ) ) total_loss = loss_weight * output_loss ` update_op ` operation that updates the given variables . y_col : string or list , column/s in ` dataframe ` that has the target data . else_expression = g ( -1 , 1 ) + depthwise_kernel.shape [ 2 : ] ) The generator should return the same kind of data as accepted by ( 'shape= ' + str ( self.shape ) ) if self.shape else `` , x = tf.cast ( x , tf.bool ) if isinstance ( validation_freq , int ) : data_format='channels_first ' , y.append ( x [ : , self._recurrent_dropout_mask = None # we do n't support init parameter with symbolic op , so eval it first as assert np.array_equal ( a_list [ i ] , a_list [ i + 1 ] ) if target_max is not None : K.get_uid ( ) whether prediction values are 1 or 0 . return init_tuple + int_shape 'output ' ) super ( TensorBoard , self ) .__init__ ( multiple output tensors , or is already connected # use the new accumulator and the * old * delta_accumulator def dropout ( self ) : # Build self.input_names and self.output_names . `` `` '' RMSProp optimizer . occasional wildly incorrect prediction . 'batch_size ' : batch_size , WITH_NP , cntk_two_dynamicity=True , axes=1 ) # # Support def test_eval_generator_with_sample_weight ( self ) : self.data_format , expr_ndim = ndim ( then_expression ) used for the linear transformation of the recurrent state . For example , if ` y_true ` is [ 0 , 1 , 1 , 1 ] and ` y_pred ` is [ 1 , 0 , 1 , 1 ] model = keras.models.Model ( inputs , state [ 0 ] ) legacy_embedding_support = generate_legacy_interface ( initial_state._keras_shape = keras_shape return self.cell.weights output = reshape ( output , from .load_backend import count_params [ 1 , 1 , 0 , 0 ] then value of ` result ( ) ` would be 2 . config.pop ( 'rank ' ) y._uses_learning_phase = False headers=self.headers ) if len ( output_names ) == 1 : if min_value is None : # Notes label_length : tensor ( samples,1 ) containing the sequence length for const._uses_learning_phase = False inner_inputs = inputs ( if the model has a single output and no metrics ) update_ops.append ( from .. utils.generic_utils import object_list_uid return C.transpose ( x , axis ) ` optimizer ` , ` loss ` , ` metrics ` or ` sample_weight_mode ` . y = expand_dims ( y , -1 ) deserialized = func_load ( serialized , closure=closure ) os.remove ( tmp_filepath ) pool_out = pool_out [ : , : , assert_args_presence ( args , doc , member , name ) while node_index < len ( node_data_list ) : if _has_nchw_support ( ) : # mse = [ 5 , 52 ] at the appropriate time . Note that the callbacks expects positional 'one integer per RNN state ) . ' ) W_constraint='maxnorm ' , if self.l1 : we will create a new global session . return super ( SimpleRNN , self ) .call ( inputs , dummy_w_2d = K.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) params = layer._canonical_to_params ( list of numpy arrays representing elif self.data_format == 'channels_last ' : # assert len ( val_seq.logs ) == 12 * 5 shape , p=p , dtype=dtype , seed=seed ) from tensorflow.keras.layers import ZeroPadding3D self.kernel = self.add_weight ( shape= ( input_dim , self.units ) , class CategoricalCrossentropy ( MeanMetricWrapper ) : return tf.nn.l2_normalize ( x , axis=axis ) return_sequences=return_sequences , Output shape tuple mae = keras.losses.MeanAbsoluteError ( ) output_index : The index of the model output for which the metric # We ca n't use isinstance here because it would require x = C.splice ( C.constant ( value=0 , shape=prefix_shape ) , x , axis=axis ) for i in range ( ndim_diff ) : embeddings_constraint='max_norm ' ) model.save_weights ( fname ) `` `` '' Calculates the number of true negatives . if len ( self.output_names ) > 1 : x_shape.append ( s ) reduction : ( Optional ) Type of loss reduction to apply to loss . * E731 \ # independently of ` data_format ` . one = ones = Ones height_padding = conv_utils.normalize_tuple ( padding [ 0 ] , 2 , ( left_dim2_crop , right_dim2_crop ) , # build an all-zero tensor of shape ( samples , output_dim ) fn = k.function ( [ x ] , [ y ] ) class CustomDialect ( csv.excel ) : ` false_positives ` and ` false_negatives ` that are used to compute the AUC . outputs = layers.Dense ( 3 ) ( x ) num_chunks += 1 def ensure_value_to_cell ( value ) : ) ) num_words = kwargs.pop ( 'nb_words ' ) A tuple ` ( data , labels ) ` . recurrent_constraint=recurrent_constraint , assert diff < 1e-8 outbound_layer = None clip_max = False work with autoencoders ) , size : Integer list/tuple or tensor recurrent_initializer='orthogonal ' , assert out_eval > 0 , ( Passing a hash will verify the file after download . The command line from .recurrent import SimpleRNNCell x = repeat_elements ( x , width_factor , axis=4 ) self.writer = None from . import io_utils if ( k == KC ) & ( cntk_dynamicity ) : output_shape = list ( output_shape ) a_list = [ ] 'sgd ' , ( if ) . base_config.pop ( 'data_format ' ) return deserialize ( config ) scale : Float , standard deviation of the normal distribution . if len ( input_shape ) ! = 2 : but input timestep is not a fixed number . keras.backend.eval ( layer.states [ 0 ] ) , state , atol=1e-4 ) kullback_leibler_divergence , name , dtype=dtype ) samplewise_center=False , file_hash='4d44cc38712099c9e383dc6e5f11a921 ' ) elif len ( input_data ) == 4 : config.pop ( 'dtype ' ) class subdirectories ( default : False ) . if not isinstance ( self.embeddings_metadata , str ) : `` `` '' A Network is a directed acyclic graph of layers . check_single_tensor_operation ( 'sigmoid ' , ( 4 , 2 ) , WITH_NP ) sample_weights=self.sample_weights , assert isinstance ( _epsilon , float ) callback_model = model._get_callback_model ( ) super ( BinaryAccuracy , self ) .__init__ ( queue_length : Queue length for keeping write_images = False ' ` output == new_states [ 0 ] ` for ' cooldown : number of epochs to wait before resuming output_shapes.append ( layers_to_output_shapes [ key ] ) def test_model_with_external_loss ( ) : in which cases the cells get stacked on after the other in the RNN , initializer='zeros ' ) y_true_rank = K.ndim ( y_true ) class SeparableConv1D ( _SeparableConv ) : if thresholds is not None : self.bias_c = self.bias [ self.units * 6 : self.units * 7 ] new_cols = ( ( cols - 1 ) * strides [ 1 ] + kernel_size [ 1 ] beta_constraint : Optional constraint for the beta weight . class Regularizer ( object ) : default : ` ( 256 , 256 ) ` . y_pred : Predicted values , a ` Tensor ` of arbitrary dimensions . input_length = np.array ( [ seq_len_0 , seq_len_1 ] , dtype=np.int32 ) if _LEARNING_PHASE in { 0 , 1 } : from tensorflow.keras.layers import LSTMCell if not isinstance ( shape , ( list , tuple ) ) : f [ ' x ' ] = 'abcd ' from .. models import save_model name='recurrent_kernel ' ) import types if not ( len ( int_shape ( x ) ) == len ( start ) == len ( size ) ) : def test_regression_predict_shape_correct_num_test_1 ( ) : for name in self.output_names : self._trainable_weights = [ ] self.activity_regularizer is not None ) : num_samples = 32 if steps_per_epoch is not None : def _check_pydot ( ) : from tensorflow.keras.activations import deserialize # ... stop sometime . subset : Subset of data ( ` `` training '' ` or ` `` validation '' ` ) if 'class_weight ' ) self._input_layers.append ( layer ) 'mode ' : self.mode , old_layer = keras.layers.SeparableConv2D ( 5 , nb_row=3 , nb_col=3 , name='conv ' ) var : Variance of batch . # ( computed tensor , compute mask ) loss='mse ' , raise ValueError ( 'CNTK Backend : the input of static rnn ' x = tf.nn.relu6 ( x ) kernel_constraint=kernel_constraint , raise ValueError ( 'Invalid padding : ' + str ( padding ) ) new_layer1 = keras.layers.Dropout ( rate=3 , name='drop ' ) if data_format == 'channels_first ' and tf_data_format == 'NHWC ' : name : String , must be unique within a model . vocabulary + 1 ) . if not all ( K.is_tensor ( v ) for v in all_inputs ) : for ( i , shape ) in enumerate ( shapes ) ] def in_test_phase ( x , alt , training=None ) : if not node.inbound_layers : If send_as_json is set to True , the content type of the request will be is_training=True ) random_tensor = rng.binomial ( noise_shape , p=retain_prob , dtype=x.dtype ) name='global_avgpool3d ' ) if x.shape [ 0 ] ! = y.shape [ 0 ] : 'GRU ( reset_after=False ) is not compatible with CuDNNGRU ' ) 'CNTK\ 's CPU version is not fully optimized , ' Theano 's arange : if only one argument is provided , _remove_long_seq = sequence._remove_long_seq # TODO : make it public ? 'Please provide different names for the metrics you have added . ' fit_inputs [ : -1 ] , batch_ids ) + [ fit_inputs [ -1 ] ] outputs = K.separable_conv2d ( from tensorflow.keras.utils import to_categorical # Compute the full input spec , including state raise AttributeError ( 'Layer ' + self.name # Aliases . return unpack_singleton ( averages ) segmentation , which first computes the IOU for each semantic class and then to avoid creating confusion on the docs website . return np.clip ( x , min_value , max_value ) } , default=get_json_type ) .encode ( 'utf8 ' ) model.add ( Dense ( 3 , name='morty ' ) ) for w , org_w in zip ( new_model.get_weights ( ) , org_weights ) : input_length = K.variable ( input_length ) # Find the rectangle heights based on ` summation_method ` . np.concatenate ( [ np.identity ( 2 ) ] * num_kernels , axis=1 ) ) ' ` get_session ` is not available when ' y_shape.pop ( -2 ) A dense tensor . parallel_model = multi_gpu_model ( model , gpus=target_gpu_id ) if steps is not None : The first argument passed to ` fn ` is the accumulator which is the from .utils.generic_utils import serialize_keras_object x_z = K.bias_add ( x_z , self.input_bias_z ) # Here we use tf.tile to mimic behavior of np.repeat so that matrix_x = K.bias_add ( matrix_x , self.input_bias ) def test_preprocess_weights_for_loading_gru_incompatible ( ) : All Keras optimizers support the following keyword arguments : # If obj is a python 'type ' class _UpSampling ( Layer ) : defaults to the [ Keras Directory ] ( /faq/ # where-is-the-keras-configuration-filed-stored ) . # Create placeholders to build the model on top of . val = pickle.loads ( val ) _LEARNING_PHASE == 1 ) ) : horizontal_flip=False , data_rec = load_from_binary_h5py ( load_function , file_like ) hash_algorithm = 'md5 ' gamma : Tensor by which to scale the input . x , gamma , beta , reduction_axes , epsilon ) self.cooldown_counter -= 1 class_name = type ( path ) .__name__ _config = json.load ( f ) except OSError : broadcast_beta = None W_constraint='maxnorm ' , metrics_updates = [ ] subsample=strides , return x_weights computed_data = [ ] # List of tuples ( input , mask ) . 'either a single ' old_layer = keras.layers.SimpleRNN ( 2 , init='normal ' , self.units : pool_size=2 , padding='valid ' , name='maxpool2d ' ) prev_output = states [ 0 ] dilation_rate : an integer or tuple/list of 3 integers , specifying from keras.utils.conv_utils import convert_kernel If ` top_k ` is set , recall will be computed as how often on average a class if len ( self.outputs ) ! = 1 : ndim=None , def _add_inbound_node ( self , input_tensors , output_tensors , `` `` '' Functional interface to the ` Subtract ` layer . untar_fpath = os.path.join ( datadir , fname ) dims = list ( range ( 1 , x_ndim ) ) + [ 0 ] def weighted_masked_objective ( fn ) : dtype : Output data type . 'does not accept positional arguments . ' `` `` '' Layer that averages a list of inputs . outs = fit_function ( fit_inputs ) seed=None ) : Building a question answering system , an image classification model , a Neural Turing Machine , or any other model is just as fast . The ideas behind deep learning are simple , so why should their implementation be painful ? { 'keras.layers.CuDNNGRU ' : keras.layers.CuDNNGRU , 'You will have to compile your model again ' param_shape = list ( input_shape [ 1 : ] ) return hasher.hexdigest ( ) the product over all dimensions . 3D tensor of shape ` ( num_samples , n , features ) ` . if isinstance ( output_shape , ( tuple , list ) ) : mask : Mask tensor . def __call__ ( self , y_true , y_pred , sample_weight=None ) : max_queue_size=max_queue_size ) def backend ( ) : from .load_backend import maximum # Private attributes to implement compatibility with Layer . return expand_dims ( result ) % ( str ( shape ) , nones ) ) 'CNTK backend : argument % s is not found in inputs . ' padding=self.padding ) regularizer=self.bias_regularizer , initializer : An Initializer instance ( callable ) . shape = K.int_shape ( w_img ) return tf.foldl ( fn , elems , initializer=initializer , name=name ) tensor : A tensor instance ( potentially sparse ) . tensors += [ K.learning_phase ( ) ] merge_mode=mode , scale=True , * * kwargs ) : `` `` '' Adagrad optimizer . estimators in scikit-learn , ` build_fn ` should provide default values for layer_configs = config StrictVersion ( tf.__version__.split ( '- ' ) [ 0 ] ) < StrictVersion ( ' 1.8.0 ' ) ) : _y.append ( np.sum ( np.stack ( __y , axis=-1 ) , axis=-1 ) ) # test that new updates are the same with both models def _get_current_tf_device ( ) : ins = x + y + sample_weights + [ 0 ] validation_steps : Only relevant if ` validation_data ` for key , value in obj.items ( ) : for node in nodes : sample_weight : Optional ` Tensor ` whose rank is either 0 , or the same rank as class History ( Callback ) : val_data = val_x + val_y + val_sample_weights Array of predictions ( if the model has a single output ) def kullback_leibler_divergence ( y_true , y_pred ) : y_pred , y_true=y_true , sample_weight=sample_weight ) ) 'mode ' : ' r ' } if source ! = target : state tensors is 1 ( for RNN and GRU ) or 2 ( for LSTM ) . self.bias_i = self.bias [ self.units * 4 : self.units * 5 ] self.backward_layer.set_weights ( weights [ nw // 2 : ] ) y = K.tile ( x , n ) K.separable_conv2d ( dummy_x_2d , dummy_w_2d , dummy_w1x1_2d , > > > kvar = K.zeros ( ( 3,4 ) ) from keras_applications import densenet assert data_keras_home == os.path.dirname ( load_backend._config_path ) via ` K.set_session ( sess ) ` . if isinstance ( constant , list ) : output_tensors= [ input_tensor ] , __return_sequences__ : Boolean . Whether to return the last output self.totals [ k ] += v * batch_size def lecun_uniform ( seed=None ) : if not hasattr ( self , 'predict_function ' ) : return [ opt ] , kwargs , [ ] base_config = super ( LSTM , self ) .get_config ( ) conv_out = conv_out.dimshuffle ( ( 0 , 2 , 3 , 4 , 1 ) ) if self._reshape_required : dropout=dropout , ' '' ) expects ' values = K.cast ( values , self.dtype ) model._make_test_function ( ) with open ( path ) as f : name='recurrent_kernel ' ) will not train on it , and will evaluate ' a generator or Sequence instance . Instead pass sample ' if do_reshape : NAME_SCOPE_STACK.append ( name ) pointwise_kernel_shape = int_shape ( pointwise_kernel ) strides = strides + ( 1 , ) It defaults to ` print ` ( prints to stdout ) . super ( Conv3DTranspose , self ) .__init__ ( params=params , raise ValueError ( 'Found a sample_weight with shape ' # This is identical to the following : super ( Adagrad , self ) .__init__ ( * * kwargs ) h._uses_learning_phase = True TypeError : if ` fn ` is not callable . class indices ( integers ) to layer.name + ' '' is part of a cycle . ' ) if K.backend ( ) in ( 'tensorflow ' , 'cntk ' ) : base_config = super ( SimpleRNNCell , self ) .get_config ( ) > > > inputs = K.placeholder ( shape= ( 2 , 4 , 5 ) ) warnings.warn ( 'The ` Highway ` layer is deprecated ' # Wrap loss function with signature ` ( y_true , y_pred , * * kwargs ) ` x = f [ ' x ' ] with K.name_scope ( self.name ) : ' else expressions . ndim ( condition ) = ' chunk_id = 0 expected_outputs = np.repeat ( inputs_vals [ ... , None ] , repeats=2 , axis=-1 ) return w * K.cast ( K.greater_equal ( w , 0 . ) , K.floatx ( ) ) x = np.random.random ( input_shape ) return ( name in arg_spec.args ) k_d = K.eval ( K.concatenate ( [ K.variable ( x_dense_1 ) , K.variable ( x_dense_2 ) ] ) ) @ interfaces.legacy_get_updates_support def check_array_length_consistency ( inputs , targets , weights=None ) : yi = y [ i ] self.shuffle = shuffle metrics_utils.ConfusionMatrix.FALSE_POSITIVES : self.false_positives , ' ` steps_per_epoch ` is not the same as the ' * * kwargs ) : noise_shape = ( input_shape [ 0 ] , input_shape [ 1 ] , 1 , 1 ) if len ( y.shape ) < 3 : categorical [ np.arange ( n ) , y ] = 1 A list of loss weights of python floats . output_index : The index of the model output for which the metric name is from .pooling import AveragePooling1D add_edge ( dot , inbound_layer_id , cell = Cell ( state_size=self.units ) 'could not access provided bucket { } '.format ( self.bucket_path ) ) that there may be cases in which this attribute is return isinstance ( x , theano.tensor.sharedvar.TensorSharedVariable ) input_c=input_c , if y_ndim == 2 : check_single_tensor_operation ( 'cos ' , ( 4 , 2 ) , WITH_NP ) execute=lambda arg : print ( arg ) , class_weight=None , all_outputs.append ( [ ] ) # # ` sample_weight ` is neither a dict nor a list . model.compile ( loss='categorical_crossentropy ' , # does not yet exist ) are re-enqueued , and the process ` directory ` ( or absolute paths if ` directory ` is None ) of the from .load_backend import zeros inputs = None X_train [ 1000 ] if clip_max : yield no conversion is made . assert ( shape [ 0 ] == layer.filters and of arrays and their shape must match tensor_map [ id ( x ) ] = ( y , mask ) successive_outputs.append ( outputs ) 'dictionary : `` ' + name + ' '' . ' raise ValueError ( 'In ` Conv2DTranspose ` , with padding mode ` same ` , ' dropout_W=0.1 , idx = key + self.start model.summary ( ) for s_k , s_np in zip ( last_states_k , last_states_np ) : elif self.verbose == 2 : ValueError : In case of invalid ` merge_mode ` argument . x_list = [ ] K.set_floatx ( old_floatx ) from .. utils.io_utils import H5Dict outs = [ ] > > > tf_session = K.get_session ( ) check_single_tensor_operation ( 'random_normal_variable ' , ( 2 , 3 ) , WITH_NP , # Arguments else , 2D tensor with shape ` ( batch_size , units ) ` . line_length = line_length or 65 import warnings y_col='class ' , # If the shape of y_true is ( num_samples , 1 ) , flatten to ( num_samples , ) # multiprocessing . We resort to an int if layer is None or node_index : `` `` '' Computes the cosine similarity between the labels and predictions . v = output_values.asarray ( ) from .. import initializers from tensorflow.keras.applications.resnet import ResNet101 model : A Keras model instance . reduction=Reduction.SUM , name='mse_1 ' ) True if we can proceed with overwrite , False otherwise . self.trainable = trainable y_rev = self.backward_layer.call ( inputs , * * kwargs ) x -= K.mean ( x , axis=1 , keepdims=True ) beam_width : if ` greedy ` is ` False ` : a beam search decoder will be used y = T.reshape ( x , ( x.shape [ 0 ] , T.prod ( x.shape [ 1 : ] ) ) ) normed , mean , stdinv = trained raise ValueError ( ' ` num_thresholds ` must be > 1 . ' ) should be the same as the size of the cell output . raise ValueError ( 'Batch dot requires inputs of rank 2 or more . ' ) rows , cols = 1 , 2 elif filename.startswith ( self._gcs_prefix ) : gamma = ones_like ( x ) return_in_sub = [ ret for code_inner in innerfunction for ret in width = 224 def __init__ ( self , schedule , verbose=0 ) : loss = convert_custom_objects ( loss_config ) def all ( x , axis=None , keepdims=False ) : if steps is None : shutil.rmtree ( path ) y = k.placeholder ( ndim=2 ) @ interfaces.legacy_spatialdropout1d_support from .merge import multiply return tf.stop_gradient ( variables ) # a get to avoid failing if the key is missing for i , layer in enumerate ( self._input_layers ) : grads ) if os.path.exists ( fpath ) : init=C.initializer.uniform ( self.bias_i_i , def _map_graph_network ( inputs , outputs ) : `` `` '' Converts binary labels into -1/1 . '' '' '' g : Tensor , the gradient tensor self.predict_function = K.function ( inputs , padding , kernel_shape , while mask_.ndim < x.ndim + 1 : A binary matrix representation of the input . The classes axis # Recover loss functions and metrics . name = 'param_ ' + str ( i ) height_shift_range=0.2 , layers in your model . self.b_carry = self.add_weight ( shape= ( input_dim , ) , if is_match_fn ( file_path ) : for idx , _input in enumerate ( self.model.input ) } can specify them via the ` target_tensors ` argument . It can be outputs_info= [ None ] + initial_states , def expand_dims ( x , axis=-1 ) : calling them with the keyword argument ` initial_state ` . from .pooling import MaxPooling2D self._helper_bilinear ( data_format , 2 , 2 ) embeddings_data=None , self.queue.put ( future , block=True ) from __future__ import absolute_import for unroll in unroll_options : i * stride_row + kernel_size [ 0 ] ) h_c = self.recurrent_conv ( h_tm1_c , 'histogram_freq was set to 0 ' ) return y , [ y ] value = np.reshape ( value , shape ) ' was passed for an output of shape ' + str ( shape ) equal to { -epsilon , 1+epsilon } for a small positive epsilon value will In the case of binary classification , return [ output_mask ] + state_mask any_sub_list = any ( isinstance ( m , list ) for m in metrics ) self.input_spec = InputSpec ( ndim=len ( input_shape ) , losses.CategoricalCrossentropy ) len_dim1 = conv_utils.conv_output_length ( len_dim1 , self.pool_size [ 0 ] , at the level of the first layer handle_method ( n , met ) np.ones ( ( num_samples , units ) ) ) top_k=self.top_k , z_list = [ k.eval ( k.gather ( k.variable ( ref ) , k.variable ( inds , dtype='int32 ' ) ) ) self.input_spec [ 0 ] = InputSpec ( shape= ( batch_size , None ) + input_shape [ 2:5 ] ) inbound_layers.append ( inbound_layer ) kshp=kernel_shape , # Updates indexed by None are unconditional # Functions or classes with less than 'MIN_CODE_SIZE ' lines can be ignored to_file='vae_mlp.png ' , `` `` '' Converts CTC labels from dense to sparse . self.queue = None warnings.warn ( ' ` Sequential.model ` is deprecated . ' * ` SUM ` : Scalar sum of weighted losses . = ( TP - TP_A ) / ( P - P_A ) output = K.dot ( inputs , self.kernel ) The use of ` keras.utils.Sequence ` guarantees the ordering assert_allclose ( k_s_d , k_d , atol=1e-05 ) def pool2d ( x , pool_size , strides= ( 1 , 1 ) , input_shape [ 2 ] + top_pad + bottom_pad , def handle_class ( name , member ) : for states_at_step in successive_states : name='bn ' ) A tensor , result of 1D convolution . Number of samples to be predicted at once . if v.ndim == 1 : from . import conv_utils if not is_all_none ( previous_mask ) : for pooling_class in [ pooling.GlobalMaxPooling3D , with pytest.raises ( ValueError ) : if layer.__class__.__name__ == 'ConvLSTM2D ' : rand = K.eval ( K.random_uniform ( ( 200 , 200 ) , min_val , max_val ) ) for j in range ( len ( output_shapes ) ) : # Add metric names from layers . x._uses_learning_phase = True for each sample at index i in a batch will be used as initial if isinstance ( x , tuple ) : state_shape = [ ( input_shape [ 0 ] , dim ) for dim in state_size ] units : Positive integer , dimensionality of the output space . mean = _reshape_dummy_dim ( mean , [ 0 ] ) def test_layer_sharing_at_heterogeneous_depth_order ( ) : ( without it , the shape of the dense outputs can not be computed ) . or list of arrays of predictions dtype : Integer dtype to use . devs_squared = tf.square ( x - m ) self.state_spec.append ( InputSpec ( shape=shape ) ) # Convert the result back to the input type . Faster than sigmoid . # Batch is ending , calculate batch time array ( [ [ 1. , 0. , 0 . ] , y_pred_step = y_pred_step [ 0 : input_length_step [ 0 ] ] kwargs [ 'input_shape ' ] = ( None , ) U_regularizer='l2 ' , self.mode = mode if ` return_state ` : a list of tensors . The first tensor is if arg not in [ 'self ' , 'args ' , 'kwargs ' ] The reduced tensor . layers.GlobalAveragePooling1D ] ] input_tensors = [ ] equal to exactly 0 or 1 . scale /= max ( 1. , fan_in ) for each threshold value . If neither thresholds nor top_k are set , the from tensorflow.keras.layers.experimental import * 'merge_mode : % s ' % ( self.merge_mode ) ) return y f_skip_idxs = ctc_create_skip_idxs ( Y ) epochs=epochs , import tensorflow as tf 'recurrent_dropout ' : self.recurrent_dropout , x._keras_shape = ( shape [ 0 ] , shape [ 1 ] , 1 , shape [ 2 ] ) return Dot ( axes=axes , normalize=normalize , * * kwargs ) ( inputs ) using ` one-hot ` representation , please use ` CategoricalCrossentropy ` loss . 'do_validation ' : do_validation , name='embeddings ' , signature += ' ) ` ' if beta.dtype ! = tf.float32 : A tensor with the product of elements of ` x ` . Currently , specifying any ` dilation_rate ` value ! = 1 is kernel = kernel.dimshuffle ( ( 2 , 3 , 0 , 1 ) ) 'Expected a symbolic tensor instance . ' ) else : that is , a different set of filters is applied at each different patch theano.tensor.TensorConstant ) ) : outputs= [ x_placeholder + 1 . ] , name = _to_snake_case ( prefix ) + ' _ ' + str ( K.get_uid ( prefix ) ) self.state_size = units from .convolutional import Conv1D output_masks=output_masks , assert o._keras_shape == ( None , None , 5 ) layer._outbound_nodes.append ( self ) `` `` '' Creates a 1D tensor containing a sequence of integers . A list of weights values ( Numpy arrays ) . self.embeddings_regularizer = regularizers.get ( embeddings_regularizer ) input_prob_matrix_0 = np.asarray ( # masking of two last timesteps for second sample only from .convolutional import Cropping3D if name.startswith ( ' _ ' ) : out = model.train_on_batch ( [ input_a_np , input_b_np ] , label classes ( 2 or more ) . Here we assume that labels are given as a ` one_hot ` 'gpu ' : 2 } } , def init_pool ( seqs ) : if tf_data_format == 'NWC ' : ` fit ` / ` evaluate ` , the unreduced vector loss is passed to the optimizer but go_backwards=False , mask=None , constants=None , generator.on_epoch_end ( ) outs = model.test_on_batch ( x , y , derived from the input shape . `` `` '' Built-in loss functions . def local_conv1d ( inputs , kernel , kernel_size , strides , data_format=None ) : inputs with shape ` ( batch , time , channels , ... ) ` . val = H5Dict ( self.data.create_group ( attr ) ) # len max_time_steps array of batch_size x depth matrices _convert_binary_labels , def __init__ ( self , input , name='convert_to_batch ' ) : `` `` '' Cropping layer for 3D data ( e.g . spatial or spatio-temporal ) . self.updates.append ( K.update ( a , new_a ) ) weights = weights [ : num_param ] reset_metrics=False ) input_tensor = Input ( tensor=x , model = Sequential ( [ Dense ( 2 , input_shape= ( 3 , ) ) ] ) 3D tensor with shape : ` ( batch_size , steps , input_dim ) ` the targets ( i.e . we are weighting timesteps , not samples ) . skip_mismatch : Boolean , whether to skip loading of layers # ( e.g . a model such as A ( B ( A ( B ( x ) ) ) ) ) model.add ( LSTM ( 64 , input_dim=64 , input_length=10 , return_sequences=True ) ) if id ( x ) not in [ id ( ct ) for ct in computable_tensors ] : metric_fn , y_true , y_pred , weights=weights , mask=mask ) return Sequential ( layers= [ origin_layer ] + layers , and just `` binary_accuracy '' for the second output , 'Layer shapes : % s , % s ' % ( shape1 , shape2 ) ) if version.endswith ( '+ ' ) : kwargs= { } , y_shape = K.shape ( y ) # ( custom class ) or if the loss function that is wrapped is Otherwise we call ` compute_mask ` of the inner layer at each time step . sample_weight=val_sample_weight , def test_random_variables ( self ) : name : String , name of returned Keras variable reference_output_tensors = node.output_tensors return h5wrapper with tf_file_io.FileIO ( target_filepath , mode='wb ' ) as target_f : any file-like object implementing the method ` write ` that accepts val_inputs = val_x + val_y + val_sample_weights + [ 0 ] t , f = cntk_func_tensors ( function_name , [ x_shape , y_shape ] , * * kwargs ) def test_orthogonal ( tensor_shape ) : def __init__ ( self , data_format=None , * * kwargs ) : ` ( batch_size , channels , pooled_rows , pooled_cols ) ` ' [ ' + str ( inbound_node_index ) + ' ] [ ' self.seed = seed of RGB data , it should have value 3 , and in case self.outputs = None if ( k == KC ) & ( cntk_dynamicity ) : invalid_thresholds ) ) str ( input_shape ) + ' : model has ' x , gamma , beta , reduction_axes , from .io_utils import HDF5Matrix mean = C.plus ( shifted_mean , shift ) def _logcosh ( x ) : from .load_backend import categorical_crossentropy losses += l.get_losses_for ( inputs ) with np.load ( path , allow_pickle=True ) as f : ) , log_p_curr.shape [ 0 ] ) , 'int32 ' ) metric_name = get_metric_name ( metric , is_weighted ) conversions= [ ( 'init ' , 'embeddings_initializer ' ) , return self.noise_shape if self.noise_shape else K.shape ( inputs ) 'thresholds ' : self.thresholds [ 1 : -1 ] , self.best = -np.Inf split in half , for GRU biases are reshaped . reason='cntk doesn\'t support gradient in this way . ' ) initializers.truncated_normal , expected_last_state [ 0 ] += ( num_timesteps - 2 ) mask , sample_weight=weights ) def on_train_begin ( self , logs=None ) : `` `` '' Converts loss to a list of loss functions . bias_initializer = self.bias_initializer custom_objects=custom_objects ) ) Can be used to feed the model miscellaneous data y = recall if has_seq : self.metrics_func.outputs , _GLOBAL_CUSTOM_OBJECTS.clear ( ) epochs=epochs , recurrent_dropout=0.1 , if compile : super ( Reduce , self ) .__init__ ( name=name , dtype=dtype ) boolean , whether the object follows the Sequence API . base_config = super ( Lambda , self ) .get_config ( ) raise AttributeError ( 'Layer must be stateful . ' ) for i , node in enumerate ( layer._inbound_nodes ) : self.output_padding = output_padding # Compute the full input spec , including state and constants def h5wrapper ( * args , * * kwargs ) : # Gather info about inputs and outputs . It crops along spatial dimensions , i.e . height and width . if 'KERAS_BACKEND ' in os.environ : if not os.path.exists ( untar_fpath ) : name='hinge ' ) : ( e.g . ` ( 2 , 3 ) * ( 4 , 3 , 5 ) - > ( 2 , 4 , 5 ) ` ) add_update `` `` '' Reverse a tensor along the specified axes image_data_generator=image_data_generator , if self._build_input_shape : from .common import set_epsilon nodes_in_decreasing_depth.append ( node ) axes = [ 1 , 0 ] + list ( range ( 2 , ndim ) ) low , high , seed=seed ) ( shape , dtype=dtype ) y_train = np.empty ( ( num_train_samples , ) , dtype='uint8 ' ) if class_id is not None : def random_normal_variable ( shape , mean , scale , dtype=None , name=None ) : y_true : Ground truth values , with the same shape as 'y_pred ' . that uses a different behavior at train time and test time . 'float64 ' from tensorflow.keras.preprocessing.image import * featurewise_std_normalization=featurewise_std_normalization , if K.backend ( ) ! = 'tensorflow ' : inner_init='uniform ' , method of the layer was called ) . self.pointwise_constraint = constraints.get ( pointwise_constraint ) return T.ge ( x , y ) ValueError : If input dimension is less than 3 . 'you can convert them to the expected format via : \n ' os.remove ( fpath ) return _LEARNING_PHASE gamma_initializer : Initializer for the gamma weight . It is a variant of Adam based on the infinity norm . [ 0 , 0 , 1 , 0 ] then the false negatives value would be 1 . averages.append ( np.float64 ( outs_per_batch [ -1 ] [ i ] ) ) return model `` `` '' Computes mean and std for batch then apply batch_normalization on batch . weight_values , Return ` None ` . Use descendant classes instead . y_shape = shape ( y ) inputs = np.asarray ( array ( [ [ 0. , 0. , 0 . ] , # there is a bug in cntk 2.1 's unpack_batch implementation assert np.abs ( np.mean ( rand ) - mean ) < std * 0.015 targets = np.zeros ( batch_size , dtype='int32 ' ) weights matrix , self._init_graph_network ( * args , * * kwargs ) weight_names.append ( name.encode ( 'utf8 ' ) ) if x is None or len ( x ) == 0 : if sys.version_info [ 0 ] == 2 : Numpy array ( s ) of predictions . ' generator based on the ' return K.in_train_phase ( noised , inputs , training=training ) output_cntk = getattr ( KC , function_name ) ( * ( placeholders + variables ) , * * kwargs ) ' outputs , but you passed ' * ` y.shape [ 0 ] ` : 100 : do not append to output shape , supports masking , typically for unused timesteps in a losses_utils.squeeze_or_expand_dimensions ( If None is passed , the loss is assumed unconditional varkw , and defaults to a python 2/3 compatible ` ArgSpec ` . < tf.Tensor 'Placeholder_11:0 ' shape= ( 2 , 3 ) dtype=float32 > input_shape [ 1 ] , [ ( v , np.zeros ( ( self.num_thresholds , ) ) ) for v in self.weights ] ) def __init__ ( self , size=2 , * * kwargs ) : ( one size per state ) . check_single_tensor_operation ( 'cumprod ' , ( 4 , 2 ) , WITH_NP ) ' '' . Need data ' 'mem ' : 1 , self.axes = axes output_shape [ 2 ] ) layers_with_complete_input.append ( layer.name ) * * kwargs : The keyword arguments that are passed on to ` fn ` . 'the underlying RNNs . ' reduction_axes = list ( range ( x.ndim - 1 ) ) [ k.variable ( x_val ) , k.variable ( y_val ) ] , * * kwargs ) ( 1 , -1 , feature_dim ) ) ) # Step-based predictions . if isinstance ( metrics , dict ) : return mean , variance `` `` '' Loss base class . from tensorflow.keras.layers import PReLU if not hasattr ( generator_output , '__len__ ' ) : return K.resize_images ( inputs , self.size [ 0 ] , self.size [ 1 ] , # Let 's use this cell in a RNN layer : Unrolling can speed-up a RNN , elems : tensor str ( ( batch_size , self.units ) ) loss='mse ' , https : //openreview.net/forum ? id=ryQu7f-RZ ) return h5file_ [ 'data ' ] [ : ] shape1 : tuple or None . Shape of the first tensor stateful=stateful , 'got { } '.format ( mask.shape ) ) raise ValueError ( 'Invalid shape for y : ' + str ( y.shape ) ) # First , we create all layers and enqueue nodes to be processed kernel_shape = kernel.shape.as_list ( ) @ interfaces.legacy_dense_support input_tensors= [ input_tensor ] , batch_size : size of batch axis . _runner ( initializers.lecun_uniform ( ) , tensor_shape , 'Got ' + str ( len ( input_shape ) ) + ' inputs . ' ) `` `` '' Permutes the dimensions of the input according to a given pattern . losses , sample_weight , reduction=self.reduction ) v.constraint = constraint if bias_shape == ( 2 * units * n_gates , ) : allclose=False , num_batches = ( size + batch_size - 1 ) // batch_size # round up # bias for hidden state - just for compatibility with CuDNN # the model is no longer sequential INTERPOLATION = 'interpolation ' has shape ` ( input_dim , output_dim ) ` , x = C.clip ( x , 0.0 , 1.0 ) # We assume our devices do n't change during our lifetime . [ [ 0.30176 , 0.28562 , 0.0831517 , 0.0862751 , 0.0816851 , 0.161508 ] , def _hash_file ( fpath , algorithm='sha256 ' , chunk_size=65535 ) : def cntk_func_tensors ( function_name , shapes_or_vals , * * kwargs ) : from keras.utils.io_utils import save_to_binary_h5py conda update -q conda print ( `` Training using single GPU or CPU .. '' ) assert K.is_keras_tensor ( keras_placeholder ) is False return Minimum ( * * kwargs ) ( inputs ) _runner ( initializers.lecun_normal ( ) , tensor_shape , > > > K.is_keras_tensor ( k_var ) str ( len ( weight_values ) ) new_layer = keras.layers.Dense ( 2 , use_bias=True , unconditional , or conditional on inputs to this model output = K.bias_add ( output , self.bias , data_format='channels_last ' ) inputs_o = inputs from .load_backend import sqrt for i , p in zip ( input_shape , placeholder_shape ) : assert_not_compatible ( gru ( cudnn=True ) , gru ( ) , # then cache them here . When any of these outputs is queried later , we initial_states_k = [ K.variable ( s ) for s in initial_states_np ] # now model.output_shape == ( None , 8 , 64 ) masks : List of masks ( tensors or None ) . config = super ( Conv3D , self ) .get_config ( ) new_shape * = tf.constant ( np.array ( [ height_factor , width_factor ] , new_layer = keras.layers.Conv1D ( 5 , 3 , output = repeat_elements ( output , width_factor , axis=2 ) warnings.warn ( 'Network returning invalid probability values . ' assert message in ex.value.message layer._num_constants = num_constants kernel_shape = kernel.eval ( ) .shape # in case of a shared variable return C.element_select ( C.greater ( x , 0 ) , res , alpha * res ) e.g . ` validation_freq= [ 1 , 2 , 10 ] ` runs validation at the end def mean_squared_error ( y_true , y_pred ) : ` ( batch , steps , channels ) ` while ` `` channels_first '' ` raise ValueError ( 'No such layer : ' + name ) A ` MaxoutDense ` layer takes the element-wise maximum of return T.pow ( x , a ) old_layer = keras.layers.BatchNormalization ( mode=0 , name='bn ' ) if 'keras_version ' in model_weights_group : ` label_smoothing=0.2 ` means that we will use a value of ` 0.1 ` for label raise ValueError ( 'interpolation should be one ' result = [ get_value ( x ) for x in xs ] if len ( gpus ) < = 1 : in inputs/kernels/outputs . def _is_graph_model ( layer ) : 'implementation ' : implementation nodes_by_depth [ depth ] = [ ] false_negatives : y_true == True and y_pred < = thresholds raise ValueError ( ' ` cropping ` should have two elements . ' # based on TensorFlow 's default : normalize along rightmost dimension assert_allclose ( res [ : , 0 ] , ref , atol=1e-05 ) String : name of an optimizer ' has been deprecated . Use instead ' y._keras_shape = tuple ( shape ) if _has_compat_v1 : input_shape= ( 10 , 299 , 299 , 3 ) ) ) if skip_target_masks [ i ] : import theano.sparse as th_sparse_module return self.noise_shape learning_rate : float > = 0 . Initial learning rate . Else , we will return the global Keras session . self.executor_fn = lambda _ : ThreadPool ( workers ) 'ipykernel ' in sys.modules ) elif isinstance ( y , dict ) : # https : //github.com/keras-team/keras/issues/9343 # issuecomment-440903847 and will return a dynamically-shaped tensor instead . for layer in sorted ( layers , key=lambda x : x.name ) : assert output == [ 30 . ] if os.path.exists ( path ) : def build ( self , input_shape ) : 'An initial_state was passed that is not compatible with ' filepath = name loss = k.sum ( exp ) raise ValueError ( 'Layer was passed initial state ' `` `` '' Returns the mean loss on the given test data and labels . return tf.sparse.concat ( axis , tensors ) # input_length can be tuple if input is 3D or higher ` ( output_at_t , states_at_t_plus_1 ) ` . The call method of the 'bias_initializer ' : 'random_uniform ' , if not isinstance ( layer.input_spec , list ) : ` Loss ` instance . If the model has multiple outputs , you can use return_states.append ( T.switch ( state_mask , new_state , state ) ) for state , new_state in zip ( states , new_states ) : if len ( set_w ) > 1 : An initializer . except ValueError : trainable = getattr ( value , 'trainable ' , False ) temp = [ x ] * n if isinstance ( cropping , int ) : # bias for inputs initial_state=None , samples dimension . Indexing starts at 1 . Beyond '' . Must be implemented on all layers that have weights . paths = decode_func ( [ input_prob , input_len ] ) Then , you can install Keras itself . There are two ways to install Keras : computed_data = [ ] # List of tuples ( input , mask ) . self.executor_fn = None def add_weight ( self , # ` loss_weights ` does not match outputs . def on_predict_begin ( self , logs=None ) : `` `` '' Callbacks : utilities called at certain points during model training . def test_pooling_invalid_use ( self ) : self._output_mask_cache = { } # Automatically track variables set as attributes . file_io_module : String identifier of the file_io module import to patch . E.g self.alpha_constraint = constraints.get ( alpha_constraint ) dynamic_axis_index = 0 if axis ! = -1 : def model ( self ) : the initial state of the RNN layer . 'config ' : layer.get_config ( ) base_config = super ( Dropout , self ) .get_config ( ) if isinstance ( input_shape , list ) : from .core import Lambda def input_mask ( self ) : batch_size = shape [ :1 ] class ZeroPadding1D ( _ZeroPadding ) : x_transposed = K.reshape ( x , K.stack ( [ batch_size , self.data_format = K.normalize_data_format ( data_format ) check_single_tensor_operation ( 'all ' , ( 4 , 2 ) , WITH_NP ) self.bias = self.add_weight ( shape= ( self.filters * 4 , ) , inputs : A list of input tensors ( at least 2 ) . ( ( 5 , 4 , 6 ) , ( 5 , 3 , 6 ) , 1 ) , created_layers [ layer_name ] = layer self._reshape_required = False An epoch is an iteration over the entire ` x ` and ` y ` m = keras.metrics.SquaredHinge ( ) `` `` '' Checks if a callable accepts a given keyword argument . str ( type ( x ) ) + ' . Full input : ' regularizer=self.pointwise_regularizer , batch_ids = index_array [ batch_start : batch_end ] not is_wrapped_model ( inbound_layer ) ) : `` `` '' Provides a scope that changes to ` _GLOBAL_CUSTOM_OBJECTS ` can not escape . with shape ` ( batch , channels , steps ) ` . data_format='channels_middle ' ) K.stack ( [ 1 , num_predictions ] ) , assert len ( layer.weights ) == 3 'except for the concat axis . ' y_true = K.cast ( y_true , y_pred.dtype ) weights = self._trainable_weights [ : ] if ( hasattr ( value , 'dtype ' ) and with `` padding [ 0 ] '' and `` padding [ 1 ] '' ( resp . ) zeros left and right . x = layer ( x ) if len ( validation_data ) == 2 : A ` DataFrameIterator ` yielding tuples of ` ( x , y ) ` assert_function_style ( name , member , doc , args ) # Raise exceptions in case the input is not compatible reps = 3 will have shape ( s1 , s2 * rep , s3 ) . x , y : tensors with ndim > = 2 def getargspec ( fn ) : before declaring the evaluation round finished . node = layer._inbound_nodes [ node_index ] # test with placeholders ( no shape info ) # or by creating a placeholder if Numpy data was provided ) . val_inputs=val_inputs , if isinstance ( size , ( list , tuple ) ) : def set_params ( self , params ) : bad_attributes = [ x for x in data if len ( x ) > HDF5_OBJECT_HEADER_LIMIT ] if steps is None : depthwise_kernel : convolution kernel for the depthwise convolution . raise ValueError ( 'The ` batch_size ` argument value { } is ' 'via both kwarg and inputs list ) ' ) preprocessor=separable_conv2d_args_preprocessor ) class SGD ( Optimizer ) : def save_function ( h5file_ ) : return ` None ` . The default 'auto ' detects the hash algorithm in use . ' , but the layer was expecting ' # now the model will take as input arrays of shape ( * , 16 ) validation every 2 epochs . If a Container , specifies the epochs on if placeholders : the model is built on top of these cols = conv_utils.conv_output_length ( cols , self.kernel_size [ 1 ] , if tf.executing_eagerly ( ) : first_val = self.data [ 0:1 ] if invalid_keys : # to self._add_inbound_node ( ) . self.units = units opt , params , _ , loss = args print_row ( fields , positions ) > > > kvar = K.zeros ( ( 2,3 ) ) def test_orthogonal_init_does_not_affect_global_rng ( ) : nb_val_samples=1 , from six.moves.urllib.error import URLError self.dilation_rate [ 0 ] ) layers_by_depth = { } decrease . In this case , SpatialDropout1D will help promote independence @ pytest.mark.parametrize ( 'model_type ' , return simple_no_states for k , v in values : save_format='png ' , prog = float ( current ) / self.target # # ` loss_weights ` is invalid type . self._updates += updates # Test that ` relu ` op gets used . alpha_p = -alpha * scale self.momentum = K.variable ( momentum , name='momentum ' ) if ndim ( xi ) == ndim ( x ) : # for older versions of CNTK '3rd entry of cropping ' ) constraint=self.alpha_constraint ) _test_optimizer ( optimizers.Adadelta ( lr=1 . ) , target=0.4 ) ( decoded , log_prob ) = ctc.ctc_beam_search_decoder ( data = [ data ] cd keras to 1 . If 0 , will execute the generator on the main thread . ndim : Number of axes of the tensor . from . import recurrent beta = C.reduce_mean ( beta , axis - 1 ) K.greater ( ( self.true_positives ) , 0 ) , legacy_dense_support = generate_legacy_interface ( data : Indexable generator ( such as list or Numpy array ) np.allclose ( K.eval ( loss ) , 16 , atol=1e-2 ) node_key = self._node_key ( layer , original_node_index ) raise ValueError ( 'Invalid border mode : ' , padding ) bottom_pad = args [ 1 ] .get ( 'bottom_pad ' , 0 ) def DISABLED_test_sequential_as_downstream_of_masking_layer ( ) : super ( ELU , self ) .__init__ ( * * kwargs ) def call ( self , inputs , training=None , mask=None ) : preprocessed_input , self._per_output_metrics [ i ] , target , output , output_mask ) def test_avgpooling1d_legacy_interface ( ) : x = _preprocess_conv3d_input ( x , data_format ) if self.target is not None and current > = self.target : if layer.__class__.__name__ == 'GRU ' : return tf.argmax ( x , axis ) K.batch_set_value ( weight_value_tuples ) self.local_objects [ filepath ] = BytesIO ( ) # Check correct values has_seq = True image_data_generator=None , if isinstance ( args [ 4 ] , tuple ) : [ 0 , 1 , 0 ] # B def _make_node_key ( layer_name , node_index ) : if skip_mismatch : # # Why this name , Keras ? gamma = ones_like ( beta ) open_fn = zipfile.ZipFile raise NotImplementedError 'is constructed . ' % g ) def DISABLED_test_fit_generator_with_class_weight ( self ) : _config_path = os.path.expanduser ( os.path.join ( _keras_dir , 'keras.json ' ) ) `` `` '' Validates that the ` batch_size ` provided is consistent with InputLayer . 2D tensor with shape : ` ( nb_samples , output_dim ) ` . initial_epoch=0 ) : return updates xs.append ( reshape ( inputs [ : , : , slice_row , slice_col ] , y_ndim = len ( y_shape ) # the ` add_metric ` API . config = { 'activation ' : activations.serialize ( self.activation ) } the variance over all dimensions . # Delegate logic to ` fit_loop ` . for name , mem in inspect.getmembers ( mod ) : if K.dtype ( x ) ! = 'float16 ' : assert len ( X_train [ 0 : ] ) == len ( X_train ) , 'Incorrect shape for sliced data ' return K.mean ( K.square ( K.maximum ( 1 . - y_true * y_pred , 0 . ) ) , axis=-1 ) batch_array = tf.transpose ( tf.reshape ( tmp , reverse ( label_shape , 0 ) ) ) raise ValueError ( 'The ` batch_size ` argument must not be specified when ' input_masks : list of input masks ( a mask can be a tensor , or None ) . base_config = super ( CuDNNGRU , self ) .get_config ( ) transformation on additional static inputs ( not changing over time ) , input_prob_matrix_1 = np.asarray ( data_format : string , one of `` channels_last '' , `` channels_first '' self.csv_file.close ( ) def check_loss_and_target_compatibility ( targets , loss_fns , output_shapes ) : # Keep track of metric instance created in subclassed model/layer . shape= [ x.shape [ normalization_axis ] ] ) return ( input_shape [ 0 ] , self.filters ) + tuple ( new_space ) for t in range ( max_time_steps ) ] name : A name of the attributes to load . for k in self._values : op = K.update ( x_var , new_x ) top_paths=top_paths ) for layer in model_layers ] k_s = K.eval ( K.dot ( K.variable ( x_sparse ) , t_W ) ) batch_size=batch_size , return six.next ( _SHARED_SEQUENCES [ uid ] ) steps=steps , return w return x.dtype.name self.output_col = output_col l1 : Float ; L1 regularization factor . if K.backend ( ) == 'cntk ' : `` `` '' Check trainable weights count consistency . new_shape = tuple ( model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.AUC ( ) ] ) m_t_bar = ( 1 . - momentum_cache_t ) * g_prime + ( def test_gradient ( self ) : ` ( batch_size , dim1 , dim2 , ... dim ( n-1 ) ) ` from .. utils.io_utils import ask_to_proceed_with_overwrite return noise_shape or as a single array . We normalize this to an ordered list of l1 : L1 regularization factor ( positive float ) . details about the training history at each epoch . if not isinstance ( inputs , list ) : 'nesterov ' : self.nesterov } approximation may be poor if this is not the case . Setting ` summation_method ` to yield from ` generator ` before declaring one epoch with output shape ` ( batch , height , width , channels ) ` , def test_bias_add ( self ) : spatial_axes = list ( range ( 1 , 1 + self.rank ) ) # Sort model layers by layer name to ensure that group names are strictly 'embeddings_constraint ' : 'op , input_shape , kernel_shape , output_shape , padding , data_format ' , [ get_output_at ( node_index ) elif ndim_cond < ndim_expr : 'of shape ( samples , classes ) . ' before = np.random.randint ( 0 , 100 , size=10 ) cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) from .convolutional import Conv3DTranspose layer = self._output_layers [ i ] Concatenate all of them and return the concatenation . def test_repeat_elements ( self ) : 'expected 0 or 1 . ' % value ) super ( Embedding , self ) .__init__ ( * * kwargs ) def pad ( x , pad_info , data_format , num_dynamic_axis ) : c_axis , d_axis , h_axis , w_axis = 4 , 1 , 2 , 3 sample_weight=None , [ Learning Phrase Representations using RNN Encoder-Decoder for 'Incompatible shapes : ` values ` { } vs ` sample_weight ` { } '.format ( from which to retrieve the attribute . config = super ( Conv2D , self ) .get_config ( ) 3D tensor with shape ` ( batch , axis_to_crop , features ) ` shape=None , # transpose kernel to output_filters first , to apply broadcast `` `` '' Returns the model 's display labels for all outputs . '' '' '' from tensorflow.keras.layers import dot sys.stdout.flush ( ) 'Keras 1 argument ` samples_per_epoch ` . ' kernel_size : a tuple of a single integer , model.add ( Conv2D ( 2 , ( 1 , 1 ) , input_shape= ( 1 , 1 , 1 ) , use_bias=False , name='rick ' ) ) return x > = y layer_names = filtered_layer_names data_format=None , pool_mode='max ' ) : from .training_utils import check_num_samples ( 'min_ndim= ' + str ( self.min_ndim ) ) if self.min_ndim else `` , data in an invalid format . `` input '' will be images identical # ca n't without specifying dtype if _get_dynamic_axis_num ( x ) > 1 : batch_size = kwargs.get ( 'batch_size ' ) self.data = f [ dataset ] def stateful ( self ) : name = build_input_shape = None global _SYMBOLIC_SCOPE # Tracks mapping of Wrapper inputs to inner layer inputs . Useful when `` `` '' Helper function for all batch_ { begin | end } methods . '' '' '' new_d_a = self.rho * d_a + ( 1 - self.rho ) * K.square ( update ) pool_size=2 , padding='valid ' , name='avgpooling2d ' ) with a single channel , inbound_node_index = input_data [ 1 ] # batch ) . val [ 0 ] , six.string_types ) ) : c_axis , h_axis , w_axis = 3 , 1 , 2 ` sample_weight_mode= '' temporal '' ` in ` compile ( ) ` . `` `` '' Built-in regularizers . self.min_delta * = 1 return self._get_node_attribute_at_index ( node_index , [ Self-Normalizing Neural Networks ] ( https : //arxiv.org/abs/1706.02515 ) y_shape.pop ( ) y = K.dot ( x , self.W ) raise Weights values as a list of numpy arrays . strides= ( 2 , 2 ) , ' ( left_dim2_crop , right_dim2_crop ) , ' 'thresholds ' : self.init_thresholds , name='global_maxpool3d ' ) strides=strides , skip_mismatch=False , reshape=False ) : We do not expect any Numpy data to be provided when calling ( 2 , 2 , 2 ) will halve the size of the 3D input in each dimension . def get_layer ( self , name=None , index=None ) : # they should be the main input and ` initial_state ` `` `` '' Upsampling layer for 2D inputs . y_pred = K.constant ( y_pred ) if not K.is_tensor ( y_pred ) else y_pred x : A tensor or variable . Note : this is the parent class of all optimizers , not an actual optimizer var = tf.cast ( var , tf.float32 ) `` `` '' Calculates a file sha256 or md5 hash . If set to True , then the output of the dot product labels_2d = K.reshape ( 'floatx ' : floatx ( ) , if self.predict_function is None : return layer_losses `` `` '' Converts a sparse tensor into a dense tensor and returns it . `` `` '' Convolutional-recurrent layers . '' '' '' Data to be visualized in TensorBoard 's Embedding tab must be passed raise ValueError ( 'CNTK Backend : Set learning phase ' isinstance ( max_value , ( int , float ) ) ) : ' It could be because a worker has died . '.format ( idx ) , check_single_tensor_operation ( 'eye ' , ( 3 , 4 ) , WITH_NP , shape_or_val=False ) or ` skip_top ` limit will be replaced with this character . intermediate = re.sub ( ' ( . ) ( [ A-Z ] [ a-z0-9 ] + ) ' , r'\1_\2 ' , name ) _runner ( initializers.zeros ( ) , tensor_shape , path : Path or file object . if K.image_data_format ( ) == 'channels_last ' : of length ` ( input_dim , ) ` . axis : Concatenation axis . layers.extend ( layers_for_depth ) pattern = [ [ 0 , 0 ] ] + pattern self.inputs [ 0 ] .shape , return _LEARNING_PHASE_PLACEHOLDER 'instead . ' ) ( tuple of integers , does not include the batch axis ) check_single_tensor_operation ( 'tile ' , ( 3 , 4 , 5 ) , WITH_NP , n= ( 3 , 1 , 2 ) ) ValueError : in case of mismatch between provided layers elif sys.version_info < ( 3 , 3 ) : for s in to_list ( input_shape ) ] ) : scope , you can still rescue it by activating this option . [ batch , channels , depth , height , width ] ( for 'channels_first ' data_format ) recurrent_activation='hard_sigmoid ' , self.workers = workers sample_weight_modes.append ( mode ) value = np.full ( x.shape , value , dtype=floatx ( ) ) if data_format == 'channels_last ' : def step ( inputs , states ) : minval : A python scalar or a scalar tensor . Lower bound of the range 'or ` ( x , y ) ` . Found : ' validation_data=validation_data , kx = K.eval ( K.map_fn ( K.sum , vx ) ) `` `` '' Sum of the values in a tensor , alongside the specified axis . `` `` '' Input layer code ( ` Input ` and ` InputLayer ` ) . '' '' '' from .load_backend import round Separable convolution performs first def recurrent_constraint ( self ) : x = C.clip ( x , 0.0 , max_value ) padding='valid ' , image_shape = transpose_shape ( image_shape , 'channels_first ' , if mask is not None and not has_seq_axis ( mask ) : data : ` data ` argument passed to ` urlopen ` . x = K.placeholder ( shape= ( 1 , None , None , None , 1 ) ) self.true_positives / denom , padding=self.padding , A None entry in a shape is compatible with any dimension , A tensor of same type as ` reference ` . output_shape , batch_size = input_shape [ 0 ] if self.stateful else None number of output units , if mode = `` fan_out '' config : Output of ` get_config ( ) ` . return variable ( np.zeros ( shape ) , dtype , name ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) for the samples in the next batch . if y_ndim > 3 : callback_metrics = out_labels + [ 'val_ ' + n for n in out_labels ] Use this cross-entropy loss when there are only two label classes ( assumed to # Typecheck that all inputs are * either * value * or * symbolic . w = args [ 1 ] elif data_format == 'channels_last ' : y2 = cntk_func ( [ x ] ) [ 0 ] `` `` '' Apply 1D conv with un-shared weights . def DISABLED_test_rnn_cell_with_constants_layer ( ) : return K.constant ( 0 , shape=shape , dtype=dtype ) `` `` '' Depthwise separable 1D convolution . self.forward_layer = copy.copy ( layer ) raise ValueError ( 'Was asked to retrieve layer at index ' if hasattr ( cls , 'from_config ' ) : from tensorflow.keras.activations import elu super ( UpSampling2D , self ) .__init__ ( normalized_size , data_format , * * kwargs ) if len ( x_shape ) > 0 : * * kwargs ) node_data.append ( [ inbound_layer.name , indices = np.arange ( len ( xs ) ) if x._keras_shape [ 1 ] is not None : return tf.reduce_logsumexp ( x , axis , keepdims ) raise ImportError ( `` Failed to import theano.sparse\n '' from tensorflow.keras import Model check_single_tensor_operation ( 'prod ' , ( 4 , 2 ) , WITH_NP ) x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) input_shapes , output_shapes , arguments=None ) : self.batch_input_shape = batch_input_shape 'fallback to auto mode . ' % mode , z._keras_shape = y._keras_shape via Keras-side shape inference . ] self.best = np.Inf A tensor , the average of the inputs . Useful for shuffling HDF5 arrays f_list.append ( f ) Global dictionary of names to classes ( ` _GLOBAL_CUSTOM_OBJECTS ` ) . recurrent_constraint=recurrent_constraint , ` ( batch , ... , channels ) ` while ` 'channels_first ' ` corresponds to reshape=reshape ) Useful for e.g . connecting RNNs and convnets together . reason='The optimization is applied only with TensorFlow . ' ) reshaped_inputs.append ( x ) input_shape = ( length , kwargs.pop ( 'input_dim ' ) ) def _base_init ( self , name=None , trainable=True , dtype=None ) : ( e.g . ` [ 'dogs ' , 'cats ' ] ` ) . Default : None . if hook == 'end ' : if unroll : tuples.append ( ( sw , w ) ) input_b = keras.Input ( ( input_dim_b , ) ) # Assumes a generator that only return None for new_model in [ new_model_disk , new_model_gcs ] : layer.name + ' '' . ' If tuple of int ( length 2 ) : _ , fname = tempfile.mkstemp ( '.h5 ' ) def l2_normalize ( x , axis=-1 ) : a = k.eval ( t ) try : dtype = first_layer.dtype def _set_sublayers ( self , layer ) : If True , process the input sequence backwards and return the self._per_input_losses [ inputs_hash ] = [ ] 'the ` Conv1D ` layer with the ` dilation_rate ` ' inputs = np.random.random ( ( num_samples , ) + input_shape ) if loss_function is None : `` `` '' Calls the ` on_epoch_begin ` methods of its callbacks . if constants is None : def DISABLED_test_fit_with_class_weight ( self ) : return Model ( model.inputs , merged ) ndim_diff = expr_ndim - cond_ndim embedding_input = tf.reshape ( embedding_input , layer_map = { } # Cache for created layers . thresholds for discretizing the curve . If set , the ` num_thresholds ` kwargs_list = [ check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=None ) for params_name in params : ( classes and functions ) if name is None : initializers.get ( moving_variance_initializer ) ) def get_tuple_shape ( nb_channels ) : Layers that have no matching name are skipped . return self.predict_generator ( initializer=self.init , 'is not found in inputs . Please double ' ( 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='avgpooling2d ' ) reduction type used with built-in Keras training loops like max_value=max_value , threshold=threshold ) with _SEQUENCE_COUNTER.get_lock ( ) : if check_array_lengths : def get_gradients ( self , loss , params ) : change of less than min_delta , will count as no self.recurrent_kernel_z = self.recurrent_kernel [ : , : self.units ] reduction=losses_utils.Reduction.SUM , name='scc ' ) If ` name ` is in ` stateful_metrics ` , constants = inputs [ -num_constants : ] assert_array_equal ( data_rec , data ) class NumpyArrayIterator ( image.NumpyArrayIterator , Iterator ) : callback_metrics = list ( model.metrics_names ) raise ValueError ( 'Optimizer must have a `` lr '' attribute . ' ) group2 [ ' x ' ] = 'abcd ' self.alpha_initializer = initializers.get ( alpha_initializer ) output_shape = self.forward_layer.compute_output_shape ( input_shape ) The list should have 2 elements , of shape ` ( input_dim , output_dim ) ` if isinstance ( loss_config , dict ) and 'class_name ' in loss_config : # First , let 's define a RNN Cell , as a layer subclass . print_fn ( ' _ ' * line_length ) variance=var , Validation quantities may not be produced for every # Build self._input_layers : a list of tensors if there are more than one outputs . # Validate user data . # are only applicable to input layers : do not pass these keywords assert_allclose ( z , z_np , atol=1e-05 ) ins_batch [ i ] = ins_batch [ i ] .toarray ( ) raise ImportError ( # Update self.updates self.padding , self.strides [ 1 ] ) c = c [ 0 ] def test_conv_1d_channels_first ( ) : delta_t_median = np.median ( self._delta_ts [ hook_name ] ) to yield from ` validation_data ` generator before stopping # add the time_step axis back on_train_end=lambda logs : json_log.close ( ) regularization_losses = [ ( ( 3 , 2 , 3 ) , ( 1 , 0 , 0 ) , ( 2 , 1 , 3 ) ) , mod = importlib.import_module ( module ) n 2 self.kernel_size [ 1 ] , A JSON string . fpath = os.path.join ( path , 'test_batch ' ) stride_row , stride_col = strides 'keras.layers.CuDNNLSTM ' : keras.layers.CuDNNLSTM } ) : check_single_tensor_operation ( 'any ' , ( 4 , 2 ) , WITH_NP ) ` model = Model ( input= [ a , b ] , output=c ) ` increment = np.random.random ( ( 3 , 4 ) ) return L1L2 ( l1=l ) return self._states `` `` '' Caches metric name and function attributes for every model output . '' '' '' metrics = [ ] T.max ( T.concatenate ( [ active_skip_idxs , [ -1 ] ] ) ) + 2 + 1 [ details ] ( https : //www.tensorflow.org/guide/embedding # metadata ) check_single_tensor_operation ( 'zeros ' , ( 3 , 5 , 10 , 8 ) , elif x.ndim == 1 : self._last_update = now if not hasattr ( self , '_trainable_weights ' ) : > labels self.sparse = sparse Files in tar , tar.gz , tar.bz , and zip formats can also be extracted . if os.path.isfile ( path ) : mask = expand_dims ( mask ) return inds [ inds < ( num_classes - 1 ) ] outs = fit_function ( ins_batch ) y=None , elif len ( args ) == 1 : dropout_U=0.1 , config = { 'name ' : self.name , # Set optimizer weights . class Dot ( _Merge ) : ` ( batch , channels , upsampled_dim1 , upsampled_dim2 , upsampled_dim3 ) ` reduction_axes == [ 0 , 2 , 3 ] and metrics : a list or a list of lists or a dict of metric functions . inbound_layer = node.inbound_layers [ i ] .name sample_weight=sample_weight , # at this point additional_inputs can not be empty if id ( lp ) in _LEARNING_PHASE_CACHE : It follows : `` `` '' Initializer that generates the identity matrix . Metrics in this list will be logged as-is in ` on_epoch_end ` . raise ValueError ( 'Please provide as model inputs ' raise ValueError ( 'Output of generator should be a tuple ' as the last part of the output shape raise ValueError ( 'CNTK backend : metrics argument % s ' for layer_name , tensor in embeddings_vars.items ( ) : # in multi-threaded environments . pool_size : Integer , size of the average pooling windows . layer.kernel_r , if data_format == 'channels_first ' : check_positional_args = False unique_variables_to_update [ v ] = nv def test_functional_model_pickling ( ) : if hasattr ( y , '_uses_learning_phase ' ) : from .generic_utils import CustomObjectScope score_array * = mask assert_value_equality = kwargs.pop ( 'assert_value_equality ' , True ) from .load_backend import is_sparse `` `` '' Context that returns a temporary filename and deletes the file on exit if output = f ( [ 10. , 20 . ] ) and what the model expects . output = permute_dimensions ( output , [ 0 , 2 , 3 , 1 ] ) new_shape = K.concatenate ( [ x_shape [ 1 : ] , return resnet50.preprocess_input ( * args , * * kwargs ) from .training import Model [ rmsprop : Divide the gradient by a running average of its recent magnitude ( 'conv2d_transpose ' , ( 2 , 3 , 8 , 9 ) , ( 3 , 3 , 2 , 3 ) , ( 2 , 2 , 8 , 9 ) , from .. engine.base_layer import InputSpec return_sequences=return_sequences , reshaped_inputs.append ( x ) @ pytest.mark.parametrize ( 'dtype ' , [ `` , 'beerfloat ' , 123 ] ) if 0 < self.dropout < 1 and self._dropout_mask is None : y = reference [ indices ] return output_tensors , output_masks , output_shapes str ( spec.shape ) + ' , found shape= ' applied to the variable after an optimizer update . # if current layer is wrapped Model layer.kernel_o , output_masks = unpack_singleton ( output_masks ) 'You passed : x= ' + str ( x ) super ( MeanAbsoluteError , self ) .__init__ ( def __init__ ( self , name='mean ' , dtype=None ) : Default : ` True ` . `` `` '' Normalizes a NumPy array . if isinstance ( generator_output , tuple ) : reduction_axes , epsilon=1e-3 ) : def test_is_keras_tensor ( self ) : previous_mask = _collect_previous_mask ( inputs ) # transpose kernel to put filters first if x.ndim == 3 : input_shape , # Deduplicate output names to handle Siamese networks . old_layer = keras.layers.AveragePooling3D ( include_optimizer : If True , save optimizer 's state together . output = layer ( inputs , initial_state=initial_state ) assert speedup > 3 for inputs in last_ones : model.add ( layer ) return ret return C.to_batch ( const_a ) d_axis , h_axis , w_axis = 2 , 3 , 4 bias_constraint=bias_constraint , 'input_shapes ' , `` `` '' Wraps a loss function in the ` Loss ` class . # Identical prediction test case : a ` StopIteration ` exception . if shape_or_val : model = Xception ( weights=None , # This will never loop forever thanks to the test above . y : array-like , shape ` ( n_samples , ) ` or ` ( n_samples , n_outputs ) ` self.minval = minval if symbolic_weights : if has_arg ( cell.call , 'constants ' ) : `` `` '' Built-in optimizer classes . `` `` '' Generate class predictions for the input samples . # Gather layer inputs . `` `` '' Start the handler 's workers . else : `` `` '' Sets the learning phase to a fixed value . output = K.local_conv1d ( inputs , self.kernel , self.kernel_size , self.strides ) to the function . model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.TruePositives ( ) ] ) def non_trainable_weights ( self , weights ) : elif len ( args ) == 4 and isinstance ( args [ 3 ] , int ) : by ` Network ` ( one layer of abstraction above ) . containing the initial values for the states used in # Raises pointwise_initializer=pointwise_initializer , return list ( x ) return json.load ( f ) 'verbose ' : verbose , self.backward_layer.build ( input_shape ) def _set_inputs ( self , inputs , outputs=None , training=None ) : if sensitivity < 0 or sensitivity > 1 : # CNTK currently do n't support cond op , so here we use Checks that the directory is empty afterwards . if not getattr ( v , '_keras_initialized ' , False ) : b_regularizer='l2 ' , programs ` shasum ` and ` sha256sum ` can compute the hash . self.bias = None # Cache newly created input layer . del shape [ index ] steps : The steps provided as an argument to fit/evaluate/predict . elif source == 'GRU ( reset_after=True ) ' : check_single_tensor_operation ( 'argmin ' , ( 4 , 2 ) , WITH_NP , axis=1 ) above_threshold = np.clip ( above_threshold , 0.0 , max_value ) # Return tensor including _keras_shape and _keras_history . prefix = self.__class__.__name__.lower ( ) from .core import Masking mean = theano.tensor.as_tensor_variable ( mean ) update_ops = metric_fn.update_state ( y_true , sample_weight=weights ) input_len_tensor = K.placeholder ( shape= ( None ) , dtype='int64 ' ) return self._compute_elemwise_op_output_shape ( shape2 , shape1 ) inputs : input tensor or list of input tensors . If int : the same symmetric cropping mean_absolute_percentage_error , name , dtype=dtype ) ` data_format= '' channels_last '' ` , or ` ( None , 128 ) ` for variable-length init='normal ' , `` `` '' Returns the loss value & metrics values for the model in test mode . kwargs [ 'initial_state ' ] = initial_state `` `` '' Converts loss weights to a list of loss weights . specificity , num_thresholds=num_thresholds , name=name , dtype=dtype ) return np.zeros ( shape , dtype=dtype ) model.compile ( optimizer , loss='mse ' , sample_weight_mode= [ 'temporal ' ] ) barstr = ' % % % dd/ % d [ ' % ( numdigits , self.target ) layers_to_output_shapes [ shape_key ] = output_shapes [ j ] recurrent_regularizer='l1 ' , output = repeat_elements ( x , depth_factor , axis=1 ) while unprocessed_nodes : if not isinstance ( axes , ( list , tuple ) ) : size = step if current is None : axis % = rank batch_hook ( batch , logs ) train_gen = datagen.flow_from_directory ( model.history = cbks.History ( ) ' ` sample_weight ` { } '.format ( layer = keras.layers.GRU ( units ) count_mode = 'samples ' `` `` '' Computes the categorical hinge metric between ` y_true ` and ` y_pred ` . out = x [ tuple ( [ py_slice ( i , i + j ) for ( i , j ) in zip ( start , size ) ] ) ] if isinstance ( lp , int ) : outputs_info= [ np.int32 ( 1 ) , log_first , np.int32 ( 1 ) , log_first ] ) computed_data.append ( tensor_map [ id ( x ) ] ) return cls.from_config ( @ functools.wraps ( func ) inbound_nodes_data = layer_data [ 'inbound_nodes ' ] from tensorflow.keras.applications.vgg16 import decode_predictions # Create an input tensor and call ` layer ` on the input tensor . masks.append ( K.ones_like ( input_i , dtype='bool ' ) ) list_of_classes = autogen.read_page_data ( page , 'classes ' ) 'The input { } could not be retrieved . ' idx = len ( x_train ) if self.monitor_op ( current , self.best ) : def load_data ( path='reuters.npz ' , num_words=None , skip_top=0 , val_outs = test_loop ( model , val_function , val_inputs , return z # this is a logistic regression in Keras 'we expect the following devices to be available : % s . ' def implementation ( self ) : for x in outputs : directory , # for enqueuer indexing . # Methods def set_learning_phase ( value ) : print ( '\nEpoch % 05d : % s improved from % 0.5f to % 0.5f , ' x : A candidate tensor . # the kwargs of the function . metric names have to be made unique by appending an integer . x._keras_shape [ 1 ] + py_sum ( padding ) , considered during deserialization . if validation_split : to yield from ` generator ` before stopping . return C.abs ( x ) config.pop ( 'dilation_rate ' ) callbacks=None , base_config = super ( Adamax , self ) .get_config ( ) def __init__ ( self , learning_rate=0.001 , beta_1=0.9 , beta_2=0.999 , channels will be equal to ` filters_in * depth_multiplier ` . name=None , def set_image_dim_ordering ( dim_ordering ) : # Fill in the output mask cache . super ( SimpleRNN , self ) .__init__ ( cell , model.add ( Conv2D ( 10 , ( 1 , 1 ) , input_shape= ( 1 , 1 , 1 ) , name='rick ' ) ) return tf.less ( x , y ) def _set_per_output_metric_attributes ( self , metrics_dict , output_index ) : shape = ( num_samples , ) + batch_out.shape [ 1 : ] def get_weights ( self ) : space = input_shape [ 2 : ] `` You probably need to pip install nose-parameterized '' ) y = T.repeat ( x , rep , axis=axis ) indices = np.arange ( len ( x_test ) ) open_fn = tarfile.open field='data ' , name='train_function ' , if isinstance ( x_weight , dict ) and output_names [ 0 ] in x_weight : depth_multiplier=depth_multiplier , return resnet50.decode_predictions ( * args , * * kwargs ) dtype=K.dtype ( self.outputs [ i ] ) ) batch_size=None , def NASNetMobile ( * args , * * kwargs ) : return C.alias ( x , name=name ) if _is_tf_1 ( ) : This function is only available with the TensorFlow backend 'for each key in : ' + str ( names ) ) bias_shape = ( 4 , ) from .advanced_activations import ReLU z = self.recurrent_activation ( x_z + recurrent_z ) if len ( x.shape ) > 2 or len ( y.shape ) > 2 : dims = [ y_ndim - 1 ] + list ( range ( y_ndim - 1 ) ) def serialize ( activation ) : return x + y return C.assign ( variable , variable * momentum + value * ( 1 . - momentum ) ) 'You should provide one ` ' + weight_type + ' ` ' check_two_tensor_operation ( 'greater_equal ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) previous_sources = get_source_inputs ( x , if len ( input_shape ) < 2 : return C.exp ( x ) for x in to_list ( output ) ] _input_tensors = [ ] else : loss_type = loss.fn if is_loss_wrapper else type ( loss ) # Compute specificity at that index . tensor_index = self._input_coordinates [ i ] [ 2 ] # we always set the loss reduction type to be ` SUM_OVER_BATCH_SIZE ` .. ValueError : In case of improperly formatted config dict . # If the wrapper modifies the inputs , use the modified inputs to _ , h0 = parse_shape_or_val ( ( num_samples , output_dim ) ) raise ValueError ( 'Invalid data_format : ' , data_format ) times = [ ] self.writer = tf.summary.FileWriter ( self.log_dir ) # assumes first uid will always be the same from .merge import Dot lambda : y_true ) name=None , seed=None ) : > > > print ( kvar ) for update in updates : cells.append ( { 'class_name ' : cell.__class__.__name__ , [ Rectifier Nonlinearities Improve Neural Network Acoustic Models ] ( A sparse tensor representation of the labels . 'bias_initializer ' : initializers.serialize ( self.bias_initializer ) , 'noise_shape ' : self.noise_shape , training ) . False : overwrite existing file , # in TensorFlow 's CTC implementation callbacks._call_batch_hook ( 'test ' , 'begin ' , batch_index , batch_logs ) # Cast the mask to floatX to avoid float64 upcasting in Theano 'epsilon ' : self.epsilon } output_array = model.predict ( input_array ) `` `` '' Functional interface to the ` Multiply ` layer . output_mask = mask corresponds to inputs with shape if metric in [ 'accuracy ' , 'acc ' ] : base_config = super ( Permute , self ) .get_config ( ) if kernel_shape is None : super ( Poisson , self ) .__init__ ( poisson , name , dtype=dtype ) ins_batch = slice_arrays ( ins [ : -1 ] , batch_ids ) + [ ins [ -1 ] ] negative_part = tf.nn.relu ( -x + threshold ) line += str ( fields [ i ] ) 'this was done on purpose . The fit and evaluate APIs will not ' us = [ K.zeros ( shape , name='u_ ' + str ( i ) ) from theano.ifelse import ifelse return log_probs , mask _assert_sparse_module ( ) self.init_thresholds = thresholds from .load_backend import cast_to_floatx initial_output = T.unbroadcast ( initial_output , 0 , 1 ) volume_shape = tuple ( int_or_none ( v ) for v in volume_shape ) slices.append ( py_slice ( None , None , None ) ) `` `` '' Proxy for tensorflow.python.lib.io.file_io.FileIO class . Mocks the class x = tf_image_ops.resize_nearest_neighbor ( x , new_shape ) '= ' + str ( expr_ndim ) ) # # Multi-backend Keras and tf.keras : steps_done = 0 This layer creates a convolution kernel that is convolved neg = -self.alpha * K.relu ( -inputs ) from tensorflow.keras.utils import SequenceEnqueuer return np.stack ( x , axis=axis ) # Call layer . if isinstance ( inputs , list ) : x_test = x_test [ indices ] x = repeat_elements ( x , depth_factor , axis=1 ) _assert_has_capability ( T.nnet , 'relu ' ) # Default values of symbolic_weights is /variable return mobilenet.decode_predictions ( * args , * * kwargs ) inner_activation='hard_sigmoid ' , # Override set_weights for backward compatibility of Keras 2.2.4 optimizer # of the positive part of the input and target_shape = [ ] If a ` bucket_name ` is provided , either as an input argument or by setting the except AttributeError : padding = _preprocess_border_mode ( padding ) def switch ( condition , then_expression , else_expression ) : x , _ , _ = generator_output A._outbound_nodes dtype : Datatype of the input . x -= alpha * negative_part self.history.setdefault ( k , [ ] ) .append ( v ) label ` 0 ` and ` 0.9 ` for label ` 1 ` `` self.thresholds = metrics_utils.parse_init_thresholds ( return 1 - C.reshape ( result , shape= ( -1 , ) ) workers : Integer . Maximum number of processes to spin up def mean_squared_logarithmic_error ( y_true , y_pred ) : proceed = ask_to_proceed_with_overwrite ( filepath ) super ( Conv2D , self ) .__init__ ( # result = max ( 0 , 1-y_true * y_pred ) = [ 1.6^2 + 1.7^2 + 1.5^2 ] / 3 return ( batch_size , ) + shape [ 1 : ] for k in range ( w.shape [ 0 ] ) : [ batch_axis ] ) ] ` ( samples , timesteps , output = repeat_elements ( output , height_factor , axis=3 ) return_state=return_state , 'driver ' : 'core ' , # Simple lookup in custom objects # # ` validation_data ` is neither a tuple nor a triple . self.thresholds = [ 0.5 ] but the function accepts a ` * * kwargs ` argument . fn : The loss function to wrap , with signature ` fn ( y_true , y_pred , `` `` '' Produces a prompt asking about overwriting a file . # ` [ h2 , c2 , h1 , c1 ] ` . if len ( set_y ) > 1 : `` `` '' Reuters topic classification dataset . '' '' '' weight_value_tuples.append ( ( symbolic_weights [ i ] , return inputs [ slices ] max_queue_size=10 , if allowed_positional_args is None : signature = ' ` ' + object_name + ' ( ' # If the channels are not in the last axis , move them to be there : losses += layer.get_losses_for ( inputs ) if isinstance ( output_mask , list ) : strides : tuple of 3 integers . if self._layers and isinstance ( self._layers [ 0 ] , InputLayer ) : from tensorflow.keras.utils import * reduction=losses_utils.Reduction.NONE ) and replaces them by the corresponding dynamic shapes of the tensor . def DenseNet121 ( * args , * * kwargs ) : layer_map [ original_input_layer ] = newly_created_input_layer new_shape = new_shape [ num_dynamic_axis : ] @ interfaces.legacy_recurrent_support # in case of a shared variable normed = tf.nn.batch_normalization ( __Note on passing external constants to RNNs__ y = np.random.random ( ( samples , target_dim ) ) `` padding [ 0 ] '' , `` padding [ 1 ] '' and `` padding [ 2 ] '' zeros left and right . callbacks=None , ( 'separable_conv2d ' , ( 2 , 3 , 5 , 6 ) , ( 4 , 3 ) , 2 , 'valid ' , 'channels_first ' ) , ] kept_nodes = 1 return tf_keras_backend.binary_crossentropy ( layer_map [ _original ] = _cloned if layer.name == 'input1 ' : def iter ( self ) : weight , mode = get_output_sample_weight_and_mode ( self.kernel , self.name + '.build ( batch_input_shape ) ` . ' ) number of elements in values . def prod ( x , axis=None , keepdims=False ) : if not isinstance ( axes , int ) : from .base_layer import Layer , Node , InputSpec By default the file at the url ` origin ` is downloaded to the sample_weight=None , msg = 'Invalid argument `` % s '' passed to K.function with Theano backend ' % key def states ( self ) : model.compile ( loss=custom_mse , optimizer=CustomSGD ( ) , metrics= [ 'acc ' ] ) feed_dict = { self.model.input : embeddings_data [ 0 ] [ batch ] } w_img = tf.squeeze ( weight ) def assert_input_compatibility ( self , inputs ) : batch_size = self._validate_or_infer_batch_size ( _LEARNING_PHASE_CACHE [ id ( lp ) ] = int_lp batch_val.append ( val_data [ -1 ] ) if hasattr ( self.forward_layer , 'non_trainable_weights ' ) : 'strides ' : self.strides , flag = False will then be the sum of all individual losses . def test_spatialdropout1d_legacy_interface ( ) : from .load_backend import batch_flatten x_mid_dims = x_shape [ 1 : -1 ] T.TensorConstant ) ) return tf.control_dependencies ( control_inputs ) # new : i , f , c , o output_shape , for pooling_class in [ layers.GlobalMaxPooling1D , factors by which to downscale ( vertical , horizontal ) . if stop is None : yield nested_metrics = [ generic_utils.to_list ( m ) for m in metrics ] first_cropped_axis , second_cropped_axis , third_cropped_axis ) ` return ' '' [ % s ] '' ' % ( ' , '.join ( map ( str , k ) ) ) relies on multiprocessing , if device_type not in [ 'cpu ' , 'gpu ' ] : decoded_length = np.zeros ( ( num_samples , ) , dtype=np.int ) 'Use the functional API instead . ' ) self._node_key ( inbound_layer , node_index ) , 0 ) binary_accuracy , name , dtype=dtype , threshold=threshold ) num_classes = 20 from .load_backend import zeros_like The sigmoid activation : ` 1 / ( 1 + exp ( -x ) ) ` . self._is_graph_network = False def DISABLED_test_training_and_eval_methods_on_backend_tensors_single_io ( ) : non_sequences=constants , def _cache_output_metric_attributes ( self , metrics , weighted_metrics ) : two standard deviations from the mean are dropped and re-picked . 'When feeding symbolic tensors to a model , we expect the ' inputs = keras.Input ( shape= ( 5 , ) ) def get_word_index ( path='imdb_word_index.json ' ) : > to_categorical ( labels ) def __init__ ( self , cropping , if not hasattr ( tensor , '_keras_history ' ) : `` `` '' Reset learning phase flag for cntk backend . The convolution operation is implemented differently in different backends . def pool3d ( x , pool_size , strides= ( 1 , 1 , 1 ) , padding='valid ' , shape = [ ] If ` sample_weight ` is ` None ` , weights default to 1 . WEIGHTED_MEAN = 'weighted_mean ' ` `` same '' ` may be supported in the future . U_regularizer='l1 ' , # Raise exceptions in case the input is not compatible layer = layer_class ( units , recurrent_initializer='identity ' ) 'activity_regularizer ' : regularizers.l2 ( 0.01 ) , from skimage.transform import resize y_pred , assert layer.get_losses_for ( x ) == [ y ] for state in initial_state : beta = tf.cast ( beta , tf.float32 ) outputs [ 0 ] , # 1D vectors or scalars . def symbolic ( func ) : 'it must contain 2 ( x_val , y_val ) ' k_input_lens = K.variable ( input_lens , dtype= '' int32 '' ) class GaussianNoise ( Layer ) : self.seen += batch_size # on the format ` ( rows , cols , input_depth , depth ) ` , layer = node.inbound_layers [ i ] A tensor , the sum of the inputs . def predict_generator ( model , generator , new_shape = K.concatenate ( [ K.expand_dims ( batch_size ) , root : String ; root url of the target server . return shape_or_val.shape , shape_or_val allclose=True , return { 'l1 ' : float ( self.l1 ) , @ trainable.setter def get_json_type ( obj ) : assert 9e-38 < p2 < = 1e-37 if _axis is None : if hasattr ( ins [ 0 ] , 'shape ' ) : mae = MAE = mean_absolute_error tmp_target_tensors = [ ] h , y = _set_keras_shape_for_reduction ( x , y , axis , keepdims ) # transpose ( and reshape ) input and recurrent kernels layer_test ( layers.MaxPooling2D , return tf.size ( x , name=name ) return T.nnet.sigmoid ( x ) model.compile ( optimizer , loss='mse ' , loss_weights= ( 0.5 , 0.5 ) ) output of ` get_weights ` ) . assert_allclose ( f ( [ x_np , y_np ] ) [ 0 ] , z_np , atol=1e-05 ) epochs to run before a new validation run is performed , e.g . layer_test ( convolutional.AveragePooling1D , thresholds : ( Optional ) A float value or a python list/tuple of float model.compile ( loss='categorical_crossentropy ' , optimizer='rmsprop ' ) with the custom object . global _SEQUENCE_COUNTER out_pad_d = out_pad_h = out_pad_w = None for i , out in enumerate ( outs ) : ( if validation is enabled in ` fit ` ) , and ` val_acc ` assert not ( w == org_w ) .all ( ) spatial_axes= ( 1 , ) ) def assert_list_pairwise ( z_list , def test_TensorBoard ( tmpdir , update_freq ) : v1_variable_initialization ( ) value = value.astype ( dtype ) split_at = int ( len ( x [ 0 ] ) * ( 1 . - validation_split ) ) process_layer ( layer_data ) quantity and if no improvement is seen for a 'patience ' number axis_2 = 3 x = x * tf.cast ( tf.greater ( x , threshold ) , floatx ( ) ) # convert dense predictions to labels # Special case for finite generators ` validation_freq=2 ` runs validation every 2 epochs . If a list , initial_state = to_list_or_none ( initial_state ) if len ( first_line.strip ( ) ) == 0 : print ( '\nEpoch % 05d : ReduceLROnPlateau reducing ' class Orthogonal ( Initializer ) : self.stateful = False py_slice ( padding [ 0 ] [ 0 ] , input_shape [ 1 ] + padding [ 0 ] [ 0 ] ) , h_tm1_o = h_tm1 * rec_dp_mask [ 3 ] input_masks , output_masks , for constant in constants ] zipped_inputs = zip ( self.targets , self.outputs , self.loss_functions , Theano backend ( Theano tensor type ) and TF backend ( TF TensorShape ) . func : Function applied to kernel of each gate . self.sk_params.update ( params ) a weight ( float ) to apply to the model 's loss for the samples output_shape = ( None , ) + output_shape inputs : tensor # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) filters = 2 w = np.fliplr ( np.flipud ( w ) ) else : pos = K.relu ( x ) op = T.nnet.abstract_conv.AbstractConv2d_gradInputs ( padding=padding , data_format=data_format , pool_mode=pool_mode , parallel_model = multi_gpu_model ( model , 2 ) self.model.save ( filepath , overwrite=True ) loss_fn , losses.SparseCategoricalCrossentropy ) ) : def DISABLED_test_stateful_invalid_use ( layer_class ) : from .load_backend import pattern_broadcast 'b_constraint ' : constraints.MaxNorm ( 1 ) } , > > > K.floatx ( ) # Prepare output masks . as 'states ' . def __call__ ( self , inputs ) : layer_weights = weights [ : num_param ] return self._layers # on the format ` ( steps , input_depth , depth ) ` , [ 1 , 1 , 0 , 0 ] then the accuracy would be 1/2 or .5 . self._per_input_losses [ inputs_hash ] += losses interpreted as class LeakyReLU ( Layer ) : config = json.loads ( json_string ) initializer=init , parallel_iterations=1 ) skipgrams = sequence.skipgrams 'samples ' : num_samples , target_size : Tuple of integers ` ( height , width ) ` , str ( x_shape ) + ' and ' + str ( y_shape ) y_squashed_shape = tf.stack ( [ y_shape [ 0 ] , y_shape [ 1 ] , y_squashed_dim ] ) dot.add_edge ( pydot.Edge ( inbound_layer_id , ` `` multi_output '' ` : list with the values of the different columns , # in the case where all inputs are value arrays . raise ValueError ( 'Layer was passed initial state ' from .load_backend import int_shape [ f_active , log_f_probs , b_active , log_b_probs ] , _ = theano.scan ( histogram_freq=0 , outputs = results [ 0 ] _GLOBAL_CUSTOM_OBJECTS.update ( objects ) score_array = K.mean ( score_array , 'of a tensor that comes from a Keras layer ' k.variable ( x_val ) , k.variable ( y_val ) , * * kwargs ) self._output_layers.append ( layer ) old_layer = keras.layers.Deconvolution2D ( 5 , 3 , 3 , depthwise_kernel = expand_dims ( depthwise_kernel , 1 ) out = keras.layers.Dense ( 4 ) ( added ) if dim_ordering not in { 'tf ' , 'th ' } : ( 'W_regularizer ' , 'kernel_regularizer ' ) , name='d ' ) mean_squared_logarithmic_error , name=name , reduction=reduction ) x_shape , x_val = parse_shape_or_val ( x_shape_or_val ) self.noise_shape = noise_shape self.bias_f = self.bias [ self.filters : self.filters * 2 ] def _preprocess_conv2d_filter_shape ( filter_shape , data_format ) : x_train = x_train.transpose ( 0 , 2 , 3 , 1 ) validation_split=0. , if specificity < 0 or specificity > 1 : if old_name in kwargs : model_config = json.loads ( model_config.decode ( 'utf-8 ' ) ) return theano.foldl ( lambda x , acc : fn ( acc , x ) , assert len ( x.shape ) == 4 - ( 1 if num_dynamic_axis > 0 else 0 ) < tf.Tensor : id=9 , shape= ( ) , dtype=int32 , numpy=4 > object_type='method ' ) if isinstance ( noise_shape , list ) : new_model = pickle.loads ( state ) containing the configuration of a layer . count=4 ) m = keras.metrics.Mean ( ) return tf.reduce_max ( x , axis , keepdims ) def DISABLED_test_zero_padding_3d ( data_format , padding ) : y_train [ ( i - 1 ) * 10000 : i * 10000 ] ) = load_batch ( fpath ) from tensorflow.keras.preprocessing import * if len ( n ) < len ( shape ) : # Padding the axis cleanup_callback ] ) `` `` '' Returns the name corresponding to the given metric input . to which to save the augmented pictures being generated if hasattr ( self , 'csv_file ' ) and not self.csv_file.closed : to have the norm between a lower bound and an upper bound . self.kernel = self.add_weight ( shape= ( input_dim , self.units * 4 ) , for a in axis_list : > > > xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes= ( 1 , 2 ) ) assert_args_presence ( init_args , member.__doc__ , member , name ) self.save_best_only = save_best_only pool_size= ( 2 , 2 , 2 ) , strides= ( 2 , 2 , 2 ) , padding='valid ' , ValueError : if ` input_shape ` and ` output_shape ` do not match . # new_model.train_on_batch ( None , val_out ) for instance batch norm updates are conditional on the layer 's inputs . return _reshape_batch ( x , shape ) return tf.SparseTensor ( indices , vals_sparse , label_shape ) # to keep the existing behavior . This is only useful when use outs.append ( 0 . ) class TopKCategoricalAccuracy ( MeanMetricWrapper ) : bias_constraint='unit_norm ' , return T.round ( x , mode='half_to_even ' ) [ Batch Normalization : Accelerating Deep Network Training by def temporal_padding ( x , padding= ( 1 , 1 ) ) : new_shape = transpose_shape ( new_shape , data_format , `` `` '' Resize the images contained in a 4D tensor of shape # so it may have an extra batch axis with 1 , it is not needed pointwise_kernel_shape , num_samples = 1000 validate_filenames=True ) : shape= ( self.units , self.units ) , 'inputs using K.expand_dims ( x , 0 ) ' ) from tensorflow.keras.applications.inception_v3 import preprocess_input It can either wrap an existing tensor ( pass an ` input_tensor ` argument ) str ( len ( self._input_layers ) ) + ' tensor inputs . ' ) # Normalize and set self.inputs , self.outputs . output_tensors = to_list ( build_input_shape = config.get ( 'build_input_shape ' ) return T.log ( T.sum ( T.exp ( x ) , axis=axis , keepdims=keepdims ) ) # File found ; verify integrity if a hash was provided . def normalize_inference ( ) : if py_any ( list ( is_symbolic ( x ) for x in ( shape , p ) ) ) : Same shape as the input . x_np = np.random.random ( x_shape ) applies mid-point summation scheme for ` ROC ` . For PR-AUC , interpolates slices.append ( py_slice ( None , None , -1 ) ) with K.name_scope ( 'weight_regularizer ' ) : ` keras.utils.hdf5_utls.H5Dict ` object , which can wrap HDF5 metrics= [ keras.metrics.SparseTopKCategoricalAccuracy ( ) ] ) def test_on_epoch_end_threads ( ) : self.filepath = filepath check_single_tensor_operation ( 'max ' , ( 4 , 2 ) , WITH_NP ) `` `` '' Saves weights into the HDF5 group . padding , data_format , masks.append ( None ) weights = convert_nested_bidirectional ( weights ) # mse = [ ( ( 4 - 1 ) ^2 + ( 8 - 9 ) ^2 ) / 2 , ( ( 12 - 2 ) ^2 + ( 3 - 5 ) ^2 ) / 2 ] mean = tf.cast ( mean , tf.float32 ) is_chunked = chunk_attr in self.data.attrs noise_shape = [ symbolic_shape [ axis ] if shape is None else shape th_padding = 'full ' mask=None , # KTF.function ( ) . pool_size=pool_size , strides=strides , legacy_pooling2d_support = generate_legacy_interface ( 'and columns . ' ) parameter should always be used . def conv_transpose ( x , w , output_shape , padding , data_format , dilation_rate=1 ) : W_regularizer='l1 ' , self.activity_regularizer = regularizers.L1L2 ( l1=l1 , l2=l2 ) depthwise_kernel_shape = int_shape ( depthwise_kernel ) epoch ) shape=cntk_shape , batch_shape = ( None , ) + tuple ( shape ) dtype stddev=self.stddev ) ` recurrent_activation='sigmoid ' ` . if isinstance ( value , C.cntk_py.Function ) : # CONVOLUTIONS ` ( inputs , targets ) ` or ` ( inputs , targets , sample weights ) ` . self.count = self.add_weight ( return w / ( K.epsilon ( ) + K.sqrt ( K.sum ( K.square ( w ) , if not hasattr ( self , 'train_function ' ) : def __init__ ( self , input_dim , output_dim , namespace [ k ] = v res = load_function ( * _args , * * _kwargs ) To be used together with the dropout variant `` AlphaDropout '' . new_out = new_model.predict ( x ) if len ( args ) > 1 : layer_class_name = layer.__class__.__name__ model.add ( Dense ( 32 , input_shape= ( 16 , ) ) ) 3D tensor with shape ` ( batch_size , timesteps , input_dim ) ` , from .convolutional import UpSampling1D @ interfaces.legacy_cropping3d_support `` categorical '' will be 2D one-hot encoded labels , self._init_graph_network ( self.inputs , strides=self.strides , feed_output_shapes.append ( data_format=self.data_format , def __getstate__ ( self ) : for l in getattr ( self , '_layers ' , [ ] ) : new_p = p.constraint ( new_p ) self.states = [ K.zeros ( get_tuple_shape ( self.cell.state_size ) ) ] def __init__ ( self , data_format='channels_last ' , * * kwargs ) : new_states_temp.append ( C.element_select ( m , n , s ) ) dimensions . `` `` '' Extracts a slice from a tensor . Note that ` print_tensor ` returns a new tensor identical to ` x ` return TFOptimizer ( identifier ) # On top of new , non-Keras tensors model.train_on_batch ( x , y ) def slice ( x , start , size ) : 'optimizer . ' ) # remove added dim # since it does not include iteration at head of the weight list . Set initialize_weights ( src ) .get_weights ( ) ) auto_padding= [ False ] ) # Update the depth of inbound nodes . y_classes = np.reshape ( y , y.shape [ 0 ] ) axis : Int specifying the channels axis . ` axis=-1 ` a single tensor ( for a single-output model ) , a list of tensors , assert key in layers_to_output_shapes class KerasClassifier ( BaseWrapper ) : j = Input ( shape= ( 32 , ) , name='input_j ' ) raise TypeError ( 'All layers in a Sequential model ' self.bias_o , y_placeholder = K.placeholder ( shape= ( ) ) return mobilenet_v2.decode_predictions ( * args , * * kwargs ) restore_best_weights : whether to restore model weights from K.batch_set_value ( weight_value_tuples ) own_weights = weights [ : num_param ] for i in range ( shape ( elems ) [ 0 ] ) : it is in fact the `` stop '' argument and `` start '' is 0 . class SimpleRNN ( RNN ) : z = K.bias_add ( z , self.bias ) from .pooling import AvgPool1D # Note that in this case train_output and test_output are the same pointer . elif initializer is None : bias_constraint='unit_norm ' , result_t = K.identity ( result_fn ( * args , * * kwargs ) ) 'should be None or `` temporal '' . ' batch = slice ( i , i + step ) One of { 'sum ' , 'mul ' , 'concat ' , 'ave ' , None } . score2 = nested_model ( input2 ) epoch_logs [ 'val_ ' + l ] = o input shape : ` output_shape = f ( input_shape ) ` i += 1 mode='max ' ) return C.square ( x ) ` metrics= { 'output_a ' : 'accuracy ' , 'output_b ' : [ 'accuracy ' , 'mse ' ] } ` . result_t._metric_obj = self from tensorflow.keras.applications.vgg19 import decode_predictions return C.random.uniform ( 'inputs ' in kwargs and 'outputs ' in kwargs ) : first_axis_to_crop , second_axis_to_crop , third_axis_to_crop ) ` cell_losses = self.cell.get_losses_for ( inputs ) if weights_shape is not None and values_shape is not None : alt : What to return otherwise `` `` '' Calculates the number of true positives . directory , image_data_generator , model.add ( Conv2D ( 2 , ( 1 , 1 ) , input_shape= ( 1 , 1 , 1 ) , name='rick ' ) ) model.add ( Dense ( 32 , input_shape= ( 500 , ) ) ) steps_per_epoch=None , ` nb_feature ` ` Dense ( input_dim , output_dim ) ` linear layers . # Subclassed networks are not serializable A numpy array of class predictions . def bucket_path ( self ) : Do not use in a model -- it 's not a valid layer ! ( 'pool3d ' , ( 3 , 3 , 8 , 5 , 9 ) , ( 2 , 3 , 2 ) , ( 1 , 1 , 1 ) , feed_output_names = self._feed_output_names A variable ( including Keras metadata ) , filled with ` 0.0 ` . enabling more time-efficient parallelization on the GPU . with a zero for every element that is masked . def wrapper ( f ) : attr = attr.decode ( 'utf-8 ' ) then_expression_fn , output_shape [ 1 ] ) elif isinstance ( value , np.ndarray ) : input_shape= ( 3 , 32 , 32 ) , padding='same ' , ) ) kwargs= { 'W_regularizer ' : regularizers.l2 ( 0.01 ) , 'but was passed an input_mask : ' # Apply model on slice for _ in axis : axes : Integer or iterable of integers . self.data_format , self.interpolation ) with tf_file_io_proxy ( 'keras.engine.saving.tf_file_io ' ) as file_io_proxy : def get_slice ( data , i , parts ) : ' It could be because a worker has died . ' self.kernel , This is to be used for Model subclasses , which do not know at instantiation result._keras_shape = ( x._keras_shape [ 0 ] , total_size = -1 `` `` '' Evaluates the model on a data generator . batch_count = int ( len ( index_array ) / batch_size ) class CIFAR10Sequence ( Sequence ) : except mp.TimeoutError : `` `` '' Instantiates an all-zeros variable . specify ` shuffle=False ` when calling fit ( ) . output = -target * C.log ( output ) - ( 1.0 - target ) * C.log ( 1.0 - output ) handle_function ( name , mem ) feed_output_shapes.append ( None ) if count_mode == 'samples ' : if isinstance ( layer , ( Model , Sequential ) ) : { 'go_backwards ' : False } , if node.arguments : def test_check_array_length_consistency ( ) : `` `` '' Depthwise separable 2D convolution . return constraints `` `` '' Calls the ` on_test_batch_begin ` methods of its callbacks . truncate=True ) The most common situation would be input_shape , depthwise_conv2d = depthwise_conv it is in fact the `` stop '' argument . if max_value is not None and max_value < 0. : `` `` '' Utilities for ImageNet data preprocessing & prediction decoding . assert len ( inputs ) == 1 min_delta = kwargs.pop ( 'epsilon ' ) for data_format in [ 'channels_last ' ] __y.append ( signal.convolve ( x [ i , k ] , w [ k , j ] , mode=padding ) ) spatial_axes= ( 0 , 1 ) ) def test_zero ( tensor_shape ) : return serialize_keras_object ( loss ) `` `` '' Returns list of metrics from the given layers . rnn_constants = [ ] then_expression = then_expression ( ) 'should be defined . Found ` None ` . ' ) include the batch axis ) , e.g . ` input_shape= ( 10 , 128 ) ` for time series def batch_shape ( shape ) : x_test , labels_test = f [ 'x_test ' ] , f [ 'y_test ' ] config : Configuration dictionary . mask = node.output_masks [ tensor_index ] if padding == 'causal ' : x = tf.reshape ( warnings.warn ( 'Can save best model only with % s available , ' activity_regularizer='l2 ' ) config = { 'l1 ' : self.l1 , ( 'conv2d ' , ( 2 , 3 , 5 , 6 ) , ( 4 , 3 , 3 , 4 ) , 'valid ' , 'channels_first ' ) , if shuffle == 'batch ' : with open_fn ( file_path ) as archive : config [ 'build_input_shape ' ] = self._build_input_shape self.model = self.__call__ ( * * self.filter_sk_params ( self.__call__ ) ) out = tf.matmul ( x , y ) if 'layer_names ' not in f.attrs and 'model_weights ' in f : # Do not run tests in the build folder `` `` '' Checks if batch axes are the same for Numpy arrays . if start_char is not None : # modify the input spec to include the state . feed_dict = { } self._feed_targets.append ( target ) steps : Total number of steps ( batches of samples ) from .tensorflow_backend import * NEG_INF = -1e10 is passed , or if the specified batch size does not match the > > > K.is_keras_tensor ( keras_var ) for i , key in enumerate ( output_shape_keys ) : `` `` '' Average pooling for temporal data . seq : Sequence object `` `` '' Map the function fn over the elements elems and return the outputs . model.add ( keras.layers.Bidirectional ( rnn ( output_dim , output = softmax ( output ) # new_model.compile ( 'rmsprop ' , 'mse ' ) hash -r or list of scalars ( if the model has multiple outputs signature += ' , ' depthwise_regularizer='l1 ' , return reference [ indices ] the bias should be either a vector or dtype = kwargs.get ( 'dtype ' ) def size ( x , name=None ) : # This requires a specific way to figure out the if var.dtype ! = tf.float32 : rnn_layer = deserialize_layer ( config.pop ( 'layer ' ) , tensors representing 'should have the same length . ' ) with tf.name_scope ( 'replica_ % d ' % gpu_id ) : if self.train_function is None : from tensorflow.keras.datasets.cifar10 import load_data from tensorflow.keras.layers import Minimum 'items , however it contains % d items ' % ( true/false ) positives but not the ratio that is precision ( see Davis original_keras_version , indices = np.arange ( len ( x ) ) return K.mean ( _logcosh ( y_pred - y_true ) , axis=-1 ) if use_sequence_api and workers == 0 : validation_steps=None , b_regularizer='l1 ' , # Compute true mean while keeping the dims for proper broadcasting . embedding_size = np.prod ( embedding_input.shape [ 1 : ] ) def pop ( self ) : return_sequences=False , 'float16 ' # check if binary classification This metric creates two local variables , ` true_positives ` and input_spec ( list of class instances ) class SpatialDropout3D ( Dropout ) : ` new_conv_dim1 ` , ` new_conv_dim2 ` and ` new_conv_dim3 ` values might have `` `` '' Glorot normal initializer , also called Xavier normal initializer . self.updates.append ( opt_update ) spatial_axes= ( 0 , 1 , 2 ) ) return_state=return_state , def __init__ ( self , directory , image_data_generator , specificity : A scalar value in range ` [ 0 , 1 ] ` . `` `` '' Called at the end of evaluation or validation . # TF input shape : ( samples , rows , cols , input_depth ) if not any ( v is value for v in self._trainable_weights ) : ` metric = y_pred - y_true * log ( y_pred ) ` new_shape = ( 1 , 1 , 1 , 1 , bias_shape [ 0 ] ) `` `` '' Recurrent layers backed by cuDNN . '' '' '' x = x.dimshuffle ( ( 0 , 3 , 1 , 2 ) ) from .losses import mean_absolute_percentage_error stream : Any file-like object implementing the method ` read ` that returns from tensorflow.keras.applications.densenet import decode_predictions ' % s ! = % s . ' % ( shape1 [ axes [ 0 ] ] , shape2 [ axes [ 1 ] ] ) num_words = max ( [ max ( x ) for x in xs ] ) which must be executed kwargs [ 'params ' ] = params return [ output_mask ] + state_mask * 2 def convert_weights ( weights , from_cudnn=True ) : initializer=self.bias_initializer , of a normal convolution , i.e. , from something that has the shape of the config [ 'arguments ' ] [ key ] = np.array ( arg_dict [ 'value ' ] ) self.padding , allowed_positional_args= [ 'units ' ] , if self.keys is None : > > > K.normalize_data_format ( None ) serialized , _ , closure = func_dump ( test_func ) return x , y , sample_weights send [ 'epoch ' ] = epoch def kernel_size ( self ) : return super ( MeanMetricWrapper , self ) .update_state ( class _SeparableConv ( _Conv ) : outputs = keras.layers.Bidirectional ( rnn ( output_dim ) , keepdims : whether the drop or broadcast the reduction axes . def load_wrapper ( * args , * * kwargs ) : assert K.get_session ( ) .run ( fetches= [ x , y ] ) == [ 30. , 40 . ] model.run_eagerly = False update_func = C.combine ( [ u.output for u in u_ops ] ) samples in different successive batches . 'unit_forget_bias ' : self.unit_forget_bias , # First , let 's define a RNN Cell , as a layer subclass . if sample_weight_mode is None : ( if validation and accuracy monitoring are enabled ) . # model.compile ( optimizer , loss='mse ' , sample_weight_mode= [ 'temporal ' ] ) raise TypeError ( 'Error when checking model ' + exception_prefix 'node_indices ' : self.node_indices , if val_gen and workers > 0 : output_length = conv_utils.conv_output_length ( input_shape [ 1 ] , does not have a defined input shape . from .load_backend import moving_average_update return padding if node_key in self._network_nodes : custom_objects=custom_objects , axes=None ) : if len ( pool_size ) == 2 : decoded_dense.append ( dense_tensor ) `` `` '' Reconstruct node by linking to inbound layers def _test_optimizer ( optimizer , target=0.6 ) : fn : Callable to inspect . self.add_update ( [ K.moving_average_update ( self.moving_mean , assert y_train.shape == ( 150 , 1 ) , 'HDF5Matrix shape should match input array ' step = batch_size // parts def normalize_conv ( func ) : 'but it received ' + str ( len ( states ) ) if losses.is_categorical_crossentropy ( loss ) : mask_vals [ -1 , -1 ] = 0 # final timestep masked for last sample config [ 'padding ' ] = config [ 'padding ' ] [ 0 ] input_shape = K.int_shape ( x ) [ 1 : ] shape=recurrent_kernel_shape , def __init__ ( self , name='categorical_hinge ' , dtype=None ) : data_format='channels_first ' , return C.softmax ( x , axis=axis ) name : Name given to the model from keras_applications import resnet50 if len ( x_weight ) ! = len ( output_names ) : if len ( n_s ) > 0 : callbacks._call_batch_hook ( 'test ' , 'end ' , batch_index , batch_logs ) else_expression = else_expression ( ) z = T.neq ( x , y ) * * kwargs ) ` . not a positional-only argument ) . def ResNet101V2 ( * args , * * kwargs ) : if data_format == 'channels_last ' : name='bias ' , kernel_size = ( args [ 2 ] , args [ 3 ] , kwargs.pop ( 'kernel_dim3 ' ) ) if match : if 'keras_version ' in f.attrs : # We 're done `` `` '' Computes the sensitivity at a given specificity . This assumes a one-to-one mapping between input_length = 10 if you wish to apply activation function # Assuming old interface . reduce_lr = ReduceLROnPlateau ( monitor='val_loss ' , factor=0.2 , return symbolic_fn_wrapper schedule : a function that takes an epoch index as input ` a [ i ] ` and ` b [ i ] ` . if out_dim is not None and target_dim ! = out_dim : by providing lower or upper bound estimate of the AUC . past_values.append ( C.sequence.past_value ( p , s ) ) # First layer in model : check that it is an input layer . super ( _Pooling3D , self ) .__init__ ( * * kwargs ) _FLOATX = 'float32 ' if 'constants ' in kwargs : RuntimeError : if no session is available # If any of ` initial_state ` or ` constants ` are specified and are Keras from numpy.testing import assert_allclose for i , ( argname , arg ) in enumerate ( zip ( argnames , args ) ) : shuffle=shuffle , # for hot fix , ignore all the . except the first one . update_ops = metric_fn.update_state ( y_true , y_pred , sample_weight=weights ) # We recommend doing this with under a CPU device scope , inputs_i = inputs return { dropout_W=0.2 , from tensorflow.keras.layers import ConvLSTM2D def __init__ ( self , fn , name=None , dtype=None , * * kwargs ) : grads = [ if self.merge_mode is None : if i in self.skip_target_indices : integers or None entries . raise ValueError ( ' ` steps=None ` is only valid for a generator ' # to hook up with keras model For instance , ` batch_shape= ( 10 , 32 ) ` indicates that base_config = super ( Reshape , self ) .get_config ( ) layer = layer_class ( units , return_sequences=False , all_inputs.append ( x ) if isinstance ( fit_inputs [ -1 ] , int ) : self.output_masks = output_masks weights = self._trainable_weights [ : ] + self._non_trainable_weights [ : ] [ graphviz ] ( https : //graphviz.gitlab.io/download/ ) and [ pydot ] ( https : //github.com/erocarrera/pydot ) ( used by [ visualization utilities ] ( https : //keras.io/visualization/ ) to plot model graphs ) . self.input_names.append ( name ) inputs = ( self._feed_inputs return K.all ( K.concatenate ( masks , axis=0 ) , axis=0 , keepdims=False ) result = C.times ( x , y , output_rank=y_ndim - 2 + int ( y_expanded ) ) import keras output_names ) # update gate if self.validation_data and self.histogram_freq : for l_s , i_s in zip ( last_states , initial_states ) : validation_split=validation_split , for cell in self.cells : const._keras_shape = const.shape self._seen_so_far = current input_length : tensor ` ( samples , ) ` containing the sequence length for # you need to assign it . end = -end sequential_like = False raise ImportError ( ' ` save_weights ` requires h5py . ' ) strides=self.strides , for pooling_class in [ pooling.GlobalMaxPooling1D , layer.trainable = False if not hasattr ( self , 'test_function ' ) : # model.compile ( optimizer , loss='mse ' , loss_weights= { 'lstm ' : 0.5 } ) def test_sequential_model_pickling ( ) : def is_tensor ( x ) : conv_out = T.nnet.conv2d ( x , depthwise_kernel , ( 'init ' , 'kernel_initializer ' ) , def noised ( ) : TensorFlow data tensors , the default ` None ` is equal to for ( m , m1 ) in zip ( range ( pool_size [ 2 ] ) , range ( -pool_size [ 2 ] , 0 ) ) : layers_by_depth : dict mapping ints ( depth ) vhats = [ K.zeros ( 1 , name='vhat_ ' + str ( i ) ) inputs = keras.Input ( ( 4 , 3 ) ) 'instead of ` None ` . ' ) class_id=self.class_id , Use Keras if you need a deep learning library that : shape= ( self.units , self.units * 4 ) , from tensorflow.keras.layers import SeparableConv2D dtype : The dtype of the weight . class ConvertToBatch ( C.ops.functions.UserFunction ) : x_expanded = False name = str ( name ) + ' _ ' + str ( counts [ name ] ) if strides ! = ( 1 , 1 ) : dtype outputs= [ score_sum , score1 , score2 ] , output_tensors.append ( tensor ) # save , load , etc . will recognize custom object by name 'Error when checking ' + exception_prefix `` `` '' Maximum value in a tensor . # This is necessary for shared layers that have inputs at different Expects ` inputs ` to be a list ( potentially with 1 element ) . ` sample_weight_mode= '' temporal '' ` in ` compile ( ) ` . This argument input_tensors , output_tensors , for p , g , m , v , vhat in zip ( params , grads , ms , vs , vhats ) : ref = [ 1.73308 , 3.81351 ] k_label_lens ) ) return simple_rnn_with_extra_mock_state output /= output.sum ( axis=-1 , keepdims=True ) def cumprod ( x , axis=0 ) : # helper function # ` ( 2 * 3 * self.units , ) ` , so that we can distinguish the classes x += bias # Keep track of the old value config = { 'class_name ' : str ( identifier ) , 'config ' : { } } def test_reset_uids ( self ) : self.set_weights ( self._initial_weights ) if len ( output_names ) > 1 : `` `` '' Checks if validation should be run this epoch . # LINEAR ALGEBRA callback.on_test_end ( logs ) mean , scale , seed=seed ) ( shape , dtype=dtype ) A tensor , result of 3D pooling . def _validate_or_infer_batch_size ( self , batch_size , steps , x ) : if datapath not in list ( self.refs.keys ( ) ) : new_states.append ( states_at_step [ i ] ) which to run validation , e.g . ` validation_freq= [ 1 , 2 , 10 ] ` runs `` `` '' Creates a tensor by tiling ` x ` by ` n ` . final_output = C.sequence.unpack ( final_output , 0 , no_mask_output=True ) 'Layer ' + layer.name # Legacy support e.g : ` [ ' a ' , ' b ' , ' b ' , ' c ' ] - > [ ' a ' , ' b ' , 'b_2 ' , ' c ' ] ` sys.stdout.write ( info ) if not self.merge_mode : Function that converts input kernel to the other format . self.return_sequences = layer.return_sequences x_h = K.dot ( inputs_h , self.kernel_h ) tokenizer_from_json = text.tokenizer_from_json y = layer ( x ) ( eg . verbosity , batch size , number of epochs ... ) . ratio=height_factor ) variables : List of variables . for name in names : 'dimension to enable padding . ' % base_shape ) elif isinstance ( k , Iterable ) and not is_zero_dim_ndarray : cropping [ 1 ] , 2 , ` y_true ` and ` y_pred ` should have the same shape . thread . self.arguments = arguments if arguments else { } layers = [ clone ( layer ) for layer in model.layers ] for node , depth in nodes_depths.items ( ) : # Applies the same workaround as in ` RNN.__call__ ` self.mode = 'auto ' strides=strides , 'squeeze ' , { 'axis ' : 2 } , new_weights = [ np.random.random ( w.shape ) for w in org_weights ] def dot ( x , y ) : if len ( kwargs ) > 0 : if len ( loss ) ! = len ( output_names ) : sample_weight = K.cast ( sample_weight , K.floatx ( ) ) name = unique_name The unique name . [ Convolutional LSTM Network : A Machine Learning Approach for normalized_padding = ( conv_utils.normalize_tuple ( padding , 2 , 'padding ' ) , ) if isinstance ( sample_weight_mode , dict ) : xs = [ K.placeholder ( shape=shape , dtype=dtype ) x = C.input ( gradient to keep at each time step . initial_epoch=0 , Weighted loss ` Tensor ` of the same type as ` losses ` . If ` reduction ` is samples are drawn from a uniform distribution x , tuple ( past_values ) + tuple ( rnn_constants ) ) if any ( x is None for x in grads ) : Three lists : output_tensors , output_masks , output_shapes for start , stop , step in ( ( 0 , 5 , 1 ) , ( -5 , 5 , 2 ) , ( 0 , 1 , 2 ) ) : if cls is None : step_function , self.strides = conv_utils.normalize_tuple ( strides , 1 , 'strides ' ) from .. layers import Conv2D raise TypeError ( 'Unrolling isn\'t possible with ' y = np.max ( y , axis=-1 ) def _old_normalize_batch_in_training ( x , gamma , beta , reduction_axes , output_mask = self.layer.compute_mask ( inner_inputs , inner_mask ) 'scale ' : self.scale , 'TensorFlow optimizers do not ' def __init__ ( self , dims , * * kwargs ) : def _uniquify ( names ) : ( 'pool2d ' , ( 3 , 6 , 7 , 3 ) , ( 3 , 3 ) , ( 1 , 1 ) , legacy_global_pooling_support = generate_legacy_interface ( # computed_masks might be used in the future . if preds.shape [ -1 ] == 1 : val_enqueuer_gen = iter_sequence_infinite ( val_data ) training_utils.standardize_weights ( ref , sw , cw , mode ) `` `` '' Calculates the number of false negatives . with K.name_scope ( scope_name ) : forward_updates = self.forward_layer.get_updates_for ( inputs ) ops : list of ops to run . new_layer = keras.layers.SimpleRNN ( 2 , kernel_initializer='normal ' , It avoids overflows caused by taking the exp of large inputs and shape = np.array ( x ) .shape # This design was chosen for Python 2.7 compatibility . if [ [ `` $ KERAS_BACKEND '' == `` cntk '' ] ] || [ [ `` $ MODE '' == `` PEP8_DOC '' ] ] || [ [ `` $ MODE '' == `` API '' ] ] ; then globs = dict ( list ( globs.items ( ) ) + list ( custom_objects.items ( ) ) ) if isinstance ( then_expression , tf.Tensor ) : ` ( batch_size , timesteps , features ) ` and elif ( len ( y.shape ) == 2 and y.shape [ 1 ] == 1 ) or len ( y.shape ) == 1 : * * sk_params : model parameters & fitting parameters ` ( samples , channels , rows , cols ) ` if ` data_format='channels_first ' ` model.fit ( ... , if device not in available_devices : if avg > 1e-3 : if val_function and val_inputs : targets=None , The default type of the returned tensor is ` 'int32 ' ` to epochs to run before a new validation run is performed . If a list , # We remove the endpoint thresholds as an inverse of how the thresholds reason='theano returns tuples for update ops ' ) ' ( only `` valid '' is supported ) : ' + padding ) y_ndim = K.ndim ( y ) raise ValueError ( 'Optimizer weight shape ' `` `` '' Creates a ` Sum ` instance . cache : # Import backend functions . from .load_backend import exp of the ` state_size ` . params : dictionary ; the parameters to be checked `` `` '' Computes the precision of the predictions with respect to the labels . name : Optional name for the object . masks = [ ] self._feed_outputs.append ( self.outputs [ i ] ) super ( AveragePooling1D , self ) .__init__ ( pool_size , strides , if x_ndim < 2 or y_ndim < 2 : callbacks.model.stop_training = False size=num_identical , replace=False ) custom_objects=custom_objects , initializer : The first value used ( elems [ -1 ] in case of None ) t = getattr ( k , function_name ) ( k.variable ( x_val ) , * * kwargs ) A Keras variable with the shape of x filled with ones . initializer=self.pointwise_initializer , border_mode='valid ' , 'same ' , 'channels_first ' ) , [ 'get_value ' , 'count_params ' , # separate biases for input and recurrent kernels `` `` '' Utilities for preprocessing sequence data . self._layers = [ ] def filters ( self ) : # ` RNN ( return_state=True ) ` since the state will be returned as the same yield output_shape=input_shape , ' is incompatible with layer ' dilation_rate ) : tensor_map = { } # Map { reference_tensor : ( corresponding_tensor , mask ) } epochs = 3 compute_output_shape ( input_shape ) weights_list = [ 1 . ] * len ( output_names ) out_pad_d ) which may take variable length input . if hasattr ( C , 'unpack_batch ' ) and _get_cntk_version ( ) > = 2.2 : function : The function to be evaluated . return _broadcast_normalize_batch_in_training ( x , gamma , beta , skip_target_weighing_indices.append ( i ) model = load_from_binary_h5py ( load_function , filepath ) raise ValueError ( 'Invalid concat axis for sparse matrix : ' , axis ) _test_bucket_env_key = 'GCS_TEST_BUCKET ' elif hasattr ( f.file , 'close ' ) : offset=16 ) .reshape ( len ( y_test ) , 28 , 28 ) sensitivities = K.switch ( w = np.transpose ( w , ( 1 , 2 , 0 ) ) # Set input spec . tensor with same shape and dtype as x . loss_config = losses.get ( loss_config ) y_true , from tensorflow.keras.layers import GlobalAveragePooling1D if threshold ! = 0. : pip install tensorflow==1.14.0 -- progress-bar off ; # Collect losses that are dependent on inputs if self.mode not in [ 'auto ' , 'min ' , 'max ' ] : raise TypeError ( 'Unknown function type : ' , function_type ) `` `` '' Abstract optimizer base class . if callbacks.model.stop_training : i * stride_row + kernel_size [ 0 ] ) new_shape_temp = [ ] self.saver.save ( self.sess , old_layer = keras.layers.Convolution2D ( 5 , 3 , 3 , print ( ' # # # # # # # Xception benchmark - np i/o ' ) seed : random seed to ensure determinism . for y , loss , shape in zip ( targets , loss_fns , output_shapes ) : [ [ 1. , 0. , 0 . ] , [ 0. , 1. , 0 . ] , [ 0. , 0. , 1 . ] ] , `` `` '' Utilities for backend functionality checks . '' '' '' a fraction ` rate ` of input units to 0 at each update during training time , squared_hinge , name=name , reduction=reduction ) shape_key += ' _ % s_ % s ' % ( node_index , tensor_index ) output = layer ( inputs , initial_state=initial_state [ 0 ] ) y=None , self._output_layers = [ ] t_before_callbacks = time.time ( ) dictionary . It does not handle layer connectivity e : float . New value of epsilon . embeddings_metadata or embeddings_data ) : if self.implementation == 1 : assert len ( layer.get_losses_for ( x ) ) == 2 class KLDivergence ( LossFunctionWrapper ) : call ( x , mask=None ) : Where the layer 's logic lives . metric_obj.add_update ( update_op ) permutation.append ( 1 ) j += 1 val_outs = model.evaluate_generator ( The validated batch_size , auto-inferred from the first layer if __call__ ( x , mask=None ) : Wrapper around the layer logic ( ` call ` ) . out._keras_shape = tuple ( shape ) indices = tf.cast ( indices , tf.int64 ) self.on_predict_begin ( ) # to reset variable state after each epoch of training . pydot.Dot.create ( pydot.Dot ( ) ) c_axis , d_axis , h_axis , w_axis = 1 , 2 , 3 , 4 if callable ( alt ) : return self._get_node_attribute_at_index ( 0 , 'output_tensors ' , training=training ) # Compute the full inputs , including state if i not in self.skip_target_indices from .. models import Sequential if name in weight_names : ` activation ` is not ` None ` , it is applied to the outputs as well . is applied to depth , height , and width . str ( loss_weights ) + ' - expected a list of dicts . ' ) if has_arg ( cell.call , 'constants ' ) : f.close ( ) will be included in the generator . show_shapes : whether to display shape information . output_masks= [ None for _ in self.outputs ] , input_mask , output_mask : Mask tensors . Same caveats apply as return metric # Ignore warnings which are verbose and unrelated to Keras # header names for the different log elements outputs = K.bias_add ( assert json.dumps ( old_layer.get_config ( ) ) == json.dumps ( new_layer.get_config ( ) ) return densenet.DenseNet121 ( * args , * * kwargs ) symmetric padding values for height and width : The need for transposed convolutions generally arises def __getattr__ ( self , attr ) : `` `` '' Reshapes a tensor to the specified shape . epsilon=epsilon ) conv_out = _postprocess_conv2d_output ( h_tm1_h = h_tm1 * rec_dp_mask [ 2 ] def __init__ ( self , build_fn=None , * * sk_params ) : if hasattr ( layer , 'losses ' ) : dev = C.device.use_default_device ( ) noise_shape = ( input_shape [ 0 ] , 1 , input_shape [ 2 ] ) summary_value.simple_value = value.item ( ) def __exit__ ( self , exc_type , exc_val , exc_tb ) : layer , inputs_np=x , class Permute ( Layer ) : super ( _GlobalPooling1D , self ) .__init__ ( * * kwargs ) fn : Callable that will be called upon each element in ` elems ` output_metrics = generic_utils.to_list ( metrics.get ( name , [ ] ) ) object in order to avoid duplicate data The softplus activation : ` log ( exp ( x ) + 1 ) ` . if isinstance ( shape [ 0 ] , int ) or shape [ 0 ] is None : ( 'inner_init ' , 'recurrent_initializer ' ) , of x.dot ( y.T ) , although we never have to calculate the off-diagonal self.device = None from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input or list of scalars ( if the model has multiple outputs `` `` '' Computes the cross-entropy loss between true labels and predicted labels . output = outputs [ -1 ] where dTP == TP_B - TP_A . def _has_compat_v1 ( ) : weighted_assign_add ( label , pred , weights_tiled , mapping input names to Numpy arrays . save_attributes_to_hdf5_group ( g , 'weight_names ' , weight_names ) condition : scalar tensor ( ` int ` or ` bool ` ) . A tensor with maximum values of ` x ` . model = keras.models.Model ( inputs=x , outputs=y ) self._per_output_weighted_metrics [ i ] ) for n in group.attrs [ ' % s % d ' % ( name , chunk_id ) ] ] ) for name in available_devices ] return self._get_node_attribute_at_index ( 0 , 'input_masks ' , oov_char : words that were cut out because of the ` num_words ` name : ( Optional ) string name of the metric instance . test_func = get_test_func ( ) computed_tensors = [ x [ 0 ] for x in computed_data ] file_hash='87aedbeb0cb229e378797a632c1997b6 ' ) if has_arg ( self.layer.call , 'mask ' ) : target_shape.append ( x.shape [ axis ] ) gamma_regularizer=None , ` PR ` curve , interpolates ( true/false ) positives but not the ratio that is self.keys = None ` ( ( top_pad , bottom_pad ) , ( left_pad , right_pad ) ) ` axes [ 0 ] += x.ndim return name in accepted_name or member.__module__ in accepted_module ( computed using the aforementioned variables ) . The ` num_thresholds ` variable assert [ tuple ( s ) for s in output_shape ] == expected_output_shape test_cases.append ( [ ( None , 3 , 4 , 5 ) , ( None , 2 , 3 , 4 ) , ( 2 , 3 ) ] ) # Convert the binary labels to -1 or 1 . cell = StackedRNNCells ( cell ) K.bias_add ( x , b , data_format='channels_middle ' ) weights [ 10 ] ] , axis=-1 ) # previous and candidate state mixed by update gate # for Theano and CNTK output_padding=None , output_shape : target shape of the array , with at most pointwise_constraint : Constraint function applied to name='global_avgpool2d ' ) from tensorflow.python.keras.utils.data_utils import _hash_file axes = [ ] parallel_model.predict ( x , batch_size=batch_size ) on_epoch_end : called at the end of every epoch . return val if ndim == 2 : # saving the shape to avoid converting sparse tensor to dense name in arg_spec.kwonlyargs ) if y.ndim == 3 : 2 * padding [ 1 ] + output_padding [ 1 ] ) ( 'conv2d ' , ( 1 , 6 , 5 , 3 ) , ( 3 , 4 , 3 , 2 ) , 'valid ' , 'channels_last ' ) , A symbolic shape ( which is itself a tensor ) . element-wise operations . from .losses import squared_hinge if id ( x ) not in source_tensors_ids : step_function_np , config.pop ( 'kernel_regularizer ' ) super ( MeanMetricWrapper , self ) .__init__ ( name=name , dtype=dtype ) f [ ' z ' ] = array 'distribution ' : self.distribution , batch_index = 0 new_dim = conv_utils.conv_output_length ( target_shape = tf.stack ( target_shape ) __y = [ ] `` `` '' Instantiates an all-zeros variable of the same shape as another tensor . normalized_size = conv_utils.normalize_tuple ( size , 3 , 'size ' ) self.stop_signal = None kwargs = { } # Transpose all inputs so that batch size is the last dimension . strides , # List of tensors , 1:1 mapping with input_tensor . cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) The metric creates two local variables , ` true_positives ` and ` false_positives ` class ProgbarLogger ( Callback ) : use_bias=use_bias , raise RuntimeError ( 'You tried to call ` count_params ` on ' base_config = super ( GaussianDropout , self ) .get_config ( ) return os.path.join ( self.bucket_path , filename ) def test_check_bad_shape ( ) : `` `` '' Layer that computes the maximum ( element-wise ) a list of inputs . spatial dimensions . return x.numpy ( ) vae.summary ( ) `` `` '' Returns predictions for the given test data . Keras Optimizer instance ( it will be returned unchanged ) . import keras_applications from .pooling import GlobalAveragePooling2D return uses_correlation [ original_backend ] ! = current_uses_correlation @ normalize_conv inputs : Input tensor , or list/tuple of input tensors . self.bias_c , tensorboard -- logdir=/full_path_to_your_logs super ( ConvertToBatch , self ) .__init__ ( [ input ] , as_numpy=False , name=name ) # TODO : somehow this capture mechanism does n't work for TF feed_dict = dict ( zip ( tensors , batch_val ) ) def call ( self , 'of `` nearest '' or `` bilinear '' . ' ) def test_stateful_invalid_use ( layer_class ) : old_layer = keras.layers.LSTM ( input_dim=5 , input_length=3 , unique_names.append ( name ) The docstring is left unchanged from .load_backend import conv2d one_hot_matrix = C.ops.one_hot ( indices , num_classes ) ImageNet Classification ] ( https : //arxiv.org/abs/1502.01852 ) bias_constraint : Constraint function applied to the bias vector model.add ( Bidirectional ( LSTM ( 10 ) ) ) self.x , self.y = x_set , y_set str ( index ) + ' but model only has ' elif isinstance ( n , list ) : self.true_positives = self.add_weight ( return { 'class_name ' : obj.__class__.__name__ , # we find beforehand the arrays that need conversion . if isinstance ( target_tensors , list ) : mock_module = Mock ( ) featurewise_center=False , # as first layer in a sequential model : workers=1 , if constants is None : if self.state_spec is not None : w_img = tf.transpose ( w_img ) def from_str ( key ) : if shape1 is None or shape2 is None : add_shape = ref_tensor_t.shape [ 1 : ] `` `` '' Returns the Callback Model for this Model . '' '' '' { 'go_backwards ' : True , 'mask ' : None } , 'input mask ' ) > > > var_transposed = K.transpose ( var ) from_logits=from_logits ) # Store the traversal order for layer sorting . assert isinstance ( args [ 2 ] , dict ) self.target is not None and current < self.target ) : rng = RandomStreams ( seed=seed ) def get_session ( ) : ' in the save file . ' # Input is unmasked . Append all 1s to masks , def process_node ( layer , node_data ) : tmp_filepath = os.path.join ( tempfile.gettempdir ( ) , signature = inspect.signature ( fn ) beta.dimshuffle ( shuffle_pattern ) , if self.bias is not None : except : pattern = list ( range ( y_ndim ) ) from_logits=False , res = C.elu ( x ) # sub_w : submodel_wrapper self._initial_weights = kwargs.get ( 'weights ' ) if six.PY2 : updated_metrics_dict [ metric_name ] = metric_fn relevant_nodes = [ ] model = Model ( x , y ) assert not ask_to_proceed_with_overwrite ( '/tmp/not_exists ' ) if _get_dynamic_axis_num ( i_s ) == 0 and _get_dynamic_axis_num ( l_s ) == 1 : new_layer = keras.layers.GlobalAvgPool3D ( data_format='channels_first ' , # if inputs were squashed , we have to reshape the matmul output . f.write ( file_like.read ( ) ) states = self.forward_layer.states kernel = C.swapaxes ( kernel , 0 , 2 ) if batch_index == 0 : _input_tensors.append ( input_tensor ) 'expected no data , but got : ' , data ) metrics= [ keras.metrics.CategoricalCrossentropy ( ) ] ) kernel_size , Assumed overridden : def test_foldr ( self ) : i.e . if it is connected to one incoming layer . The area within the interval is ( slope / total_pos_weight ) times val_outs = model.evaluate ( to only focus on significant changes . x = k.variable ( x_np ) regularizer=self.depthwise_regularizer , # get the updates from the inner layer . rho : float > = 0 . > > > kvar = K.random_uniform_variable ( ( 2,3 ) , 0 , 1 ) if not isinstance ( input_shape , list ) or len ( input_shape ) < 2 : output_tensors=self.outputs , k_s = K.concatenate ( [ K.variable ( x_sparse_1 ) , K.variable ( x_sparse_2 ) ] ) # Split single set of biases evenly to two sets . The way of import numpy as np max_value=self.max_value , return x , tf_data_format def repeat_elements ( x , rep , axis ) : reduction_axes = [ i for i in range ( x.ndim ) if mean.broadcastable [ i ] ] `` `` '' Context manager for mock patching ` tensorflow.python.lib.io.file_io ` in tests . super ( MeanIoU , self ) .__init__ ( num_classes , name=name , dtype=dtype ) or , the input is ` None ` and import pydot # Do n't call InputLayer multiple times . This does not use a dictionary . new_states = [ ] if isinstance ( self.recurrent_initializer , initializers.Identity ) : for key in value_conversions : total_loss = 0 . data_format='NCHW ' ) `` `` '' Returns the mean accuracy on the given test data and labels . { 'CustomSGD ' : CustomSGD , 'custom_mse ' : custom_mse } ) : regularizer=self.b_regularizer , layer.backward_layer.add_loss ( 0 , inputs=x ) 'must be a list , a tuple , or a function . ' ) slice_row = py_slice ( i * stride_row , conv_out = conv_out [ : , : , : , : i , : ] grad_parameter_dict = { } ' Please consider using the ` keras.utils.Sequence ' input_h = initial_state [ 0 ] loss = 0.5 * x^2 if |x| < = d initial_state = [ keras.Input ( ( units , ) ) for _ in range ( num_states ) ] max_value = _to_tensor ( max_value , x.dtype.base_dtype ) input_length=inputs_np.shape [ 1 ] if unroll else None , # For backwards compatibility x = r'\u ' raise TypeError ( `` ` initializer ` must be a tensor or None '' ) save_format : one of `` png '' , `` jpeg '' sub_w_first_node = { } self.go_backwards = go_backwards def subtract ( inputs , * * kwargs ) : return h , [ h ] raise ValueError ( 'No data provided for `` ' + e.args [ 0 ] variable = theano.shared ( value=value , `` `` '' Reshapes an output to a certain shape . will then be the * weighted sum * of all individual losses , def output ( self ) : return serialize_keras_object ( regularizer ) reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE ) reps = T.concatenate ( [ [ 1 ] , add_shape ] , 0 ) return _clone_sequential_model ( model , input_tensors=input_tensors ) Generator yielding batches . def depthwise_conv ( x , w , padding , data_format ) : self._set_inputs ( x ) if candidate_vars : It draws samples from a truncated normal distribution centered on 0 if ` class_mode ` is ` `` input '' ` or ` None ` no extra column is needed . # Expand dim of weights to match ndim of values , if required . with gzip.open ( paths [ 2 ] , 'rb ' ) as lbpath : if axis_without_batch ! = -1 and axis_without_batch not in output_dimensions : it is flattened prior to the initial dot product with ` kernel ` . model.compile ( 'sgd ' , metrics= [ keras.metrics.MeanSquaredError ( ) ] ) # Update model updates and losses : # if obj is a serializable Keras class instance pad=pad , bias_constraint=bias_constraint , summation for decreasing intervals . `` `` '' Retrieves the input mask tensor ( s ) of a layer at a given node . `` `` '' Instantiate an input data placeholder variable . self.inputs = None # self.losses return th_sparse_module and isinstance ( tensor.type , th_sparse_module.SparseType ) bias_regularizer='l2 ' ) weights : list of Numpy arrays of sample weights . ` [ batch_size ] ` . elif states is None : return all_matrix data_shape = data [ i ] .shape self.monitor_op = lambda a , b : np.less ( a , b - self.min_delta ) self._values [ k ] [ 0 ] += v * ( current - self._seen_so_far ) return [ get_value ( x ) for x in xs ] if batch_size is None and steps is None : if chunk : ( ie . `` linear '' activation : ` a ( x ) = x ` ) .x for state in self.states : # Check that layer is an InputLayer . 'when doing step-wise ' A tensor with the same data as ` x ` but reduced dimensions . # assert 12 * 5 < = len ( val_seq.logs ) < = ( 12 * 5 ) + 2 # the queue may be full . class MaxNorm ( Constraint ) : if not custom_objects : def __init__ ( self , name='mean_squared_logarithmic_error ' , dtype=None ) : 'by CNTK backend . Please set ` dilation_rate ` to 1 . ' def DISABLED_test_convolutional_recurrent ( data_format , return_sequences , use_mask ) : @ K.eager # returns a compiled model name in arg_spec.kwonlyargs ) outbound_layer = self.outbound_layer.name from abc import abstractmethod normalizer : function to be called on data when retrieved check_single_tensor_operation ( 'mean ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) if str ( _hash_file ( fpath , hasher , chunk_size ) ) == str ( file_hash ) : def control_dependencies ( control_inputs ) : filter_shape = ( filter_shape [ 4 ] , filter_shape [ 3 ] , input_shape [ 1 ] , raise TypeError ( 'Expected ` target_tensors ` to be a tensor , ' # Due to the recommendations in [ 2 ] , i.e . warming momentum schedule embeddings_metadata=None , if ( y_pred_rank - y_true_rank == 1 ) and ( y_pred_shape [ -1 ] == 1 ) : config [ 'output_shape ' ] = output_shape isinstance ( path , dict ) or del reduction_axes [ self.axis ] if not sequential_like : i.e . maximum integer index + 1 . for dim in self.cell.state_size ] Note that the 'out of vocabulary ' character is only used for dramatically depending on ` num_thresholds ` . The ` thresholds ` parameter can be raise ValueError ( ' ` y ` argument is not supported when data is ' callbacks=None ) : if do_validation : ( which acts on each input channel separately ) . ( one for each output ) . inputs : input tensor or list of inputs tensors to mark self._non_trainable_weights = [ ] Note that CNTK does not support yet the ` bilinear ` upscaling if _ == C.FreeDimension : from .common import normalize_data_format write_images=write_images , inputs_h = inputs * dp_mask [ 2 ] if axis ! = axes [ 0 ] : # Except permission denied . for i in range ( len ( model.layers ) ) : for _ in x.shape : x_shape = ( 2 , 3 , 4 , 5 ) self.input_spec = InputSpec ( min_ndim=2 ) `` `` '' Returns the default float type , as a string . as accepted by ` test_on_batch ` . # this test ensures that models serialized prior to version 2.1.2 can still be K.set_floatx ( dtype ) Input tensor or list of input tensors . self.kernel_size = conv_utils.normalize_tuple ( kernel_size , 1 , 'kernel_size ' ) if ndim ( var ) == ndim ( x ) and shape ( var ) [ 0 ] == 1 : uninitialized_vars = [ ] def serialize ( constraint ) : Training samples where ` n_samples ` is the number of samples # ( no input shape specified ) , x_var = K.variable ( x ) from keras_applications import nasnet if self.write_images : y = K.gather ( x , indices ) h5dict [ 'model_config ' ] = model_config if file_io_module is None : `` `` '' Trains the model on data generated batch-by-batch by a Python generator out3 = model.predict ( np.ones ( ( num_samples , timesteps ) ) ) name='conv ' ) assert tuple ( g.shape ) == tuple ( c.shape ) batch_size=None , list of Numpy arrays ( if the model has multiple outputs ) or sample_weight : Optional ` Tensor ` whose rank is either 0 , or the same rank as ` `` causal '' ` results in causal ( dilated ) convolutions , # as intermediate layer in a Sequential model return K.cast ( K.size ( losses , name=scope ) , losses.dtype ) var_np = np.random.random ( other_shape ) from .pooling import GlobalMaxPool3D self moving_mean_initializer='zeros ' , name='LSTM ' ) # if _LEARNING_PHASE is static warnings.warn ( self.reset_metrics ( ) reduction_axes = list ( range ( len ( input_shape ) ) ) input_shape [ 2 ] ) rng = np.random.RandomState ( self.seed ) sample_weight : Optional ` Tensor ` whose rank is either 0 , or the same rank padding=padding , metrics_updates.extend ( m.updates ) inputs_hash = object_list_uid ( inputs ) obj : the object to serialize if ( num_dynamic_axis > 0 and x = args [ 0 ] strides= ( 2 , 2 ) , dilation_rate= ( 1 , 2 ) ) isinstance ( loss_fn , losses.BinaryCrossentropy ) or if do_validation and should_run_validation ( validation_freq , epoch ) : def test_rnn_output_num_dim_larger_than_2_masking ( self ) : value is computed and used to evaluate the corresponding sensitivity . param_shape [ i - 1 ] = 1 nadam = Nadam Note that this is a * linear * layer ; group3 = group2 [ 'group3 ' ] output_shape By default , we consider that output encodes a probability distribution . base_config = super ( _GlobalPooling3D , self ) .get_config ( ) broadcast_gamma = tf.reshape ( gamma , target_shape ) print_fn ( 'Model : `` { } '' '.format ( model.name ) ) summary = tf.Summary ( ) [ Getting started with the functional API ] ( https : //keras.io/getting-started/functional-api-guide ) `` `` '' Instantiates a variable and returns it . self._feed_input_shapes = [ ] base_config = super ( Flatten , self ) .get_config ( ) if axis ! = -1 and axis not in output_dimensions : < tf.Tensor 'Cast_1:0 ' shape= ( 2 , 3 ) dtype=float16 > layer = layer_map [ layer ] @ interfaces.legacy_zeropadding2d_support 'data_format ' : self.data_format , where ` limit ` is ` sqrt ( 6 / ( fan_in + fan_out ) ) ` @ keras_modules_injection if x [ 0 ] .shape [ 0 ] % batch_size ! = 0 : if np.max ( key ) + self.start < self.end : 'outputs that are at least 3D , i.e . that have ' activations.serialize ( self.recurrent_activation ) , print ( 'baseline inference : ' , total_time ) self.recurrent_kernel_c , dictionary . ValueError : in case of incorrectly formatted data . kernel_constraint='max_norm ' , E.g . 2 will halve the input . axis : Axis to drop . mode='fan_avg ' , WITH_NP , cntk_two_dynamicity=True , axes= ( 1 , 1 ) ) `` `` '' Runs a single gradient update on a single batch of data . name=output_name + '_sample_weights ' ) os.path.basename ( filepath ) ) from .load_backend import reshape weight_values = preprocess_weights_for_loading ( layer , one_hot_matrix , reference , unroll=False , input_length=None ) : 'Please double check the model and inputs in ' unroll=unroll , top-k predictions . if len ( sample_weight_mode ) ! = len ( output_names ) : computed for the samples in one batch will be reused as initial states 'batch_input_shape argument . ' ) return ( not explicitly_on_cpu and gpus_available ) len ( self._collected_trainable_weights ) ) : return out y_shape_or_val , or instead draw them in chronological order . out = model.fit ( [ input_a_np , input_b_np ] , when using process based threading . weights [ 6 ] , Updating and clearing custom objects using ` custom_object_scope ` __Note on specifying the initial state of RNNs__ mode , the direction is automatically inferred new_shape = ( 1 , ) + bias_shape printable_module_name='initializer ' ) from .load_backend import switch write_grads=False , _DISABLE_TRACKING = threading.local ( ) from .pooling import GlobalMaxPooling3D x = tf.nn.avg_pool ( x , pool_size , strides , def prepare_sample_weights ( output_names , sample_weight_mode , gamma_initializer='uniform ' , pool_size= ( 2 , 2 , 2 ) , padding='valid ' , name='maxpool3d ' ) def load_function ( h5file_ ) : all_outs = [ ] def __init__ ( self , dtype=None , These values are similar to values from a ` RandomNormal ` If PIL version 1.1.3 or newer is installed , ` `` lanczos '' ` is also `` `` '' Utilities common to CIFAR10 and CIFAR100 datasets . output_shape = output_shape [ :1 ] + output_shape [ 2 : ] if 'input_shape ' in kwargs or 'batch_input_shape ' in kwargs : return map ( C.stop_gradient , variables ) return arg_spec raise ValueError ( 'The dimension and the size of indices should match . ' ) depth_keys = list ( model._nodes_by_depth.keys ( ) ) ' ( ' + cls_name + ' ) ' , x = tf.transpose ( x , ( 0 , 2 , 1 ) ) # NWC - > NCW int_shape = K.int_shape ( tensor ) [ start_idx : ] rand = K.eval ( K.random_normal ( ( 200 , 200 ) , def categorical_crossentropy ( y_true , y_pred , from_logits=False , label_smoothing=0 ) : neither ` `` channels_last '' ` or ` `` channels_first '' ` . def is_indexed_slices ( grad ) : module_objects=all_classes , training=training , if value.shape ! = ( batch_size , dim ) : 'but the model expects a list of ' + str ( len ( names ) ) # Standardize the inputs . class Flatten ( Layer ) : # To prevent a slowdown , pool3d = pool # input shape known ? then we can compute the output shape _x = x [ rep : ( rep + 1 ) ] np.array ( [ [ 10 ] , [ 11 ] ] ) ) # Handle negative axes with shape ( output_length , feature_dim , filters ) self._add_inbound_node ( input_tensors=inputs , A Keras model instance . If an optimizer was found String , the current default float type . padding : Currently only support ` `` valid '' ` ( case-insensitive ) . if self.use_bias : num_states = len ( self.cell.state_size ) pooling.GlobalAveragePooling1D ] ] # Attempt automatic input shape inference . def _preprocess_conv2d_input ( x , data_format , force_transpose=False ) : 'and you are passing both ` kernel_size ` and ` strides ` ' self.kernel_size = conv_utils.normalize_tuple ( kernel_size , 2 , 'kernel_size ' ) for state_mask , state_t , state_tm1 self.local_objects = None from .. utils.layer_utils import get_source_inputs epochs = 2 if on_batch_end is not None : validation_freq=1 , ' A target array with shape ' + str ( y.shape ) ( when full , workers could block on ` put ( ) ` ) name=name + '_metrics ' if name else None ) def check_composed_tensor_operations ( first_function_name , x = tf.expand_dims ( x , 1 ) preprocessor=None , Reference of the model being trained . ( 2 , 2 ) # default stddev=std ) ) def bias_regularizer ( self ) : 'sample_weight ' ) name , input_dim : dimensionality of the input ( integer ) . updated during training . The more updates a parameter receives , if self.mock_gcs : tuple ` ( x_val , y_val , val_sample_weights ) ` of Numpy arrays Input tensor , unchanged . if node_key not in self._network_nodes : assert os.listdir ( os.path.join ( tmpdir , 'examples ' ) ) interpolation=interpolation updates = self._updates [ : ] `` `` '' Apply batch normalization on x given mean , var , beta and gamma . old_layer = keras.layers.Deconvolution2D ( 5 , nb_row=3 , nb_col=3 , return self.fn ( y_true , y_pred , * * self._fn_kwargs ) depthwise_kernel_shape , data_format ) @ interfaces.legacy_spatialdropoutNd_support return [ K.int_shape ( x_elem ) for x_elem in x ] # Update dimensions of weights to match with mask . accumulator , for instance ` lambda acc , x : acc + x ` return [ tuple ( shape1 ) , tuple ( shape2 [ : -1 ] ) ] shape = K.shape ( data ) def __init__ ( self , input , shape , name='reshape_with_batch ' ) : x = tf.where ( condition , then_expression , else_expression ) cropping_all_dims = transpose_shape ( cropping_all_dims , elif isinstance ( n , tuple ) and len ( n ) ! = len ( shape ) : b_constraint='unitnorm ' , def __init__ ( self , num_classes , name=None , dtype=None ) : from keras.utils.generic_utils import custom_object_scope class_id=None , `` `` '' Type of AUC Curve ( ROC or PR ) . '' '' '' `` `` '' Retrieves a live reference to the global dictionary of custom objects . A list of tuples of array indices . if isinstance ( cell , ( list , tuple ) ) : ( 'b_regularizer ' , 'bias_regularizer ' ) , nested_states = nested_states [ : :-1 ] weights2 = saving.preprocess_weights_for_loading ( layer , weights1 ) the axes of ` output ` . assert len ( layer.trainable_weights ) == 3 target = C.transpose ( target , permutation ) 'fallback to auto mode . ' % ( self.mode ) , initial.append ( C.to_batch ( s ) ) target = 'GRU ( reset_after=True ) ' self.data [ attr ] = val not K.tensorflow_backend._get_available_gpus ( ) ) , or a constrained dictionary search . value = np.asarray ( value , dtype=dtype ) import importlib * * params : Dictionary of parameter names mapped to their values . batch_input_shape = tuple ( batch_input_shape ) elif hasattr ( y , '_keras_shape ' ) : 'metrics ' : callback_metrics , specifying the strides of the convolution . sample_weight = None h_tm1_i = h_tm1 hasattr ( self , 'compute_mask ' ) ) input mask for Embedding . model = keras.models.model_from_json ( model.to_json ( ) ) itself=False , weights [ 7 ] , normalize_inference , of the layer ( i.e . it should match the out_width = conv_utils.deconv_length ( width , new_layer = keras.layers.ConvLSTM2D ( 5 , ( 3 , 3 ) , name='conv ' ) `` `` '' Fused version of ` normalize_batch_in_training ` . return [ ( i * batch_size , min ( size , ( i + 1 ) * batch_size ) ) BaseMeanIoU = object new_shape = tuple ( new_shape_temp ) md5_hash : Deprecated in favor of 'file_hash ' . random.shuffle ( sequence ) y._keras_shape = ( np.prod ( x._keras_shape ) , ) output = np.log ( output / ( 1 - output ) ) 'moving_variance_initializer ' : if accept_all and arg_spec.varkw is not None : return eval_fn ( [ ] ) [ 0 ] self.false_positives = self.add_weight ( or set ( dir ( Sequence ( ) ) ) .issubset ( set ( dir ( seq ) + [ 'use_sequence_api ' ] ) ) ) ' weights , did you set ` model.trainable ` without calling ' if they 're in plain Keras format . def logcosh ( y_true , y_pred ) : The hook will be passed three arguments ; `` `` '' Applies an activation function to an output . def gru ( cudnn=False , * * kwargs ) : with K.name_scope ( 'metrics ' ) : prefix_shape = list ( base_shape ) correctly ( see ` lecun_normal ` initialization ) and the number of inputs load_function : The function to wrap , with requirements : pattern [ -1 ] = a0 states = states [ : -self._num_constants ] while i < len ( cntk_axis ) : constants_shape = None from .load_backend import local_conv1d ` output = ( x - mean ) / sqrt ( var + epsilon ) * gamma + beta ` samples = [ init ( ( 2 , 2 ) ) for _ in range ( 2 ) ] kwargs [ 'mask ' ] = K.reshape ( mask , inner_mask_shape ) The purpose of this class is to be able to tests model saving/loading to/from def elu ( x , alpha=1.0 ) : ' ` shape ` does not include the batch ' desired = K.clip ( norms , 0 , self.max_value ) `` `` '' Flattens the input . Does not affect the batch size . c : float > = 0 . Gradients will be clipped 'value ' + str ( value ) piecewise linear activation function over the inputs . [ [ 0.30999 , 0.309938 , 0.0679938 , 0.0673362 , 0.0708352 , 0.173908 ] , origin = 'https : //www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz ' return output_shape + state_shape + copy.copy ( state_shape ) @ interfaces.legacy_dropout_support auto_padding= [ False , padding , padding , padding ] , depth_multiplier , super ( Adagrad , self ) .set_weights ( weights ) steps_done += 1 global _SHARED_SEQUENCES 'reset_after ' : self.reset_after } [ 0.279611 , 0.452966 , 0.0204795 , 0.0209126 , 0.0194803 , 0.20655 ] , 'you should specify ' 'must be in interval [ 0 , 1 ] . ' % level ) x , gamma , beta , mean , var , 'spatial ' , epsilon ) `` `` '' Resets wait counter and cooldown counter . self.forward_layer.state_spec = state_specs [ : num_states // 2 ] K.pool2d ( x , pool_size=pool_size , data_format='channels_middle ' ) broadcast_beta = T.reshape ( beta , target_shape ) # Without merging should be decoded as : `` AABB '' , with merging as : `` AB '' . you can use ` noise_shape= ( batch_size , 1 , features ) ` . from .. import models while dynamic_axis_index < nones : state updates , e.g . when we need to update a layer 's internal state # is the same as the corresponding input . weighted_metrics=None , # dtype='bool ' is only available since Theano 0.9.0 ' a tuple of 2 ints ' input_shapes = to_list ( input_shapes ) new_model.train_on_batch ( None , val_out ) old_layer = keras.layers.MaxPooling2D ( if has_arg ( self.cell.call , 'training ' ) : from .convolutional import Cropping1D verbose=1 , model.add ( Reshape ( ( 3 , 4 ) , input_shape= ( 12 , ) ) ) input_length : tensor ` ( samples , 1 ) ` containing the sequence length for # ( used by Sequential models ) x_shape = K.shape ( x ) sliced_shape = list ( shape ) `` ` sh # If the input tensor ( s ) had not previous Keras history , check_single_tensor_operation ( 'squeeze ' , ( 4 , 3 , 1 ) , WITH_NP , axis=2 ) scale : Scaling factor ( positive float ) . std = np.sqrt ( 2 . / fan_in ) while scaling the entire tensor . classes = model.predict ( x_test , batch_size=128 ) `` `` '' 2D deconvolution ( transposed convolution ) . op , initial_epoch=initial_epoch ) max_queue_size=10 , value : A constant value ( or list ) super ( Huber , self ) .__init__ ( def DISABLED_test_check_not_failing ( ) : ins = x variables : tensor or list of tensors to consider constant with respect if all ( [ s is not None return self.classes_ [ classes ] inputs=computed_tensors ) for epoch in range ( initial_epoch , epochs ) : if ( axis == -1 or axis == x.ndim - 1 ) and x.ndim == 2 : y = np.sqrt ( x ) WITH_NP , axes= ( 2 , 1 ) ) kwargs.pop ( 'forget_bias_init ' ) from tensorflow.keras.applications.xception import decode_predictions def __init__ ( self , x , y , image_data_generator , self.kernel_shape = ( output_length , set ` axis ` to ` 0 ` to constrain each weight vector def evaluate ( self , feed_sample_weight_modes ) padding : Tuple of 2 integers , how many zeros to x = th_sparse_module.csr_matrix ( name=name , dtype=dtype ) from tensorflow.keras.layers import MaxPooling1D def sin ( x ) : dim1_padding = conv_utils.normalize_tuple ( padding [ 0 ] , 2 , Recurrent Neural Networks ] ( https : //arxiv.org/abs/1512.05287 ) assert dense._inbound_nodes [ 1 ] .get_config ( ) [ 'inbound_layers ' ] == [ 'input_b ' ] def to_yaml ( self , * * kwargs ) : def _wait_queue ( self ) : num_features = 5 `` `` '' Built-in metrics . config [ k ] = K.eval ( v ) if K.is_tensor ( v ) else v distribution = distribution.lower ( ) output_shape=antirectifier_output_shape ) ) base_config = super ( Embedding , self ) .get_config ( ) ` validation_split ` is set in ` ImageDataGenerator ` . assert_allclose ( np.squeeze ( out ) , np.squeeze ( out2 ) , atol=1e-05 ) # Restore original input spec return eager_fn_wrapper epsilon=1e-3 , `` `` '' Squeeze or expand last dimension if needed . is_training=False from keras import losses string , name of the model 's unique metric name a = h5file_ [ 'data1 ' ] .attrs [ 'attr ' ] get_weights containing the truth labels . styles = [ arg + `` : '' not in words for arg in args ] from .convolutional import SeparableConv1D output_shapes : list of output shape tuples . self.padding = conv_utils.normalize_padding ( padding ) # Historically , ` Sequential ` was once 'is deprecated with TensorFlow 2.0 . ' output = C.transpose ( output , permutation ) return C.reduce_sum ( all_matrix ) def multi_gpu_model ( model , gpus=None , cpu_merge=True , cpu_relocation=False ) : ` self._add_inbound_node ( last_layer ) ` initializer=self.kernel_initializer , K.depthwise_conv2d ( dummy_x_2d , dummy_w_2d , f = self.predict_function # Update dimensions of ` sample_weight ` to match with ` losses ` if possible . return C.tanh ( x ) A bool tensor . origin , return C.sin ( x ) super ( LossFunctionWrapper , self ) .__init__ ( reduction=reduction , name=name ) 'Merge mode should be one of ' `` `` '' Element-wise maximum of two tensors . @ K.symbolic vhat_t = K.maximum ( vhat , v_t ) the losses and metrics to TensorBoard after each batch . The same `` `` '' Fashion-MNIST dataset . all_output_shapes = set ( def fit ( self , x , y , sample_weight=None , * * kwargs ) : # Check if callbacks have not been already configured batch_size , steps_per_epoch , x ) ` class_id ` is indeed a correct label . exp = np.exp if x is None or isinstance ( x , list ) : epsilon=1e-3 ) : # pragma : no cover sample dimension is assumed either the same as the input : return self._get_node_attribute_at_index ( 0 , 'input_tensors ' , 'You passed : x= ' + str ( x ) ) inputs_f = inputs summary # result = max ( 0 , 1-y_true * y_pred ) = [ 1.6 + 1.7 + 1.5 ] / 3 ' Keras tensors and non-Keras tensors ' files , groups and dicts with a common API . input_tensors=self.inputs , for layer in self.layers : function_name = identifier o = layers.Lambda ( func ) mode = 'temporal ' 'of `` nearest '' or `` bilinear '' . ' ) be trained via backprop or not ( assuming 'sgd ' , ` ( 1 - rate ) * norm + rate * norm.clip ( min_value , max_value ) ` . return T.minimum ( x , y ) else : printable_module_name='loss function ' ) from tensorflow.keras.layers import Cropping3D 'CNTK backend warning : GPU is not detected . ' min_val = -1 . input_tensor : Optional tensor to use as layer input beta = tf.reshape ( beta , [ -1 ] ) for i in indices_for_conversion_to_dense : def predict_on_batch ( self , x ) : for i , s in enumerate ( t._keras_shape ) : ` data_format= '' channels_first '' ` , unconcatenated_outs [ i ] .append ( batch_out ) return image.array_to_img ( x , ( 'conv3d ' , ( 2 , 3 , 4 , 5 , 4 ) , ( 3 , 3 , 3 , 3 , 4 ) , 'same ' , 'channels_first ' ) , was never compiled in the first place ) . from tensorflow.keras.applications.resnet_v2 import preprocess_input identity = Identity preds2 = model2.predict ( [ np.random.random ( ( 1 , 32 ) ) ] ) For example , if ` y_true ` is [ [ 2 ] , [ 1 ] ] and ` y_pred ` is self._build_input_shape = input_shape # ( batch_size , dim1 , dim2 , ... ) - > ( dim1 , dim2 , ... , batch_size ) 'keras.backend ' , 'keras.engine ' , The generator is expected to loop over its data # do not slice the learning phase def _contain_seqence_axis ( x ) : and hasattr ( C.sequence , reduce_fun_name ) : ` `` binary '' ` : 1D numpy array of binary labels , class Sum ( Reduce ) : losses , _ , sample_weight = squeeze_or_expand_dimensions ( 2 . Values passed to ` sk_params ` with H5Dict ( filepath , mode= ' w ' ) as h5dict : x_test , y_test = np.array ( xs [ idx : ] ) , np.array ( labels [ idx : ] ) elif isinstance ( self._output_shape , ( tuple , list ) ) : `` `` '' Segment-wise linear approximation of sigmoid . training/validation . ` bytes ` data ( e.g . ` io.BytesIO ` ) . 'work with the natural order of states if you ' y_pred , ' or outdated because the { } file hash does not match ' loss_weight ) in enumerate ( zipped_inputs ) : input_uid = object_list_uid ( inputs ) `` `` '' Returns the loss corresponding to the loss input in ` compile ` API . '' '' '' _ , x = parse_shape_or_val ( ( num_samples , timesteps , input_dim ) ) def __init__ ( self , learning_rate=0.001 , rho=0.9 , * * kwargs ) : # input_a = keras.backend.variable ( val_a ) def _get_executor_init ( self , workers ) : return T.max ( x , axis=axis , keepdims=keepdims ) f = k.function ( [ y ] , [ exp ] , updates= [ ( x , update ) ] ) mean = mean.dimshuffle ( shuffle_pattern ) @ pytest.mark.parametrize ( 'initializer ' , get_custom_objects ( ) .clear ( ) python : 2.7 out_labels = out_labels or [ ] multi-GPU data parallelism . It works in the following way : or list of Numpy arrays if the model has multiple inputs . TypeError : if ` config ` is not a dictionary . __Easy extensibility.__ New modules are simple to add ( as new classes and functions ) , and existing modules provide ample examples . To be able to easily create new modules allows for total expressiveness , making Keras suitable for advanced research . new_model.train_on_batch ( x2 , y2 ) if self.trainer is None : histogram_freq=histogram_freq , x_f = K.bias_add ( x_f , self.bias_f ) `` `` '' Decorator used in TensorFlow 2.0 to exit the Keras graph . std = np.sqrt ( 1 . / fan_in ) idx = future.idx proba = self.model.predict ( x , * * kwargs ) 'use the functional API . ' ) # Update weights with mask . class_name ) ) if constraint is not None : raise ValueError ( 'Error when checking model ' 'as model targets . ' ) from tensorflow.keras.layers import subtract self._num_constants = None # Retrieve a slice of the input . lambda : else_expression ) def __init__ ( self , * args , * * kwargs ) : ` ( batch_size , pooled_dim1 , pooled_dim2 , pooled_dim3 , channels ) ` value = value.astype ( np.float32 ) val_x , val_y = validation_data def round ( x ) : from FullArgSpec . def test_batch_normalization ( self , axis , x_shape ) : stdinv = theano.tensor.as_tensor_variable ( stdinv ) accumulator.name = str ( name ) 'left_pad ' : 3 } , original_backend : Keras backend the weights were trained with , as a string . sub_n_last_node [ layer.name ] = sub_n_nodes [ -1 ] 'steps ' : steps_per_epoch , model.add ( Dense ( units=10 , activation='softmax ' ) ) def bias_add ( x , y , data_format ) : x , y , image_data_generator , else : # pragma : no cover return y_pred , y_true reason='theano returns tuples for updates ; cntk buggy ' ) Adadelta is a more robust extension of Adagrad shift = C.stop_gradient ( shift ) output = y * y_rev imshp=None , self._input_layers = [ ] 'Please double check how the gradient node ' input_dim=None , int ) : [ 1 , 0 , 0 ] , # A A boolean : Whether the argument is a Keras tensor . to be fed to an LSTM layer . generator : Generator yielding batches of input samples @ interfaces.legacy_zeropadding3d_support return [ out1 , out2 ] # Final result : 2.6 assert json.dumps ( old_layer.get_config ( ) ) == json.dumps ( new_layer2.get_config ( ) ) output2 = layers.Dense ( 1 , name='output2 ' ) ( x ) def one_hot ( indices , num_classes ) : It transforms an objective function ` fn ( y_true , y_pred ) ` # layer = Dense ( 32 ) code = inspect.getsource ( member ) .split ( '\n ' ) def DenseNet169 ( * args , * * kwargs ) : y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) # On model saving input_shape= ( height , width , 3 ) , ` validation_data ` will override ` validation_split ` . names : List of expected array names . output_masks.append ( mask ) `` `` '' Calls the ` on_train_end ` methods of its callbacks . validation_steps=None , for a in axis : if _get_dynamic_axis_num ( c ) == 1 : `` `` '' Softplus of a tensor . simply scaled by the given value . If ` sample_weight ` is a tensor of size Shape inference : self._delta_ts = defaultdict ( lambda : deque ( [ ] , maxlen=self.queue_length ) ) 'Dilated separable 1D convolution is currently not supported ' def test_constant ( tensor_shape ) : val_use_sequence_api ) It is the topological form of a `` model '' . A Model # now : model.output_shape == ( None , 3 , 2 , 2 ) assert len ( history.history ) < 20 x_weights = [ ] self.depthwise_initializer = initializers.get ( depthwise_initializer ) on_epoch_begin : called at the beginning of every epoch . elif sys.version_info < ( 3 , 3 ) : volume_shape = ( volume_shape [ 0 ] , volume_shape [ 4 ] , raise ValueError ( 'The output of the `` schedule '' function ' def test_cumsum ( self ) : or list/tuple if multiple classes . for metric_name , metric_fn in metrics_dict.items ( ) : return a * x + b module_objects=globs , model.add ( Dense ( 64 , input_shape= ( 10 , ) , activation='relu ' ) ) count = 0 if self.center : on the layer 's weights variables , not on any inputs tensors ) . `` `` '' Multi-GPU training utilities . '' '' '' Weights can be converted in both directions between ` LSTM ` and ` CuDNNSLTM ` raise IndexError log_p_next = T.set_subtensor ( [ 0 , 0 , 1 , 0 ] then the true negatives value would be 1 . dataframe , from . import embeddings def _make_test_function ( self ) : return forward_weights + backward_weights self.nesterov = nesterov if len ( set ( id ( x ) for x in self.inputs ) ) ! = len ( self.inputs ) : def __init__ ( self , value=0 ) : ( 'filter_length ' , 'kernel_size ' ) , `` `` '' Just your regular densely-connected NN layer . add_loss z_list = [ ] first_function_args , * * kwargs : Used for backwards compatibility . assert np.allclose ( decode ( merge_repeated=False ) , [ np.array ( [ [ 0 , 0 , 1 , 1 ] ] ) ] ) model = keras.models.Model ( [ input_a , input_b ] , [ output_a , output_b ] ) tensor_indices= [ ] , if obj in custom_objects : 'Basically steps_per_epoch = samples_per_epoch/batch_size . ' # TODO remove this if statement when Theano without nodes_by_depth [ depth ] .append ( node ) if num_dynamic_axis == 1 and len ( shape ) > 0 and shape [ 0 ] == -1 : base_config = super ( Cropping1D , self ) .get_config ( ) # misc functions ( e.g . loss function ) kernel_regularizer='l1 ' , # Inference ( only if doing validation from data tensors ) . `` `` '' Invokes the ` LossFunctionWrapper ` instance . y : Tensor or variable . classes = proba.argmax ( axis=-1 ) sample_weight=None , function_type = config.pop ( 'function_type ' ) from . import network data = [ raise RuntimeError ( 'The layer has never been called ' computable_tensors = [ ] output_values = self.metrics_func.eval ( input_dict , as_numpy=False ) self._total_width = len ( bar ) if any ( args_not_in_doc ) : keras_placeholder = K.placeholder ( shape= ( 2 , 4 , 5 ) ) 'variance_scaling ' ] ) reduce_axes.append ( a ) dataset or a dataset iterator test_split : Fraction of the dataset to be used as test data . `` `` '' Utilities for ImageNet data preprocessing & prediction decoding . '' '' '' ` batch_dot ` results in a tensor or variable with less dimensions b_regularizer='l1 ' , if len ( connections ) > 1 : def _call_metric ( metric_obj , * args , * * kwargs ) : new_layer_2 = keras.layers.UpSampling1D ( 3 , name='us1d ' ) A Numpy array . val_function=None , x_identity = K.identity ( x_placeholder ) Tuple of Numpy arrays : ` ( x_train , y_train ) , ( x_test , y_test ) ` . from tensorflow.keras.layers import ELU Keras ( κέρας ) means _horn_ in Greek . It is a reference to a literary image from ancient Greek and Latin literature , first found in the _Odyssey_ , where dream spirits ( _Oneiroi_ , singular _Oneiros_ ) are divided between those who deceive men with false visions , who arrive to Earth through a gate of ivory , and those who announce a future that will come to pass , who arrive through a gate of horn . It 's a play on the words κέρας ( horn ) / κραίνω ( fulfill ) , and ἐλέφας ( ivory ) / ἐλεφαίρομαι ( deceive ) . if name in [ 'accuracy ' , 'acc ' ] : skip_target_weighing_indices ) : # If the learning is either dynamic , or set to training : If set to 2 ( LSTM/GRU only ) , Can be run on non-Keras tensors . reduction=losses_utils.Reduction.SUM , name='bce_1 ' ) return serialize_keras_object ( metric ) warnings.warn ( 'The ` AtrousConvolution2D ` layer ' if dynamic_axis_num > len ( cntk_shape ) : the quantity monitored will not be overwritten . 'should have rank ' + str ( self.rank + 2 ) + ' . ' _u = getattr ( x , '_uses_learning_phase ' , False ) self.rho = rho check_batch_axis : Boolean ; whether to check that return unique_names # the batch normalization should be applied along the rightmost axis . feed_output_shapes.append ( output_shape [ : -1 ] + ( 1 , ) ) ( 'max_ndim= ' + str ( self.max_ndim ) ) if self.max_ndim else `` , self.input_names = [ ] and ` n_features ` is the number of features . padding_all_dims = ( ( 0 , 0 ) , ) + self.padding + ( ( 0 , 0 ) , ) fn = module_objects.get ( function_name ) raise TypeError ( 'The added layer must be ' for c in constant : a ` Tensor ` whose rank is either 0 , or the same rank as ` y_true ` , axis_without_batch = -1 if axis == -1 else axis - 1 This is done with processes or threads . h_tm1_z = h_tm1 * rec_dp_mask [ 0 ] shape = ( 5 , 5 , 5 ) `` `` '' IMDB sentiment classification dataset . '' '' '' > > > K.eval ( K.eye ( 3 ) ) raise EnvironmentError ( `` `` '' Test one batch , two beams - hibernating beam search . '' '' '' epsilon=epsilon ) _runner ( initializers.RandomNormal ( mean=0 , stddev=1 ) , tensor_shape , self.check_params ( sk_params ) i = 0 from tensorflow.keras.layers import GlobalMaxPooling2D base_config = super ( _GlobalPooling2D , self ) .get_config ( ) x_placeholder = K.placeholder ( shape= ( ) ) weights = [ np.array ( 0 ) ] + weights raise ValueError ( ' { } needs a docstring . '.format ( name ) , masks : List of computed output mask values . axis=-1 ) , from .load_backend import sign set_weights for module in modules : scale /= max ( 1. , float ( fan_in + fan_out ) / 2 ) def DISABLED_test_TensorBoard ( tmpdir , update_freq ) : if constants_shape is not None : # cntk calculate everything in float , so do n't need case from bool / int `` `` '' Bidirectional wrapper for RNNs . ImportError : if PIL is not available . b_regularizer='l2 ' , self.output_names = [ ] raise ValueError ( 'The model has ' + str ( len ( self.outputs ) ) from tensorflow.keras.layers import minimum 'Use ` get_output_mask_at ( node_index ) ` ' def test_one ( tensor_shape ) : super ( Accuracy , self ) .__init__ ( accuracy , name , dtype=dtype ) TFOpt = tf.train.Optimizer from tensorflow.keras.layers import ZeroPadding2D def save_model ( model , filepath , overwrite=True , include_optimizer=True ) : model_config [ 'class_name ' ] = model.__class__.__name__ raise ValueError ( 'When passing a list as loss_weights , ' from tensorflow.keras.layers import Multiply `` `` '' Maps ` sample_weight ` or ` class_weight ` to model outputs . _gcs_copy ( tmp_filepath , filepath , overwrite ) get_custom_objects ( ) [ 'MyObject ' ] = MyObject class MaxPooling2D ( _Pooling2D ) : x._uses_learning_phase = _u or uses_learning_phase ' a number of samples that can be ' def assert_list_keras_shape ( t_list , z_list ) : if dims > 0 and x.shape [ 0 ] == C.InferredDimension : .format ( self.name , input_shape ) ) old_layer = keras.layers.Embedding ( 1 , 1 , dropout=0.0 , name='d ' ) Fraction of the units to drop for origin='https : //s3.amazonaws.com/img-datasets/mnist.npz ' , outs [ i ] += float ( batch_out ) return outputs strides : stride integer . return x.shape.rank 3D tensor with shape ` ( batch_size , timesteps , input_dim ) ` . def test_slice ( self , shape , start , size ) : var = tf.reshape ( var , [ -1 ] ) conda config -- set always_yes yes 'bias_constraint ' : constraints.serialize ( self.bias_constraint ) output_shape = ( batch_size , self.filters , return T_softsign ( x ) self.factor = factor if None in [ shape1 , shape2 ] : self.min_ndim = min_ndim 'six > =1.9.0 ' , if all ( [ isinstance ( x , int ) for x in args [ 2:5 ] ] ) : x = x + reshape ( bias , ( 1 , ) + bias_shape ) updates The current release is Keras 2.3.0 , which makes significant API changes and add support for TensorFlow 2.0 . The 2.3.0 release will be the last major release of multi-backend Keras . Multi-backend Keras is superseded by ` tf.keras ` . ` ( batch , filters , new_conv_dim1 , new_conv_dim2 , new_conv_dim3 ) ` callbacks.on_predict_end ( ) for i in range ( len ( outputs ) ) : assert not model.uses_learning_phase feed_sample_weight_modes = self._feed_sample_weight_modes * * params : ignored ( exists for API compatibility ) . symbolic_weights = layer.weights input2_depth = depth x_aggregate = concatenate ( xs , axis=0 ) if not match : dilation=cell.dilation_rate [ 1 ] ) for ( k , k1 ) in zip ( range ( pool_size [ 0 ] ) , range ( -pool_size [ 0 ] , 0 ) ) ] # Collect input shapes to build layer . result.append ( C.times ( xi , yi , output_rank=y_ndim - 2 + int ( y_expanded ) ) ) border_mode=th_padding , kullback_leibler_divergence , name=name , reduction=reduction ) x_shape = ( 1 , ) + shape + ( 4 , ) raise ValueError ( 'When using causal padding in ` Conv1D ` , ' x : Input tensor or variable . initializers.serialize ( self.recurrent_initializer ) , batch_logs = { 'batch ' : batch_index , 'size ' : len ( batch_ids ) } > > > kvar = K.variable ( np.random.random ( ( 2,3 ) ) ) you can pass a 2D array with shape return [ self.kernel , self.recurrent_kernel , self.bias ] outputs.append ( output_t ) for ( i , p ) in enumerate ( params ) ] input_shape , depthwise_regularizer=None , n_gates : Number of gates ( 4 for LSTM , 3 for GRU ) . def is_running ( self ) : group2 = group1 [ 'group2 ' ] epochs=1 , converted.append ( ( 'kernel_size ' , 'kernel_dim * ' ) ) when their absolute value exceeds this value . if mask.shape ! = inputs.shape [ :2 ] : if y.shape [ 1 ] > 1 : if hasattr ( C , 'pad ' ) : from . import convolutional_recurrent `` `` '' Leaky version of a Rectified Linear Unit . zca_whitening=False , loss_fn = losses.get ( loss ) all spatial dimensions . # with a range of TensorFlow versions . len ( args ) == 1 and 'outputs ' in kwargs or entire 2D feature maps instead of individual elements . If adjacent pixels second_function_args , dilation_rate=2 , [ Delving Deep into Rectifiers : Surpassing Human-Level Performance on config = { } C.output_variable ( # for k in config [ 'config ' ] .keys ( ) : def _serialize_model ( model , h5dict , include_optimizer=True ) : from tensorflow.keras.layers import Add conv_out = K.conv2d ( x , w , strides=self.strides , set_image_data_format ( _image_data_format ) kernel_shape = _preprocess_conv3d_filter_shape ( kernel_shape , data_format ) have simply been skipped . One : You can specify the initial state of RNN layers symbolically by return K.tensorflow_backend._get_available_gpus ( ) + [ '/cpu:0 ' ] model = keras.models.Model ( [ inputs , init_state ] , outputs ) new_layer = keras.layers.Cropping2D ( data_format='channels_last ' , name='c2d ' ) if self.append : to apply a different weight to every timestep of every sample . auto_padding= [ False , padding ] , self.bias_i = None y_shape [ : y_ndim - 1 ] ] ) # Used only in conjunction with graph-networks This allows you to optionally specify min_val = -2 . if not _is_tf_1 ( ) : strides=strides , if not self._is_started : self.device = device target_tensors = None rate = kwargs.pop ( 'atrous_rate ' ) _UID_PREFIXES [ prefix ] += 1 ones , w = np.fliplr ( np.flipud ( w ) ) self.output_tensors = output_tensors super ( Dropout , self ) .__init__ ( * * kwargs ) def __init__ ( self , target_shape , * * kwargs ) : match=r ' . * load . * [ 0-9 ] + layers into . * [ 0-9 ] + layers . ' ) : spatial_axes ) # cntk does n't support gradient as symbolic op , to hook up with keras model , use_multiprocessing=False , For estimation of these metrics over a stream of data , the function creates an [ batch , channels , height , width ] ( for 'channels_first ' data_format ) self.kernel_o , if the ` data_format ` is ` `` channels_first '' ` . For instance , in a ` Dense ` layer the weight matrix break if sorted ( reduction_axes ) == list ( range ( ndim ( x ) ) ) [ : -1 ] : callbacks._call_end_hook ( 'predict ' ) config [ 'num_constants ' ] = self._num_constants [ On the importance of initialization and momentum in deep learning ] ( shape.insert ( index , 1 ) def ndim ( self ) : assert os.path.isdir ( os.path.join ( tmpdir , 'examples ' ) ) for i in indices : sample_weights : Optional list of sample weight arrays . return self.data [ idx ] of the 1st , 2nd , and 10th epochs . converted.append ( ( 'kernel_size ' , 'nb_row/nb_col ' ) ) kwargs = getattr ( self , '_function_kwargs ' , { } ) if not expand_nested : class LogCoshError ( MeanMetricWrapper ) : of the layer uses ` K.in_training_phase ( ) ` return _FLOATX the test samples , used for weighting the loss function . x : Tensor to print . the callback will write the metrics and losses to TensorBoard every return weight , mode 'channels_last ' , assert len ( x.shape ) == 3 - ( 1 if num_dynamic_axis > 0 else 0 ) if hasattr ( instance , 'get_config ' ) : # test __getitem__ # Rebuild the label as a table including input/output shapes . from .load_backend import function number of parameters ) . raise KeyError return vgg19.VGG19 ( * args , * * kwargs ) def test_conv2d_legacy_interface ( ) : unroll=unroll , value_conversions= { 'consume_less ' : { 'cpu ' : 0 , shape= ( None , input_dim ) ) epochs : Integer . Number of epochs to train the model . for layer_name in embeddings_vars.keys ( ) } regularizers.serialize ( self.embeddings_regularizer ) , raise ValueError ( 'Asked to get ' + attr_name str ( len ( sample_weight_mode ) ) + 'sample_weight_modes ' ) output_arrays = [ ] def test_one_hot ( self ) : A ` pydot.Dot ` instance representing the Keras model or if bad_attributes : 'or a tuple of 2 tuples of 2 ints ' def _assert_sparse_module ( ) : ( useful for resuming a previous training run ) . keras1_args = { 'samples_per_epoch ' , 'val_samples ' , computable_tensors.append ( x ) _LEARNING_PHASE = True from tensorflow.keras import Sequential # We were passed a model as first layer . first_val = self.normalizer ( self.data [ 0:1 ] ) target_shape = T.stack ( * target_shape ) A model instance . self.reset_after = reset_after def test_lecun_normal ( tensor_shape ) : axis_list = [ axis ] ` ( rows , cols , input_depth , output_depth ) ` , 'requested permute on dynamic axis , ' str ( len ( params ) ) 'instantiated via ' batch_input_shape = ( [ 0.230246 , 0.450868 , 0.0389607 , 0.038309 , 0.0391602 , 0.202456 ] , callbacks=None ) : test_doc1 , model.add ( Dense ( 32 , input_dim=500 ) ) 6.63339 # output beam 1 steps_per_epoch : Total number of steps ( batches of samples ) # if inbound_layer is Model kwargs [ 'input_shape ' ] = ( self.input_dim , ) for i in self.shared_axes : layer = RNN ( cell ) self.seen = 0 nb_worker=1 , pickle_safe=False , max_q_size=3 ) weight_values [ i ] ) ) b_constraint : instance of the [ constraints ] ( .. /constraints.md ) module , metric = metrics_module.get ( metric ) input , output . > > > kvar = K.random_normal_variable ( ( 2,3 ) , 0 , 1 ) elif len ( validation_data ) == 3 : def dropped_inputs ( ) : has reset gate applied to hidden state before matrix multiplication . The WITH_NP = [ KTH , KNP ] update_freq='epoch ' ) : x = np.random.random ( ( num_samples , input_dim ) ) A tensor of the cumulative product of values of ` x ` along ` axis ` . object_name = func.__name__ `` `` '' Learning rate scheduler . u , _ , v = np.linalg.svd ( a , full_matrices=False ) ( tf_data_format == 'NCHW ' and assert history.history [ 'val_accuracy ' ] [ -1 ] > 0.75 if K in [ KTF , KTH ] : if not model.built : from tensorflow.keras.layers import Softmax return _get_dynamic_axis_num ( x ) `` `` '' Calls the ` on_test_begin ` methods of its callbacks . self._uses_inputs_arg = True class SimpleRNNCell ( Layer ) : constraint=self.gamma_constraint ) assert input1_depth ! = -1 layers : list of layers to add to the model . targets=targets , self._num_constants = len ( constants ) 'output_masks ' , 'The reshape did not take place . ' ) output_mask = masks [ i ] if masks else None alpha=self.negative_slope , and ` `` bicubic '' ` . K.reshape ( weights , [ 1 , -1 ] ) , [ num_thresholds , 1 ] ) to the ` Conv2D ` layer , except that weights are unshared , batch_size = 40 padding , data_format , layer_data : layer config dict . ` ( batch , new_rows , new_cols , filters ) ` with CustomObjectScope ( { 'MyObject ' : MyObject } ) : line = `` assert np.allclose ( log_prob_pred_np , log_prob_pred ) max_time_steps = 6 # Nodes that can not yet be processed ( if the inbound node to a weight ( float ) value , used for weighting the loss function ' 3+ dimensional targets . ' ) variable = th_sparse_module.as_sparse_variable ( model.add ( LSTM ( 32 , input_shape= ( 10 , 64 ) ) ) broadcast_gamma = C.reshape ( gamma , target_shape ) check_single_tensor_operation ( 'var ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) input_shape [ 4 ] + padding [ 2 ] [ 0 ] + padding [ 2 ] [ 1 ] ) rescaled by the corresponding element in the ` sample_weight ` vector . If 'valid ' , 'channels_first ' , ( 2 , 2 ) ) , ( fit_inputs [ 0 ] .shape [ 0 ] , val_inputs [ 0 ] .shape [ 0 ] ) ) The variable ` x ` updated . if H5Dict.is_supported_type ( filepath ) : ( 'nb_val_samples ' , 'validation_steps ' ) , fan_in = shape [ 0 ] if function_type == 'function ' : w ) seed=seed ) steps=None , 'mask should have ` shape= ( samples , time ) ` , ' val = H5Dict ( val ) depth = input_shape [ d_axis ] if itself : base_config = super ( RepeatVector , self ) .get_config ( ) dilation_rate=self.dilation_rate ) summation_method : ( Optional ) Specifies the Riemann summation method used raise ValueError ( 'To clone a ` Sequential ` model , we expect ' # ( since serialized nodes refer to layers by their name ) . self.true_negatives [ min_index ] / denom , _callbacks = [ cbks.BaseLogger ( new_layer_1 = keras.layers.SpatialDropout2D ( rate=0.5 , if num_time_step is not None and num_time_step is not C.FreeDimension : def __init__ ( self , filename , separator= ' , ' , append=False ) : base_config = super ( Adam , self ) .get_config ( ) for k in self.params [ 'metrics ' ] : axes.append ( self.axes [ i ] % K.ndim ( inputs [ i ] ) ) w_pad = pool_size [ 0 ] - 2 if pool_size [ 0 ] % 2 == 1 else pool_size [ 0 ] - 1 flat_shape = ( num_rows , num_cols ) compute_output_shape def decode ( merge_repeated ) : from the new rank of ` y_pred ` . ( ` keras.utils.Sequence ` ) object in order to avoid class Multiply ( _Merge ) : TypeError : if there are no layers in the model . super ( LeakyReLU , self ) .__init__ ( * * kwargs ) batch_shape : A shape tuple ( integer ) , including the batch size . def batchnorm_args_preprocessor ( args , kwargs ) : class Loss ( object ) : networks ] ( http : //jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf ) from tensorflow.keras.layers import SimpleRNNCell from .layer_utils import get_source_inputs old_layer = keras.layers.MaxPool1D ( pool_length=2 , if len ( shape ) ! = 2 : if not is_model ( inbound_layer ) and ( write_graph = False self._feed_sample_weights = [ def _clone_functional_model ( model , input_tensors=None ) : check_single_tensor_operation ( 'ones_like ' , ( 3 , 5 , 10 , 8 ) , WITH_NP ) In ` 'channels_first ' ` mode , the channels dimension shifted_mean = C.minus ( x , shift ) In other words , it flattens each data samples of a batch . early convolution layers ) then regular dropout will not regularize the check_single_tensor_operation ( 'spatial_3d_padding ' , x_shape , WITH_NP , from unittest.mock import patch for layer in self._layers : dtype_list.append ( k.dtype ( t ) ) > > > keras.backend.floatx ( ) from .np_utils import normalize in addition to the output . The returned elements of the conversions= [ ( ' p ' , 'rate ' ) ] ) constants_np=None , self.on_batch_end = lambda batch , logs : None groups=x.shape [ 0 ] ) # Check that all arrays have the same length . # and we make no assumptions about it . assert y._keras_shape == ( None , 8 ) subsample= ( 2 , 2 ) , for ( a , p ) in enumerate ( pad_info ) : beta_initializer='zeros ' , filepath : String , path to the file to save the weights to . layer.bias_f , self.embeddings_data = embeddings_data if len ( update ) ! = 2 : def l1 ( l=0.01 ) : reason= '' cntk does not support slice to 0 dimension '' ) for rep in range ( shape [ axis ] - 1 ) : check_single_tensor_operation ( 'std ' , ( 4 , 2 ) , WITH_NP ) from .load_backend import repeat unroll_options = [ True , False ] def handle_value ( k ) : def __init__ ( self , sequence , run_options = config_pb2.RunOptions ( output_partition_graphs=True ) shape=depthwise_kernel_shape , kwargs [ 'input_shape ' ] = ( kwargs.pop ( 'input_dim ' ) , ) ( 'val_samples ' , 'steps ' ) , def on_train_batch_end ( self , batch , logs=None ) : callbacks.on_batch_begin ( batch_index , batch_logs ) epsilon=epsilon , tf_keras_backend.batch_set_value ( tuples ) If ` True ` , use process-based threading . warnings.warn ( 'The ` forget_bias_init ` argument ' tensor_index : Tensor_index from which ` tensor ` comes from . return updated_metrics_dict if isinstance ( axis , tuple ) : def exp ( x ) : self.output_names.append ( layer.name ) K.set_learning_phase ( 1 ) weights [ 0 ] = weights [ 0 ] [ : , 0 , : , : ] pytest.main ( [ __file__ ] ) config [ 'size ' ] = self.size [ 0 ] Skip the data if it is ` None ` . if enqueuer is not None : super ( EarlyStopping , self ) .__init__ ( ) def count_params ( x ) : _ , depthwise = parse_shape_or_val ( kernel_shape if any ( K.is_tensor ( t ) for t in x ) : uses_learning_phase = y._uses_learning_phase # length . Will support it in next release . _test_optimizer ( optimizers.Adamax ( lr=1 . ) ) 'on a list of at least 2 inputs . ' broadcast_beta = None return ( device is not None and device.device_type.lower ( ) == device_type ) ' ( ( left_dim1_pad , right_dim1_pad ) , ' name=layer.name ) batch_size = 7 > > > x = K.placeholder ( shape= ( 32 , 28 , 3 ) ) elif isinstance ( val , ( list , tuple ) ) : in addition to the output . arguments= { ' i ' : i , image in TensorBoard . model.add ( Reshape ( ( 6 , 2 ) ) ) [ padding [ 0 ] [ 0 ] , padding [ 0 ] [ 1 ] ] , unroll : Boolean ( default False ) . states = tuple ( initial_states ) ' based on the ` keras.utils.Sequence ` class . ' 'but the layer received an input with shape ' if callable ( x ) : if len ( unconcatenated_outs ) == 1 : new_rows , new_cols , filters ) ` if data_format='channels_last ' . and the last part from either ` int_shape ` ( if provided ) state_shape = [ ( input_shape [ 0 ] , self.units ) for _ in self.states ] return resnet.ResNet101 ( * args , * * kwargs ) self.kernel_o = self.kernel [ : , self.units * 3 : ] raise ValueError ( 'An empty Model can not be used as a Layer . ' ) def test_value_manipulation ( self , function_name ) : return tuple ( result ) return tf.reverse ( x , axes ) output_shapes= [ batch_input_shape ] ) pointwise_kernel = tf.expand_dims ( pointwise_kernel , 0 ) self.use_steps = True raise ValueError ( 'Unknown entry in ` target_tensors ` ' # handle k < 1 and k > = predictions.shape [ 1 ] cases to match TF behavior sample_weights [ i ] constants=constants , def test_invalid_data_format ( ) : for i in range ( num_batches ) ] function_type = 'function ' import gzip if self.seen < self.target : depthwise_initializer : Initializer for the depthwise kernel matrix from .merge import maximum # e.g . CUDA_VISIBLE_DEVICES=0,2 python keras_mgpu.py x , broadcast_gamma , broadcast_beta , 'spatial ' , epsilon ) ( i.e . send_as_json is set to False ) . state for the sample of index i in the following batch . result = K.switch ( 'Use ` get_output_at ( node_index ) ` instead . ' ) return [ output ] + states # now model.output_shape == ( None , 28 , 28 , 32 ) gamma_initializer='ones ' , return load_function ( * args , * * kwargs ) # Make sure that all input tensors come from a Keras layer . rows = conv_utils.conv_output_length ( rows , use_bias : Boolean , whether the layer uses a bias vector . for x in output_tensors ] ( hasattr ( loss , '__name__ ' ) and raise ValueError ( 'CNTK Backend : ` eval ` method on ' be treated as a different class super ( MaxoutDense , self ) .__init__ ( * * kwargs ) then the true positives value is 2 . If the weights were specified as class SensitivityAtSpecificity ( SensitivitySpecificityBase ) : # We do n't want those compilation to show up in Theano profiler . condition = n > = c g = C.constant ( 0 , shape=v.shape , name='keras_grad_placeholder ' ) def test_atrousconv1d_legacy_interface ( ) : # Note input_shape will be list of shapes of initial states and initial_state = self.states def _padding ( x , pattern , axis ) : # pragma : no cover Pads these dimensions with respectively b_constraint='unitnorm ' , `` `` '' Gets the datatype of the dataset . at their default values return C.clip ( x , min_value , max_value ) are_ones = K.expand_dims ( are_ones , 0 ) # Whereas if you specify the input shape , the model gets built continuously assert len ( val_seq.logs ) == 12 * 5 depthwise_regularizer=depthwise_regularizer , masks = to_list ( masks ) mask = C.to_sequence_like ( mask , rnn_inputs ) containing the prediction , or output of the softmax . layer ( computed_tensors , * * kwargs ) ) y_pred , if extract : self._make_predict_function ( ) if beta.dtype ! = tf.float32 : # Use the inputs ; training , evaluating , predicting . if layer.__class__.__name__ == 'LSTM ' : if volume_shape : output = parallel_model.predict ( x ) is_zero_dim_ndarray = isinstance ( k , np.ndarray ) and k.ndim == 0 get_output_mask_at ( node_index ) 'sample_weight_mode= '' temporal '' ' return self.cell.bias_initializer `` `` '' Computes Kullback-Leibler divergence loss between ` y_true ` and ` y_pred ` . x = np.transpose ( x , ( 0 , 2 , 1 ) ) patience=0 , check_two_tensor_operation ( 'dot ' , ( 4 , 2 ) , ( 5 , 2 , 3 ) , WITH_NP ) sample_weight = K.repeat_elements ( 'Received inputs with shapes ' # Prepare inputs , delegate logic to ` predict_loop ` . if hasattr ( self , 'batch_input_shape ' ) : if callable ( then_expression ) : initial_state = self.cell.input_conv ( initial_state , new_out = new_model.predict ( x ) @ interfaces.legacy_gaussiandropout_support paths = [ ] d = cPickle.load ( f ) printable_module_name='output_shape function in Lambda layer ' ) op , # # Installation from tensorflow.keras.activations import tanh mask = mask.astype ( np.bool ) m.reset_states ( ) 'unroll ' : self.unroll , `` `` '' Loads CIFAR100 dataset . shape = tuple ( [ None for _ in range ( ndim ) ] ) If you never set it , then it will be ` `` channels_last '' ` . name='binary_crossentropy ' , class ZeroPadding2D ( _ZeroPadding ) : model.add ( keras.layers.Embedding ( 10 , input_size , self.pointwise_kernel = self.add_weight ( if hasattr ( self.layer , 'activity_regularizer ' ) : ' than or equal to rank of then and ' if len ( output_shape ) == 1 : self.queue.task_done ( ) self.build ( input_shape= ( None , ) + inputs.shape [ 1 : ] ) from .recurrent import _generate_dropout_mask tf.summary.histogram ( ' { } _out_ { } '.format ( layer.name , i ) , reduce_result = sum ( x , axis , keepdims=keepdims ) return inception_resnet_v2.decode_predictions ( * args , * * kwargs ) Consider a custom object ` MyObject ` ( e.g . a class ) : def _gcs_copy ( source_filepath , target_filepath , overwrite=True ) : ( handled by Network ) , nor weights ( handled by ` set_weights ` ) . if mask.dtype ! = np.bool : assert x.ndim == 2 return self.accumulator [ 0 ] metrics_names.extend ( [ m.name for m in self._compile_metric_functions ] ) def test_truncated_normal ( tensor_shape ) : new_layer = keras.layers.UpSampling2D ( ( 2 , 2 ) , data_format='channels_last ' , if cond_ndim < expr_ndim : elif ndim > 2 : def __init__ ( self , callbacks=None , queue_length=10 ) : y = to_list ( y , allow_tuple=True ) return tf_math_ops.cumprod ( x , axis=axis ) # In-place input splitting which is not only super ( Adadelta , self ) .set_weights ( weights ) 'Bilinear upscaling with factors other than ( 2 , 2 ) ' return regularization if not input_length : You can set RNN layers to be 'stateful ' , which means that the states return tf.squeeze ( x , [ axis ] ) from .load_backend import is_variable from .load_backend import eager self.kernel_size [ 0 ] * input_dim , def softplus ( x ) : # Add an endpoint `` threshold '' below zero and above one for either if is_accepted ( name , member ) or member_too_small ( member ) : learning_rate = kwargs.pop ( 'lr ' , learning_rate ) try : tensor_map = { } * * kwargs ) self.seen += batch_size self.kernel = self.add_weight ( shape= ( input_shape [ -1 ] , self.units ) , A tuple of scalars , ` ( fan_in , fan_out ) ` . old_layer = keras.layers.Convolution3D ( 5 , 3 , for axis in _axes : def image_dim_ordering ( ) : metrics_utils.ConfusionMatrix.FALSE_NEGATIVES : self.false_negatives , return new_output , n_s def _check_started ( self ) : @ interfaces.legacy_upsampling1d_support from .load_backend import min raise TypeError ( `` ` fn ` must be callable . '' ) # test stacked bidirectional layers distribution='normal ' , `` `` '' Transposes a tensor and returns it . self._set_sample_weight_attributes ( prefix = 'input ' `` `` '' This wrapper applies a layer to every temporal slice of an input . `` `` '' Computes the mean absolute error between the labels and predictions . sample_weight=sample_weight ) output_shape = self.compute_output_shape ( input_shape ) return ( name in arg_spec.args ) new_layer1 = keras.layers.GaussianDropout ( rate=0.6 , name='drop ' ) node_indices.append ( None ) ` batch_shape= ( ... ) ` to all the first layers in your model . noise_shape = ( input_shape [ 0 ] , input_shape [ 1 ] , 1 , 1 , 1 ) if hasattr ( w , 'name ' ) : from .pooling import MaxPooling2D sequence : a sequence function which yields data bias_initializer=bias_initializer , weighted_metrics , K.normalize_data_format ( 'channels_middle ' ) input_lens = np.expand_dims ( np.asarray ( [ 5 ] ) , 1 ) def accuracy ( y_true , y_pred ) : outputs = K.conv2d_transpose ( return tf.nn.dropout ( x , rate=level , noise_shape=noise_shape , seed=seed ) state_updates loss = logs.get ( 'loss ' ) m = keras.metrics.Hinge ( ) self.updates.append ( K.update ( vhat , vhat_t ) ) output ) . This can also be a list/tuple of integers * * self._open_args ) if hasattr ( self , 'clipvalue ' ) : # model.compile ( optimizer , loss='mse ' , loss_weights= [ 0.5 ] ) assert K.int_shape ( o ) == ( None , 4 , 5 ) last_ones.append ( self.queue.get ( block=True ) ) * * kwargs ) : weighted_metrics = convert_custom_objects ( ' the ` keras.utils.Sequence ` class . ' ) We expect labels to be provided as integers . If you want to provide labels # recomute steps per epochs in case if Sequence changes it 's length content_type = response.info ( ) .get ( 'Content-Length ' ) callbacks.set_model ( callback_model ) self.log_values = [ ] axes : Integer or tuple of integers , return test_func return self.cell.use_bias if self.initial_decay > 0 : return [ ] if isinstance ( dilation_rate , int ) : if not unroll and mask is not None : calling ` reset_states ` with the keyword argument ` states ` . The value of @ interfaces.legacy_conv3d_support if dynamic_axis_index < nones : layer = created_layers [ layer_name ] ` ( batch_size , steps , features ) ` > > > K.ndim ( inputs ) ` ( batch , first_axis_to_pad , second_axis_to_pad , third_axis_to_pad , K.depthwise_conv2d ( dummy_x_2d , dummy_w_2d , self.epsilon = epsilon conv2d = conv if key in self : converted.append ( ( 'forget_bias_init ' , 'unit_forget_bias ' ) ) return tf.exp ( x ) self.target = target to_yaml random_tensor = rng.binomial ( x.shape , p=retain_prob , dtype=x.dtype ) _EPSILON = float ( e ) b = K.variable ( np.random.random ( bias_shape ) ) return deserialized for i in range ( len ( input_shapes ) ) : # second state should not be incremented for last two timesteps variance , Used to implement efficient stacked RNNs . 'bias ' : self.bias , # On top of new , non-Keras tensor fields = [ name py_slice ( top_pad , input_shape [ 2 ] + top_pad ) , 'of tensors . ' ) input_shape , if hasattr ( value , '_metric_obj ' ) : outs = model.predict_on_batch ( x ) _SHARED_SEQUENCES [ self.uid ] = self.sequence output_tensors : list of output tensors . target_devices , x.set_value ( np.asarray ( value , dtype=x.dtype ) ) def DISABLED_test_learning_phase ( ) : return None seq_len_0 = 5 Tensor with same type and shape as ` initializer ` . from .pooling import GlobalAvgPool2D # The StackedConvRNN2DCells is n't implemented yet . g_shape = copy.copy ( then_expression.get_shape ( ) ) ( 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='maxpool2d ' ) h5dict : keras.utils.io_utils.HD5Dict instance . Concatenate the results ( on CPU ) into one big batch . steps=steps , _BACKEND = _backend mode : one of { auto , min , max } . return C.reshape ( x , new_shape ) directory=None , # before the final log , so the results are different but scale check_single_tensor_operation ( 'resize_images ' , x_shape , 'you should pass a 2D sample_weight array . ' ) metric_fn , name=metric_name ) ins : List of tensors to be fed to the Keras function . load_function : A function that takes a ` h5py.File ` , reads from it , and with K.name_scope ( loss_name ) : assert_allclose ( out_2 , new_out_2 , atol=1e-05 ) # the disable_tracking decorator without altering the class SparseCategoricalAccuracy ( MeanMetricWrapper ) : # Note : in this case , ` any ` and ` all ` are equivalent since we disallow def __init__ ( self , name='squared_hinge ' , dtype=None ) : self.name + ' : expected axis ' weights += layer.non_trainable_weights [ dim == 1 for dim in noise_shape ] ) if s : orthogonal = Orthogonal use_sequence_api = is_sequence ( generator ) from .. models import Model if layer.data_format == 'channels_first ' : self.file_flags = `` return x.get_value ( ) verbose=0 , mode='auto ' , min_delta=1e-4 , cooldown=0 , min_lr=0 , data_format , random_tensor = T.patternbroadcast ( random_tensor , progbar.update ( batch_end ) weights : List of weights values ( Numpy arrays ) . recurrent_dropout=recurrent_dropout , # When using the delayed-build pattern ( no input shape specified ) , you can if len ( self._inbound_nodes ) > 1 : ( self.false_positives + self.true_negatives ) ) , callbacks._call_batch_hook ( 'train ' , 'end ' , step_index , batch_logs ) [ 'func ' , 'seq ' ] , data_format=tf_data_format , self.kernel_constraint = constraints.get ( kernel_constraint ) max_queue_size : Maximum size for the generator queue . forward_updates + backward_updates ) self.on_test_begin ( ) shape = ( 10 , 2 , 3 ) return T.sum ( x , axis=axis , keepdims=keepdims ) def test_conv1d_legacy_interface ( ) : def is_sequence ( seq ) : v._uses_learning_phase = False feed_input_shapes , 'the model was * not * compiled . ' for x in inputs : if axis is None : # perf issue , will resolve it later with cntk cond op . 'W_regularizer ' : regularizers.serialize ( self.W_regularizer ) , start : int , start of desired slice of the specified dataset x_shape = ( 1 , 4 ) + shape self.init = initializers.get ( init ) If ` sample_weight ` is given , calculates the sum of the weights of assert layer.cell.state_size == ( 8 , 8 , 16 , 16 , 32 , 32 ) The model returned by ` load_model ` fieldnames = [ 'epoch ' ] + self.keys normalized_padding = ( height_padding , width_padding ) for n , met in inspect.getmembers ( member ) : keepdims : A boolean , whether to keep the dimensions or not . index_array = np.arange ( num_train_samples ) def __init__ ( self , alpha=1.0 , * * kwargs ) : if 'input_dim ' in kwargs : x_weights.append ( x_weight.get ( name ) ) padding , data_format ) : # may contain an unused ` dtype ` argument . y._keras_shape = tuple ( y._keras_shape ) return rng.normal ( size=shape , avg=mean , std=stddev , dtype=dtype , return T.nnet.bn.batch_normalization_test ( output_tensors : list of output tensors . # Update cache ; # Epoch finished . beta = zeros_like ( mean ) elif j == 1 : outputlabels = 'multiple ' input_tensors : optional list of input tensors kwargs = self.filter_sk_params ( Sequential.predict_proba , kwargs ) if kernel_shape [ 3 ] % 2 == 0 : the precision values by the recall . current , filepath ) ) if preds.min ( ) < 0. or preds.max ( ) > 1. : initializer=initializer ) Events are sent to ` root + '/publish/epoch/end/ ' ` by default . Calls are min_delta=0 , self.size = size # If arguments were numpy array , they have been saved as `` `` '' Repeat a 2D tensor . self._compute_previous_mask = ( uses_learning_phase = any ( somewhere else ( forbidden in ` Sequential ` models ) . b_regularizer='l2 ' , Any PNG , JPG , BMP , PPM or TIF images # If all previous input tensors are available in tensor_map , getargspec = inspect.getargspec # Shape : ` ( output_length , batch_size , filters ) ` . A list of ` sample_weight ` or ` class_weight ` where there are exactly # momentum def __init__ ( self , name='hinge ' , dtype=None ) : The selected tensor . # not possible to handle 3D convnets etc . `` `` '' Computes the mean squared logarithmic error between ` y_true ` and ` y_pred ` . if write_graph : form of symbolic tensors , generators , or def __call__ ( self , w ) : slice_arrays ( sample_weights , 0 , split_at ) , uses_correlation = { 'tensorflow ' : True , score : float 'You can build it manually via : ` ' `` `` '' Instantiates an identity matrix . name=code.co_name , sample_weight : Optional weighting of each example . Defaults to 1 . Can be c = keras.layers.concatenate ( [ a , b ] ) for x_shape , y_shape , axes in test_cases : # Handle data tensors support when no input given constraint=self.kernel_constraint ) 'HDF5 file because they are larger than ' y , sample_weight , validation_split=validation_split ) self.bias_c = self.bias [ self.filters * 2 : self.filters * 3 ] def test_dense_legacy_interface ( ) : activation=activation , to the model 's outputs . If a dict , it is expected to map left_pad = args [ 1 ] .get ( 'left_pad ' , 0 ) if d1 is not None and d2 is not None and d1 ! = d2 : Tensor , output of softmax transformation . histogram_freq = 0 return T.sqr ( x ) # of the deepest model to infer input shape and dtype . from .convolutional import ZeroPadding1D inputs_f = inputs subset=None , Repeats the 1st , 2nd and 3rd dimensions class Adagrad ( Optimizer ) : if y is not None : ( if you have TensorFlow installed ) , but the only feature available will assert new_seq == [ [ 1 , 2 , 3 ] ] l : l1 : strides [ 1 ] , start_time = time.time ( ) use_cudnn = ndim < 5 and ( dev.startswith ( 'cuda ' ) or dev.startswith ( 'gpu ' ) ) horizontal_flip=horizontal_flip , computed_masks = [ x [ 1 ] for x in computed_data ] ( 'conv3d ' , ( 2 , 3 , 5 , 4 , 6 ) , ( 3 , 2 , 4 , 3 , 4 ) , 'valid ' , 'channels_first ' ) , inspect.Parameter.KEYWORD_ONLY ) ) is executed on a dedicated GPU . K.expand_dims ( batch_size ) ] ) The config of a layer does not include connectivity rate : float , drop probability ( as with ` Dropout ` ) . stride , length of history , etc. , to produce batches for will then be used to fit/predict . One of the following will be batches of 32-dimensional vectors . `` `` '' Initializer that generates a truncated normal distribution . top_k : Optional int , indicates that the positive labels should be limited to interpolation='nearest ' , ' , '.join ( bad_attributes ) ) ) Legal arguments are the arguments of ` Sequential.fit ` from .load_backend import is_keras_tensor ( relative to ~/.keras/datasets ) . new_layer = keras.layers.LSTM ( 2 , input_shape= [ 3 , 5 ] , name= 'd ' , implementation=1 ) return [ update_total_op ] continue 'found gradient node ` % s ` which is not ' # This will never loop forever thanks to the test above . g = get_graph ( ) > > > # Any Keras layer output is a Keras tensor . ` [ 0 , 1 ] ` , or NEG_INF ( used when top_k is set ) . at the level of the first layer value : Python boolean . x : A tensor or variable to compute the activation function for . postfix_shape = list ( base_shape ) callbacks.set_model ( callback_model ) from .load_backend import separable_conv1d super ( LocallyConnected1D , self ) .__init__ ( * * kwargs ) a specific layer , or on your entire model . from keras.utils.generic_utils import transpose_shape # Set dtype . pool_size= ( 2 , 2 ) , border_mode='valid ' , name='avgpooling2d ' ) values_rank = K.ndim ( values ) enforcement of the constraint , while rate < 1.0 means that application/json . output._keras_shape = tuple ( output._keras_shape ) get_input_at from .. utils import generic_utils check_single_tensor_operation ( 'clip ' , ( 4 , 2 ) , WITH_NP , min_value=0.4 , x , mean , variant , beta , gamma , epsilon ) strides : strides tuple ( length 2 ) . Statistical Machine Translation ] ( https : //arxiv.org/abs/1406.1078 ) return tf.ones_like ( x , dtype=dtype , name=name ) kernel_constraint=kernel_constraint , beta : Tensor with which to center the input . return cls.from_config ( config [ 'config ' ] ) original = np.prod ( input_shape , dtype=int ) workers=1 , initial_states_np= [ h0 ] , ' ` batch_shape ` argument to your Input layer.\n ' This metric creates two variables , ` total ` and ` count ` that are used to from .callbacks import History # A simple CTC probability map with some repeating characters , def DISABLED_test_embedding_invalid ( input_shape ) : save_model ( model , gcs_filepath ) # model , input_tensors=input_a ) if len ( cropping ) ! = 3 : super ( Sum , self ) .__init__ ( reduction=metrics_utils.Reduction.SUM , model : Keras model instance . return np.float16 for name , value in logs.items ( ) : with gzip.open ( paths [ 3 ] , 'rb ' ) as imgpath : layer_config = layer.get_config ( ) class ActivityRegularization ( Layer ) : layer.bias_z_i , slices_dims = [ ] # + ( 30 * 30 ) * 64 parameters process each sub-batch on one GPU , then return the full def test_reuters_load_does_not_affect_global_rng ( fake_downloaded_reuters_path ) : cntk_dynamicity=True ) Boolean . top_paths = 2 ` ( samples , time , filters , output_row , output_col ) ` var = T.inv ( stdinv * * 2 ) data_gen = TimeseriesGenerator ( data , targets , ` f ( x ) = alpha * ( exp ( x ) - 1 . ) for x < 0 ` , learner = C.cntk_py.universal_learner ( p_list , u_list , update_func ) y = T.reshape ( x , shape ) assert_allclose ( out_2 , new_out_2 , atol=1e-05 ) from .. engine import Input to_display = [ 'Layer ( type ) ' , 'Output Shape ' , 'Param # ' ] ( which should be the same as the number of channels of the cell if pattern [ 1 ] > 0 : y += self.b_carry from tensorflow.python.eager import context return sample_weight def load_data ( label_mode='fine ' ) : without any modifications . target = targets [ i ] if targets else None def __setitem__ ( self , attr , val ) : if layer.__class__.__name__ == 'TimeDistributed ' : shape = [ None ] # Update properties . if len ( shape ) > 2 : `` `` '' Loads a model saved via ` save_model ` . def test_cudnn_rnn_basics ( ) : E.g . for use with categorical_crossentropy . tile_shape = tf.where ( shape_diff > 0 , expr_shape , zero_expr_shape ) layer = self._input_layers [ i ] # E731 do not assign a lambda expression , use a def stateful : Boolean ( default False ) . If True , the last state > > > kvar = K.variable ( value=val , dtype='float64 ' , name='example_var ' ) h_tm1_r = h_tm1 * rec_dp_mask [ 1 ] # Check that we did compute the model outputs , 'file because they are larger than % d bytes : % s ' base_shape = x.shape ValueError : if a batch size is specified and a generator/Sequence if skip_mismatch : 'data_format ' : 'channels_last ' , attr = 1 pattern = [ [ 0 , 0 ] , ' about its expected input shape , ' def get_metric_function ( metric , output_shape=None , loss_fn=None ) : except that values whose magnitude is more than ' ( named `` ' + layer.name the logical or over all dimensions . # TODO remove this if statement when Theano without self.strides = conv_utils.normalize_tuple ( strides , 2 , 'strides ' ) def __init__ ( self , axis=-1 , * * kwargs ) : new_layer = keras.layers.Conv2DTranspose ( subtracted = keras.layers.Subtract ( ) ( [ x1 , x2 ] ) if isinstance ( cell , Layer ) : label_ind = tf.boolean_mask ( label_array , dense_mask ) constants_np= [ c ] , from tensorflow.keras.activations import relu node_indices = [ ] with self.queue.mutex : input_depth = input_shape [ -1 ] import pickle spatial_axes= ( 1 , 2 , 3 ) ) raise ValueError ( `` If printing histograms , validation_data must be `` epochs=1 , # since ` Sequential ` depends on ` Model ` . y : Labels . callback.on_train_begin ( logs ) class L1L2 ( Regularizer ) : uses_learning_phase = True depending on the ` class_mode ` : invalid_keys = [ ` `` channels_first '' ` . validation_steps=validation_steps , # Everything has weight 1 by default . def prepare_loss_functions ( loss , output_names ) : # batch size matters , we currently do not handle mask explicitly = ( TP_B - TP_A ) / ( P_B - P_A ) result = C.reshape ( result , dilation=self.dilation_rate [ 0 ] ) self._reshape_required = True height and width of the 2D convolution window . sequential_like = True class Huber ( LossFunctionWrapper ) : metrics= [ keras.metrics.MeanIoU ( num_classes=2 ) ] ) 2 class MeanSquaredError ( MeanMetricWrapper ) : new_val_list = [ k.get_value ( x ) for x , k in zip ( x_list , test_backend ) ] return x.op.type == 'Placeholder ' from .load_backend import set_value train_dir , to ` abs ( x ) - log ( 2 ) ` for large ` x ` . This means that 'logcosh ' works mostly 'epsilon ' : self.epsilon , self.built = False self.totals [ k ] = v for l , o in zip ( out_labels , outs ) : def to_dense ( tensor ) : regularizer=self.recurrent_regularizer , ValueError : In case of invalid arguments . 'samples ' : num_train_samples , at beginning of the ` with ` statement . 'layer in your model . ' ) return save_wrapper def separable_conv1d ( x , depthwise_kernel , pointwise_kernel , strides=1 , def recurrent_initializer ( self ) : threshold=0. , * * kwargs ) : # note : ` None ` is the batch dimension . all_dims_padding = ( ( 0 , 0 ) , ) + padding + ( ( 0 , 0 ) , ) parallel_model = multi_gpu_model ( model , gpus=2 ) x : input variable . out_filters = input_shape [ 3 ] * self.depth_multiplier model.add ( RepeatVector ( 3 ) ) model.train_on_batch ( np.ones ( ( num_samples , timesteps ) ) , An operation to update the variable . can slow down your training . raise ValueError ( 'Found a sample_weight array for ' arguments : dictionary of keyword arguments that were passed to the m._call_result for m in metrics if hasattr ( m , '_call_result ' ) are from a 2D convolution if globs is None : while True : raise ValueError ( 'An operation has ` None ` for gradient . ' elif backend ( ) == 'tensorflow ' : 'to draw from the generator at each epoch . ' output = repeat_elements ( x , height_factor , axis=2 ) if hasattr ( value , 'dtype ' ) and value.dtype ! = dtype and len ( shape ) > 0 : inbound_tensor_index = node.tensor_indices [ i ] y_pred = K.cast ( y_pred > threshold , y_pred.dtype ) duplicate data when using multiprocessing . if has_arg ( self.layer.call , 'constants ' ) : tf_data_format = 'NDHWC ' group.attrs [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) class _ZeroPadding ( Layer ) : from keras.utils import multi_gpu_model x = permute_dimensions ( x , permute_pattern ) x_mid_dims , name='zp2d ' ) nodes = self._nodes_by_depth [ depth ] 'be expecting any data to be passed to { 0 } . '.format ( name ) ) if is_accepted ( name , member ) : self.strides , return tf.reduce_mean ( devs_squared , origin='https : //s3.amazonaws.com/keras-datasets/boston_housing.npz ' , the 3rd , 4th and 5th dimension will be padded . output_dimensions = list ( range ( len ( int_shape ( output ) ) ) ) sub_w_nodes = submodel_wrapper.get_nodes ( ) the mean absolute percentage error is 5e+08 . mask = unpack_singleton ( masks ) if shape is None and ndim is None : the number of samples in the current batch . if axes [ i ] == -1 : raise ValueError ( 'Unexpected bias dimensions % d , ' f = H5Dict ( path , mode= ' r ' ) output_tensors [ i ] ._keras_history = ( self , if isinstance ( shape_or_val , np.ndarray ) : tensor : Some tensor in a graph . raise ValueError ( ' Can not create group in read-only mode . ' ) K.ones_like ( prev_output ) , cond = k.greater_equal ( x , 0.5 ) if len ( weight_names ) > 0 : result [ 3 ] = nb_channels `` `` '' Computes the hinge loss between ` y_true ` and ` y_pred ` . # cntk 's batch axis is not in shape , save_format=save_format , K.pool3d ( x , pool_size=pool_size , padding='twice ' ) present class will then be treated as the default ` build_fn ` . grads.append ( g ) ` History ` object . output = _reduce_on_axis ( x , axis , 'reduce_sum ' ) def random ( shape ) : for i in range ( weights_rank ) : data [ x ] .values x : Numpy array of test data , kernel_size = ( args [ 2 ] , args [ 3 ] ) shape = self._output_shape ( input_shape ) class RNN ( Layer ) : next epoch . Ignored with the default value of ` None ` . # This saves time when the user is not using all functions . output = permute_dimensions ( output , ( 2 , 0 , 1 , 3 ) ) def trainable ( self , value ) : raise ValueError ( 'When passing a list of lists as ` metrics ` , ' new_shape = [ C.InferredDimension if _ is None else _ for _ in new_shape ] output_mask_int_shape = K.compute_output_shape ( input_shape ) [ : -1 ] for x , k in zip ( x_list , [ KTH , KTF ] ) : output_shape.append ( i ) _IMAGE_DATA_FORMAT = data_format if keras1_args.intersection ( kwargs.keys ( ) ) : raise ValueError ( 'The list of inputs passed to the model ' def AtrousConvolution2D ( * args , * * kwargs ) : layers.GlobalAveragePooling3D ] ] shape=self.kernel_shape , out_pad_h ) model 's target , which will be fed with the target data during class Dropout ( Layer ) : def test_model_custom_target_tensors ( ) : Numpy array with the same length as the input samples reason= '' cntk does not support it yet '' ) 'and ' + str ( len ( output_masks ) ) + ' output masks . ' ) d2 = y_shape [ a1 ] dilation_rate : An integer or tuple/list of 2 integers , specifying as ` embeddings_data ` . return 'float16 ' 'is not found in inputs . Please double ' adagrad = Adagrad [ x._uses_learning_phase for x in computed_tensors ] ) 'If your inputs are not batched , ' # cntk 's result shape is ( batch , 1 ) , while keras expect ( batch , ) retained with length 1 . classes=None , base_config = super ( LSTMCell , self ) .get_config ( ) layer_test ( layers.AveragePooling2D , # i have not idea what I 'm doing : garbage as inputs/outputs raise ValueError ( 'unexpected type { } for ` filepath ` '.format ( type ( filepath ) ) ) 'expected 1 or % d dimensions ' % ( bias_dims , dims ) ) repeat_dim = x._keras_shape [ axis ] yield ( x , y ) strides = ( 1 , 1 ) + strides score_array /= K.mean ( K.cast ( K.not_equal ( weights , 0 ) , K.floatx ( ) ) ) y_ndim = K.shape ( y_shape ) [ 0 ] cells = [ np.testing.assert_allclose ( ref_output , output , atol=1e-5 ) ids= [ 'orthogonal ' , None ( default ) if feeding from framework-native tensors assert y_train.dtype == np.dtype ( ' i ' ) , ( credentials must be available , see : # ` loss ` does not exist . while i < shape [ axis ] : `` `` '' Element-wise rounding to the closest integer . It should be a tuple of integers , e.g . ` ( 32 , 10 , 100 ) ` . kernel_size : An integer or tuple/list of a single integer , `` `` '' LeCun normal initializer . go_backwards=False , mask=None , constants=None , parallel_model = multi_gpu_model ( model , cpu_relocation=True ) and guarantees the single use of every input per epoch when return C.greater_equal ( x , y ) layer_weights_shape = K.int_shape ( layer.weights [ 0 ] ) initializer=self.recurrent_initializer , @ saving.allow_write_to_gcs return ( input_shape [ 0 ] , ) + tuple ( new_space ) + ( self.filters , ) elif inspect.isfunction ( mem ) : insert a ` Masking ` layer with ` mask_value=0. ` before the LSTM layer : def __init__ ( self , optimizer ) : y , a ` output_size ` attribute . This can be a single integer or a # Note that when using this delayed-build pattern def DISABLED_test_fit_generator ( ) : nodes = model._nodes_by_depth [ depth ] ` ( batch , channels , rows , cols ) ` activity_regularizer=activity_regularizer , reason='cntk has issues with negative number . ' ) name : Optional name string for the tensor . inputs = K.reshape ( inputs , inner_input_shape ) ConfusionMatrix.TRUE_POSITIVES : ( label_is_pos , pred_is_pos ) , layer.recurrent_kernel_f , 'and thus has no defined input shape . ' ) pattern = [ list ( p ) for p in pad_info ] > > > K.dtype ( K.placeholder ( shape= ( 2,4,5 ) , dtype='float32 ' ) ) def non_trainable_weights ( self ) : raise RuntimeError ( 'CuDNN RNNs are only available ' proba = self.predict ( x , batch_size=batch_size , verbose=verbose ) self.size [ 0 ] , self.size [ 1 ] , self.size [ 2 ] , ValueError : in case of invalid ` label_mode ` . 'config ' : obj.get_config ( ) } class MeanIoU ( BaseMeanIoU ) : height_factor : Positive integer . class_name = layer.__class__.__name__ ' Please specify ` steps ` or use the ' available_devices = [ _normalize_device_name ( name ) masks = self.compute_mask ( self.inputs , mask=None ) loss_functions = [ get_loss_function ( l ) for l in loss ] bias_regularizer=bias_regularizer , return C.less ( x , y ) If the current device scope is explicitly set , it returns a string with name = ' % s_alias ' % x.name of 32-dimensional vectors . if target_tensors is not None : if str ( id ( x ) ) in tensor_map : raise ValueError ( 'Could not interpret constraint identifier : ' for instance activity losses are conditional on the layer 's inputs . 'than expected . ' ) for i in range ( weights_rank , values_rank ) : fn : the target function to inspect . globs : dictionary of global objects . os.makedirs ( _keras_dir ) ' ` Function ` , ` Constant ` or ' K.ones_like ( states [ 0 ] ) , 'collapse of batch axis with inferred dimension . ' that gets passed to the output @ interfaces.legacy_deconv2d_support mask : A mask or list of masks . A mask can be outputs = outputs.dimshuffle ( axes ) auxiliary_axis = axis + 1 decrement = np.random.random ( ( 3 , 4 ) ) ` ( batch_size , timesteps , units ) ` . check_single_tensor_operation ( 'mean ' , ( 4 , 2 , 3 ) , raise Exception ( error_msg.format ( origin , e.errno , e.reason ) ) sudo dpkg -i openmpi_1.10-3.deb m = mean ( x , axis , keepdims=True ) def fit_generator ( model , broadcast_shape ) ( ` state_size [ 0 ] ` ) should be the same as placeholders.append ( KC.placeholder ( shape ) ) # install cntk xm = x.max ( axis=axis , keepdims=True ) output_shape : The shape of the output that this metric will be calculated stateful_metrics=self.stateful_metrics ) output_shape = config [ 'output_shape ' ] raise ValueError ( 'Layer ' + self.name + ' expects ' x , gamma , beta , reduction_axes , epsilon ) dtype=None , # This test aims to make sure that we walk the array from right to left from tensorflow.keras.layers import Conv2DTranspose 'Numpy arrays . Found : ' + str ( data ) [ :200 ] + ' ... ' ) old_layer = keras.layers.SimpleRNN ( input_shape= [ 3 , 5 ] , output_dim=2 , name='d ' ) self.optimizer = optimizers.get ( optimizer ) 'recurrent_regularizer ' : assert y._keras_shape == ( None , ) input , output : Input/output tensor ( s ) . Note that if the layer new_nested_states.append ( states ) return T.tile ( mask_t , reps , ndim=ndim ) supports_sparse = False # If the learning phase is * static * and set to inference : self.l2 = K.cast_to_floatx ( l2 ) ( eta // 3600 , ( eta % 3600 ) // 60 , eta % 60 ) ) str ( y.shape ) + ' . ' pad = ( 0 , 0 ) stateful_metrics=model.metrics_names [ 1 : ] ) ) An instance of ` Sequential ` reproducing the behavior self.padding , any file-like object implementing the method ` write ` that accepts def _check_trainable_weights_consistency ( self ) : return x class Node ( object ) : > > > val = np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) depth = 6 class _Pooling3D ( Layer ) : gcs_filepath = file_io_proxy.get_filepath ( filename='model.h5 ' ) random_shift = image.random_shift if y_ndim is None : new_layer = keras.layers.GlobalMaxPool3D ( data_format='channels_first ' , return active_next , log_p_next target , output , from_logits=from_logits , axis=axis ) return K.mean ( samples = [ K.get_value ( K.variable ( x ) ) for x in samples ] For Python 2 , checks if there is an argument with the given name . ids= [ 'model_func ' , 'model_seq ' ] ) K.variable ( y ) , metric_name = self._add_unique_metric_name ( metric_name , output_index ) ( necessary since each inbound layer might have several nodes , sparse_top_k_categorical_accuracy , name , dtype=dtype , k=k ) def __init__ ( self , name='root_mean_squared_error ' , dtype=None ) : based on the inputs , mask , and the inner layer . mode='average_exc_pad ' ) dimensions in ` x ` . if 'optimizer_weights ' in h5dict : recurrent_dropout=recurrent_dropout ) depth_keys = list ( self._nodes_by_depth.keys ( ) ) self.kernel_i = self.kernel [ : , : , : , : self.filters ] output_tensors = [ ] normalized_cropping = ( conv_utils.normalize_tuple ( cropping , 2 , 'cropping ' ) , ) the layers from where ` input_tensors ` originate . lr = float ( K.get_value ( self.model.optimizer.lr ) ) if len ( args ) < 3 : raise TypeError ( 'It is not possible at the moment to ' for i in range ( 1 , 6 ) : list_of_functions = autogen.read_page_data ( page , 'functions ' ) check_single_tensor_operation ( 'expand_dims ' , ( 4 , 3 , 2 ) , WITH_NP , axis=1 ) custom_objects=custom_objects ) ( 'pool2d ' , ( 3 , 3 , 8 , 5 ) , ( 2 , 3 ) , ( 1 , 1 ) , shape = weights [ 0 ] .shape weights=weights [ : num_weights ] , def test_multi_gpu_invalid_devices ( ) : a single tensor ( also of the same shape ) . axis = list ( axis ) moving_variance_initializer : Initializer for the moving variance . self.min_value = min_value sys.stdout.write ( '\r ' ) for g in grads : check_two_tensor_operation ( 'less ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) Metrics dict updated with unique metric names as keys . python : 3.6 'If ' + steps_name + ' is set , the ` batch_size ` must be None . ' ) broadcast_var , r '' '' '' Callback for creating simple , custom callbacks on-the-fly . verify the input assumptions of the layer model.add ( Bidirectional ( LSTM ( 10 , return_sequences=True ) , 'The model has ' + str ( len ( self.outputs ) ) len ( validation_data ) ) square = np.square training_utils.check_generator_arguments ( y , sample_weight ) workers=workers , scale : Whether to rescale image values to be within ` [ 0 , 255 ] ` . assert np.abs ( np.std ( rand ) - std ) < std * 0.015 When using a backend other than TensorFlow , TensorBoard will still work correct and can be found in the label for that entry . def state_size ( self ) : elems : Tensor super ( Adadelta , self ) .__init__ ( * * kwargs ) strides : An integer or tuple/list of single integer , automatically inferred from the name of the monitored quantity . batch_logs [ l ] = o fused_batch_norm = tf.compat.v1.nn.fused_batch_norm parallel_model.train_on_batch ( x , y ) when it is constant . decode_truth = [ np.array ( [ 1 , 0 ] ) , np.array ( [ [ 1 ] ] ) ] return loss_functions.append ( get_loss_function ( loss.get ( name , None ) ) ) 'Found : ' + str ( padding ) ) sys.stdout.write ( bar ) padding=padding , def __init__ ( self , * * kwargs ) : K.resize_volumes ( K.variable ( xval ) , 2 , 2 , 2 , raise ValueError ( 'Can not perform batch_dot on inputs ' if any ( K.is_tensor ( v ) for v in all_inputs ) : future = self.queue.get ( block=True ) x , C.variables.Parameter ) ) : x_train = np.array ( x [ : int ( len ( x ) * ( 1 - test_split ) ) ] ) x = expand_dims ( x , nones ) proba : array-like , shape ` ( n_samples , n_outputs ) ` if not hasattr ( self , '_non_trainable_weights ' ) : that adapts learning rates based on a moving window of gradient updates , created_layers = { } # References y_true , elif not model._is_graph_network : nodes_depths [ node ] = depth a list of strings specifies the epochs on which to run validation . class_mode : one of `` binary '' , `` categorical '' , `` input '' , `` multi_output '' , last_output._uses_learning_phase = True # we should return a not-None mask pool_size=2 , strides=2 , padding='valid ' , name='maxpool2d ' ) # Assuming convolution kernels ( 1D , 2D or 3D ) . tensor_shape = K.shape ( tensor ) f , self.layers , reshape=reshape ) if layer is not None : model.compile ( 'sgd ' , metrics= [ keras.metrics.LogCoshError ( ) ] ) old_lr = float ( K.get_value ( self.model.optimizer.lr ) ) ( 'W_constraint ' , 'embeddings_constraint ' ) ] , baseline=None , nested_metrics = [ ] from tensorflow.keras.layers import LSTM from keras.backend import cntk_backend as KC mean = 0 . return C.element_select ( condition , go_backwards=False , origin='https : //s3.amazonaws.com/text-datasets/imdb_word_index.json ' , x = tf.expand_dims ( x , spatial_start_dim ) is required when using this layer as the first layer in a model . if shape [ :2 ] ! = ( layer.kernel_size [ 0 ] , 1 ) or shape [ 3 ] ! = layer.filters : A list of loss objective functions . metrics_utils.ConfusionMatrix.FALSE_NEGATIVES : self.false_negatives if isinstance ( model , Sequential ) : arg_spec = inspect.getargspec ( fn ) deserialized [ key ] = convert_custom_objects ( value ) `` `` '' IMDB sentiment classification dataset . 'use_bias ' : self.use_bias , 2 * padding [ 2 ] + output_padding [ 2 ] ) elif K.is_tensor ( target_tensors ) : dilation_rate = ( dilation_rate , 1 ) input_dim = kwargs.pop ( 'input_dim ' ) return inputs , initial_state , constants strides = ( 1 , ) + strides if ndim ( mean ) > 1 : ` KerasClassifier ` or ` KerasRegressor ` . The ` __call__ ` method of the `` `` '' Creates a generator to extract data from the queue . return output , [ output ] if dev.type ( ) == 0 : if hasattr ( x , 'shape ' ) : class_weight=None , ValueError : In case of invalid arguments for ( self.false_positives / Has no effect when ` steps_per_epoch ` is not ` None ` . inputs = [ inputs [ 0 ] ] if metric not in [ 'accuracy ' , 'acc ' , 'crossentropy ' , 'ce ' ] : act * = transform_weight from . import recurrent h5py = None create the model 's weights under the scope of the CPU . return dot ( x , y ) self.embeddings_metadata = embeddings_metadata or { } ` values ` . This is ultimately returned as ` sum ` . mean : Float , mean of the normal distribution . `` `` '' Decorator used in TensorFlow 2.0 to enter the Keras graph . `` `` '' Retrieves the input shape ( s ) of a layer at a given node . return T.cos ( x ) # Bring d to the second dimension in y print ( 'Epoch % 05d : early stopping ' % ( self.stopped_epoch + 1 ) ) y = backend.dot ( inputs , w_i ) dot.add_edge ( pydot.Edge ( inbound_layer_id , layer_id ) ) dilation_rate=self.dilation_rate [ 0 ] ) y_pred : tensor ( samples , time_steps , num_categories ) containing the from .pooling import GlobalAvgPool3D if validation_steps : 'input_dim ' : self.input_dim } class SensitivitySpecificityBase ( Metric ) : Tuple ( input_shape , input_dtype ) . Both could be None if the layer The predictions are accumulated in a confusion matrix , weighted by from .convolutional_recurrent import ConvLSTM2D assert o._input_dtypes == 'float16 ' alpha_constraint=None , if alpha ! = 0 : def DISABLED_test_train_on_batch_with_class_weight ( self ) : raised if ` x ` is a generator or ` Sequence ` instance and ` batch_size ` is self.gamma_initializer = initializers.get ( gamma_initializer ) def test_rnn_cell_with_constants_layer_passing_initial_state ( ) : input_tensor._uses_learning_phase = False assert np.allclose ( log_prob_truth , log_prob_pred ) dilation_rate : an integer or tuple/list of a single integer , specifying input_dict = { } in order to capture the string summary . shape2 = list ( input_shape [ 1 ] ) `` `` '' Loads the MNIST dataset . input_shape [ 2 ] + padding [ 1 ] [ 0 ] + padding [ 1 ] [ 1 ] , self.n_classes_ = len ( self.classes_ ) layer.call ( computed_tensor , * * kwargs ) ) section `` Note on passing external constants '' below . ` [ batch_size , num_classes ] ` . check_single_tensor_operation ( 'softmax ' , ( 4 , 5 , 3 ) , WITH_NP , axis=1 ) saving.load_weights_from_hdf5_group_by_name ( ' ` sparse_categorical_crossentropy ` instead , ' assert len ( K.eval ( t ) ) == 1 if z_shape is not None : if not hasattr ( _DISABLE_TRACKING , 'value ' ) : from .convolutional import Conv3D return resnet_v2.decode_predictions ( * args , * * kwargs ) # convert negative indices . layer._inbound_nodes [ 0 ] .inbound_layers ) ) : self._set_metric_attributes ( ) self.output_dim = output_dim for x in xs ] state_and_io_size = 5 `` `` '' 2D Pooling . # Tile labels by number of thresholds # Arguments input_tensors [ i ] == origin_node.output_tensors [ tensor_indices [ i ] ] This can also be a list/tuple of integers if self.input_length is None : Every layer should expose ( if appropriate ) an ` input_spec ` attribute : def argmax ( x , axis=-1 ) : update_total_op = K.update_add ( self.total , value_sum ) if input_shape [ i ] is None : 'fallback to auto mode . ' % ( mode ) , initial_state = [ np.random.random ( ( num_samples , units ) ) losses += self._per_input_losses [ inputs_hash ] return ( name in arg_spec.args or dropout=dropout , 'Received : `` { } '' '.format ( def DISABLED_test_fit_generator_with_sample_weight ( self ) : dilation_rate : an integer or tuple/list of n integers , specifying dim2_cropping = conv_utils.normalize_tuple ( cropping [ 1 ] , 2 , if 'init ' in kwargs : for spec_dim , dim in zip ( spec.shape , x_shape ) : # sample_weight= [ sample_weight [ 1 ] , # threshold method to account for floating point imprecisions . self.output_shapes = output_shapes alpha = _to_tensor ( alpha , x.dtype.base_dtype ) shape1.pop ( axes [ 0 ] ) globs [ 'Model ' ] = models.Model losses = self._losses [ : ] sparse : Boolean , whether the placeholder should have a sparse type . 3D tensor with shape : ` ( batch , new_steps , filters ) ` targets = np.array ( [ [ i ] for i in range ( 50 ) ] ) return super ( Bidirectional , self ) .__call__ ( inputs , * * kwargs ) `` `` '' Selects ` x ` in train phase , and ` alt ` otherwise . 'check the model and inputs . ' % argument.name ) match=r ' . * has shape . * but the saved . * shape . * ' ) : is commonly smaller than the others , if the size of the dataset Nadam is RMSprop with Nesterov momentum . weights += layer.weights assert_allclose ( old_weights [ i ] [ j ] , new_weights [ j ] , atol=1e-05 ) 'You can build it manually via : ' obj = super ( Metric , cls ) .__new__ ( cls ) # Collect unconditional updates . 'You passed : y= ' + str ( y ) ) `` `` '' Computes the output shape of the layer . raise TypeError ( 'Not JSON Serializable : % s ' % ( obj , ) ) def batch_get_value ( xs ) : '2nd entry of cropping ' ) self.bias = self.add_weight ( shape= ( input_dim * self.depth_multiplier , ) , if isinstance ( x_weight , list ) and len ( x_weight ) == 1 : `` `` '' Layer that concatenates a list of inputs . Assumes that fit_function returns a list , labeled by out_labels . recurrent_kernel = np.transpose ( recurrent_kernel , history : object 'Theano is out of date . ' # slices along the repeat axis raise ValueError ( 'Invalid pool_mode : ' + str ( pool_mode ) ) Each time a layer is connected to some new input , def save_function ( h5file ) : # a few lines above . output_shapes , # python 2 has 'next ' , 3 has '__next__ ' 2D tensor with shape : ` ( batch_size , sequence_length ) ` . Epoch at which to start training ` constants ` are lists of tensors ( or None ) . if on_epoch_end is not None : raise ValueError ( 'The channel dimension of the inputs to ' x = x ( ) if not isinstance ( axis , list ) : return np.prod ( self.shape ) self._per_output_weighted_metrics = ( ` ( batch , upsampled_rows , upsampled_cols , channels ) ` assert_allclose ( x.sum ( axis=1 ) , kx , atol=1e-05 ) elif validation_split and 0 . < validation_split < 1. : ' 0 or 1 . ' ) 'related to any parameters in the model . ' y = K.dot ( x , self.W_carry ) K.set_floatx ( dtype ) strides , data_format ) : conda info -a raise ValueError ( ' ` multi_gpu_model ` is only available ' np.random.seed ( 1337 ) # Add any potential unconditional model-level loss . def predict ( self , x , * * kwargs ) : 5D tensor with shape : # model.add_metric ( mean ( values ) , name='mean ' ) normalized_dtype_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , 'kernel_regularizer ' : regularizers.serialize ( self.kernel_regularizer ) , `` `` '' MinMaxNorm weight constraint . x_shape = None out_pad_h , out_pad_w = self.output_padding from tensorflow.keras.layers import AlphaDropout self.input_spec = [ InputSpec ( ndim=5 ) ] if ( x_shape is not None ) and ( x_shape [ 0 ] is not None ) : name='b_carry ' ) fused_batch_norm = tf.compat.v1.nn.fused_batch_norm y_rev = K.reverse ( y_rev , 1 ) gpus : Integer > = 2 or list of integers , number of GPUs or node_index : The layer 's position ( e.g . via enumerate ) in a list of normalized_rs_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , restore_best_weights=False ) : noise_shape : A 1-D ` Tensor ` of type ` int32 ` , representing the def sigmoid ( x ) : self.kernel_shape = kernel_shape if isinstance ( value , ( theano.tensor.TensorVariable , dynamic_batch_size = True the logical and over all dimensions . self.recurrent_kernel_f ) ) raise TypeError ( 'The ` mode ` argument of ` BatchNormalization ` ' for _ in shape : generator , `` `` '' Layers that augment the functionality of a base layer . from .callbacks import BaseLogger horizontal_flip=True , `` `` '' Computes the categorical hinge loss between ` y_true ` and ` y_pred ` . on_batch_begin=lambda batch , logs : print ( batch ) ) hook_name = 'on_ { mode } _batch_ { hook } '.format ( mode=mode , hook=hook ) layer , node_index , _ = tensor._keras_history for layer_data in config [ 'output_layers ' ] : `` `` '' Decorator to wrap metric ` result ( ) ` with identity op . if has_arg ( cls.from_config , 'custom_objects ' ) : ValueError : If ` identifier ` can not be interpreted . `` `` '' Element-wise value clipping . name='W_carry ' ) [ 0. , 0. , 1 . ] ] , dtype=float32 ) if dev.type ( ) == 0 and dilation_rate ! = 1 : each entry describes one required input : if target_class == 'CuDNNGRU ' : save_prefix : Str . Prefix to use for filenames of saved pictures seed=None , `` `` '' Layers that augment the functionality of a base layer . '' '' '' @ six.wraps ( func ) u_t = K.maximum ( self.beta_2 * u , K.abs ( g ) ) `` `` '' Utility class for generating batches of temporal data . dot.set ( 'labeljust ' , ' l ' ) self.maxval = maxval if any ( K.is_tensor ( v ) for v in all_inputs ) : x_h = matrix_x [ : , 2 * self.units : ] if id ( x ) not in unique_tensors_ids : 'input_length ' : self.input_length } return T.zeros_like ( x , dtype=dtype ) placeholder = K.placeholder ( shape=shape , name=name ) return resnet.ResNet152 ( * args , * * kwargs ) cache_dir = os.path.join ( os.path.expanduser ( '~ ' ) , '.keras ' ) recurrent_initializer=recurrent_initializer , input when this layer is the first one in a model . updated_per_output_weighted_metrics.append ( def _step ( inputs , * states ) : axis : Integer , the axis that should be normalized . return self.gain * K.eye ( ( shape [ 0 ] , shape [ 1 ] ) , dtype=dtype ) if flag : if n in conflict_counter : return isinstance ( tensor , tf.SparseTensor ) original_keras_version , # ( samples , timesteps , rows , cols , filters ) use_bias=True , ` data [ i ] ` , ` data [ i-r ] ` , ... ` data [ i - length ] ` time_index = time_index [ : :-1 ] 'size of your Input Layer : { } ' input_tensors.append ( input_tensor ) border_mode='valid ' , h_tm1 = states [ 0 ] # previous memory If unspecified , ` max_queue_size ` will default to 10 . for real valued inputs . super ( Cropping2D , self ) .__init__ ( normalized_cropping , `` `` '' Pooling layers . '' '' '' x += reshape ( bias , ( 1 , ) + bias_shape ) tensor_indices=tensor_indices , ` call ` method of the layer at the call that created the node . def test_ctc_decode_greedy ( self ) : states = [ T.squeeze ( state [ -1 ] ) for state in states ] _UID_PREFIXES = defaultdict ( int ) assert rand.shape == ( 200 , 200 ) inputs : Tensor of temporal data of shape ( samples , time , ... ) self._make_test_function ( ) # Check that all tensors required are computable . ` sk_params ` takes both model parameters and fitting parameters . Legal model 'not a list . Maybe you meant to use ' if 'acc ' in self.monitor : specify ` stateful=True ` in the layer constructor . def stack ( x , axis=0 ) : for cell in self.cells : custom_objects=custom_objects ) if self.stopped_epoch > 0 and self.verbose > 0 : name='global_maxpool2d ' ) specified the file will be saved at that location . weights : List of weights values ( Numpy arrays ) . ` ( symmetric_dim1_pad , symmetric_dim2_pad , symmetric_dim3_pad ) ` . if not extension : for ndims in [ 1 , 2 , 3 ] : from .load_backend import pool2d fname : Name of the file . If an absolute path ` /path/to/file.txt ` is return resnet50.ResNet50 ( * args , * * kwargs ) class SparseCategoricalCrossentropy ( LossFunctionWrapper ) : If None is passed , the updates are assumed unconditional . requests.post ( self.root + self.path , `` `` '' Flatten a tensor . class _GlobalPooling1D ( Layer ) : weight = K.placeholder ( outputlabels ) 'If your data is in the form of symbolic tensors , ' self.outputs = to_list ( outputs , allow_tuple=True ) xs.append ( reshape ( inputs [ : , slice_length , : ] , for matrix_cond , ( label , pred ) in loop_vars.items ( ) : `` `` '' Returns the metric function corresponding to the given metric input . dimension 2 of ` y ` has been summed over . ( ` dot_axes [ 1 ] ` = 2 ) assert_value_equality=False ) > > > b = K.placeholder ( ( 2 , 2 ) , sparse=True ) def member_too_small ( member ) : self.placeholders = inputs def __init__ ( self , cropping= ( 1 , 1 ) , * * kwargs ) : eta_format = ' % d : % 02d ' % ( eta // 60 , eta % 60 ) if static_batch_size is not None : def conv1d ( x , kernel , strides=1 , padding='valid ' , from .load_backend import stop_gradient values to determine the truth value of predictions ( i.e. , above the If both are specified , ` shape ` is used . target = K.placeholder ( Legal arguments are the arguments dot.set ( 'dpi ' , dpi ) signature = autogen.get_class_signature ( cls ) None ) ) self.data [ '_is_group ' ] = True super ( ConvLSTM2D , self ) .__init__ ( cell , preds2 = model2.predict ( np.random.random ( ( 1 , 32 ) ) ) provide an ` input_shape ` argument ( tuple of integers or ` None ` , does not ValueError : In case of invalid layer name or index . old_layer = keras.layers.Cropping3D ( dim_ordering='tf ' , name='c3d ' ) h5_file_args = { 'backing_store ' : False , for rep_axis in range ( ndims ) : # not supported learning_phase p1 = K.eval ( K.foldl ( lambda a , b : a * b , vx ) ) a tuple ` ( inputs , targets , sample_weights ) ` . kernel_size : An integer or tuple/list of n integers , specifying the 'Received : { } . '.format ( type ( path ) ) ) on_train_end=lambda logs : [ custom_opt = optimizers.rmsprop from keras.applications import Xception if threshold ! = 0 : mode it will be reduced when the quantity `` `` '' Creates a layer from its config . # Assume list/iterable weights = [ weights [ 0 ] ] + [ np.array ( 1 . ) ] + weights [ 1 : ] inputs , recurrent_h = r * matrix_inner [ : , 2 * self.units : ] ( optionally ) returns any object . raise ValueError ( 'Operands could not be broadcast ' class tf_file_io_proxy ( object ) : new_model.train_on_batch ( x , y ) for data_format in [ 'channels_first ' , 'channels_last ' ] : def all ( cls ) : f = self.recurrent_activation ( x_f + K.dot ( h_tm1_f , member.__module__ ) def test_batchnorm_cntk ( self , x_shape ) : training=training ) if not isinstance ( validation_freq , collections.Container ) : > > > keras.backend.get_uid ( 'dense ' ) batch_dot ( x , y , axes=1 ) = [ [ 17 , 53 ] ] which is the main diagonal dense_mask = functional_ops.scan ( range_less_than , label_lengths , y = K.permute_dimensions ( y , ( 1 , 0 ) ) def save_weights_to_hdf5_group ( group , layers ) : output_shape [ 3 ] ) def padding ( self ) : if hasattr ( instance , '__name__ ' ) : sample_weight=tuple ( sample_weight ) ) # skip transitions inner_inputs = self._input_map [ input_uid ] self.bias = self.add_weight ( shape= ( output_row , output_col , self.filters ) , This callback is automatically applied to if unknown_output : # NOTE : need to flatten , since slicing in CNTK gives 2D array b_skip_idxs = ctc_create_skip_idxs ( Y [ : :-1 ] ) unrelated_updates.append ( u ) x_rep = [ s for s in splits for _ in range ( rep ) ] kernel = _preprocess_conv2d_kernel ( kernel , data_format ) reset_after : GRU convention ( whether to apply reset gate after or globs = globals ( ) # All layers . available_devices ) ) with name_scope ( `` ) : keras_shape [ j ] , elif len ( args ) == 4 : class SpecificityAtSensitivity ( SensitivitySpecificityBase ) : output = _reduce_on_axis ( x , axis , 'reduce_mean ' ) self.pool_size = conv_utils.normalize_tuple ( pool_size , 2 , 'pool_size ' ) return self.accumulator raise ValueError ( 'The shape of the input to `` Flatten '' ' warnings.warn ( 'Learning Rate Plateau Reducing mode % s is unknown , ' loss = reduce_weighted_loss ( weighted_losses , reduction ) `` `` '' Computes the crossentropy metric between the labels and predictions . timesteps = 6 assert len ( inner_model.weights ) == 0 data_format='channels_middle ' ) return K.repeat ( inputs , self.n ) 'the notion of `` input shape '' is ' x_i = self.input_conv ( inputs_i , self.kernel_i , self.bias_i , 'seed ' : self.seed , from the name of the monitored quantity . raise ValueError ( ' ` class_weight ` not supported for ' return isinstance ( x , C.variables.Parameter ) elif hasattr ( cropping , '__len__ ' ) : data_format=data_format , dilation_rate=dilation_rate ) signature = autogen.get_function_signature ( function_ ) 'kernel_constraint ' : constraints.serialize ( self.kernel_constraint ) , return super ( RNN , self ) .get_losses_for ( inputs=inputs ) # out = model.train_on_batch ( [ input_a_np , input_b_np ] , `` `` '' Loads the Reuters newswire classification dataset . assert 'group4 ' in group3 f = theano.function ( [ ] , x.shape , profile=False ) return metrics_module.get ( metric ) ValueError : In case of mismatch between the provided source = 'GRU ( reset_after=True ) ' min_ndim : Integer , minimum rank of the input . attr_name : Human-readable attribute name , for error messages . # Bidirectional and stateful Code within a ` with ` statement will be able to access custom objects ` ( batch , padded_rows , padded_cols , channels ) ` ( 'conv2d ' , ( 1 , 7 , 6 , 3 ) , ( 3 , 3 , 3 , 4 ) , 'same ' , 'channels_last ' ) , set to ` True ` . origin='https : //s3.amazonaws.com/text-datasets/imdb.npz ' , raw_code = code.encode ( 'raw_unicode_escape ' ) skip_target_weighing_indices ) : return K.hard_sigmoid ( x ) padding=self.padding , self.kernel_r , elif isinstance ( target_tensors , dict ) : if mask.ndim ! = 2 : self.saver = tf.train.Saver ( list ( embeddings_vars.values ( ) ) ) self.kernel = self.add_weight ( states = states [ len ( cell.state_size ) : ] if not self.callbacks : If unspecified , ` batch_size ` will default to 32 . assert np.alltrue ( decode_pred_np == decode_pred ) if training in { 0 , False } : if len ( generator_output ) == 2 : for value in obj : threshold values in [ 0 , 1 ] . A threshold is compared with prediction if ndim ( mean ) == ndim ( x ) and shape ( mean ) [ 0 ] == 1 : input_filter = input_shape [ 3 ] super ( Lambda , self ) .__init__ ( * * kwargs ) variance * = sample_size / ( sample_size - ( 1.0 + self.epsilon ) ) 'on a list of at least 2 inputs ' ) dialect=CustomDialect ) class Adadelta ( Optimizer ) : x = x + reshape ( bias , ( 1 , bias_shape [ 0 ] , 1 , 1 ) ) the initial state of the RNN layer . new_layer = keras.layers.Conv3D ( 5 , ( 3 , 3 , 4 ) , name='conv ' ) `` `` '' Calls ` save_function ` on an in memory ` h5py.File ` . if mode == 'rb ' : `` `` '' Generates class probability predictions for the input samples . def DISABLED_test_convert_weights ( ) : from .. import constraints alpha = 1.6732632423543772848170429916717 Repeats each temporal step ` size ` times along the time axis . ' and multiple workers may duplicate your data . ' entries = backend_module.__dict__ it is neither an Integer nor a Sequence . batches of image data , which is useful to use in ` input_tensors ` and turns them into ` output_tensors ` x_rep.set_shape ( x_shape ) result = T.set_subtensor ( output [ : , padding [ 0 ] : x.shape [ 1 ] + padding [ 0 ] , : ] , x ) y_dset = f.create_dataset ( 'my_labels ' , ( 200 , 1 ) , dtype= ' i ' ) if update_tn : class RandomUniform ( Initializer ) : from keras.models import Sequential `` `` '' A dict-like wrapper around h5py groups ( or dicts ) . warnings.warn ( ' ` transform_bias ` argument is deprecated and ' if filepath.startswith ( self._gcs_prefix ) : the given ` y_col ` column with class values as strings . interpolation : Interpolation method used to self.root = root assert 'group2 ' in group1 global NAME_SCOPE_STACK from tensorflow.keras.applications.vgg19 import VGG19 alpha_regularizer : regularizer for the weights . def test_serialization ( ) : # All layers in order of horizontal graph traversal . `` `` '' Returns a list of batch indices ( tuples of indices ) . smoothed , meaning the confidence on label values are relaxed . expand_nested=False , bias_constraint=None , masks.append ( K.expand_dims ( mask_i ) ) self.on_batch_end = on_batch_end cropping : int or tuple of int ( length 2 ) for item in ( self [ i ] for i in range ( len ( self ) ) ) : The multiplicative noise will have value=1.0 , def _has_nchw_support ( ) : def moving_average_update ( x , value , momentum ) : def test_resize_images ( self ) : width_padding = conv_utils.normalize_tuple ( padding [ 1 ] , 2 , x2 = Dense ( 32 ) ( input_2 ) resnet = None batch_sizes = [ ] `` `` '' Locally-connected layer for 1D inputs . where there is a mismatch in the number of weights , assert ( normalized_rs_X_train.shape [ 1 ] == 5 ) updated.append ( v ) # # the rank of output arrays should be at least 3D . then_expression : either a tensor , or a callable that returns a tensor . layer_map [ layer ] = new_layer # Note : the shape is intentionally different from CuDNNGRU biases assert json.dumps ( old_layer.get_config ( ) ) == json.dumps ( new_layer_1.get_config ( ) ) t = K.arange ( start ) 'or calling fit ( ) with some data . ' if value is None : def test_Bidirectional_with_constants_layer_passing_initial_state ( ) : pattern : A tuple of from .load_backend import softsign output_b = keras.layers.Dense ( output_dim_b ) ( c ) shape : Shape of the placeholder check_single_tensor_operation ( 'sin ' , ( 4 , 2 ) , WITH_NP ) padding ] , if cond_ndim > expr_ndim : input_dim_b = 5 from .load_backend import softmax if steps is not None and batch_size is not None : reference : A tensor . print_row ( fields , positions ) # Case 1 : generator-like . Input is Python generator , 'from keras.utils import to_categorical\n ' This layer supports masking for input data with a variable number stride_w , kernel_w , assert not out2._uses_learning_phase name : A string name for the foldl node in the graph use_multiprocessing=use_multiprocessing ) size : integer . Upsampling factor . if layer.supports_masking : len_dim2 = conv_utils.conv_output_length ( len_dim2 , self.pool_size [ 1 ] , 'hence the notion of `` layer input '' ' name= ' W ' , layer.bias_c , 'center ' : self.center , model.add_loss ( 1 ) output_k = K.eval ( output_k ) class_sample_weight = None 'For multi-output layers , ' `` `` '' Sets the weights of the optimizer , from Numpy arrays . download = True def get_initial_state ( self , inputs ) : `` `` '' A ` Network ` is way to compose layers : the topological form of a ` Model ` . kernel_regularizer : Regularizer function applied to json_string : JSON string encoding a model configuration . num_dynamic_axis = _get_dynamic_axis_num ( x ) String , the name of the backend Keras is currently using . * * kwargs : Additional keyword arguments to be passed to ` call ( ) ` . value_sum = K.sum ( values ) def test_model_with_partial_loss ( ) : if isinstance ( v , ( np.ndarray , np.generic ) ) : Computation is done in batches . 'W_constraint ' : constraints.serialize ( self.W_constraint ) , return T.nnet.bn.batch_normalization ( x , gamma , beta , mean , sqrt ( var + epsilon ) , x._keras_shape = ( None , dim ) self.W_carry = self.add_weight ( shape= ( input_dim , input_dim ) , from_config x = switch ( training , x , alt ) reshape : Reshape weights to fit the layer when the correct number padding=padding , data_format=data_format ) mask=None , arguments=None , * * kwargs ) : # It does not affect users of the underlying layers , only users of the elems , initializer , name=name ) [ 0 ] def layers ( self ) : # Returns for input in ... : classes=classes , or 5D tensor with shape : 'loss function identifier : ' , identifier ) return ( self.uses_learning_phase and from tensorflow.keras.applications.resnet import ResNet152 # ( samples , rows , cols , filters ) from .. utils.io_utils import save_to_binary_h5py along with the images . `` `` '' Dummy decorator used in TensorFlow 2.0 to enter the Keras graph . '' '' '' input_masks=input_masks , super ( LocallyConnected2D , self ) .__init__ ( * * kwargs ) dummy_x_1d = K.variable ( np.ones ( ( 4 , 8 , 2 ) ) ) _axis.append ( _ if _ > = 0 else _ + len ( shape ) ) new_layer = keras.layers.ZeroPadding2D ( ( ( 1 , 2 ) , ( 3 , 4 ) ) , assert ( len ( result ) == 2 ) def test_model_saving_to_binary_stream ( ) : y = self.forward_layer.call ( inputs , data_format=self.data_format ) ` on_epoch_end ` . The method ` __getitem__ ` should return a complete batch . return x / np.expand_dims ( l2 , axis ) if getattr ( cell , 'output_size ' , None ) is not None : return Conv1D ( * args , * * kwargs ) constraint=None ) : data_format=data_format , size : Tuple of ints . return self.cell.reset_after assert bce_obj.reduction == Reduction.SUM data_format='channels_last ' , if not self._is_graph_network : class _GlobalPooling2D ( Layer ) : if threshold ! = 0.5 : # These properties will be set upon call of self.build ( ) output._keras_shape [ axis_1 ] * = height_factor ( i.e. , values in [ 0 , 1 ] ) . cntk_axis.append ( x.dynamic_axes [ dynamic_axis_index ] ) # Restore old value # replace all None in int_shape by K.shape padding : One of ` `` valid '' ` or ` `` same '' ` ( case-insensitive ) . def DISABLED_test_on_epoch_end_threads_sequence_change_length ( ) : return [ update_total_op , K.update_add ( self.count , num_values ) ] 'Expected to see ' + str ( len ( names ) ) + ' array ( s ) , ' archive_format='auto ' , fi class_weight=class_weight ) factor : factor by which the learning rate will return tf.equal ( x , y ) super ( SpatialDropout2D , self ) .__init__ ( rate , * * kwargs ) state_updates = [ ] ( Theano , TensorFlow or CNTK ) , which we augment with certain return False return scale * np.random.randn ( * shape ) .astype ( dtype ) + mean y = outputs def get_input_mask_at ( self , node_index ) : y = x.dimshuffle ( pattern ) stop = self.shape [ 0 ] If the model is not defined under any preceding device file_io_proxy.delete_file ( gcs_filepath ) # cleanup def DISABLED_test_model_saving_to_binary_stream ( ) : 'name ' : layer.name , # ( e.g . weight regularizers ) . model.compile ( 'sgd ' , metrics= [ keras.metrics.CategoricalHinge ( ) ] ) if issparse ( fit_inputs [ i ] ) and not K.is_sparse ( feed [ i ] ) : base_config = super ( Precision , self ) .get_config ( ) return K.in_train_phase ( dropped_inputs , inputs , y_true : The ground truth values . # of the deepest model to infer input shape and dtype . raise RuntimeError ( 'You must compile your model before using it . ' ) `` `` '' Validates arguments passed when using a generator . '' '' '' ` f ( x ) = x for x > = 0 ` , for step_index in range ( steps_per_epoch ) : loss_weights = training_config [ 'loss_weights ' ] ( str ( ishape ) for ishape in layer.input_shapes ) ) name : String , name of returned Keras variable . return ( num_samples , ) + tuple ( self._output_shape ) if 'implementation ' in config and config [ 'implementation ' ] == 0 : pool_size = ( 1 , ) + pool_size + ( 1 , ) batch_outs = to_list ( batch_outs ) ' ( same as input shape ) . ' `` ` args , kwargs , _converted = conv2d_args_preprocessor ( args , kwargs ) def resize_volumes ( x , depth_factor , height_factor , width_factor , data_format ) : > > > keras_placeholder = K.placeholder ( shape= ( 2 , 4 , 5 ) ) origin=base + fname , See [ this script ] ( legal_params_fns.append ( self.build_fn.__call__ ) return K.permute_dimensions ( inputs , ( 0 , ) + self.dims ) # Prepare input arrays and training function . z = K.dropout ( K.variable ( val ) , level=-0.5 ) the same value will be used for both . result = tf.reshape ( result , output_shape ) legacy_separable_conv2d_support = generate_legacy_interface ( shapes ) , in which case requesting ` input_shape ` will raise # # ` loss ` does not exist . # Save the outputs for merging back together later . 'list elements should be `` int '' . ' ) name='us2d ' ) _is_path_instance ( path ) layer_weights = model_weights_group [ name ] self.kernel_size [ 0 ] , custom_objects=dict ( list ( _GLOBAL_CUSTOM_OBJECTS.items ( ) ) `` `` '' Sets the value of the fuzz factor used in numeric expressions . from keras.utils.data_utils import validate_file # Attempt to create an image of a blank graph if hasattr ( x , '_keras_history ' ) : f = K.function ( inputs= [ x_placeholder ] , outputs= [ x_identity ] ) input only . Maximum size for the generator queue . Tuple : `` `` '' Calculates how often predictions matches labels . ' ` padding= ( ( top_pad , bottom_pad ) , ( left_pad , right_pad ) ) ` ' , def DISABLED_test_ReduceLROnPlateau_backwards_compatibility ( ) : assert isinstance ( node.input_masks , list ) input_spec = to_list ( input_spec ) 'to shape ` % s ` , but input shape is ` % s ` . Currently ' def losses ( self ) : self.input_spec = InputSpec ( shape=input_shape ) recurrent_constraint : Constraint function applied to W_regularizer='l1 ' , self.input_spec = InputSpec ( ndim=5 , axes= { channel_axis : input_dim } ) b = -a * alpha_p * rate padding=padding , if self.unrelated_updates is not None : if depth not in nodes_by_depth : layer = cls ( cell , * * config ) ` Specificity ` measures the proportion of actual negatives that are correctly if self._initial_weights is not None : def test_preprocess_weights_for_loading_cudnn_rnn_should_be_idempotent ( layer_class , inputs = K.variable ( inputs ) kernel_initializer : Initializer for the ` kernel ` weights matrix ' or a ` batch_shape ` argument . Note that ' def predict_loop ( model , f , ins , warnings.warn ( 'The TensorBoard callback does not support ' d = d_decoded class GaussianDropout ( Layer ) : bool , whether ` fn ` accepts a ` name ` keyword argument . output_shape = self._compute_elemwise_op_output_shape ( output_shape , def zeros ( shape , dtype=floatx ( ) , name=None ) : ( it will be wrapped as a Keras Optimizer ) . self._handle_per_output_metrics ( if self.queue.unfinished_tasks == 0 or self.stop_signal.is_set ( ) : top_k : ( Optional ) Unset by default . An int value specifying the top-k WITH_NP , cntk_two_dynamicity=True ) strides= ( 1 , 1 , 1 ) , > > > keras.backend.backend ( ) is a generator . Total number of steps ( batches of samples ) K.greater ( p [ 1 : ] , 0 ) , `` input '' , or None . Default : `` categorical '' . def _step ( inputs , mask , output_tm1 , * states ) : batch_input_shape = ( batch_size , ) + tuple ( input_shape ) thresholds : ( Optional ) A list of floating point values to use as the line_length = line_length or 98 if loss_weights is None : py_slice ( padding [ 0 ] [ 0 ] , input_shape [ 2 ] + padding [ 0 ] [ 0 ] ) , super ( PReLU , self ) .__init__ ( * * kwargs ) y = np.expand_dims ( x , 1 ) # Creating dataset to store features y_expanded = False overwrite : Whether we should overwrite any existing `` `` '' Check if the current device is explicitly set on the device type specified . def MobileNet ( * args , * * kwargs ) : if 'type ' in arg_dict and arg_dict [ 'type ' ] == 'ndarray ' : shuffle=False , inputs_r = inputs ( ( 2 , 5 ) , ( 0 , 1 ) , ( 2 , 3 ) ) , return self._merge_function ( reshaped_inputs ) from tensorflow.keras.layers import GlobalMaxPooling1D new_layer_2 = keras.layers.SpatialDropout2D ( 0.5 , model.compile ( 'sgd ' , loss=keras.losses.Poisson ( ) ) stateful_metrics=None ) : > > > keras.backend.epsilon ( ) base_config = super ( Adagrad , self ) .get_config ( ) https : //www.tensorflow.org/guide/embedding ) . return_sequences : Boolean . Whether to return the last output . check_single_tensor_operation ( 'zeros_like ' , ( 3 , 5 , 10 , 8 ) , WITH_NP ) K.variable ( x ) , # try to cast to float to run the model 'mismatch in shape ( { } vs { } ) . '.format ( the * weighted sum * of all individual losses , weighted by the additional_inputs = [ ] def _collect_previous_mask ( input_tensors ) : weight_names = _uniquify ( weight_names ) variables_to_update [ matrix_cond ] ) ) A tuple ` ( nodes , nodes_by_depth , layers , layers_by_depth ) ` . def _get_existing_metric ( self , name=None ) : raise ValueError ( self.end_of_epoch_signal.wait ( timeout=30 ) if expand_nested and isinstance ( layer , Model ) : among the labels of a batch entry is in the top-k predictions . input_tensors = _input_tensors filter_shape = ( filter_shape [ 3 ] , filter_shape [ 2 ] , 1e-05 self._is_graph_network = True return tf_file_io.FileIO ( name , mode ) 'default ' : None } } ) str ( get_tuple_shape ( dim ) ) * including the batch size * . assert_input_compatibility ( ) [ 1. , 0. , 0 . ] ] , dtype=float32 ) # The model owns this layer node . top_pad = kwargs [ 'padding ' ] .get ( 'top_pad ' , 0 ) if inputs is None : x = np.pad ( x , [ ( 0 , 0 ) , ( 0 , 0 ) ] + [ ( 0 , 1 ) for _ in pool_size ] , return y_pred , y_true , sample_weight `` `` '' Pooling layers . losses.cosine_similarity , with K.name_scope ( self.__class__.__name__ ) : return load_wrapper user_kwargs = kwargs.copy ( ) # # You have just found Keras . In other words , you could use ` grid_search ` to search for the best if has_raise and `` # Raises '' not in doc : from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 return mask_ _convert_model_weights ( cudnn_model , model ) seed=self.seed ) stride=self.strides [ 1 ] , nodes_in_progress.remove ( node ) 'bias_regularizer ' : regularizers.serialize ( self.bias_regularizer ) , _SEQUENCE_COUNTER += 1 for s , p in zip ( states , place_holders ) : ' has multiple inbound nodes , ' shapes : Optional list of expected array shapes . dtype = K.floatx ( ) to be passed to ` json.dumps ( ) ` . This checks that the tensor ( s ) ` input ` self.recurrent_kernel_f , output_mask = mask output.append ( dot ( x_flatten , `` `` '' Apply 2D conv with un-shared weights . output_ls_copy = [ ] value.dtype ! = np.float64 ) : shape= ( num_thresholds , ) , if self.input_dim : values = getattr ( self._inbound_nodes [ node_index ] , attr ) 'recurrent_constraint ' : constraints.serialize ( self.recurrent_constraint ) , When using the TensorFlow backend , for different applications . # tf.where needs its condition tensor f = K.function ( inputs= [ x_placeholder ] , # yields inputs ( not targets and sample weights ) . if has_arg ( fn , name ) : steps_per_epoch=10000 , epochs=10 ) # or Sequence object , or iterator . def convert_nested_time_distributed ( weights ) : def _init_subclassed_network ( self , name=None , * * kwargs ) : output channels . The ` depth_multiplier ` argument controls how many sensitivity , num_thresholds=num_thresholds , name=name , dtype=dtype ) representing input for the batch of samples at a certain optimizer : String ( name of optimizer ) or optimizer instance . slice_arrays ( sample_weights , split_at ) ) if ( ( algorithm == 'sha256 ' ) or training_utils.check_generator_arguments ( `` `` '' Sets sample weight related attributes on the model . '' '' '' the generator as they ca n't be passed easily to children processes . y_pred = ops.convert_to_tensor ( y_pred ) 'alpha_regularizer ' : regularizers.serialize ( self.alpha_regularizer ) , Precision slope = dTP / dP # only convert between different types _LEARNING_PHASE = -1 padding = _preprocess_padding ( padding ) 'float16 . ' % dtype ) output_shape = tf.concat ( [ output_shape [ : -1 ] , y_trail_dims ] , 0 ) true_negatives : y_true == False and y_pred < = thresholds old_layer = keras.layers.MaxPool1D ( 2 , padding='valid ' , name='maxpool1d ' ) `` `` '' Decorated function with ` add_update ( ) ` . '' '' '' raise TypeError ( ' ` model_from_config ` expects a dictionary , ' cudnn_model = _make_nested_model ( input_shape , cudnn_layer , layer , node_index , tensor_index = x._keras_history from .load_backend import eye 'metrics ' : model._compile_metrics , for s in shapes : `` `` '' Layers that can merge several inputs into one . '' '' '' the output will have shape ` ( samples , 2 , dim ) ` . return sample_weight kshape.pop ( axis ) if isinstance ( a , C.Axis ) : if ( isinstance ( curve , metrics_utils.AUCCurve ) and inputs = layers.Input ( ( 3 , ) ) cols = conv_utils.conv_output_length ( cols , self.pool_size [ 1 ] , true_fn=lambda : tf.constant ( 0 , dtype=start.dtype ) , # Keep track of metric function . backend_list ) : with K.name_scope ( 'activity_regularizer ' ) : unroll=unroll , metric_name = ' % s_ % d ' % ( base_metric_name , j ) self.moving_mean = self.add_weight ( decode_func = K.function ( [ input_prob_tensor , input_len_tensor ] , `` `` '' Abstract class for different global pooling 1D layers . def recurrent_conv ( self , x , w ) : `` `` '' Glorot uniform initializer , also called Xavier uniform initializer . first_layer = layer.layers [ 0 ] # Note on specifying the initial state of RNNs def test_in_train_phase ( self , training ) : training_config = json.loads ( training_config.decode ( 'utf-8 ' ) ) raise TypeError ( 'Unrecognized keyword arguments : ' + str ( kwargs ) ) mask = mask [ 0 ] # add a x - > x^2 layer if x_ndim is None : for j in range ( w.shape [ 1 ] ) : def input_shape ( self ) : output_dim_a = 1 `` `` '' Minimum value in a tensor . self._call_batch_hook ( _TEST , 'begin ' , batch , logs=logs ) self.padding , self.strides [ 2 ] ) if alpha == 1 : weight_values = [ layer_weights [ weight_name ] for weight_name in weight_names ] assert len ( input_shape ) > = 3 specifying the learning phase . return T.sgn ( x ) first_log = K.log ( K.clip ( y_pred , K.epsilon ( ) , None ) + 1 . ) # This acts just like the ` trainable ` attribute of any layer instance . def metrics_names ( self ) : def to_list ( x ) : initializers.serialize ( self.moving_variance_initializer ) , return False 'which has { } dimensions . '.format ( len ( output.shape ) ) ) ) super ( SGD , self ) .__init__ ( * * kwargs ) t = K.cast ( self.iterations , K.floatx ( ) ) + 1 loss_fns : a list of the loss functions corresponding to the model outputs . bias_initializer=bias_initializer , rows = input_shape [ 1 ] if losses is None : input_shapes.append ( input_shape ) 'at least one item . ' ) h = K.dot ( inputs , self.kernel ) ( E.g. , ` mask ` is not used at all ) value = kwargs.pop ( 'forget_bias_init ' ) add_unprocessed_node ( layer , node_data ) `` `` '' Called at the end of a training batch in ` fit ` methods . self.loss = outputs [ 0 ] Dictionary : configuration dictionary . parallel_model.compile ( loss='categorical_crossentropy ' , values : Per-example value . from the static batch size of the InputLayer . Lastly , ValueError will be ( a mask can be a tensor , or None ) . rescaled to yield 'count ' , initializer='zeros ' ) the axes to compute the standard deviation . If ` None ` ( default ) , but that may change in the future . # need to be fixed in GA . check_two_tensor_operation ( 'not_equal ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) raise TypeError ( 'Layer can receive at most 4 positional arguments . ' ) from .. models import clone_model expand_nested : whether to expand nested models into clusters . if ( dtype ( x ) == 'float64 ' and eta = time_per_unit * ( self.target - current ) inputs=y_pred , # e.g . when loading model from file placeholders will be created . `` `` '' Element-wise absolute value . def assert_blank_before ( name , member , doc , keywords ) : import json the ` embeddings ` matrix output1 = layers.Dense ( 1 , name='output1 ' ) ( x ) def __init__ ( self , layer , * * kwargs ) : updates = self.layer.get_updates_for ( inner_inputs ) super ( TensorBoard , self ) .set_model ( model ) inputlabels = ' , '.join ( output_shape=output_shape , padding=padding , data_format=data_format , ' returned by a Keras layer , or by ` Input ` ) ' ) while i > = 0 : on_batch_end=None , ' outputs , but you passed loss_weights= ' A mask tensor output_shape : Expected output shape from function . ( 1 , -1 , feature_dim ) ) ) super ( SeparableConv1D , self ) .__init__ ( mock_fio.read = self.local_objects [ filepath ] .read y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) self.on_batch_end ( batch , logs=logs ) updates=self.state_updates , class Conv2DTranspose ( Conv2D ) : variable._uses_learning_phase = False Build from x._keras_shape implementation : Implementation mode , either 1 or 2 . layer = layer.layers [ 0 ] s = K.sum ( e , axis=axis , keepdims=True ) if self.stateful or self.return_state : l2 [ l2 == 0 ] = 1 def test_cudnn_rnn_canonical_to_params_lstm ( ) : inputs : list of Numpy arrays of inputs . model.add ( layers.Dense ( 3 ) ) raise ValueError ( 'Can not do batch_dot on inputs ' 'must be set . ' ) old_layer = keras.layers.UpSampling2D ( ( 2 , 2 ) , dim_ordering='tf ' , name='us2d ' ) return Conv2D ( * args , * * kwargs ) if k not in namespace : ` ( batch_size , channels ) ` If unspecified , ` workers ` will default to 1 . If 0 , will def VGG19 ( * args , * * kwargs ) : To discretize the AUC curve , a linearly spaced set of thresholds is used to 'data . '.format ( hash_algorithm , file_hash ) ) if len ( input_shape ) < 4 : shuffle : whether to shuffle the data ( default : True ) to_file : File name of the plot image . dtype=None ) : beam_width = 2 elif interpolation == 'bilinear ' : res = copy.deepcopy ( self.sk_params ) This is the expected shape of your inputs # y : ( b_size , d , ... ) output_shape = tf.shape ( result ) output_shape = copy.copy ( input_shape ) arguments [ 'mask ' ] = mask self.name + ' : expected dtype= ' callback.on_train_end ( logs ) # new : ( kernel_rows , kernel_cols , stack_size , filters ) K.zeros_like ( self.true_positives ) ) 'with TensorFlow backend . ' ) pointwise_regularizer=pointwise_regularizer , return permute_dimensions ( output , ( 1 , 0 , 2 ) ) for k , v in logs.items ( ) : kernel_regularizer='l1 ' , from tensorflow.keras.layers import GlobalAveragePooling2D `` `` '' Copies a file to/from/within Google Cloud Storage ( GCS ) . x , cell.kernel_size [ 0 ] , if stop is None : return [ K.tile ( initial_state , [ 1 , self.cell.state_size ] ) ] `` `` '' Returns the serializable config of the metric . '' '' '' idx_identical = np.random.choice ( num_classes , if data_format == 'channels_first ' and dilation_rate ! = ( 1 , 1 ) : config = { 'data_format ' : self.data_format } return mock_fio lr will be reduced when the quantity `` `` '' Computes the squared hinge metric between ` y_true ` and ` y_pred ` . return False for i in range ( len ( node.inbound_layers ) ) : assert output == [ 11 . ] return tf.one_hot ( indices , depth=num_classes , axis=-1 ) if self.use_bias : from keras.utils.generic_utils import func_load def generator_methods_args_preprocessor ( args , kwargs ) : else_expression ) cce = keras.losses.CategoricalCrossentropy ( ) shape1 = list ( input_shapes [ 0 ] ) `` `` '' Handles calling metric functions . count_mode : One of `` steps '' or `` samples '' . pass with and without a real GCS bucket during testing . See example below . field : String ; JSON field under which the data will be stored . for i in range ( len ( weight_values ) ) : if isinstance ( class_weight , dict ) : _LEARNING_PHASE = value return_state=return_state , validation_split=None ) : from .. layers.core import Lambda name = '/ ' + ' : '.join ( name.lower ( ) .replace ( '/ ' , `` ) .split ( ' : ' ) [ -2 : ] ) output = batch_dot ( x_aggregate , kernel ) x = k.placeholder ( ( 1 , 1 ) ) `` `` '' Retrieves the output mask ( s ) of the previous node . and returns a single tensor , the concatenation of all inputs . ' ` cell.state_size ` . Received ` state_spec ` = { } ; ' padding='valid ' , shuffle_pattern = list ( range ( ndim ) ) You may also consider installing the following * * optional dependencies * * : inbound_layers = [ ] from .losses import binary_crossentropy with pytest.raises ( ValueError , `` `` '' Returns whether ` x ` is a placeholder . # Theano expects the parameters to always have x.ndim dimensions . implementation later . if not self.layers : for i in range ( len ( node.inbound_layers ) ) : for i , a in enumerate ( _axis ) : width . constraint=self.pointwise_constraint ) return output , [ h ] probs = self.model.predict ( x , * * kwargs ) if data_format='channels_last ' . losses += cell_losses padding=self.cell.padding , return variable class Highway ( Layer ) : embedding.metadata_path = embeddings_metadata [ layer_name ] assert len ( layer.updates ) == 4 Similar test as test_load_weights_between_noncudnn_rnn ( ) but has different ( num_old_batch , ) + self.from_shape ) ) original_keras_version , def __init__ ( self , padding=1 , * * kwargs ) : filters , node_indices : a list of integers , the same length as ` inbound_layers ` . initial_epoch=initial_epoch , u_ops.append ( u ) for t in range ( seq_len_0 ) ] + # Pad to max_time_steps = 8 `` `` '' Replicates a model on different GPUs . computed_masks = [ x [ 1 ] for x in computed_data ] ( except maybe the last one ) . mask_slice , n_s , s ) ) # y : ( b_size , y1 , ... , d , ... , yn ) # version is the same . def test_setfloatx_incorrect_values ( self , dtype ) : target_max=None , target_min=None ) : pytestmark = pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , if pv.shape ! = w.shape : inner_products.append ( xi.dot ( yi ) ) labels = np.concatenate ( [ labels_train , labels_test ] ) ValueError : If ` data_format ` is neither self.inputs = [ x ] fan_in , fan_out = initializers._compute_fans ( tensor_shape ) layer_class = CuDNNGRU if cudnn else GRU 'go_backwards ' : go_backwards } , self.output_names = [ y_np = random ( y_shape ) return output , [ h , c ] ( 0.1 , 0.0 , 0.8 ) , # max_value is zero self.outputs = [ output_tensor ] if i not in skip_target_weighing_indices fan_out = np.sqrt ( np.prod ( shape ) ) return to_categorical ( indices , num_classes ) ` strides ` ! = 1 , as described accumulators = [ K.zeros ( shape , name='accumulator_ ' + str ( i ) ) # List of shape tuples , shapes of output_tensors . if dims < 3 : model.add ( Embedding ( 1000 , 64 , input_length=10 ) ) follow_links=follow_links , 'CNTK Backend : ` go_backwards ` is not supported with ' class UnitNorm ( Constraint ) : self.skip_target_indices.append ( i ) all of the same shape except for the concatenation axis , shape= ( ) , # DeviceSpec rather than a string . This is done for compatibility self.writer.close ( ) y = np.transpose ( y , ( 0 , 2 , 1 ) ) bias_regularizer=bias_regularizer , if shape [ 1 ] is None : True = `` after '' ( CuDNN compatible ) . self._states = None # Prepare list of loss functions , same size as model outputs . validation epoch if validation is performed . Validation result keys self.append = append layer.bias_r_i , * ` WEIGHTED_MEAN ` : Scalar sum of weighted values divided by sum of weights . nodes_in_decreasing_depth = [ ] at successive epochs , as well as validation loss values `` `` '' Functional interface to the ` Dot ` layer . A tensor with shape equal to the concatenation of ` x ` 's shape return 'th ' `` `` '' Retrieves a layer based on either its name ( unique ) or index . class_weight=None , for ( i , shape ) in enumerate ( shapes ) ] A list of converted weights values ( Numpy arrays ) . ` ( samples , new_rows , new_cols , filters ) ` if data_format='channels_last ' . base_config = super ( _Pooling1D , self ) .get_config ( ) metrics = [ ] # for data_format in [ 'channels_first ' , 'channels_last ' ] ' ( symmetric_dim1_pad , symmetric_dim2_pad , symmetric_dim3_pad ) , ' config.pop ( 'data_format ' ) # to be the same shape as its two model = Sequential ( [ Dense ( 1 , activation='sigmoid ' , input_shape= ( 3 , ) ) ] ) check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , WITH_NP , axes= ( 1 , 2 ) ) out = f ( * args , * * kwargs ) if start is None : if isinstance ( mask , list ) : # Here 's how to use the cell to build a stacked RNN : # Crop the input 2D images or feature maps target , output_shape [ axis ] += shape [ axis ] merging model weights under the scope of the CPU or not . } assert all ( not getattr ( x , '_uses_learning_phase ' ) for x in outputs ) old_layer = keras.layers.Deconvolution2D ( 5 , 3 , nb_col=3 , output_shape= ( 6 , 7 , 5 ) , and/or metrics ) . The attribute ` model.metrics_names ` will give you match = [ m for m in self._metrics if m.name == name ] with custom_object_scope ( { 'MyObject ' : MyObject } ) : bar += ( '= ' * ( prog_width - 1 ) ) from .. import callbacks as cbks super ( SparseCategoricalAccuracy , self ) .__init__ ( x = permute_dimensions ( x , [ 0 , 2 , 3 , 1 ] ) def permute_dimensions ( x , pattern ) : `` `` '' Element-wise sigmoid . Let x 's shape be ( 100 , 20 ) and y 's shape be ( 100 , 30 , 20 ) . self._output_shape = output_shape ) if is_keras_tensor : if md5_hash is not None and file_hash is None : x = k.variable ( val ) check_single_tensor_operation ( 'argmin ' , ( 4 , 2 ) , WITH_NP ) self.b_carry = None name='accumulator_ ' + str ( i ) ) save_prefix=save_prefix , def _convert_dtype_string ( dtype ) : seed = np.random.randint ( 10e6 ) allowed_positional_args= [ 'cropping ' ] , output = repeat_elements ( x , depth_factor , axis=2 ) def test_training_and_eval_methods_on_symbolic_tensors_multi_io ( ) : from .load_backend import bias_add 'beta_initializer ' : initializers.serialize ( self.beta_initializer ) , `` `` '' Multiply the values in a tensor , alongside the specified axis . if len ( padding ) ! = 2 : raise ValueError ( 'Unknown data_format : ' + str ( data_format ) ) shape [ 2 ] , if weights is None : self.learning_rate = K.variable ( learning_rate , name='learning_rate ' ) for layer in index.get ( name , [ ] ) : inputs = keras.Input ( batch_shape= ( 1 , timesteps , dim ) ) with the layer input to produce a tensor of outputs . xs = [ [ w for w in x if skip_top < = w < num_words ] for x in xs ] @ interfaces.legacy_lambda_support is_weighted : Boolean indicating whether the given metrics are weighted . def _handle_metrics ( self , input_shapes = [ x._keras_shape for x in inputs ] self.best = np.Inf if self.monitor_op == np.less else -np.Inf kernel_regularizer=kernel_regularizer , Currently , the ` .fit ( ) ` method of the ` Sequential ` model class from tensorflow.python.framework import device as tfdev self.recurrent_constraint = constraints.get ( recurrent_constraint ) x += reshape ( bias , ( 1 , 1 , 1 , bias_shape [ 0 ] ) ) ` ( batch , depth , old_layer = keras.layers.Deconvolution2D ( 5 , 3 , 3 , output_shape= ( 6 , 7 , 5 ) , # than 2 , need manual eval if data_format not in { 'channels_first ' , 'channels_last ' } : for ( i , p ) in enumerate ( params ) ] var = k.variable ( var_np ) which is useful to use with ` model.predict_generator ( ) ` ) . K.greater ( dp , 0 ) , assert ' # Theano-like behavior example ' in generated if is_sparse ( tensor ) : output_masks ) : `` `` '' Computes the Poisson metric between ` y_true ` and ` y_pred ` . xs = [ [ start_char ] + [ w + index_from for w in x ] for x in xs ] class RandomNormal ( Initializer ) : shape= ( ) , dtype=np.float32 , separable_conv2d = separable_conv # Make a list of masks while making sure self._recurrent_dropout_mask is None ) : name='kernel ' , List of metrics . cudnn_layer = TimeDistributed ( cudnn_layer ) initial_states_np= [ ] , initializer=self.beta_initializer , @ pytest.mark.skipif ( K.backend ( ) == 'cntk ' , reason='Not supported ' ) shape , p=p , dtype=dtype , seed=seed ) check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=0 ) ( in which case its weights are n't yet defined ) . self.backward_layer.constants_spec = constants_spec ` ( batch , channels * depth_multiplier , new_rows , new_cols ) ` return ( key in self.data ) or ( key in self.data.attrs ) str ( inbound_tensor_index ) + ' ] ' ) super ( RootMeanSquaredError , self ) .__init__ ( name , dtype=dtype ) # choose to manually build your model by calling def transpose ( x ) : exception_prefix= '' ) : super ( DataFrameIterator , self ) .__init__ ( elif mode == 'wb ' : if tf.__version__ > = ' 2.0.0 ' : from .load_backend import is_tensor K.zeros_like ( self.false_positives ) ) # config has the same thresholds . def update ( self , current , values=None ) : chosen so that the mean and variance of the inputs are preserved unprocessed_nodes = { } mean=0. , if level < 0. or level > = 1 : def call_metric_function ( metric_fn , When running a model loaded from file , the input tensors target_size= ( height , width ) , model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.FalsePositives ( ) ] ) model.add ( Lambda ( antirectifier , num_values = K.cast ( K.size ( values ) , self.dtype ) if node_key in self._network_nodes : self.cells = cells from tensorflow.keras.layers import Conv3D self.kernel_z = self.kernel [ : , : self.units ] callbacks._call_batch_hook ( 'train ' , 'end ' , batch_index , batch_logs ) self.data_format , check_batch_axis=True , x = expand_dims ( x , 3 ) `` `` '' Validates a network 's topology and gather its layers and nodes . inputs_f = inputs * dp_mask [ 1 ] if pydot is None : output_shape , layer.count_params ( ) ] raise Exception ( error_msg.format ( origin , e.code , e.msg ) ) x_c = self.input_conv ( inputs_c , self.kernel_c , self.bias_c , max_queue_size=max_queue_size , as a list of Numpy arrays . inputs_np , if layer.__class__.__name__ == 'Conv3D ' : loss : String ( name of objective function ) or objective function or The current implementation does not include the feedback loop on the for layer in model_layers : _ , c = parse_shape_or_val ( ( num_samples , output_dim ) ) automatically inferred from the ` y_col ` , layer.recurrent_kernel_h , 'due to technical limitations . ' x_sparse_2 = sparse.csr_matrix ( ( x_d , ( x_r , x_c ) ) , shape= ( 4 , 5 ) ) really hard . then the precision value is 2/ ( 2+1 ) ie . 0.66 . If the weights were specified as steps = input_shape [ 1 ] initial_state , conversions= [ ( ' p ' , 'rate ' ) , updated_per_output_weighted_metrics = [ ] For the first two cases , ` batch_size ` must be provided . return data_format input1 = keras.layers.Input ( shape= ( 16 , ) ) def _remove_blanks ( inds , num_classes ) : current = logs.get ( self.monitor ) 'verbose ' : verbose , _runner ( initializers.RandomUniform ( minval=-1 , maxval=1 ) , tensor_shape , if len ( weight_values ) ! = len ( symbolic_weights ) : `` `` '' Utilities related to metrics . '' '' '' self.bias_h_i , return outputs , states from .load_backend import square fan_in , _ = initializers._compute_fans ( tensor_shape ) self.W = self.add_weight ( shape= ( input_dim , input_dim ) , name='kernel ' ) kernel = _preprocess_conv3d_kernel ( kernel , data_format ) return weight 'reduction [ % s ] not implemented ' % self.reduction ) # to insert before the current layer Note that when P_A == 0 the above calculation simplifies into @ pytest.mark.parametrize ( 'rnn_type ' , [ 'lstm ' , 'gru ' ] , ids= [ 'LSTM ' , 'GRU ' ] ) def tile ( x , n ) : inbound_layer_name = input_data [ 0 ] x = K.truncated_normal ( shape , self.mean , self.stddev , output_shape = input_shape [ :2 ] + ( rows , cols , cell.filters ) def is_categorical_crossentropy ( loss ) : [ batch , height , width , channels ] ( for 'channels_last ' data_format ) `` `` '' A context manager that specifies control dependencies . # iteration to 0 . 'dynamic shape is not supported now . ' name : String , name for the variable to create . x = tf.transpose ( x , pattern ) conversions=None , if 'implementation ' in kwargs : n = tuple ( [ 1 for _ in range ( len ( shape ) - len ( n ) ) ] ) + n merged = [ ] `` `` '' Recurrent layers backed by cuDNN . send [ k ] = v.item ( ) from .load_backend import random_normal def recurrent_identity ( shape , gain=1. , dtype=None ) : return tf_file_io.file_exists ( filename ) elif len ( generator_output ) == 3 : # Final result : 1.6 layer.recurrent_kernel_o , layer = rnn_layer_class ( units , * * rnn_layer_kwargs ) if ` class_mode ` is ` `` raw '' ` or ` `` multi_output '' ` it should contain 'For multi-output layers , ' self._wait_queue ( ) padding = 'valid ' Default : None . If not provided , the list of classes will be train_dir , py_slice ( left_pad , input_shape [ 2 ] + left_pad ) , model.add ( keras.layers.Bidirectional ( rnn ( output_dim ) , merge_mode=mode ) ) self._reset_batch_timing ( ) sample_weight = mask > > > K.int_shape ( kvar ) from .advanced_activations import LeakyReLU grads , global_step=self.iterations ) python -c `` import keras.backend '' See [ optimizers ] ( /optimizers ) . def _clone_sequential_model ( model , input_tensors=None ) : if x is None : Number of samples per gradient update . def test_ctc_decode_beam_search_no_merge ( self ) : > > > K.is_keras_tensor ( keras_input ) # An Input is a Keras tensor . the output will be a tensor of shape ` ( batch_size , 1 ) ` self.histogram_freq = histogram_freq # Make sure we do n't override any entries from common , such as epsilon . array_to_img.__doc__ = image.array_to_img.__doc__ 'because symbolic tensors are expected to produce ' from .merge import Subtract corresponding value of ` sample_weight ` . 'data_format ' : self.data_format } 'ill-defined for the layer . ' # include the metrics that were added in compile API of a nested model . x += reshape ( bias , ( 1 , 1 , 1 , 1 , bias_shape [ 0 ] ) ) return tf.square ( x ) reset_metrics=True ) : ( idxs < b_active.dimshuffle ( 0 , ' x ' ) ) [ : :-1 , : :-1 ] ) if ( getattr ( y , '_uses_learning_phase ' , False ) or loss : Scalar tensor to minimize . Ignored with the default value of ` None ` . # Check dtype . `` `` '' Base class for convolutional-recurrent layers . `` `` '' A ` Network ` is way to compose layers : the topological form of a ` Model ` . '' '' '' save_to_dir=None , bias_regularizer='l1 ' , `` `` '' Sets the value of a variable , from a Numpy array . if transposed : # Set backend based on KERAS_BACKEND flag , if applicable . def is_sparse ( tensor ) : % ( len ( cntk_shape ) , dynamic_axis_num ) ) [ How can I install HDF5 or h5py to save my models in Keras ? ] ( without calling ` model.compile ` again . Apologies for the inane API , but Theano makes this `` `` '' Constrains the weights to be non-negative . 'after instantiation . ' _runner ( initializers.identity ( ) , tensor_shape , from . import wrappers @ interfaces.legacy_separable_conv2d_support self.on_train_end ( ) from keras.engine import training_utils from .. legacy import interfaces from .losses import mean_absolute_error # create place holder preprocessor=convlstm2d_args_preprocessor ) specifying the amount of padding along the depth , height , and 'return_sequences ' : self.return_sequences , Hard sigmoid activation : weights = losses_utils.broadcast_weights ( `` `` '' Instantiates a Model from its config ( output of ` get_config ( ) ` ) . start , stop = key.start , key.stop 'unroll ' : self.unroll } opt , params , _ = args shape = K.int_shape ( x ) K.set_value ( state , np.zeros ( ( batch_size , dim ) ) ) should contain the images and the second element return np.array ( value , dtype ) axes.append ( self.axes [ i ] ) assert max_val - 0.015 < np.max ( rand ) < = max_val stateful , global _LEARNING_PHASE normalizer=normalizer ) output_tensors = unpack_singleton ( output_tensors ) new_node_index = node_conversion_map [ node_key ] max_num_labels_tns = tf.stack ( [ label_shape [ 1 ] ] ) ( or an instance of ` Sequence ` ) . for out in outs : broadcast_beta , ( output_shape [ 0 ] , 1 ) + output_shape [ 2 : ] ) nested_states = [ ] zero_grad_eval_fn = k.function ( [ x ] , [ zero_grad [ 0 ] ] ) corresponding target should be ignored or not . from tensorflow.keras.layers import ReLU `` `` '' Builds a Enqueuer from a Sequence . # Check that no item in ` data ` is larger than ` HDF5_OBJECT_HEADER_LIMIT ` config [ 'clipnorm ' ] = self.clipnorm self.data.close ( ) state_spec = to_list ( state_spec ) def get_file ( fname , assert set ( z_shape ) < = set ( ( None , 1 ) ) input_c = initial_state [ 1 ] base_config = super ( ConvLSTM2D , self ) .get_config ( ) if not hasattr ( cell , 'state_size ' ) : if is_symbolic ( x ) : `` `` '' Save the model after every epoch . ( if accuracy monitoring is enabled ) . from mock import patch return K.resize_volumes ( inputs , model.add ( LSTM ( 32 , return_sequences=True ) ) if has_arg ( fn , params_name ) : `` `` '' Element-wise truth value of ( x < y ) . `` `` '' Takes data & label arrays , generates batches of augmented data . successive_outputs.append ( output ) `` `` '' Fast GRU implementation backed by [ CuDNN ] ( https : //developer.nvidia.com/cudnn ) . weights = [ ] name='bias ' , strict=False ) 'also use the ` input_length ` argument.\n ' base_metric_name = metric_name A model is callable on non-Keras tensors . # verify same expected output for ` unroll=true/false ` # 4D convolution in th order . This shape has the same effective shape as FC_SHAPE download = True List of standardized input arrays ( one array per model input ) . lambda i : K.sum ( vx [ i ] ) , # decode utf8 self.forward_layer.build ( input_shape ) 'Received : ` gpus= % s ` ' % gpus ) placeholders = [ ] from .. utils import Sequence # the model will take as input an integer matrix of size ( batch , input_length ) . # TODO : make this a parameterized test On the [ Keras Slack channel ] ( https : //kerasteam.slack.com ) . Use [ this link ] ( https : //keras-slack-autojoin.herokuapp.com/ ) to request an invitation to the channel . self.writer = tf.summary.FileWriter ( self.log_dir , z_list.append ( k.eval ( k.switch ( cond , then_expr , else_expr ) ) ) ( 'nb_worker ' , 'workers ' ) , num_classes = 2 ` `` same '' ` results in padding the input such that if isinstance ( shape_or_val , tuple ) : return y , [ ] from tensorflow.python.ops import functional_ops def test_unweighted_high_sensitivity ( self ) : assert abs ( output.max ( ) - target_max ) < lim cell.build ( input_shape ) self.recurrent_kernel_c ) ) sin = np.sin fan_in = np.sqrt ( np.prod ( shape ) ) class SeparableConv2D ( _SeparableConv ) : show_shapes=False , `` `` '' Layer that computes the minimum ( element-wise ) a list of inputs . # Theano expects ` ( depth , input_depth , space ) ` . ' was passed non-serializable ' version = C.__version__ for weight in layer.weights : if not _has_nchw_support ( ) or force_transpose : if not all ( isinstance ( v , np.ndarray ) or sub_n_first_node [ layer.name ] = sub_n_nodes [ 0 ] elif hasattr ( cell.state_size , '__len__ ' ) : # loss_weight_2 * output_2_loss_fn ( ... ) idx = 2 `` `` '' Regularizer for L1 and L2 regularization . new_layer = keras.layers.Embedding ( input_dim=4 , output_dim=2 , name= 'd ' , if hasattr ( self , 'clipnorm ' ) : depthwise_constraint=depthwise_constraint , epsilon=self.epsilon ) super ( LSTMCell , self ) .__init__ ( * * kwargs ) layer.name + '.\n ' if mask is None : for i in range ( batch_size ) : if not cond_ndim : raise ValueError ( ' ` class_weight ` must contain ' # implement a simple RNN If set , the layer will not create a placeholder tensor . weight_value_tuples = [ ] Effectively , this means that rate=1.0 stands for strict return K.in_train_phase ( dropped_inputs , inputs , training=training ) save_function ( obj , filepath , overwrite , * args , * * kwargs ) filter_shape=depthwise_kernel_shape , g._dense_shape = g_shape return tf.reduce_mean ( x , axis , keepdims ) return Sequential ( layers=layers , name=model.name ) if steps_done == 1 : constraints.serialize ( self.pointwise_constraint ) ) def DISABLED_test_check_bad_shape ( ) : `` `` '' Normalizes inputs and targets provided by users . # the returned Numpy arrays . # to be the same shape as its two def _create_mean_metric ( value , name=None ) : # tf.where needs its condition tensor elif isinstance ( loss , collections.Sequence ) : shape = ( bias.shape [ 0 ] , 1 , 1 ) before computing the test split . output_masks : list of output masks 'instead to initialize with ones . ' , stacklevel=3 ) from .load_backend import logsumexp `` `` '' Prepares sample weights for the model . `` `` '' Sequential model class . be equal to the number of samples of your state_shape = state_shape [ 0 ] from keras import initializers from keras import layers return trainable_weights self.layer.build ( child_input_shape ) `` `` '' Global max pooling operation for temporal data . x = np.array ( [ [ 1 , 1 , 1 , 1 , 1 ] ] ) self.input_dim = input_dim when=lambda arg : True , 'has shape ` % s ` , the second axis ' if K.image_data_format ( ) == 'channels_last ' : self._t_enter_batch = time.time ( ) new_p = p_t # Note : batch_dot implementation is different for axis = mean.broadcastable.index ( False ) weights = weights [ num_param : ] val_x , val_y , val_sample_weight = validation_data n : Tensor , actual norm of ` g ` . nb_feature=4 , from tensorflow.keras.layers import Subtract from .advanced_activations import ThresholdedReLU raise ValueError ( 'RNN cell does not support constants ' ) ValueError : in case of invalid ` data_format ` argument . layer=layer , ` ( batch_size , : ) ` . x_test , y_test = load_batch ( fpath ) shape : Shape tuple , expected shape of the input threshold=self.threshold ) x : A tensor or variable deserialization . Else , an average of the metric over time will be displayed . The parameter name 'varkw ' is changed to 'keywords ' to fit the loss = { 'output1 ' : 'mse ' , 'output2 ' : 'mse ' } raise ValueError ( 'When specifying ` unroll=True ` , ' Note that because this implementation if not dtype : assert outputs.op.name.lower ( ) .endswith ( '/relu ' ) def state_updates ( self ) : self.value = value self.recurrent_kernel_c = ( layers_depths [ node.outbound_layer ] = depth workers=1 , amsgrad=False , * * kwargs ) : mask_slice = C.ops.slice ( mask , time_axis , i , i + 1 ) json.dumps ( node.arguments ) self.recurrent_kernel_r , self.data.attrs [ attr ] = val thresholds : A float value or a python list or tuple of float thresholds in model.compile ( loss='mse ' , optimizer='sgd ' , metrics= [ 'acc ' ] ) from keras.utils.test_utils import tf_file_io_proxy input_dict , mock_fio = MagicMock ( ) while overwrite not in ( ' y ' , ' n ' ) : ValueError : In case of mismatch between the provided input data to be the time dimension . pool_size : tuple of 2 integers . t_W = K.variable ( W ) if hasattr ( self.cells [ -1 ] .state_size , '__len__ ' ) : op , input_shape , WITH_NP , # We use a Value to provide unique id to different processes . if layer.data_format == 'channels_last ' : batch_input_shape : Shape tuple , including the batch axis . reshaped_inputs.append ( x_transposed ) nesterov : boolean . Whether to apply Nesterov momentum . # serialize and save the layers in layer_configs [ cuDNN ] ( https : //docs.nvidia.com/deeplearning/sdk/cudnn-install/ ) ( recommended if you plan on running Keras on GPU ) . assert_list_pairwise ( output_arrays ) return _IMAGE_DATA_FORMAT if self.cell.data_format == 'channels_first ' : # inputs . model.add ( LSTM ( 10 ) ) round = np.round def ones ( shape , dtype=floatx ( ) , name=None ) : pass embeddings_constraint=None , return K.max ( inputs , axis=steps_axis ) self.kernel_size [ 1 ] , output_shape = transpose_shape ( output_shape , 'channels_first ' , be ` min ` , etc . In ` auto ` mode , the direction is 'you should specify the ` steps_per_epoch ` ' applied to the bias . k_labels = K.variable ( labels , dtype= '' int32 '' ) # Output shape check_two_tensor_operation ( 'batch_dot ' , ( 4 , 2 , 3 ) , ( 4 , 5 , 3 ) , # SHAPE OPERATIONS def __init__ ( self , learning_rate=0.002 , beta_1=0.9 , beta_2=0.999 , * * kwargs ) : raise ValueError ( 'Received an empty batch . ' normalize : Whether to L2-normalize samples along the model.add_loss ( lambda : 1 ) where ` alpha ` is a learned array with the same shape as x . elif initial_state is not None : raise TypeError ( 'The model has multiple outputs , so ` ' clipvalue : float > = 0 . Gradients will be clipped first_connection ] embeddings_constraint : Constraint function applied to Decorated function that wraps ` update_state_fn ( ) ` with ` add_update ( ) ` . If you have installed TensorFlow with pip , you should be able def set_epsilon ( e ) : be automatically included with these to correctly handle predictions subtracted = keras.layers.subtract ( [ x1 , x2 ] ) implementing an efficient stacked RNN . install_requires= [ 'tensorflow > =2.2.0 ' , elif i == 1 : learning_rate : float > = 0 . Initial learning rate , defaults to 1 . array ( [ [ 1. , 2. , 3 . ] , from tensorflow.keras.constraints import MinMaxNorm if not ( len ( int_shape ( x ) ) == len_start == len_size ) : kernel_shape = self.kernel_size + ( self.filters , input_dim ) self.on_batch_begin = on_batch_begin config = { 'padding ' : self.padding , return T.grad ( loss , variables ) new_c.append ( C.sequence.broadcast_as ( c , rnn_inputs ) ) return C.slice ( x , cntk_axes , begin_index , end_index , strides ) from tensorflow.python.ops import image_ops as tf_image_ops return K.mean ( inputs , axis= [ 1 , 2 ] ) # This is suboptimal , but it is the best we can do with the info if on_train_end is not None : If set to 1 , the RNN will use more matrix products , new_layer = keras.layers.LSTM ( 2 , kernel_initializer='normal ' , 'Please update your layer call . ' ) mean = tf.reshape ( mean , [ -1 ] ) save_to_binary_h5py ( save_function , f ) the ` training ` flag defaults to ` K.learning_phase ( ) ` . self.gamma = None legacy_spatialdropoutNd_support = generate_legacy_interface ( def test_update_sub ( self ) : raise NotImplementedError ( 'Must be implemented in subclasses . ' ) __doc__ = image.DataFrameIterator.__doc__ `` `` '' Pad the 2nd , 3rd and 4th dimensions of a 5D tensor self._layers.append ( value ) `` `` '' Utilities used in convolutional layers . if ndim ( var ) > 1 : self.output_names , loss_weights ) inputs = K.variable ( inputs_vals ) except ( tarfile.TarError , RuntimeError , samples_per_epoch = kwargs.pop ( 'samples_per_epoch ' ) if old_value in value_conversions [ key ] : step = min ( self.batch_size , n_samples - i ) raise ValueError ( 'Improperly formatted model config . ' ) # update delta_accumulator `` `` '' Applies batch normalization on x given mean , var , beta and gamma . while cond_float.ndim < then_expression.ndim : kwargs = { } sys.stdout.isatty ( ) ) or backend_module = importlib.import_module ( _BACKEND ) if id ( x ) in [ id ( i ) for i in inputs_ls ] : new_layer = keras.layers.SeparableConv2D ( 5 , ( 3 , 3 ) , assert not tmpdir.listdir ( ) def rnn ( step_function , inputs , initial_states , cell can also take the optional argument ` constants ` , see 'axis ' : self.axis , if axis ! = 1 : `` `` '' Callback that prints metrics to stdout . return _convert_dtype_string ( x.dtype ) from tensorflow.keras.models import clone_model ( batch_size , new_rows , new_cols , filters ) return np.tile ( x , n ) import collections if ` x ` has shape ( samples , dim ) and ` n ` is ` 2 ` , need_convert = not has_seq_axis ( inputs ) fit_inputs = x + y + sample_weights + [ 1 ] from .. utils.generic_utils import has_arg raise NotImplementedError ( 'CNTK only supports ` nearest ` interpolation . ' ) For example , if ` y_true ` is [ 0 , 1 , 1 , 1 ] and ` y_pred ` is [ 0 , 1 , 0 , 0 ] # Properly set learning phase result = C.cross_entropy_with_softmax ( output , target ) return alt assert abs ( output.min ( ) - target_min ) < lim learning_rate : float > = 0 . Learning rate . [ class_weight [ cls ] for cls in y_classes if cls in class_weight ] ) ` ( batch , new_steps , filters ) ` assert outputs.op.name.lower ( ) .endswith ( '/leakyrelu ' ) } , y_true , y_pred , self.thresholds , sample_weight=sample_weight ) save_format='png ' , broadcast_gamma = None new_p = p + v raise ValueError ( 'Invalid Reduction Key % s . ' % key ) from .advanced_activations import ELU 'Here , a tensor specified as ' self.l2 = l2 save_best_only : if ` save_best_only=True ` , warnings.warn ( 'The ` AtrousConvolution1D ` layer ' legal_params_fns.append ( self.build_fn ) if len ( input_shape ) ! = 4 : C.AVG_POOLING , if isinstance ( obj , list ) : 'false_positives ' , Use this crossentropy loss function when there are two or more label classes . stddev=stddev ) if index < nones : TFOpt = tf.compat.v1.train.Optimizer b_active_next , log_b_next = ctc_update_log_p ( __Note on using statefulness in RNNs__ assert input1_depth == input2_depth shape = None res.update ( override ) 2 * [ np.zeros ( ( 1 , depth ) , dtype=np.float32 ) ] ) from tensorflow.keras.applications.mobilenet import MobileNet mask_t = expand_dims ( mask_t ) raise ImportError ( ' ` load_model ` requires h5py . ' ) if sample_weight is not None and len ( sample_weight.shape ) ! = 2 : assert f [ ' x ' ] == 'abcd ' class DataFrameIterator ( image.DataFrameIterator , Iterator ) : states = y [ 1 : ] + y_rev [ 1 : ] print_row ( to_display , positions ) > > > k_var = tf.placeholder ( 'float32 ' , shape= ( 1,1 ) ) layer.compute_mask ( computed_tensors , 'Use ` get_input_mask_at ( node_index ) ` ' return np.any ( x , axis=axis , keepdims=keepdims ) assert o [ 0 ] ._keras_shape == ( None , 3 , 2 , 1 ) ms = [ K.zeros ( shape , name='m_ ' + str ( i ) ) # shape : batch , filters , output_length skip_target_masks = skip_target_masks or [ False ] * len ( outputs ) result = C.reshape ( x , new_shape ) if shape is None : batch_size : Integer or ` None ` . # input shape : ` ( samples , time ( padded with zeros ) , input_dim ) ` expected output shape when an element-wise operation is _assert_has_capability ( T.nnet , 'elu ' ) if len ( input_shape ) < self.rank + 2 : candidate_vars.append ( v ) An integer . if d1 ! = d2 : name='kernel ' , if node.inbound_layers : import scipy.sparse as sparse dimension indices , e.g . ` ( 0 , 2 , 1 ) ` . if self.wait > = self.patience : step_function , if not is_tensor ( elems ) : from theano.tensor.signal import pool is purely for UX purposes . x = expand_dims ( x , 1 ) binary dropout mask that will be multiplied with the input . outputs = layers.ReLU ( ) ( inputs ) group3 [ ' y ' ] = [ b'efg ' , b'hij ' , b'klmn ' ] if isinstance ( value , np.ndarray ) : if x.dtype.base_dtype == tf.bool : is not supported when ` x ` generator , or ` Sequence ` instance , val_function = None x_shape = tf.shape ( x ) raise TypeError ( 'For the ` ' + new_arg + ' ` argument , ' scale = 1.0507009873554804934193349852946 thus running faster on CPU but consuming more memory . in reverse chronological order . fn=ctc_step , return obj.tolist ( ) self.monitor_op = lambda a , b : np.greater ( a , b + self.min_delta ) if count > 1 : if cache_key in self._output_mask_cache : dtype=x.dtype , embeddings_freq=0 , name='kernel ' ) if ndim ( x ) is not None and ( ndim ( x ) > 2 or ndim ( y ) > 2 ) : activation=activation , [ Zero-Bias Autoencoders and the Benefits of Co-Adapting Features ] ( re.findall ( r '' \s * return \S+ '' , code_inner , re.MULTILINE ) ] `` `` '' Makes the metric name unique and adds it to the model 's metric name list . defaults to ` [ .33 , .55 , .67 , 1 . ] ` . kwargs [ 'constants ' ] = constants reduction=losses_utils.Reduction.SUM , name='mse_1 ' ) num_classes : The possible number of labels the prediction task can have . return x [ slices ] is saved ( ` model.save ( filepath ) ` ) . dilation_rate= ( 1 , 1 ) , # Legacy aliases a ` call ( input_at_t , states_at_t ) ` method , returning def foldl ( fn , elems , initializer=None , name=None ) : shuffle : Whether to shuffle the data ( default : True ) 'starting with a freshly initialized ' conflict_counter = { n : 0 for n , count in occurrences.items ( ) if count > 1 } spatial_axes= ( 1 , 2 ) ) ) * * kwargs ) model.add ( Conv2D ( 3 , ( 1 , 1 ) , name='morty ' ) ) if dim is None : 'with value % s is not supported , ' from .. legacy.layers import Highway if layer.input_spec is None : key_loss_classes = ( losses.MeanSquaredError , losses.BinaryCrossentropy , # to the input layer we just created . def validate_file ( fpath , file_hash , algorithm='auto ' , chunk_size=65535 ) : `` `` '' Computes an output mask tensor . weight_values = K.batch_get_value ( symbolic_weights ) If True , the network will be unrolled , config [ 'depthwise_initializer ' ] = ( config [ 'output_padding ' ] = self.output_padding padding='same ' , data_format=data_format ) if old_weights [ i ] : This is a near direct port of the internal Numpy function allowed_positional_args= [ 'stddev ' ] , and ` batch_size ` is not ` None ` because they are mutually Tensor with dtype ` dtype ` . data_format='channels_middle ' ) self.padding , self.strides [ 0 ] ) def load_model ( filepath , custom_objects=None , compile=True ) : batch_dot results in a tensor with less dimensions than the input . training=training , if 'transform_bias ' in kwargs : inputs : 4D tensor with shape : ultimately returned as ` binary accuracy ` : an idempotent operation that simply def clone ( layer ) : assert dot.get_node ( inbound_layer_id ) # then call node.inbound_layer on them . `` `` '' Categorical crossentropy with integer targets . assert_not_compatible ( gru ( reset_after=True ) , gru ( ) , directory : string , path to the directory to read images from . If ` None ` , out , err = capsys.readouterr ( ) always ignore first dimension of y if id ( x ) in tensor_map : if self.data_format == 'channels_first ' : initial_epoch : Integer . _PREDICT = 'predict ' return sample_weight * class_sample_weight 'CNTK backend : when constructing trainer , ' from . import noise elif time_per_unit > = 1e-3 : export MINICONDA= $ HOME/miniconda strides= ( 2 , 2 ) , return tf.identity ( x ) assert dense._inbound_nodes [ 1 ] .input_tensors == [ b ] pointwise_kernel_shape = ( 1 , ) * self.rank + pointwise_kernel_shape 'parts ' : num_gpus } ) ( x ) validation_data=None , `` `` '' Builds a queue out of a data generator . name='m_ ' + str ( i ) ) chunk_attr = ' % s % d ' % ( attr , 0 ) gamma = tf.cast ( gamma , tf.float32 ) # This list holds the available devices . kx = K.eval ( K.foldl ( lambda a , b : a + b , K.variable ( x ) ) ) A function object . # Test invalid use cases If a Keras tensor is passed : model.add ( Cropping2D ( cropping= ( ( 2 , 2 ) , ( 2 , 2 ) ) ) ) `` `` '' Checks if a callable accepts a given keyword argument . def ctc_path_probs ( predict , Y , alpha=1e-4 ) : from tensorflow.keras.applications.inception_resnet_v2 import decode_predictions b_constraint=None , If no global Keras session exists at this point : filter_dilation=dilation_rate , ` ( samples , time , output_row , output_col , filters ) ` current_uses_correlation = True val_inputs=None , super ( Adamax , self ) .__init__ ( * * kwargs ) `` `` '' Callback that terminates training when a NaN loss is encountered . def test_conv3d_legacy_interface ( ) : ( e.g . will not include losses that depend on tensors the size of the recurrent state # And to the following : always ignore first dimension of ` y ` p= [ level , 1 - level ] ) `` `` '' Adds metric tensor to the layer . from keras.utils.io_utils import HDF5Matrix converted.append ( ( new_name , old_name ) ) return tf.stack ( x , axis=axis ) 'strides ' : self.strides , model.add ( layers.Dense ( 2 , input_shape= ( 3 , ) ) ) h5py.File or h5py.Group object from which to load the model for layer_data in config [ 'input_layers ' ] : enqueuer = GeneratorEnqueuer ( new_layer = keras.layers.GlobalMaxPool3D ( name='global_maxpool3d ' ) ' ` can accept only ' def test_rnn_constants ( self ) : return K.concatenate ( [ pos , neg ] , axis=1 ) Default : hyperbolic tangent ( ` tanh ` ) . self.recurrent_kernel_o ) ) indices = tf.transpose ( tf.reshape ( indices , [ 2 , -1 ] ) ) return map ( tf.stop_gradient , variables ) x : The input tensor . if backend ( ) == 'theano ' : recurrent_dropout = 0 . raise ValueError ( 'Invalid backend . Missing required entry : ' + e ) output , states = self._process_batch ( inputs , initial_state ) `` `` '' Computes the shape of the resultant of an elementwise operation . 'but instead got the following list of ' then the false negatives value is 2 . If the weights were specified as self.depth_multiplier = depth_multiplier base_config = super ( _Conv , self ) .get_config ( ) if layer.name : false negatives . This metric creates one local variable , ` accumulator ` PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/integration_tests ; # test when batch_size = 1 , that is , one sample only # Find the index of the threshold where the specificity is closest to the return C.sigmoid ( x ) the function if a real GCS bucket is not available for testing . _p_prev [ active_skip_idxs + 2 ] , p_prev [ active_skip_idxs ] ) def serialize ( initializer ) : self.beta_initializer = initializers.get ( beta_initializer ) # out = model.train_on_batch ( [ input_a_np , input_b_np [ :2 ] ] , regularizer=self.kernel_regularizer , ( eg . L1 or L2 regularization ) , applied to the main weights matrix . idx += 1 if self.read_only : return super ( RNN , self ) .get_losses_for ( inputs ) ' ( tuple of integers , ' @ symbolic input_tensors.append ( self._cache_output_metric_attributes ( metrics , weighted_metrics ) for m in metrics : backward_losses = self.backward_layer.get_losses_for ( inputs ) # add dim to kernel ( always same format independently of data_format ) raise TypeError ( ' ` get_value ` can only be called on a variable . ' self.recurrent_kernel_i ) label_is_neg = K.equal ( target_shape.append ( 1 ) set_y = set_of_lengths ( targets ) self.beta_1 = K.variable ( beta_1 , name='beta_1 ' ) ' have ' + str ( len ( weight_values ) ) prev_output * = rec_dp_mask `` `` '' Wraps a stateless metric function with the Mean metric . '' '' '' ( left_dim3_pad , right_dim3_pad ) ) ` y._keras_shape = ( 1 , ) * len ( x._keras_shape ) if keepdims else ( 1 , ) accumulator = fn ( accumulator , elems [ i ] ) Mean accuracy of predictions on ` x ` wrt . ` y ` . const_a = C.reshape ( const_a , shape ) arguments=arguments histogram_freq : frequency ( in epochs ) at which to compute activation ' saving model to % s ' def spatial_2d_padding ( x , padding= ( ( 1 , 1 ) , ( 1 , 1 ) ) , data_format=None ) : h = states [ 0 ] return weights max_queue_size=10 , known * = dim transfers the file to GCS if ` filepath ` starts with `` gs : // '' . def deserialize ( config , custom_objects=None ) : from tensorflow.keras.layers import Permute states = new_states [ : len ( states ) ] raise ValueError ( 'Please do not pass a dictionary ' error = y_pred - y_true dense_layer.add_loss ( lambda : 0 , inputs=a ) if shape : self._updates = [ ] loss = cce ( self.recurrent_kernel = self.add_weight ( from tensorflow.keras.layers import Dot return iter ( self.data ) label_smoothing : Float in [ 0 , 1 ] . When 0 , no smoothing occurs . When > 0 , we return out # load weights from first model of the convolution along the width and height . def test_statefulness ( initializer ) : Parameters : A tensor with shape equal to the concatenation of x 's shape `` `` '' Checks if conversion on kernel matrices is required during weight loading . output_sample , _ = step_function ( inputs [ : , 0 ] , initial_states + constants ) C.MAX_POOLING , self._uses_inputs_arg = has_arg ( self.call , 'inputs ' ) def _uses_dynamic_learning_phase ( self ) : return tf_keras_backend.random_normal ( 'Update your method calls accordingly . ' , stacklevel=3 ) 'however ` cell.state_size ` is ' 'was not an Input tensor , ' return np.power ( x , a ) WITH_NP , from_logits=True ) 'keras_applications > =1.0.6 ' , kept_idx = K.greater_equal ( K.random_uniform ( noise_shape , recurrent_activation=recurrent_activation , `` `` '' Gets the model 's input specs . b_regularizer='l2 ' , mean=1.0 , datas = [ data1 , data2 , attr ] x = K.placeholder ( shape= ( None , 3 , 4 ) ) x = temporal_padding ( x , ( left_pad , 0 ) ) `` `` '' Turn a n-D tensor into a 2D tensor where mode + self.file_flags , def spatial_3d_padding ( x , padding= ( ( 1 , 1 ) , ( 1 , 1 ) , ( 1 , 1 ) ) , data_format=None ) : top_paths : if ` greedy ` is ` False ` , if hasattr ( C , 'unpack_batch ' ) : of values are present but the shape does not match . # If there are multiple inputs , then x , tf.stack ( [ -1 , prod ( shape ( x ) [ 1 : ] ) ] , if value.shape ! = ( batch_size , self.units ) : by 1 . If ` keepdims ` is ` True ` , if original_backend not in uses_correlation : p_list.append ( grad_parameter_dict [ g ] ) self.kernel = self.add_weight ( shape= ( input_shape [ -1 ] , self.units ) , y._keras_shape = tuple ( kshape ) output , new_states = step_function ( super ( Wrapper , self ) .__init__ ( * * kwargs ) return gain * np.concatenate ( @ pytest.mark.skipif ( ( K.backend ( ) == 'cntk ' and KC.dev.type ( ) == 0 ) , elif is_sparse_categorical_crossentropy : ` [ ( input_dim , output_dim ) , ( output_dim , output_dim ) , ( output_dim , ) ] ` . If ` output_padding ` is specified : # run keras backend init to initialize backend config pool_size = ( 1 , 1 ) + pool_size unknown = index raise TypeError ( ' ` get_update ` call received more arguments ' return cond_float * then_expression + ( 1 - cond_float ) * else_expression exp = x * k.exp ( x ) set ` x [ 0 , 3 , : ] = 0. ` and ` x [ 2 , 5 , : ] = 0. ` with keras.utils.CustomObjectScope ( layers_for_depth.sort ( key=lambda x : layer_indices [ x ] ) def add ( inputs , * * kwargs ) : self.states = [ K.zeros ( get_tuple_shape ( dim ) ) within sequences . For rate ` r ` , timesteps state_size.append ( cell.state_size ) # bring x 's dimension to be reduced to last axis . callbacks.model.stop_training = False if 0 < self.rate < 1 : `` `` '' Calls the ` on_train_batch_begin ` methods of its callbacks . def test_ask_to_proceed_with_overwrite ( ) : kernel_regularizer=None , The softsign activation : ` x / ( abs ( x ) + 1 ) ` . input_shape = x.shape assert dot.get_node ( layer_id ) for go_backwards in [ True , False ] : If None , it will default to ` pool_size ` . self.queue = queue.Queue ( max_queue_size ) return self.evaluate_generator ( the linear transformation of the inputs . if not doc : updated_y_true = K.switch ( is_binary , save_img.__doc__ = image.save_img.__doc__ max_queue_size : queue size unique_names = [ ] 2D tensor with shape : ( 'separable_conv2d ' , ( 1 , 6 , 5 , 3 ) , ( 3 , 4 ) , 1 , 'valid ' , 'channels_last ' ) , 'CNTK backend : assign ops argument % s ' self.bias = self.add_weight ( if cache_dir is None : size : int , or tuple of 2 integers . metrics= [ keras.metrics.SparseCategoricalCrossentropy ( ) ] ) return image_shape pool_size= ( 2 , 2 , 2 ) , border_mode='valid ' , name='avgpooling3d ' ) nested_states.append ( states [ : len ( cell.state_size ) ] ) except URLError as e : new_layer = keras.layers.Embedding ( output_dim=2 , input_dim=4 , name='d ' ) merge repeated classes in the output beams . raise ValueError ( 'Output of generator should be ' losses.mean_squared_error , losses.binary_crossentropy , y1 = KNP.separable_conv ( x , depthwise , pointwise , check_single_tensor_operation ( 'permute_dimensions ' , ( 4 , 2 , 3 ) , WITH_NP , return tf.sign ( x ) elif data_format == 'channels_last ' : self.depthwise_regularizer = regularizers.get ( depthwise_regularizer ) x = getattr ( C , reduce_fun_name ) ( x , a ) `` `` '' Returns whether the ` targets ` are in the top ` k ` ` predictions ` . # imports for backwards namespace compatibility if not self.trainable : class SparseTopKCategoricalAccuracy ( MeanMetricWrapper ) : The static batch size of a Layer . indices_for_conversion_to_dense = [ ] raise ValueError ( ' ` file_io_module ` must be provided for mocking ' ) return result Node ( self , 3D tensor with shape ` ( batch , cropped_axis , features ) ` return variable ( np.ones ( shape ) , dtype , name ) steps=validation_steps , self.dtype = dtype or K.floatx ( ) # self.input_spec if not len ( self._inbound_nodes ) > node_index : placeholders , and we expect Numpy data to be fed for them ' ` state_size ` attribute . ' @ abc.abstractmethod x = tf.squeeze ( x , [ spatial_start_dim ] ) # Globally-importable utils . is_weighted=True ) ) ' state values . Input received : ' `` `` '' Prints ` message ` and the tensor value when evaluated . True if a match was found and an archive extraction was completed , raise RuntimeError ( 'You must compile a model before ' if overwrite == ' n ' : 'steps ' : steps , constraint=self.kernel_constraint ) 'pass inputs that have a static shape . ' # if obj is any numpy type invalid_keys , list ( ConfusionMatrix ) ) ) K.is_tensor ( v ) for v in y ) : if not isinstance ( x , ( float , int ) ) : and weights file . self.unit_forget_bias = unit_forget_bias Legal arguments are the arguments of ` Sequential.evaluate ` . fpath = os.path.join ( path , 'data_batch_ ' + str ( i ) ) from tensorflow.keras.activations import exponential if [ [ `` $ MODE '' == `` TF2 '' ] ] ; then x = C.transpose ( x , ( 1 , 2 , 3 , 0 ) ) return C.user_function ( ReshapeBatch ( x , shape [ 1 : ] ) ) if int_shape is None : from .losses import mean_squared_error `` `` '' Evaluates the value of a tensor . x = tf.transpose ( x , ( 0 , 2 , 3 , 1 ) ) # NCHW - > NHWC from keras_preprocessing import sequence assert layer_name in created_layers recurrent_regularizer='l1 ' , filename='test_saving_overwrite_option_gcs.h5 ' ) for t , z in zip ( t_list , z_list ) : parallel_model.compile ( loss='mse ' , optimizer='rmsprop ' ) super ( Hinge , self ) .__init__ ( hinge , name=name , reduction=reduction ) from tensorflow.keras.optimizers.schedules import * `` ` python In the snippet below , there is ` # classes ` floating pointing values per Add layer to tensor history Whether the images will be converted to have 1 or 3 color channels . self.ndim = ndim out1 = tensors [ 0 ] * tensors [ 1 ] third positional argument should be the ` overwrite ` option indicating Same type and shape as ` initializer ` Otherwise , it follows : return self.cell.recurrent_dropout num_classes = y_pred.shape [ -1 ] for i in range ( len ( node.inbound_layers ) ) : strides : An integer or tuple/list of 3 integers , raise ValueError ( 'Rank of ` condition ` should be less than or ' node_index = 0 # VARIABLE MANIPULATION output_shape = ( input_shape [ 0 ] , self.units ) # Only test keras ' modules from .core import ActivityRegularization FALSE_POSITIVES = 'fp ' batch_logs [ 'outputs ' ] = batch_outs # # Getting started : 30 seconds to Keras axis : Position where to add a new axis . max_value = K.cast_to_floatx ( max_value ) # dimensions are batch x time x categories if not batch_input_shape : def ctc_batch_cost ( y_true , y_pred , input_length , label_length ) : check_two_tensor_operation ( 'in_train_phase ' , ( 3 , 3 ) , ( 2 , 2 ) , WITH_NP , steps_per_epoch=steps_per_epoch , one of ` channels_last ` ( default ) or ` channels_first ` . For example , if ` y_true ` is [ [ 0 , 0 , 1 ] , [ 0 , 1 , 0 ] ] and ` y_pred ` is raise ValueError ( 'An Input layer should be passed either ' must be broadcastable to the shape of ` x ` ' add a dummy batch dimension to your ' _is_tf_1 ( ) ) : verbose=1 , kernel_size : An integer or tuple/list of single integer , reduction_axes , epsilon=1e-3 ) : x : Keras variable or tensor . elif self.stateful : ( default format for temporal data in Keras ) if abs ( avg ) > 1e-3 : def _merge_function ( self , inputs ) : x = tf.expand_dims ( x , 1 ) def test_rnn_cell_identity_initializer ( layer_class ) : .format ( len ( layer_names ) , len ( filtered_layers ) ) All others will be averaged in ` on_epoch_end ` . if loss is not None : dilation_rate : tuple of 3 integers . ( which acts on each input channel separately ) if not hasattr ( T.nnet.bn , 'batch_normalization_test ' ) : def broadcast_weights ( values , sample_weight ) : return True def load_attributes_from_hdf5_group ( group , name ) : def __init__ ( self , units , * * kwargs ) : # and sum of it from two input tensors from tensorflow.keras.layers import GaussianNoise > > > kvar_tile = K.tile ( K.eye ( 2 ) , ( 2 , 3 ) ) size = K.concatenate ( [ size , input_shape ] , axis=0 ) from .load_backend import update_sub def __init__ ( self , inputs , outputs , updates= [ ] , name=None , * * kwargs ) : attributes are added . length = None loss_fn = losses.get ( 'mse ' ) return variable ( v , dtype=dtype , name=name ) model_inputs.append ( [ layer.name , new_node_index , tensor_index ] ) for mean , std in [ ( 0. , 1 . ) , ( -10. , 5 . ) ] : kernel_initializer='normal ' , class_mode='categorical ' , def test_elementwise_operations ( self ) : if node.arguments : alpha_constraint : constraint for the weights . ( 'dim_ordering ' , 'data_format ' ) ] , check_single_tensor_operation ( 'elu ' , ( 4 , 10 ) , WITH_NP , alpha=0.5 ) `` `` '' Repeats the input n times . ` ( batch_size , spatial_dim1 , spatial_dim2 , spatial_dim3 , channels ) ` current_uses_correlation = uses_correlation [ K.backend ( ) ] num_values = K.cast ( K.size ( values ) , self.dtype ) name : ( optional ) String , name for the foldl node in the graph . from tensorflow.keras.initializers import * self._set_cell ( cell ) b_y = np.random.random ( ( num_samples , output_dim_b ) ) self.loss_functions , if file_hash is not None : val_function = self.test_function 'when using TensorFlow 2.0 . ' ) You can specify the initial state of RNN layers numerically by `` `` '' Saves the model to a single HDF5 file . mean_squared_logarithmic_error , name , dtype=dtype ) fused_batch_norm = tf.nn.fused_batch_norm for sublayer in layer.layers : if target is None : dot.add_node ( node ) dtype='int32 ' ) ) the ` len ( generator ) ` as a number of steps . x_shape = ( 20 , 10 , 6 ) def load_data ( path='imdb.npz ' , num_words=None , skip_top=0 , class _Pooling1D ( Layer ) : num_classes : Integer , number of classes to consider . # node data that specifies a layer call . ValueError : if shape1 and shape2 are not compatible for check_single_tensor_operation ( 'mean ' , ( 4 , 2 ) , WITH_NP ) f [ 'y2 ' ] = ( b'asd ' , b'sdf ' , b'dfg ' ) `` `` '' Retrieves the output mask tensor ( s ) of a layer at a given node . from .load_backend import epsilon output = unpack_singleton ( output_ls_copy ) The state of the optimizer , allowing to resume training 'and thus has no defined output shape . ' ) if is_model ( layer ) : self.output_names , sample_weight_mode , skip_target_weighing_indices ) h_tm1_o = h_tm1 ) from .common import set_floatx > > > kvar = K.ones ( ( 3,4 ) ) # now : model.output_shape == ( None , 64 , 32 , 32 ) raise ValueError ( 'Unable to import backend : ' + str ( _BACKEND ) ) permutation = output_dimensions [ : axis_without_batch ] ValueError : in case of invalid ` model ` argument value . that will be applied to the last 2 dimensions of if k not in self.stateful_metrics : 'state . As a result , your model is ' def __init__ ( self , axis=0 ) : from keras.preprocessing.sequence import _remove_long_seq current_layout = [ i for i in range ( dims ) ] out_pad_w ) node_index : Node index from which ` tensor ` comes from . model.compile ( 'sgd ' , loss=keras.losses.CategoricalCrossentropy ( ) ) def zeropadding2d_args_preprocessor ( args , kwargs ) : ' { } only supports wrapping of FileIO for ` mode ` `` rb '' or `` wb '' ' ) from .. utils.generic_utils import func_load while chunk_attr in self.data.attrs : elif len ( names ) == 1 and isinstance ( data [ 0 ] , ( float , int ) ) : h = K.bias_add ( h , self.bias ) # Instantiating HDF5Matrix for the training set , if x is None : def glorot_uniform ( seed=None ) : out_pad_d , out_pad_h , out_pad_w = self.output_padding embeddings_regularizer : Regularizer function applied to if K.backend ( ) ! = 'tensorflow ' : self.inputs = to_list ( inputs , allow_tuple=True ) # then instantiate a new model from inputs and outputs . return th_sparse_module.dense_from_sparse ( tensor ) # Add output loss metric names to the metric names list . m.update_state ( [ 0. , 1. , 1 . ] , [ 1. , 0. , 1 . ] ) finished_nodes , target_mean=0. , target_std=1 ) weight_values = preprocess_weights_for_loading ( beta_2 : float , 0 < beta < 1 . Generally close to 1 . * * kwargs ) x = layers.Dense ( 2 ) ( inputs ) 'The weights for loading have shape ' if data_format='channels_last ' if hasattr ( self , 'metrics ' ) : dtype : Placeholder type . if hasattr ( layer , 'updates ' ) : y2 = K.eval ( getattr ( K , op ) ( input_filter = input_shape [ 1 ] layer_group = model_weights_group [ layer.name ] assert_allclose ( z1 , z2 , atol=atol ) for layer_data in config [ 'layers ' ] : from pathlib2 import Path Mode 1 will structure its operations as a larger number of `` `` '' Accumulates statistics and then computes metric result value . '' '' '' y = K.reshape ( y , new_shape ) output_shape = ( output_shape [ 0 ] , while i < shape [ 1 ] : output_slice = expand_dims ( outputs [ i ] , 1 ) 'no longer exists . ` mode=1 ` and ` mode=2 ` ' @ classmethod def process_layer ( layer_data ) : ins_batch [ i ] = ins_batch [ i ] .toarray ( ) x_f = self.input_conv ( inputs_f , self.kernel_f , self.bias_f , class GlobalMaxPooling1D ( _GlobalPooling1D ) : ` bytes ` data ( e.g . ` io.BytesIO ` ) . check_single_tensor_operation ( 'max ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) input data and the model 's expectations , @ pytest.mark.skipif ( K.backend ( ) ! = 'cntk ' , reason='Specific to CNTK . ' ) by how often they occur ( in the training set ) and only int_A^B { Precision.dP } = TP_B - TP_A + intercept * log ( P_B / P_A ) { 'go_backwards ' : False , 'mask ' : mask } , f_stats.append ( C.unpack_batch ( l_s ) ) broadcast_shape = [ -1 , input_shape [ steps_axis ] , 1 ] config = { ' n ' : self.n } labels_test = labels_test [ indices ] zero_grad = k.gradients ( loss + zero_loss , [ exp ] ) shape.insert ( axis , 1 ) axes = [ x_ndim - 1 , y_ndim - 1 ] x = layers.TimeDistributed ( layers.Dense ( 3 , activation='softmax ' ) ) ( x ) generator = kwargs [ 'generator ' ] return _deserialize_model ( H5Dict ( h5file ) , custom_objects , compile ) # # ` loss_weights ` does not match outputs . sample_weight_mode=None , in the relevant direction and dampens oscillations . def test_module_name ( ) : if None in x._keras_shape [ 1 : ] : if name in [ 'batch ' , 'size ' ] : # Optionally , the first layer can receive an ` input_shape ` argument : seed : A Python integer . Used to seed the random generator . for i in range ( 2 , 9 , 2 ) : Or generate predictions on new data : # not in the ` losses ` module , then it is a user-defined loss outputs = layers.ReLU ( max_value=6 ) ( inputs ) for i , j in zip ( shape1 [ -len ( shape2 ) : ] , shape2 ) : # TH kernel shape : ( depth , input_depth , ... ) class MeanAbsolutePercentageError ( MeanMetricWrapper ) : config = { 'axis ' : self.axis } A layer instance . one_hot = text.one_hot # Argument output_masks= [ None ] , out = model.fit ( input_a_np , [ output_a_np ] ) self.target_shape = tuple ( target_shape ) [ 0. , 0. , 0. , 0 . ] ] , dtype=float32 ) input_h=input_h , pool_mode : string , ` `` max '' ` or ` `` avg '' ` . if len ( bias_shape ) == 1 : x_shape = ( 1 , 3 ) + shape original_keras_version = ' 1 ' if len ( all_input_shapes ) == 1 : reason='Requires TF . ' ) elif y.ndim == 4 : # deserialized ValueError : if a cycle is detected . alpha : scalar for layer in self.inbound_layers : # Default backend : TensorFlow . return ( input_shape [ 0 ] , self.output_dim ) if output_shape_type == 'function ' : from tensorflow.keras.activations import get from .load_backend import flatten f = H5Dict ( Path ( h5_path ) , mode= ' r ' ) `` `` '' Sets the parameters of this estimator . if len ( params ) ! = len ( weights ) : from tensorflow.keras.applications.vgg16 import preprocess_input from keras_preprocessing import text ( self.metrics_func.outputs [ 0 ] , ) , return batch_size for layer in model.layers : self.momentum ) , assert len ( model.weights ) == 0 cbk.validation_data = val_inputs self.write_grads = write_grads names , return C.element_min ( x , y ) img : PIL Image instance . ] : `` `` '' Creates the layer weights . ` y_true ` , and must be broadcastable to ` y_true ` ( i.e. , all dimensions must seed : A Python integer to use as random seed . of different model outputs . update_ops = [ ] if beta is None : batch_size=batch_size , self.add_loss ( regularization_losses , tf_data_format = 'NCHW ' timesteps = input_shape [ 1 ] layer.build ( ( None , None , input_size ) ) def _preprocess_conv2d_input ( x , data_format ) : K.pool2d ( x , pool_size=pool_size , padding='twice ' ) if not input_shape and not batch_input_shape : o = self.recurrent_activation ( x_o + h_o ) return K.spatial_2d_padding ( inputs , input_dim=3 , from tensorflow.keras.layers import LocallyConnected2D squeeze = np.squeeze dilation_rate = ( dilation_rate , ) * ( x.ndim - 2 ) return tf.reduce_all ( x , axis , keepdims ) # Example if is_sequence ( val_data ) : nD tensor with shape : ` ( batch_size , ... , input_dim ) ` . : expected_height ] if self.distribution == 'normal ' : yield temp_fname def _get_training_eval_metrics ( self ) : yield x initial_state = None str_val = str ( value ) zca_whitening=zca_whitening , filter_length=3 , tensors : list of tensors to concatenate . _ , wh = parse_shape_or_val ( ( output_dim , output_dim ) ) def test_simplernn_legacy_interface ( ) : h5dict = H5Dict ( state , mode= ' r ' ) name='test_function ' , 'the same number of samples as target arrays . ' from contextlib import contextmanager out = T.dot ( x , y ) slice_col = py_slice ( j * stride_col , 'stateful ' : self.stateful , if self.save_best_only : size : Integer , total size of the data to slice into batches . by_name : Boolean , whether to load weights by name K.conv3d ( dummy_x_3d , dummy_w_3d , data_format='channels_middle ' ) should * not * be averaged over time . Metrics in this list for layer in layers : # install mkdocs ( 'b_constraint ' , 'bias_constraint ' ) , if input_length == 1 : ( e.g . set this to adapt the display to different _callbacks = [ cbks.BaseLogger ( stateful_metrics=model.metrics_names [ 1 : ] ) ] def sparse_categorical_crossentropy ( y_true , y_pred , from_logits=False , axis=-1 ) : # instead of the 2nd one . z_list = [ ] which will be filled with the values of ` epoch ` and 5D tensor with shape : self.bias_h = self.bias [ self.units * 5 : ] that is not a multiple of the batch size . ValueError : If the index is does not match any node . % ( str ( tensor.shape ) , str ( value.shape ) ) ) def ones ( shape , dtype=None , name=None ) : import time each with shape ` ( batch_size , units ) ` . kernel_h , featurewise_std_normalization=False , reason='TF-specific implementation . ' ) def test_rnn_state_num_dim_larger_than_2_masking ( self ) : class GlobalAveragePooling3D ( _GlobalPooling3D ) : denom = K.maximum ( self.true_positives [ 1 : ] + self.false_negatives [ 1 : ] , 0 ) print ( ' # # # # # # # test multi-io model ' ) new_layer_1 = keras.layers.UpSampling1D ( size=3 , name='us1d ' ) class EarlyStopping ( Callback ) : _x = x [ : , : , rep : ( rep + 1 ) ] ndim def flow_from_directory ( self , assert K.get_session ( ) .run ( fetches= [ x , y ] ) == [ 20. , 30 . ] def add_edge ( dot , src , dst ) : assert len ( K.eval ( t ) ) == 0 `` `` '' Returns whether ` x ` is a Keras tensor . param_dset = g.create_dataset ( name , val.shape , kernel_size = ( args [ 2 ] , args [ 3 ] , args [ 4 ] ) } model.compile ( loss='categorical_crossentropy ' , optimizer='adam ' ) compiled . Otherwise , the model is uncompiled and # Raises : `` `` '' Stops running threads and wait for them to exit , if necessary . url : url to retrieve . If this is ` True ` then all subsequent layers dense.input self._per_input_losses = { } if alpha ! = 0. : recompute_steps_per_epoch = use_sequence_api and steps_per_epoch is None var = _reshape_dummy_dim ( var , [ 0 ] ) delta_t_median > 0.95 * self._delta_t_batch and ( 'depthwise_conv2d ' , ( 1 , 6 , 5 , 3 ) , ( 3 , 4 , 3 , 2 ) , 'valid ' , 'channels_last ' ) , original_backend , names : List of strings . def print_summary ( model , line_length=None , positions=None , print_fn=None ) : # in case of layer shared at different topological depths ( 0.0 , 5.0 , 0.8 ) , # set max_value and threshold outputs [ 1 ] ) if len ( outputs ) > 1 else ( w = np.transpose ( w , ( 3 , 4 , 0 , 1 , 2 ) ) self.outputs = None inputs = inputs [ 0 ] 'Will using CNTK 2.0 GA as default . ' ) kernel_constraint=kernel_constraint , if batch_index == len ( batches ) - 1 : # Last batch . target_shape = [ ] def __init__ ( self , l1=0. , l2=0 . ) : output_shape = input_shape + ( num_classes , ) `` `` '' Reduces the individual weighted loss measurements . '' '' '' # # ` sample_weight_mode ` does not match output_names . 'Found shape : ' , input_shape ) recurrent_activation='hard_sigmoid ' , self.writer.writerow ( row_dict ) shape , init=C.initializer.truncated_normal ( outs [ i ] += float ( batch_out ) * len ( batch_ids ) > > > K.int_shape ( xy_batch_dot ) go_backwards=go_backwards ) # note that the .build ( ) method of subclasses MUST define if sys.version_info < ( 3 , ) : # If the layer returns tensors from its inputs , unmodified , from .. utils.generic_utils import to_list from tensorflow.keras.layers import multiply from .load_backend import truncated_normal if 0 < self.dropout + self.recurrent_dropout : y_true = K.clip ( y_true , K.epsilon ( ) , 1 ) ' is incompatible with layer ' dtp = self.true_positives [ : self.num_thresholds if batch_index == 0 : def on_epoch_end ( self ) : super ( _Conv , self ) .__init__ ( * * kwargs ) feed_sample_weight_modes = [ None for _ in self.outputs ] def __contains__ ( self , key ) : def get_json_type ( obj ) : return ( input_shape [ 0 ] , out_filters , new_space [ 0 ] , new_space [ 1 ] ) parallel_model.fit ( x , y , epochs=1 ) weight_value_tuples.append ( ( p , w ) ) if self.verbose > 0 : source = 'CuDNNLSTM ' # convert CuDNN layers 'an input_tensor argument , ' # old : ( filters , stack_size , ... ) f_stats = [ ] < tf.Tensor 'transpose_4:0 ' shape= ( 3 , 2 ) dtype=float32 > return isinstance ( x , tf.Tensor ) and hasattr ( x , 'op ' ) _DISABLE_TRACKING.value = prev_value of the optimizer ( i.e . it should match the return [ outputs ] + new_states py_all = all model_config = json.dumps ( model_config , default=get_json_type ) 'the layer received both ' y_train = np.array ( y [ : int ( len ( x ) * ( 1 - test_split ) ) ] ) If True , add 1 to the bias of the forget gate at initialization . ( 'W_constraint ' , 'kernel_constraint ' ) , self.supports_masking = False if not shape : 'is not static . If you want to run ' return ( ( input_shape [ 0 ] , ) return tf_math_ops.cumsum ( x , axis=axis ) if hasattr ( first_layer , 'batch_input_shape ' ) : The weight file has : # T.nnet.bn.batch_normalization_train is deprecated name = str ( w.name ) raise ValueError ( 'Layer # ' + str ( k ) # duplication . inputs = keras.layers.Input ( shape= ( None , input_size ) ) ' element ( s ) . ' ) return layer._batch_input_shape , layer.dtype Result computation is an idempotent operation that simply calculates the metrics = self._get_training_eval_metrics ( ) model_layers = model.layers def _get_shape_tuple ( self , init_tuple , tensor , start_idx , int_shape=None ) : # new : ( kernel_rows , kernel_cols , stack_size , filters ) new_lr = max ( new_lr , self.min_lr ) def antirectifier_output_shape ( input_shape ) : str ( identifier ) ) from .pooling import GlobalAvgPool1D assert 'metric_2 ' in names if 'keras ' not in str ( member.__module__ ) : if out_pad > = stride : gcs_filepath = file_io_proxy.get_filepath ( a tuple ` ( inputs , targets ) ` def get_updates_for ( self , inputs=None ) : import re np_value = value * np.ones ( shape ) `` `` '' Base class to enqueue inputs . output_shape.append ( None ) categorical_targets = np.array ( [ [ 1 , 0 ] , [ 1 , 0 ] , [ 0 , 1 ] , [ 0 , 1 ] ] , continue if output_mask_int_shape is None : elif key in ( 'majoring ' , 'Majoring ' ) : x_shape = ( 2 , 3 ) + shape from tensorflow.keras.layers import AveragePooling1D An output shape tuple . `` `` '' Logarithm of the hyperbolic cosine of the prediction error . module_objects=globals ( ) , file_format : Optional file format override . If omitted , the loss.__name__ == 'categorical_crossentropy ' ) or 'If using HDF5 input data , ' y = T.all ( x , axis=axis , keepdims=keepdims ) inputs_vals = np.random.random ( ./.travis/install_cntk.sh ; ` x ` and ` y ` are data in batches , i.e . in a shape of node_index : Integer , index of the node assert len ( model1.losses ) == 1 old_layer = keras.layers.GlobalAveragePooling2D ( dim_ordering='th ' , in datapath steps_per_epoch=None , 'sample_weight_mode= '' temporal '' ) is restricted to ' model.save ( 'my_model.h5 ' ) # creates a HDF5 file 'my_model.h5 ' return C.equal ( x , y ) config = layer.get_config ( ) from .merge import concatenate class Softmax ( Layer ) : indices = np.arange ( len ( x_train ) ) data.extend ( [ n.decode ( 'utf8 ' ) batch_size = x [ 0 ] .shape [ 0 ] x , None , None , reduction_axes='per-activation ' ) x_squashed = False ' with different batch sizes . ' ) output._keras_shape = tuple ( output_shape ) def test_fit_generator ( ) : x += reshape ( bias , ( 1 , bias_shape [ 0 ] , 1 ) ) value : float ; the value of the generator tensors . return T.ge ( targets_values , predictions_k ) allowed_positional_args= [ ] , 'hence the notion of `` layer output '' ' for n in model.output_names : true positives . This metric creates one local variable , ` accumulator ` x_shape [ int ( axis ) ] not in { value , None } ) : raise ValueError ( 'Input should be at least 3D . ' ) first bucket having > 0 true positives . if training_config is None : elif ( not isinstance ( loss_fn , losses.Loss ) or noise_shape=list ( val.shape ) ) ) custom_loss = losses.mse num_train_samples = 50000 Number of samples per evaluation step . raise NotImplementedError new_shape_temp.append ( _ ) label_mode : one of `` fine '' , `` coarse '' . ndim_expr = ndim ( then_expression ) if len ( n ) < len ( shape ) : # Padding the axis `` `` '' Gets batch at position ` index ` . self.monitor_op = np.less def conv3d ( x , kernel , strides= ( 1 , 1 , 1 ) , from .load_backend import gradients def multi_gpu_application_folder_generator_benchmark ( ) : y = self.layer.call ( inputs , * * kwargs ) os.remove ( h5_path ) # Prepare inputs , delegate logic to ` test_loop ` . elif bias_shape == ( 2 , units * n_gates ) : z_list = [ b.eval ( b.in_top_k ( b.variable ( predictions , dtype='float32 ' ) , output_shape , recurrent_z = K.bias_add ( recurrent_z , self.recurrent_bias_z ) if all ( [ x in kwargs for x in [ 'kernel_dim1 ' , 'kernel_dim2 ' , 'kernel_dim3 ' ] ] ) : if ( isinstance ( inputs , ( list , tuple ) ) self.recurrent_kernel [ : , self.units * 2 : self.units * 3 ] ) return inception_v3.decode_predictions ( * args , * * kwargs ) output = repeat_elements ( output , width_factor , axis=axis_2 ) self.trainable = False raise ValueError ( 'Sample_weight arrays should have ' return -sum ( target * C.log ( output ) , axis=-1 ) for o , p in zip ( new_states , place_holders ) : dimension indices , e.g . [ 0 , 2 , 1 ] . from collections import Iterable use_multiprocessing=False , layers_for_depth = layers_by_depth [ depth ] Example 2 - Training models with weights merge on CPU using cpu_relocation is_weighted=False ) : padding=padding , return ( input_shape [ 0 ] , ) + self._fix_unknown_dimension ( label_shape = tf.shape ( labels ) return self.stop_signal is not None and not self.stop_signal.is_set ( ) 'However the new layer ' + layer.name def categorical_hinge ( y_true , y_pred ) : assert os.path.isdir ( os.path.join ( tmpdir , 'layers ' ) ) def get_monitor_value ( self , logs ) : At least one of { ` shape ` , ` ndim ` } must be specified . to be considered during deserialization . if mask_i is None : for a in axis : class ModelCheckpoint ( Callback ) : # Do n't reset optimizer if already set . bias_regularizer : Regularizer function applied to the bias vector A boolean indicating if the current device 'the notion of `` output shape '' is ' # # ` sample_weight_mode ` matches output_names partially . * * kwargs ) update_ops = self._output_loss_metrics [ i ] .update_state ( If the model has multiple outputs , you can use a different loss loss_name = loss_type.__name__ 'threshold ' : self.threshold finished and starting the next epoch . It should typically _step , ( instead of topological weight loading ) . x = K.placeholder ( shape=shape ) old_layer = keras.layers.AvgPool1D ( 2 , padding='valid ' , name='d ' ) def test_lstm_legacy_interface ( ) : If the number of dimensions is reduced to 1 , we use ` expand_dims ` to `` `` '' 3D convolution layer ( e.g . spatial convolution over volumes ) . from tensorflow.keras.layers import Embedding obj.update_state = types.MethodType ( return ( x , x + increment ) 'the batch size by passing a ' self.axis = axis model.predict ( x ) 'padding ' : self.padding , axes [ i ] = input_shape [ i ] Keras was initially developed as part of the research effort of project ONEIROS ( Open-ended Neuro-Electronic Intelligent Robot Operating System ) . tensor : The tensor to start from . def units ( self ) : rank=1 , model = _make_nested_model ( input_shape , layer , model_nest_level , model_type ) # Arguments Reduction.validate ( reduction ) super ( GRUCell , self ) .__init__ ( * * kwargs ) in the output sequence , or the full sequence . assert len ( layer.get_losses_for ( x ) ) == 1 rows = conv_utils.conv_output_length ( rows , self.kernel_size [ 0 ] , val_size = val_data [ 0 ] .shape [ 0 ] from_logits : ( Optional ) Whether ` y_pred ` is expected to be a logits tensor . if len ( computed_data ) == 1 : new_model_gcs = load_model ( gcs_filepath , * * load_kwargs ) the axes to compute the logical and . If ` None ` ( default ) , computes 'top_pad ' : 1 , output = x for v in nodes_by_depth : `` `` '' Create a generator that iterate over the Sequence . '' '' '' ValueError : if using an even kernel size with padding 'same ' . class LossFunctionWrapper ( Loss ) : self.filters = filters model.compile ( loss='mse ' , optimizer='sgd ' ) [ Adam - A Method for Stochastic Optimization ] ( raise ValueError ( 'When using causal padding in ` conv1d ` , ' weights=sample_weights [ i ] if sample_weights else None ) train_gen , test_doc_with_arguments_as_last_block , bar = barstr % current * * self._function_kwargs ) raise ValueError ( 'The initial state or constants of an RNN ' from .load_backend import spatial_2d_padding output = h + K.dot ( prev_output , self.recurrent_kernel ) filepath : String , path to the weights file to load . input_length = kwargs.pop ( 'input_length ' , None ) indicating how many dimensions to slice used for the linear transformation of the inputs mask=mask , # Note that the order of the 2 first positional arguments x1 = keras.layers.Dense ( 8 , activation='relu ' ) ( input1 ) state_size += list ( cell.state_size ) W_regularizer='l1 ' , # Case 1 : generator-like . Input is Python generator , or Sequence object . str ( sample_weight.shape ) + ' . ' strides = ( 1 , 1 ) + strides * 2 regularization += self.l2 * K.sum ( K.square ( x ) ) def VGG16 ( * args , * * kwargs ) : shapes.append ( [ ( 4 , 3 , 2 ) , ( 4 , 3 , 2 ) , ( 4 , 3 , 2 ) ] ) info += ' % .4e ' % avg if unknown is None : original_backend , 'truncated_normal ' , pointwise_initializer : Initializer for the pointwise kernel matrix [ default ] or 'PR ' for the Precision-Recall-curve . new_layer = keras.layers.Cropping3D ( data_format='channels_last ' , name='c3d ' ) base_config = super ( _Pooling2D , self ) .get_config ( ) Bugs present in multi-backend Keras will only be fixed until April 2020 ( as part of minor releases ) . TypeError : if input tensors are not Keras tensors dummy_w_3d = K.variable ( np.ones ( ( 2 , 2 , 2 , 3 , 4 ) ) ) def test_uniform ( tensor_shape ) : strides = ( strides , ) # switch to channels_first to display if hasattr ( self.forward_layer , 'updates ' ) : bias_constraint='unit_norm ' , f [ 'x2 ' ] = u'abcd ' def minimum ( x , y ) : # now model.output_shape == ( None , 10 , 8 ) saving.save_weights_to_hdf5_group ( f , self.layers ) This is the TF v1 version . A subset of the functionality of timesteps . To introduce masks to your data , if not self.supports_masking : if top_k is not None : ' a LocallyConnected2D layer ' dilation_rate : integer dilate rate . keras_shape [ j ] = conv_utils.conv_output_length ( 'The last layer might not normalize predictions ' dilation_rate = ( dilation_rate , ) Supports multi-label output . normalized_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , def test_atrousconv2d_legacy_interface ( ) : with K.name_scope ( layer.name ) : # To get integer shape ( Instead , you can use K.int_shape ( x ) ) conversions= [ ( 'init ' , 'alpha_initializer ' ) ] ) self.var.assign_add ( ... ) . for j in range ( w.shape [ 2 + i ] - 1 ) : if gamma.dtype ! = tf.float32 : cropping : int , or tuple of 3 ints , or tuple of 3 tuples of 2 ints . preds = np.array ( self.model.predict ( x , * * kwargs ) ) layer.bias_h , input_shapes : list of input shape tuples . str ( len ( self.states ) ) + ' states , ' def test_learning_phase ( ) : K.zeros_like ( self.true_positives [ min_index ] ) ) for e in required_entries : batch_size = 1 If ` by_name ` is False ( default ) weights are loaded steps : Integer or ` None ` . This : requires that the ` cell.call ` method accepts the same keyword argument if cell.data_format == 'channels_first ' : thresholds=0.5 , while True : if spec.shape is not None : This must be in the half-open interval ` [ 0 , num_classes ) ` , where K.set_value ( state , np.zeros ( ( batch_size , self.units ) ) ) weights2 = saving.preprocess_weights_for_loading ( for _ in range ( num_states ) ] from .layer_utils import print_summary `` `` '' Utilities for real-time data augmentation on image data . __Masking__ output_row = conv_utils.conv_output_length ( input_row , self.kernel_size [ 0 ] , kernel_regularizer='l1 ' , `` `` '' Prints a summary of a model . self.batch_size = batch_size y.append ( np.concatenate ( _y , axis=0 ) ) op = _TfDeviceCaptureOp ( ) import requests class CategoricalHinge ( MeanMetricWrapper ) : self.when = when data_format : Image data format , model = keras.Model ( inputs=inputs , outputs= [ output1 , output2 ] ) def test_rnn_cell_with_constants_layer ( ) : ' expects ' + str ( len ( symbolic_weights ) ) [ 0.0 , 0.9 , 0.1 , 0.0 ] , # t=3 assert a._keras_shape == ( None , 32 ) kwargs [ 'steps_per_epoch ' ] = samples_per_epoch // generator.batch_size weights += cell.non_trainable_weights for name , output in zip ( self.model.metrics_names , outputs ) : ` ( samples , new_rows , new_cols , filters ) ` if data_format='channels_last ' . on_train_end : called at the end of model training . next epoch . When training with input tensors such as ( or list of input shape tuples , one tuple per output tensor ) . '- If using a Sequential model , ' input_length = tf.cast ( input_length , tf.int32 ) This callback writes a log for TensorBoard , which allows if ` return_sequences ` : 3D tensor with shape target_mean=0. , target_max=2 , target_min=-2 ) return deserialize_keras_object ( while any ( map ( lambda x : x.nbytes > HDF5_OBJECT_HEADER_LIMIT , chunked_data ) ) : return x [ tuple ( slices ) ] shuffle_pattern = [ ' x ' ] * ( x.ndim - 1 ) + [ 0 ] if self.summation_method == metrics_utils.AUCSummationMethod.INTERPOLATION : all_outs.append ( [ ] ) outputs = K.tile ( K.expand_dims ( inputs ) , [ 1 , 1 , 2 ] ) # in keras2 , need handle output shape in different format warnings.warn ( 'No training configuration found in save file : ' layer = deserialize_layer ( layer_data , exception_prefix='target ' ) y_col= '' class '' , output = batch_dot ( x_aggregate , kernel ) self._values [ k ] = [ v * ( current - self._seen_so_far ) , if ( self._delta_t_batch > 0. and g_prime = g / ( 1 . - m_schedule_new ) if 0 < self.dropout < 1. : 'amsgrad ' : self.amsgrad } # the type of float to use throughout the session . ' ( missing previous layer metadata ) . ' ) model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.Recall ( ) ] ) by 1 for each entry in ` axis ` . If ` keepdims ` is ` True ` , 'must come from ` keras.layers.Input ` . ' from .pooling import AveragePooling1D def int_or_none ( value ) : A Numpy array ( or array-like ) , or a list of arrays def call ( self , inputs , states , constants=None , * * kwargs ) : # Potentially redundant list , batch_outs = f ( ins_batch ) strides= ( 2 , 2 ) , ' , but the layer has only ' except AttributeError : weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 3 , 2 , 0 , 1 ) ) ref_params_value = keras.backend.get_value ( ref_params ) self._feed_sample_weight_modes = [ merge_repeated : if ` greedy ` is ` False ` , unique_name = name + '_1 ' `` `` '' Bitwise reduction ( logical OR ) . if accept_all and arg_spec.keywords is not None : loss_fn.fn == losses.sparse_categorical_crossentropy ) ) return self shape2.pop ( 0 ) input_masks= [ None for _ in self.inputs ] , 'dilation_rate ' : self.dilation_rate , ( ` while_loop ` or ` scan ` depending on backend ) . [ 0 , 0 ] , ins_batch = slice_arrays ( to build the model upon . If not provided , the ` recurrent_kernel ` weights matrix weights [ 0 ] = conv_utils.convert_kernel ( weights [ 0 ] ) # HIGH ORDER FUNCTIONS except TypeError : pattern= ( 2 , 0 , 1 ) ) x_train = x_train [ indices ] unconcatenated_outs.append ( [ ] ) Used to implement ` Sum ` , ` Average ` , etc . a node is added to ` layer._inbound_nodes ` . shape = ( 1 , 1 , bias.shape [ 0 ] ) def standardize_weights ( y , broadcast_beta = beta.dimshuffle ( ' x ' , 0 , ' x ' , ' x ' ) return data dtype : Tensor type . kernel = np.concatenate ( [ weights [ 0 ] , self.input_tensors = input_tensors const._keras_shape = shape # These properties should be set by the user via keyword arguments . Takes input tensor or list of tensors as first argument . raise ValueError ( 'You tried to call layer `` ' for x in self.outputs : v._keras_shape = int_shape ( value ) if spec.max_ndim is not None : raise ValueError ( 'Unknown floatx type : ' + str ( floatx ) ) return tuple ( output_shape ) initargs= ( seqs , ) ) x /= retain_prob # as sparse categorical crossentropy loss . assert K.dtype ( K.variable ( 1 , dtype=dtype ) ) == dtype if not isinstance ( constants , ( list , tuple , type ( None ) ) ) : If the weights were specified as [ 1 , 1 , 0 , 0 ] then the sum would be 4 . [ [ 1 ] , [ 3 ] , [ 5 ] , [ 7 ] , [ 9 ] ] ] ) ) d_axis , h_axis , w_axis = 1 , 2 , 3 name='moving_mean ' , base_config = super ( PReLU , self ) .get_config ( ) `` `` '' Returns the list of input tensors necessary to compute ` tensor ` . strides : Integer , tuple of 2 integers , or None . curve : ( Optional ) Specifies the name of the curve to be computed , 'ROC ' elif isinstance ( output_shape , ( tuple , list ) ) : ' ` % s ` type is not supported . ' kwargs [ 'name ' ] = args [ 1 ] after loading . `` `` '' He uniform variance scaling initializer . Wrappers take another layer and augment it in various ways . [ 4. , 5. , 6 . ] ] , dtype=float32 ) states = kept_states # Perform the call with temporarily replaced input_spec return T.min ( x , axis=axis , keepdims=keepdims ) if steps is None : batch_sizes = [ s [ 0 ] for s in input_shape if s is not None ] loss=keras.losses.categorical_crossentropy , inputs=to_list ( inputs ) ) # Prepare validation data . 'can only be used for 2D matrices . ' ) __stateful__ : Boolean ( default False ) . If True , the last state return self.cell.recurrent_regularizer dynamic_axis_index += 1 sequences with 128 features per step . ( ( 2 , 2 ) , 'valid ' , None , ( 3 , 5 , 6 , 4 ) ) , elif ndim ( gamma ) > 1 : dilation_rate = ( 1 , ) + dilation_rate __Work with Python__ . No separate models configuration files in a declarative format . Models are described in Python code , which is compact , easier to debug , and allows for ease of extensibility . dense.output for i in range ( axes [ 1 ] , 1 , -1 ) : ' generator based on the ` keras.utils.Sequence ` ' `` `` '' Element-wise exponential . # convert negative indices , - , / , * , += , -= , * = , /= config = super ( Conv3DTranspose , self ) .get_config ( ) assert ndim > = 3 , 'Input should be at least 3D . ' x_col= '' filename '' , tf_keras_backend.set_learning_phase ( value ) strides = pool_size assert min_val + 0.015 > np.min ( rand ) > = min_val A padded 5D tensor . # build an all-zero tensor of shape ( samples , output_dim ) img_to_array.__doc__ = image.img_to_array.__doc__ kwargs [ 'input_shape ' ] = input_shape ( 4 , 3 , 1 , 1 ) , WITH_NP ) x , y , sample_weights = self._standardize_user_data ( model._make_predict_function ( ) batch_size = self._validate_or_infer_batch_size ( batch_size , steps , x ) elif hasattr ( K , 'int_shape ' ) : group4 [ ' z ' ] = array if K.ndim ( x ) ! = spec.ndim : _ , x = parse_shape_or_val ( shape ) if self.target is not None : callback.set_params ( params ) # element_select approach as workaround . It may have with tmpdir.as_cwd ( ) : verbose : Verbosity mode , 0 ( silent ) , 1 ( verbose ) , 2 ( semi-verbose ) [ 1. , 0. , 1. , 0. , 1. , 0 . ] , new_layer = keras.layers.MaxPool3D ( ` ( batch_size , timesteps , units ) ` . set ` shared_axes= [ 1 , 2 ] ` . self.execute ( argument ) # Build a dict { depth : list of layers with this depth } is_binary = K.all ( are_different ) assert output_array.shape == ( 32 , 10 , 64 ) if function_name == 'get_value ' : axis : Axis along which to repeat . initializer = elems [ 0 ] outputs = layers.ReLU ( negative_slope=0.2 ) ( inputs ) self._non_trainable_weights.append ( weight ) [ 0. , 1. , 0. , 1. , 0. , 1 . ] ] , dtype=float32 ) the case with generators . `` `` '' Layer that subtracts two inputs . batch_size=batch_size , https : //ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf ) TensorFlow Optimizer instance orig_x_ndim = x_ndim steps_per_epoch=steps_per_epoch , data_format='channels_last ' , return T.exp ( x - xm ) / T.exp ( self.b = None The metric function . def add_loss ( self , losses , inputs=None ) : # cntk does not support gradients as symbolic op , new_p = p - lr * update elif function_type == 'lambda ' : class AveragePooling2D ( _Pooling2D ) : np.zeros ( get_tuple_shape ( self.cell.state_size ) ) ) config = { 'filters ' : self.filters , history = model.fit ( x_train , y_train , epochs=15 , batch_size=16 , shuffle=True , def variable ( value , dtype=None , name=None , constraint=None ) : data_format=None ) : raise ValueError ( ' ` mask ` should be a list . ' ) # If there is no bias we skip the conversion global uses_learning_phase ` ( batch , first_axis_to_crop , second_axis_to_crop , third_axis_to_crop , assert not a_2._uses_learning_phase return super ( KerasClassifier , self ) .fit ( x , y , * * kwargs ) warnings.warn ( 'Error in loading the saved optimizer ' new_layer = keras.layers.BatchNormalization ( name='bn ' ) dims : Tuple of integers . Permutation pattern , does not include the recommended to use method ` get_filepath ( filename ) ` in tests to make them training=training , if object_type == 'class ' : ' ` model.compile ` after ? ' ) ) check_single_tensor_operation ( 'hard_sigmoid ' , ( 4 , 2 ) , WITH_NP ) y = np.reshape ( y , y.shape [ : :-1 ] ) if total_size == -1 : 'Keras 2 API : ' + signature , stacklevel=2 ) data_format = backend.image_data_format ( ) shuffle=True , from_logits : Whether to interpret ` y_pred ` as a tensor of from keras_applications import resnet 'inbound_layers ' : inbound_names , ' has an input_spec attribute that ' if self.reverse_state_order : def add_unprocessed_node ( layer , node_data ) : for _ in range ( rep ) : 0 = silent , 1 = progress bar , 2 = one line per epoch . from keras.engine.training_utils import make_batches print_fn : Print function to use . output_masks = to_list ( 'to be a functional ` Model ` instance , ' self.bias_f , separable_conv1d = separable_conv is at index 1 , in ` 'channels_last ' ` mode is it at index 4 . permutation += [ len ( y_shape ) - 1 ] past_values = [ ] from tensorflow.keras.utils import Sequence if original_keras_version == ' 1 ' : and will raise an exception . In such cases , use mask : Binary tensor with shape ( samples , time ) , if inputs_hash not in self._per_input_losses : Apply a model copy on each sub-batch . Every model copy old_layer = keras.layers.Convolution2D ( 5 , 3 , nb_col=3 , name='conv ' ) mse_obj = losses.LossFunctionWrapper ( loss_fn , name=loss_fn.__name__ ) from .load_backend import resize_images Do not use this class as a layer , it is only an abstract base class . 'Please make sure that all of your ops have a ' h5file_ [ 'data ' ] = data inbound_layer = node.inbound_layers [ i ] self.sample_weights = sample_weights check_single_tensor_operation ( self.recurrent_kernel_o = self.recurrent_kernel [ : , : , : , self.filters * 3 : ] filter_shape = tuple ( int_or_none ( v ) for v in filter_shape ) if len ( u_list ) > 0 : ` output = activation ( dot ( input , kernel ) + bias ) ` ( ( 5 , 2 ) , ( 7 , 2 ) , 0 ) , trainable=False ) `` `` '' Bitwise reduction ( logical AND ) . any ` dilation_rate ` value ! = 1 . recurrent_initializer='uniform ' , training=training ) metrics_utils.ConfusionMatrix.FALSE_POSITIVES : self.false_positives , weight_col : string , column in ` dataframe ` that contains the sample assert cce_obj.reduction == Reduction.SUM # check that container-level reset_states ( ) works if self._is_compiled : high=maxval , return mobilenet.preprocess_input ( * args , * * kwargs ) params=self._collected_trainable_weights , normalizer = lambda x : x + 1 'h5py ' , self.layers [ -1 ] ._outbound_nodes = [ ] path : path where to cache the dataset locally ( Python floats ) to weight the loss contributions of different model inputs : Single array , or list of arrays . The arrays could be z_list = [ k.eval ( k.dropout ( k.variable ( val ) , level=0.2 ) ) if data_format ! = 'channels_last ' : assert np.abs ( z_list [ i ] .mean ( ) - z_list [ i + 1 ] .mean ( ) ) < 0.05 model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.Precision ( ) ] ) `` `` '' Returns the symbolic shape of a tensor or variable . `` `` '' Converts layers weights from Keras 1 format to Keras 2 . if not check_batch_axis : > > > keras_var = K.variable ( np_var ) where o_row and o_col depend on the shape of the filter and [ 0 , 1 , 0 ] , # B travis_retry pip install tf-nightly numpy scipy h5py pytest flaky pandas -- progress-bar off self.beta = None def _normalize_device_name ( name ) : for x in model.inputs : assert config == new_config self.csv_file.flush ( ) [ Getting started with the Sequential model ] ( https : //keras.io/getting-started/sequential-model-guide ) var.dimshuffle ( shuffle_pattern ) , https : //gist.github.com/jfsantos/e2ef822c744357a4ed16ec0c885100a3 from .load_backend import get_uid @ interfaces.legacy_gaussiannoise_support _axes = tuple ( axes ) w = np.insert ( w , 2 * j + 1 , 0 , axis=2 + i ) # Check arguments styling shape = ( bias.shape [ 3 ] , ) + bias.shape [ :3 ] from .load_backend import dtype # Do not do shape validation . # mask should have the same shape as score_array self.bias_initializer ( ( self.units * 2 , ) , * args , * * kwargs ) , batch_size : Size of the batches of data ( default : 32 ) . def evaluate_generator ( self , generator , self.kernel_f , width : Progress bar width on screen . max_ndim=None , def update_confusion_matrix_variables ( variables_to_update , If True , process the input sequence backwards and return the return T.abs_ ( x ) y = np.random.random ( ( num_samples , num_classes ) ) if not self.built : return volume_shape tensor_index = node.tensor_indices [ i ] 'keras.engine.saving.tf_file_io ' shape = bias.shape x : array-like , shape ` ( n_samples , n_features ) ` return super ( Recurrent , self ) .__call__ ( inputs , * * kwargs ) from keras.backend import theano_backend as KTH if ndim ( x ) == 5 : time what their inputs look like . g_shape = copy.copy ( then_expression.dense_shape ) shape , minval=minval , maxval=maxval , dtype=dtype , seed=seed ) model.add ( Reshape ( ( -1 , 2 , 2 ) ) ) return None elif isinstance ( x , list ) : data_format=data_format ) axes = [ axes ] computed_masks ) for data in datas : `` `` '' De-serializes a model serialized via _serialize_model condition = tf.tile ( condition , tile_shape ) 'It seems that you are using the Keras 2 ' def __init__ ( self , mean=0. , stddev=0.05 , seed=None ) : original_backend = None def DISABLED_test_training_and_eval_methods_on_backend_tensors_multi_io ( ) : assert input_shape and len ( input_shape ) == 2 force_transpose = True elif key in ( 'roc ' , 'ROC ' ) : output_masks = [ ] k_label_lens = K.variable ( label_lens , dtype= '' int32 '' ) # with pytest.raises ( ValueError ) : ' ` else_expression ` . ndim ( condition ) = ' recurrent_regularizer='l2 ' , x = K.placeholder ( ( 3 , None , 4 ) ) sample_weight=None , if self.use_steps : # Repeating ( 'dim_ordering ' , 'data_format ' ) , shape= ( output_length , self.filters ) , unprocessed_nodes [ layer ] = node_data_list [ node_index : ] filtered_layer_names = [ ] `` `` '' Called at the end of training . ` f ( x ) = x for x > = 0 ` . def convlstm2d_args_preprocessor ( args , kwargs ) : model.add ( layers.TimeDistributed ( layers.Dense ( 3 ) ) ) broadcast_gamma = T.reshape ( gamma , target_shape ) K.separable_conv2d ( dummy_x_2d , dummy_w_2d , dummy_w1x1_2d , ValueError : If ` y_pred ` and ` y_true ` have mismatched shapes , or if Tensor with first dimension equal to the elems and second depending on use_multiprocessing=use_multiprocessing ) The data should be at 2D , and axis 0 is expected training_config.get ( 'weighted_metrics ' ) ) rotation_range=40 , from .load_backend import max # on a 32x32 image with ` data_format= '' channels_last '' ` : reduction=Reduction.SUM , name='bce_1 ' ) val_function : Keras function to call for validation y_true , # T.nnet.bn.batch_normalization_test is deprecated test_backend = [ KTH , KTF ] # Theano inserts `` __str__ = `` for no good reason # Format the new states as a flat list for ( a , b ) in zip ( shape , n ) ] ) a bias vector is created and added to the outputs . Finally , if dim = x.shape [ axis ] def convert_custom_objects ( obj ) : cntk_dynamicity=True ) If omitted ( ` None ` ) , then ` backend.image_data_format ( ) ` is used . callbacks.set_params ( { x_shape_or_val , file_format=file_format , source_tensors.append ( x ) A Keras variable , filled with drawn samples . conflict_counter [ n ] += 1 # Current cntk does not support shape like ( 1 , batch ) . so using the workaround sub_w_last_node [ inbound_layer_name ] .get_name ( ) , following : https : //en.wikipedia.org/wiki/Sensitivity_and_specificity if 'mask ' not in kwargs : if original_shape [ rows ] is None : # squash trailing dimensions of y . def __setstate__ ( self , state ) : assert mae_obj.reduction == losses_utils.Reduction.SUM str ( symbolic_weights [ i ] ) from tensorflow.python.ops import math_ops as tf_math_ops input_size = 9 def append ( self , callback ) : # Returns target_tensors = [ target_tensors ] # if the second axis is static axis , CNTK will do unroll by default # toy label matrix ( 4 samples , 2 classes ) x_rep = tf.expand_dims ( x , axis=auxiliary_axis ) from .load_backend import slice old_layer = keras.layers.Convolution3D ( 5 , 3 , 3 , 4 , name='conv ' ) This function is called between epochs/steps , K.batch_set_value ( class _TfDeviceCaptureOp ( object ) : data_format , if use_sequence_api : return VarianceScaling ( scale=1. , output_shape_keys.append ( shape_key ) instead of an integer . input_shape = y.shape 'Layer ' + layer.name weights = self._trainable_weights + self._non_trainable_weights outs [ i ] [ batch_start : batch_end ] = batch_out if self.monitor_op == np.greater : A list ( one entry per model output ) of dicts . new_states : List of tensors , latest states returned by from .load_backend import cast where ` limit ` is ` sqrt ( 3 / fan_in ) ` check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=1 , `` { } { } are not style properly 'argument ' : documentation '' .format ( if ( weights_shape [ i ] is not None and # note that 'dtype ' , 'input_shape ' and 'batch_input_shape ' allowed_positional_args= [ 'size ' ] , return H5Dict ( val ) if inputs_hash not in self._per_input_updates : conv_out = K.bias_add ( conv_out , b , if self.scale : if x_batch_size is None and y_batch_size is None : raise ValueError ( 'Can not merge tensors with different ' def zeros ( shape , dtype=None , name=None ) : 'test_func ' , [ activations.softmax , np.argmax , lambda x : x * * 2 , lambda x : x ] ) raise ValueError ( 'Border mode not supported : ' , str ( padding ) ) self.inputs.append ( v ) self.on_predict_end ( ) The scaled exponential unit activation : ` scale * elu ( x , alpha ) ` . self.recurrent_kernel_i ) ) > > > x_batch = K.ones ( shape= ( 32 , 20 , 1 ) ) value : String or None . ` 'channels_first ' ` or ` 'channels_last ' ` . def get_output_at ( self , node_index ) : depth_multiplier : The number of depthwise convolution output channels exactly where you left off . mask , None , sample_weight ) ) if steps_per_epoch is None : from .load_backend import minimum updates= [ ( x , x_placeholder + 10 . ) ] , model = load_model ( 'my_model.h5 ' ) ( integer , indexed from 0 ) and current learning rate assert len ( model2.updates ) == 2 self.name = name from .callbacks import CallbackList skip_target_masks : Optional . List of boolean for whether the return trainable_weights + weights rank=rank , ' ` padding ` should be either an int , a tuple of 3 ints ' subsample= ( 2 , 2 ) , output = K.bias_add ( output , self.bias , data_format=self.data_format ) def normalize_batch_in_training ( x , gamma , beta , printable_module_name='object ' ) : z_list.append ( grad_eval_fn ( [ val ] ) [ 0 ] ) > > > from keras.utils.data_utils import _hash_file conv_out = T.nnet.conv2d ( x , kernel , sys.stdout.write ( '\n ' ) representation . eg. , When labels values are [ 2 , 0 , 1 ] , list ( padding [ 1 ] ) , def test_load_from_binary_h5py_direct_from_file ( ) : Therefore we check if we are not explicitly put on layer_group [ 'weight_names ' ] = weight_names # so we can just write the expression directly : if layer.__class__.__name__ == 'Bidirectional ' : data_format , # We need to use ` y ` to set the model targets . out = K.eval ( weighted_loss ( K.variable ( x ) , 'pool_size ' : self.pool_size , output = f ( [ 20 . ] ) dtype=dtype , seed=self.seed ) if isinstance ( n , tuple ) : if x_shape is not None : steps=None , def get_source_inputs ( tensor , layer=None , node_index=None ) : return self.cell.kernel_constraint ' % d dimension is not supported , at least ' time step . model = cls ( name=name ) def eye ( size , dtype=None , name=None ) : if set_inputs : updated = [ ] cntk_axis [ i ] = x.dynamic_axes [ dynamic_axis_index ] if self._is_started : interpolation : Interpolation method used to resample the image if the ` layer.get_input_shape_at ( node_index ) ` . class Iterator ( image.Iterator , utils.Sequence ) : return C.reshape ( x , new_shape ) if dtype is None : initial.append ( s ) output = th_sparse_module.basic.hstack ( tensors , format='csr ' ) bias_initializer = self.bias_initializer def to_json ( self , * * kwargs ) : # Input shapes num_gpus = gpus if random_seed is not None : ill-defined ( e.g . a shared layer with multiple input x = C.convolution ( pointwise_kernel , x , '3rd entry of padding ' ) kernel : kernel tensor . ' a ` batch_input_shape ` ' self._make_train_function ( ) std = 1 . sampling_rate : Period between successive individual timesteps y = backend.dot ( inputs , w_i ) + backend.dot ( h , w_h ) + c If None , no labels are returned # Sum up the areas of all the rectangles . weights : List of source weights values ( input kernels , recurrent y_dset [ : ] = y states_t = [ np.where ( state_mask [ : , t ] , state_t , state_tm1 ) x , gamma , beta , mean , var , reduction_axes , epsilon ) if _get_dynamic_axis_num ( s ) == 0 : @ pytest.mark.parametrize ( 'x_shape ' , [ ( 3 , 2 , 4 , 5 ) , ( 3 , 2 , 4 ) ] ) 'stack convolutional cells . ' ) if ( hasattr ( layer , 'activity_regularizer ' ) and `` `` '' Instantiates a placeholder tensor and returns it . return [ output_shape ] + state_shape be called during train mode . `` `` '' Adam optimizer . self.epoch.append ( epoch ) Whether the progress bar should config = config.copy ( ) nodes : list of Node instances . strides : Integer , tuple of 3 integers , or None . Strides values . implementation , model_nest_level , layers = model.layers if K.backend ( ) == 'theano ' : the output of the layer ( its `` activation '' ) . if self.metrics_func is not None : # Create an input node to add to self.outbound_node epochs = 4 axis = axis % ndim ( tensors [ 0 ] ) batch_size : integer . def __init__ ( self , name='mean_squared_error ' , dtype=None ) : if not ( height_factor == width_factor == 2 ) : for i , x in enumerate ( input_tensors ) : output_shape = ( input_shape [ 0 ] , y = y.ravel ( ) val_inputs = val_x + val_y + val_sample_weights ( useful for visualizing what you are doing ) . fit_function = self.train_function ValueError : In case the generator yields data in an invalid format . another numpy array or a list of numpy arrays raise TypeError ( ' x dtype is not float16 , it is ' , K.dtype ( x ) ) node_key = self._node_key ( layer , node_index ) with the template model ( the argument you passed to ` multi_gpu_model ` ) , assert_allclose ( w , new_w ) we use ` expand_dims ` to make sure that ndim is at least 2 . file_hash='f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5 ' ) for kwargs in kwargs_list : validation_freq=1 ) : score_array * = weights `` `` '' Gets the static batch size of a Layer . ' elements . ' ) `` `` '' Extracts an archive if it matches tar , tar.gz , tar.bz , or zip formats . elif not self.in_cooldown ( ) : ` ( batch , filters , new_steps ) ` return C.cos ( x ) return K.dropout ( ones , rate ) its arguments , so that you could create the estimator without passing any states.append ( T.stack ( return _BACKEND if any ( m is not None for m in mask ) : save_prefix : str . Prefix to use for filenames of saved pictures 'should contain two elements . ' ) keras_shape_list = list ( x._keras_shape ) weights : Initial weights to load in the Bidirectional model log_probs = log_f_probs + log_b_probs [ : :-1 , : :-1 ] - L if dev.type ( ) == 0 and dilation_rate ! = ( 1 , 1 , 1 ) : steps = input_shape [ 2 ] self.wait = 0 def recurrent_activation ( self ) : A tiled tensor . height_factor=height_factor , str ( cond_ndim ) + ' , ndim ( then_expression ) ' def iter_sequence_infinite ( seq ) : dilation_rate : integer dilation rate . None ( default ) if feeding from framework-native while ` `` channels_first '' ` corresponds to inputs with shape If you need to , you can further configure your optimizer . A core principle of Keras is to make things reasonably simple , while allowing the user to be fully in control when they need to ( the ultimate control being the easy extensibility of the source code ) . # model = Model ( ) return K.sum ( K.switch ( all ops in the graph to the new inputs ' Keras tensors and non-Keras tensors ' ) `` `` '' Abstract nD UpSampling layer ( private , used as implementation base ) . if isinstance ( inputs , list ) and len ( inputs ) > 1 or initial_state : return tf.not_equal ( x , y ) backward_weights = preprocess_weights_for_loading ( if values_shape ! = weights_shape : # Collected trainable weights , sorted in topological order . parallel_model = model self.epochs = self.params [ 'epochs ' ] assert len ( padding [ 1 ] ) == 2 input_shape = ( input_shape [ 0 ] , output_dim ) RuntimeWarning ) target_mean=2 , target_max=2 , target_min=2 ) beta_regularizer=None , def constant ( value , dtype=None , shape=None , name=None ) : simply divides ` total ` by ` count ` . first_line = lines [ 0 ] raise ValueError ( ' ` inputs ` should be a list . ' ) assert o._keras_shape == ( None , 1 ) # Apply reduction function to the individual weighted losses . A YAML string . permute_pattern = list ( range ( y_ndim ) ) f = h5py.File ( datapath ) cell = self.cell x = np.random.random ( ( num_samples , height , width , 3 ) ) def __getitem__ ( self , attr ) : if not isinstance ( mask , list ) : old_layer = keras.layers.Dense ( 2 , bias=True , self.uid = _SEQUENCE_COUNTER batch_size = logs.get ( 'size ' , 0 ) model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.TrueNegatives ( ) ] ) if not hasattr ( x , '_keras_history ' ) : node_index = node.node_indices [ i ] str ( x ) + ' at layer `` ' y_true : tensor of true targets . x_c = np.array ( [ 4 , 3 , 2 , 3 ] , dtype=np.int64 ) self._dropout_mask = _generate_dropout_mask ( outs_per_batch.append ( outs ) e.g . ` output [ t ] ` does not depend on ` input [ t + 1 : ] ` . x = self.call ( x ) output_shape ) in [ 4 , 5 ] : if value not in self._layers : masks = [ ] for argument in self.unrelated_updates.arguments : assert len ( decode_pred_tf ) == top_paths argmax ( return None , C.cntk_py.Value ( result ) 'is redundant . ' y , self._feed_loss_fns , feed_output_shapes ) from .load_backend import transpose return loss filter_flip=not flip_filters ) to the specified pattern . metric_fn = metrics_module.MeanMetricWrapper ( model.add ( LocallyConnected1D ( 32 , 3 ) ) x_test , y_test = f [ 'x_test ' ] , f [ 'y_test ' ] if weights : Contains the following values : normed = batch_normalization ( x , broadcast_mean , broadcast_var , `` `` '' Downloads a file from a URL if it not already in the cache . self.recurrent_kernel_i = self.recurrent_kernel [ : , : , : , : self.filters ] from tensorflow.keras.layers import MaxPooling3D return x / C.abs ( x ) concat_args = kwargs.pop ( 'concat_args ' , False ) cntk_axes = _normalize_axis ( axes , x ) self.W_regularizer = regularizers.get ( W_regularizer ) travis_retry conda install -q pydot graphviz Pillow to launch TensorBoard from the command line : assert abs ( output.std ( ) - target_std ) < lim epoch_logs = { } dataset = self.data.create_dataset ( attr , val.shape , dtype=val.dtype ) trained = theano.sandbox.cuda.dnn.dnn_batch_normalization_train ( update_freq='epoch ' , 'majoring ' that does the opposite . super ( CuDNNGRU , self ) .build ( input_shape ) y.shape [ 1 ] : 30 : append to output shape pool_out = pool_out.dimshuffle ( ( 0 , 2 , 3 , 4 , 1 ) ) preds3 = model3.predict ( np.random.random ( ( 1 , 32 ) ) ) K.batch_set_value ( tuples ) def get_num_dynamic_axis ( x ) : if 'initial_state ' in kwargs : self.total = self.add_weight ( def test_H5Dict_attrs ( ) : from .. utils.generic_utils import Progbar the exact same state , without any of the code * * Note : * * for the time being , masking is only supported with Theano . return theano.tensor.as_tensor_variable ( result ) return rng.uniform ( shape , low=minval , high=maxval , dtype=dtype ) forward_losses + backward_losses ) parallel_model = multi_gpu_model ( model , gpus= [ 0 , 2 , 4 , 6 , 8 ] ) if not all ( isinstance ( v , np.ndarray ) or return tuple ( x.shape.as_list ( ) ) `` `` '' Legacy setter for ` image_data_format ` . Supported methods are ` `` nearest '' ` , ` `` bilinear '' ` , generator , specs += layer.input_spec self.recurrent_kernel_c ) 'gs : // ' prefix ) . A bucket name provided with argument precedes what is pattern [ i ] = pattern [ i + 1 ] 'with the TensorFlow backend . ' ) for i in indices_for_conversion_to_dense : from tensorflow.keras.constraints import get # different shapes , with None = > 0 __User friendliness.__ Keras is an API designed for human beings , not machines . It puts user experience front and center . Keras follows best practices for reducing cognitive load : it offers consistent & simple APIs , it minimizes the number of user actions required for common use cases , and it provides clear and actionable feedback upon user error . y_true : ` y_true ` argument of ` fn ` . in_lens [ i ] = s2 pool_size : Integer or tuple of 2 integers , on_batch_begin=None , shape [ 0 ] , * including the batch size * . name : ( Optional ) string name of the metric instance . return x / ( 1 + np.abs ( x ) ) ' ' + str ( len ( output_tensors ) ) + ' output tensors ' def squeeze_or_expand_dimensions ( y_pred , y_true=None , sample_weight=None ) : elif hasattr ( layer , 'input_shapes ' ) : def has_arg ( fn , name , accept_all=False ) : ( or list of tensors if the layer has multiple outputs ) . str ( shape ) + ' but got array with shape ' ' ( ( left_dim1_crop , right_dim1_crop ) , ' cntk_dynamicity = kwargs.pop ( 'cntk_dynamicity ' , False ) elif self.merge_mode == 'ave ' : axis : Axis along which to perform stacking . ' ` train_function ` . ' % argument.name ) ` ( batch , channels , depth , rows , cols ) ` return K.reshape ( inputs , ( K.shape ( inputs ) [ 0 ] , ) + self.target_shape ) `` `` '' Convert keras ' padding to tensorflow 's padding . if verbose == 1 : return serialize_keras_object ( constraint ) with pytest.raises ( NotImplementedError ) : # If mask is explicitly passed to __call__ , ref_output = parallel_model.predict ( x ) self._metrics = [ ] def __init__ ( self , size= ( 2 , 2 , 2 ) , data_format=None , * * kwargs ) : feed_dict [ K.learning_phase ( ) ] = False self._per_output_weighted_metrics [ i ] , i ) ) str ( spec.ndim ) + ' , found ndim= ' Otherwise ( both the output mask and the input mask are ` None ` ) : strides = [ -1 for _ in cntk_axes ] self.unroll = unroll W_regularizer='l1 ' , new_layer = keras.layers.SeparableConv2D ( 5 , ( 3 , 3 ) , name='conv ' ) these arguments are passed into ` tf.Session.run ` . name : A name of the attributes to save . shuffle=shuffle , constants_shape = input_shape [ -self._num_constants : ] reason='Requires 8 GPUs . ' ) timesteps = 2 optimizer='rmsprop ' ) def test_spatialdropout2d_legacy_interface ( ) : ' '' value = t [ 1 ] if K.is_keras_tensor ( x ) : ( e.g . set this to adapt the display to different return wrapper `` `` '' Computes log ( sum ( exp ( elements across dimensions of a tensor ) ) ) . dtype = K.floatx ( ) `` `` '' Layers that act as activation functions . def test_nn_operations ( self ) : clip_max = max_value is not None ` variables_to_update ` contains invalid keys . value , dtype=dtype , name=name , constraint=constraint ) raise ImportError ( 'RemoteMonitor requires ' def _collect_input_shape ( input_tensors ) : if issparse ( ins [ i ] ) and not K.is_sparse ( feed [ i ] ) : assert len ( run_metadata.partition_graphs ) == 0 # tensorflow does n't support float64 for conv layer before 1.8.0 The layer 's attribute ` attr ` at the node of index ` node_index ` . gets returned by the ` fit ` method of models . return DirectoryIterator ( if beta is None : self.rank = rank if batch_size is not None and training_utils.is_generator_or_sequence ( x ) : if sample_weight_mode is not None : assert_allclose ( out3 , out4 , atol=1e-5 ) return tf_keras_backend.random_binomial ( return tf.expand_dims ( x , axis ) self.seen += 1 depthwise_regularizer : Regularizer function applied to 'tar ' includes tar , tar.gz , and tar.bz files . raise ValueError ( 'function { } has no argument { } '.format ( f , name ) ) `` `` '' Binary crossentropy between an output tensor and a target tensor . if updates == [ ] : __input_length__ : Length of input sequences , to be specified 'mean ' : self.mean , oh = KNP.one_hot ( np.int32 ( indices ) , num_classes ) weight = K.variable ( initializer ( shape , dtype=dtype ) , stateful=False , if sequential model : from .merge import Maximum m.result ( ) elif class_name in _GLOBAL_CUSTOM_OBJECTS : from tensorflow.keras.constraints import MaxNorm output_mask = get_matching_mask ( mask [ i ] , output ) class TrueNegatives ( _ConfusionMatrixConditionCount ) : inspect.iscode ( x ) ] output = T.nnet.sigmoid ( output ) return super ( RNN , self ) .losses def resize_images ( x , height_factor , width_factor , data_format ) : self.bias_initializer ( ( self.units * 5 , ) , * args , * * kwargs ) , metrics_utils.ConfusionMatrix.FALSE_POSITIVES : self.false_positives # Prepare data for validation ` ( samples , time , channels , rows , cols ) ` to validate before stopping . get_output_shape_at # Set inferred batch size from the InputLayer . tensor : A tensor instance . 'beta_1 ' : float ( K.get_value ( self.beta_1 ) ) , if self.trainable and self.built : self.state_size = units f = self.refs [ datapath ] recurrent_initializer=recurrent_initializer , base_config = super ( StackedRNNCells , self ) .get_config ( ) output = T.clip ( output , epsilon ( ) , 1.0 - epsilon ( ) ) gpus = 8 def test_stop_gradient ( self ) : 'should be a list or an int . ' ) except TypeError : # Invoke all metrics added using ` compile ` . len_size = int_shape ( size ) [ 0 ] if is_tensor ( size ) else len ( size ) if node_key in model._network_nodes : y_train = HDF5Matrix ( h5_path , 'my_labels ' , start=0 , end=150 ) # the current node will be added to `` `` '' CIFAR100 small images classification dataset . int_A^B { Precision.dP } = int_A^B { slope * dP + intercept * dP / P } neg = K.relu ( -x ) if initializer is None : weight_value_tuples = [ ] # Graph network reduction : ( Optional ) Type of Reduction to apply to loss . # the gradient during training . saving.load_weights_from_hdf5_group ( # test theano shape inference when if len ( y.shape ) == 2 : self.predict_function = None return func elif mode == 'max ' : feed = ( model._feed_inputs if hasattr ( output , '_uses_learning_phase ' ) : self._output_loss_metrics = [ output_shape [ h_axis ] = conv_utils.deconv_length ( output_shape [ h_axis ] , out._keras_shape = tuple ( size ) # Create metric variables print_fn : Print function to use . if type ( obj ) .__module__ == np.__name__ : import io We update the _keras_history of the output tensor ( s ) specifying the stride length of the convolution elif self.reduction == metrics_utils.Reduction.WEIGHTED_MEAN : depthwise_initializer=depthwise_initializer , reshape=reshape ) `` `` '' Layer that applies an update to the cost function based input activity . outputs.append ( output ) or vice versa . However , there 's no conversion required between TF and CNTK . y = x.dimshuffle ( ( 0 , ' x ' , 1 ) ) step : Difference between two successive values . # get initial_state from full input spec update_delta = ( x - tf_math_ops.cast ( value , x.dtype ) ) * decay and a single floating point value per feature for ` y_true ` . metric_name : Metric name that corresponds to the metric specified by the input_shape=volume_shape , def test_regression_predict_shape_correct_num_test_0 ( ) : self.strides , shape , mean=mean , stddev=stddev , dtype=dtype , seed=seed ) if len ( self.layers ) < = index : # merge input and recurrent biases into a single set return AUCSummationMethod.INTERPOLATION beta , gamma , if not connections : from tensorflow.keras.activations import hard_sigmoid embeddings_data = self.embeddings_data assert ( history.history [ 'accuracy ' ] [ -1 ] > = 0.6 ) return hasattr ( x , '_theano_placeholder ' ) and x._theano_placeholder a generator or a ` Sequence ` object for the validation data output = C.ops.argmin ( x , axis=axis [ 0 ] ) def make_batches ( size , batch_size ) : ` predict_on_batch ` . self.mask_value = mask_value filterwarnings = 'in ` SimpleRNN ` has been deprecated . ' 'bottom_pad ' : 2 , assert K.is_sparse ( k_s ) return _fused_normalize_batch_in_training ( def _set_sample_weight_attributes ( self , sample_weight_mode , None or a tensor ProgressTracker.progbar = None slice_length = py_slice ( i * stride , training_utils.check_loss_and_target_compatibility ( Fraction of the training data to be used as validation data . `` `` '' Determine if an object follows the Sequence API . raise ValueError ( 'Layer # ' + str ( k ) outputs = keras.layers.Bidirectional ( rnn ( output_dim , stateful=True ) , is_np = type ( val ) .__module__ == np.__name__ if use_cudnn : if K.backend ( ) == 'tensorflow ' and sample_size.dtype ! = 'float32 ' : # Normally the trailing 1 is added by standardize_weights if gamma is None : config = { 'theta ' : float ( self.theta ) } # NOTE ( robieta ) : This differs from tf.keras in that self.device is a return ( x - mean ) / C.sqrt ( var + epsilon ) * gamma + beta x = ifelse ( training , x , alt ) self.total_loss = self._prepare_total_loss ( masks ) def normalize_data_format ( value ) : output_shape [ 4 ] , output_masks = [ None for _ in output_tensors ] additional non-weight state . Used in , for instance , RNN inputs : Tensor if 'class_name ' not in config or 'config ' not in config : p_list = [ ] ' ( ( top_crop , bottom_crop ) , ( left_crop , right_crop ) ) . ' RuntimeError : If the layer has no inbound nodes . for i , value in enumerate ( args [ 1 : ] ) : { 'go_backwards ' : True } , features = input_shape [ 2 ] x = layers.Dense ( 5 ) ( inputs ) If tuple of 2 ints : check_single_tensor_operation ( 'cumsum ' , ( 4 , 2 ) , WITH_NP ) directory=None , `` `` '' Proxy for tensorflow.python.lib.io.file_io.file_exists class . Mocks the __Two__ : The value of ` initial_state ` should be a tensor or list of obj : object , dict , or list . if K.dtype ( x ) ! = spec.dtype : t , f = cntk_func_tensors ( function_name , [ x_shape ] , * * kwargs ) y_pred = K.cast ( y_pred , self.dtype ) x : tensor 'when doing step-wise training . ' ) output_dim = 4 `` `` '' Calls the ` on_test_end ` methods of its callbacks . def stop ( self , timeout=None ) : device_type : A string containing ` GPU ` or ` CPU ` ( case-insensitive ) . return C.splice ( * tensors , axis=axis [ 0 ] ) # sample_weight_mode= { 'lstm ' : 'temporal ' } ) # Assumed tensor - TODO ( fchollet ) additional type check ? axis = axis % x.type.ndim + 1 def _set_device ( self , device ) : global _IMAGE_DATA_FORMAT e.g . if the layer is being shared with a different data stream ) . pos = K.relu ( inputs ) 'kernel_size ' ) file_hash=None , if insecure [ 0 ] ! = ' _ ' : for axis in range ( len ( x._keras_shape ) ) : 'as a keyword argument instead . ' ) def test_fit_with_class_weight ( self ) : if self.save_weights_only : self.send_as_json = send_as_json def test_zero_padding_3d ( data_format , padding ) : state._uses_learning_phase = True assert_allclose ( new_x , K.eval ( x_var ) , atol=1e-05 ) color_mode : one of `` grayscale '' , `` rgb '' , `` rgba '' . Default : `` rgb '' . elif pool_mode == 'avg ' : assert str ( id ( x ) ) in tensor_map , 'Could not compute output ' + str ( x ) sample_weights = [ Note : Please also see _ , feature_dim , filters = kernel_shape A numpy-style shape tuple . if self.states [ 0 ] is None : self.cell._dropout_mask = None return ( input_shape [ 0 ] , input_shape [ 2 ] ) for archive_type in archive_format : self.recurrent_kernel_h , ( batch_size , filters , new_rows , new_cols ) biases= [ output_shape [ 1 ] , beta , Precision = TP / ( TP + FP ) = TP / P with `` padding [ 0 ] '' , `` padding [ 1 ] '' and `` padding [ 2 ] '' ( resp . ) zeros left and right . if training is None : `` `` '' Recurrent layers and their base classes . return conv ( x2 , w2 , padding=padding , data_format=data_format ) `` `` '' Specifies the ndim , dtype and shape of every input to a layer . tf_file_io.is_directory ( self.bucket_path ) layer_test ( legacy_layers.Highway , _LEARNING_PHASE_PLACEHOLDER = C.constant ( return [ K.tile ( initial_state , [ 1 , dim ] ) self.recurrent_bias_r = ( constraint=self.recurrent_constraint ) In a ` Conv2D ` layer with ` data_format= '' channels_last '' ` , parallel_model = multi_gpu_model ( model , gpus=1 ) fit_args = copy.deepcopy ( self.filter_sk_params ( Sequential.fit ) ) def MobileNetV2 ( * args , * * kwargs ) : # Note : we ca n't test whether the model # shape : batch , filters , output_length , input_length * kernel_size def test_dynamic_set_inputs ( ) : xs = [ [ w + index_from for w in x ] for x in xs ] ` alpha * ( exp ( x ) -1 ) ` if ` x < 0 ` . assert dense._inbound_nodes [ 1 ] .inbound_layers == [ b_layer ] datadir_base = os.path.expanduser ( cache_dir ) # input shape has None entries epsilon : Fuzz factor . from .callbacks import EarlyStopping # # On top of new , non-Keras tensor # Invoke metric functions ( unweighted ) for all the outputs . name='conv ' ) x = tf.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) self._feed_inputs.append ( placeholder ) Inconsistency will typically arise when one modifies ` model.trainable ` raise ValueError ( ' A ` Concatenate ` layer requires ' if hasattr ( reference , '_keras_shape ' ) and hasattr ( indices , '_keras_shape ' ) : if reduction in [ metrics_utils.Reduction.SUM_OVER_BATCH_SIZE , from tensorflow.keras.layers import AveragePooling2D fan_in , fan_out = _compute_fans ( shape ) `` `` '' A dense maxout layer . `` `` '' Converts input first axis to CNTK batch axis . check_single_tensor_operation ( 'resize_images ' , x_shape , reset_after=reset_after ) sequences=inputs , This function is used internally with ` self._network_nodes ` . from tensorflow.keras.layers import GaussianDropout if mode == _TRAIN : self.function = function _UID_PREFIXES = defaultdict ( int ) 'trainable ' , Custom layers and optimizers written by users x2 = depthwise_conv ( x , w1 , padding=padding , data_format=data_format ) ] , # note that filename does not matter here . the size of the cell output . from tensorflow.keras.activations import softplus sparse=sparse , with ` stddev = sqrt ( 1 / fan_in ) ` return int ( ins [ 0 ] .shape [ 0 ] ) `` `` '' Locally-connected layers . '' '' '' with K.control_dependencies ( update_ops ) : # For TF output = K.local_conv2d ( inputs , @ interfaces.legacy_global_pooling_support assert K.floatx ( ) == old_floatx self.mask = mask len_dim2 = input_shape [ 3 ] def test_maxpooling3d_legacy_interface ( ) : activation=activation , self._is_started = False self._input_map [ input_uid ] = inputs return len ( code ) < MIN_CODE_SIZE metrics_names = [ 'loss ' ] batch_size=batch_size , 'or ` ( val_x , val_y ) ` . Found : ' layer.recurrent_kernel_i , # batch_size length vector of log probabilities Input mask tensor ( potentially None ) or list of input finally : workers=1 , specificity at the given sensitivity . The threshold for the given sensitivity An integer tensor . 'config ' : cell.get_config ( ) } ) an Exception . Prefer using def from_config ( cls , config , custom_objects=None ) : ( -1 , 1 , feature_dim ) ) ) from . import Callback > > > y_batch = K.ones ( shape= ( 32 , 30 , 20 ) ) def get_output_shape_at ( self , node_index ) : outs = [ ] if self._is_file : super ( UpSampling1D , self ) .__init__ ( ( int ( size ) , ) , 'channels_last ' , * * kwargs ) min_delta : minimum change in the monitored quantity # Legacy support z = K.batch_dot ( x , y , axes ) eval_fn = function ( [ ] , [ x ] ) the first value is used ( ` elems [ 0 ] ` ) as ` initializer ` from ` elems ` if axes [ 1 ] < 0 : # batchifies original CTC code def simple_rnn ( inputs , states ) : base_config = super ( ELU , self ) .get_config ( ) weight_type : A string used purely for exception printing . _ , outputs , _ = K.rnn ( step , inputs , callbacks=callbacks , 'result ' : `` 'Base class for recurrent layers . def _get_metrics_from_layers ( layers ) : @ pytest.mark.skipif ( K.backend ( ) == 'cntk ' , if mean.ndim == 1 and x.ndim > 1 : if self._states is None : * ` call ( ) ` : Contains the logic for loss calculation using ` y_true ` , ` y_pred ` . # Here , ` x_set ` is list of path to the images assert old_config == new_config `` `` '' Retrieves the output shape tuple ( s ) of a layer . x = K.placeholder ( shape=input_shape , dtype=self._input_dtypes ) return True dataframe , If ` data_format ` is ` `` channels_first '' ` : import yaml if not hasattr ( cell , 'call ' ) : def __init__ ( self , activation , * * kwargs ) : model.compile ( loss=losses.MSE , indefinitely . An epoch finishes when ` steps_per_epoch ` for k in self.params [ 'metrics ' ] : return_states = [ ] layer = cls ( rnn_layer , * * config ) if self.write_graph : inputlabels , self._delta_ts [ hook_name ] .append ( time.time ( ) - t_before_callbacks ) postfix_shape = tuple ( postfix_shape ) assert o._keras_shape == ( None , 4 , 5 ) self._input_coordinates = [ ] [ Learning to forget : Continual prediction with LSTM ] ( ask.return_value = True if class_sample_weight is not None : This layer supports masking for input data with a variable number shape = ( dim , ) def pool2d ( x , pool_size , strides= ( 1 , 1 ) , padding='valid ' , ( where one can not access arbitrary indices ) . indices : nD integer tensor of shape info += ' % s ' % self._values [ k ] z = k.eval ( getattr ( k , second_function_name ) ( y , * * second_function_args ) ) data_format=tf_data_format ) self.samples_seen_at_last_write = self.samples_seen self.outputs , result = np.array ( result ) from tensorflow.keras.layers import Conv2D y._uses_learning_phase = x._uses_learning_phase mean.dimshuffle ( shuffle_pattern ) , ( ( 1 , 1 ) , 'valid ' , 'channels_first ' , ( 3 , 4 , 5 , 6 ) ) ] # If the output_shape [ -1 ] is not 1 , then we know output is ` categorical ` . that can be used for training models . 'config ' : config , do_validation and val_inputs = [ 0 ] if y_squashed : x_col='filename ' , from keras_applications import inception_v3 dynamic_axis_num=1 ) : global tf , projector return np.hstack ( [ func ( k ) for k in np.hsplit ( kernels , n_gates ) ] ) class SequenceEnqueuer ( object ) : x , tf_data_format = _preprocess_conv2d_input ( x , data_format , force_transpose ) ValueError : If loss weight is a dict with key not in model output names , for out in output : from .. engine.base_layer import disable_tracking def __len__ ( self ) : metric : Metric function name or reference . name = config.get ( 'name ' ) with K.name_scope ( self.forward_layer.name ) : elif isinstance ( axis , list ) : self.path = path return [ outputs ] + return_states # data has already been validated . self._compile_metric_functions.append ( metric_fn ) padding , super ( Softmax , self ) .__init__ ( * * kwargs ) 'Use ` get_input_shape_at ( node_index ) ` ' kwargs= { 'units ' : units , raise ValueError ( `` To visualize embeddings , embeddings_data must `` ` true_positives ` by the sum of ` true_positives ` and ` false_negatives ` . the ` .compile ( ) ` method of the model . def average ( inputs , * * kwargs ) : 'Input ' + str ( input_index ) depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape ( 'mask_zero ' : self.mask_zero , ( 'border_mode ' , 'padding ' ) ] ) initializer=bias_initializer , will default to 1 . If 0 , will execute the generator on the main maxlen=None , test_split=0.2 , seed=113 , name='logcosh ' ) : `` `` '' Makes a function that transforms input kernels from/to CuDNN format . raise ValueError ( 'Multiple target dimensions are not supported . ' `` `` '' See docstring for ` Model.fit_generator ` . '' '' '' if y_pred is not None : distribution centered on zero , with ` stddev = sqrt ( scale / n ) ` where n is : if len ( padding ) ! = 3 : a specific layer , or on your entire model . if accept_all : output_shape = list ( input_shapes [ 0 ] ) trainable_weights : List of variables . targets : Targets corresponding to timesteps in ` data ` . K.floatx ( ) ) elif isinstance ( loss_weights , list ) : self.bias_o_i = self.bias [ self.units * 3 : self.units * 4 ] ( input_depth * depth_multiplier , 7 ) ) return inds [ is_not_repeat ] # old : ( kernel_rows , kernel_cols , stack_size , filters ) output = C.clip ( output , epsilon ( ) , 1.0 - epsilon ( ) ) def test_func_dump_and_load_closure ( ) : def fit ( self , names = [ m.name for m in model.metrics ] def dot ( x , y ) : assert dense._inbound_nodes [ 0 ] .input_tensors == [ a ] str ( loss_weights ) ) self._feed_sample_weights ) y=None , from .load_backend import map_fn initial_state = inputs [ 1 : ] inputs_z = inputs * dp_mask [ 0 ] a ` batch_input_size= ( ... ) ` to the first layer in your model . @ interfaces.legacy_pooling2d_support strides = ( 1 , ) + strides * 2 + ( 1 , ) into a sample-weighted , cost-masked objective function raise ValueError ( 'Could not interpret regularizer identifier : ' self.input_bias_h = None import threading new_shape = tuple ( new_shape ) from keras.engine.training_utils import weighted_masked_objective before declaring ` predict_loop ` finished . class ConvRecurrent2D ( Recurrent ) : Useful to avoid clutter from old models / layers . `` `` '' Invokes the ` Loss ` instance . `` `` '' Max pooling operation for spatial data . if x.type.ndim == 0 : to retrieve the attribute . ' ` Sequential.from_config ( config ) ` ? ' ) data_npy = np.asarray ( val ) It should be a tuple of integers , e.g . ` ( 32 , 10 , 100 , 100 , 32 ) ` . initial_state = inputs [ 1 : -self._num_constants ] max_value = np.inf full_input = [ inputs ] + additional_inputs This value is ultimately returned as ` auc ` , an idempotent operation that pattern = [ i for i in range ( x.type.ndim ) ] warnings.warn ( ' ` epsilon ` argument is deprecated and ' batch_size : size of the batches of data ( default : 32 ) . y_pred , y_true=y_true ) if sorted ( reduction_axes ) == list ( range ( ndim ( x ) ) ) [ : -1 ] : return cls ( layer , * * config ) max_queue_size=10 , axis_list = list ( set ( int ( a ) for a in axis ) ) x = T.nnet.relu ( x ) update_op from ` update_state ` and result works in case result returns mean_absolute_error , name , dtype=dtype ) backward_state = initial_state [ pivot : ] 'activation ' : activations.serialize ( self.activation ) , model.add ( keras.layers.Dense ( output_dim ) ) if isinstance ( x , ( list , tuple ) ) : self.dropout , raise RuntimeError ( 'The following attributes can not be saved to ' def test_glorot_uniform ( tensor_shape ) : postfix_shape [ axis ] = pattern [ 1 ] return arg , args [ : i ] + args [ i + 1 : ] , kwargs depthwise_constraint : Constraint function applied to for i in range ( len ( z_list ) - 1 ) : input_dim_a = 10 if not self.inputs : w = x._keras_shape [ 2 ] + left_pad + right_pad _test_optimizer ( optimizers.Adamax ( lr=1. , decay=1e-3 ) ) def any ( x , axis=None , keepdims=False ) : 'it was generated by layer ' xs = np.concatenate ( [ x_train , x_test ] ) super ( MeanSquaredError , self ) .__init__ ( return dot mean=mean , def test_zeropadding2d_legacy_interface ( ) : if ( not isinstance ( output_mask , ( list , tuple ) ) and from keras.backend import load_backend Separable convolutions consist in first performing _ = fn ( [ np.ones ( ( 1 , 1 ) ) ] ) If the weights were specified as [ 1 , 1 , 0 , 0 ] then the mean would be 2 . # apply a 3x3 unshared weights convolution with 64 output filters input_a = keras.backend.variable ( val_a ) stream.write ( binary_data ) `` `` '' Callback that streams epoch results to a csv file . f = K.function ( inputs= [ x_placeholder , y_placeholder ] , self.data.attrs [ attr ] = val vertical_flip=vertical_flip , This method deals with an inherent problem sample_weight_mode = training_config [ 'sample_weight_mode ' ] return self.cell.data_format from .load_backend import clip raise ValueError ( ' Can not serialize ' , instance ) return theano.map ( fn , elems , name=name ) [ 0 ] callback.on_epoch_begin ( epoch , logs ) _axis [ i ] = ( a % ndim ) outs [ i ] = float ( batch_out ) dilation_rate : tuple of 2 integers . dtype = K.floatx ( ) update_fn = ConfusionMatrix.FALSE_NEGATIVES in variables_to_update output_mask = expand_mask ( mask , output_sample ) new_model.train_on_batch ( x2 , y2 ) kwargs = input_data [ 3 ] # ` Mean ` metric only takes a single value . py_slice ( top_pad , input_shape [ 1 ] + top_pad ) , for fine-tuning or transfer-learning models where steps_done = 0 if K.is_keras_tensor ( tensor ) ! = K.is_keras_tensor ( additional_inputs [ 0 ] ) : if K.backend ( ) ! = 'cntk ' : # Note py_slice ( None ) ) ins_batch = slice_arrays ( fit_inputs , batch_ids ) return tf_keras_backend.random_normal ( Unrolling can speed-up a RNN , args = [ args [ 0 ] , args [ 1 ] , kernel_size ] dpi=96 ) : with pytest.raises ( TypeError ) : Assume ` x = [ [ 1 , 2 ] , [ 3 , 4 ] ] ` and ` y = [ [ 5 , 6 ] , [ 7 , 8 ] ] ` def __init__ ( self , gain=1 . ) : elif len ( shape ) in { 3 , 4 , 5 } : def get_static_batch_size ( layer ) : axis=self.axis , 1 . A function super ( Recurrent , self ) .__init__ ( * * kwargs ) self.from_shape = input.shape self._metrics.append ( metric_obj ) strides = ( 1 , ) + strides + ( 1 , ) 'or a tuple of 3 tuples of 2 ints ' DeprecationWarning ) x : Tensor to reverse . value : Numpy array , initial value of the tensor . return _EPSILON skip_top : skip the top N most frequently occurring words self.monitor_op = np.less spatial_axes ) keras_shape_list.pop ( a ) from tensorflow.python.keras.utils.layer_utils import print_summary if k in self.stateful_metrics : y = K.batch_flatten ( x ) self.input_spec = [ InputSpec ( ndim=3 ) ] names ( strings ) to custom objects self._cudnn_gru = cudnn_rnn_ops.CudnnGRU ( `` `` '' Returns the value of a variable . `` `` '' Interface converters for Keras 1 support in Keras 2 . return C.one_hot ( indices , num_classes ) reason='Uses the ` string ` type for a tensor . ' ) target_gpu_ids = range ( num_gpus ) to each of the 10 timesteps , independently : _has_nchw_support ( ) ) ) and output_dim = cell.state_size ( potentially with 1 element ) . model = keras.Sequential ( ) This metric creates two local variables , ` total ` and ` count ` that are used to sample_size = K.cast ( sample_size , dtype=K.dtype ( inputs ) ) def DISABLED_test_on_epoch_end_threads ( ) : ' specified with a mix of Keras tensors and ' def init_pool_generator ( gens , random_seed=None ) : `` `` '' A ` Node ` describes the connectivity between two layers . if isinstance ( args [ 1 ] , six.string_types ) : # Final result : 5e+08 # get only first sample from above test case output_shape = [ output_shape , state_shape , state_shape ] self.scale = scale weights = self._non_trainable_weights [ : ] kx2 = K.eval ( K.map_fn ( initializer : The first value used ( ` elems [ -1 ] ` in case of None ) __doc__ = image.NumpyArrayIterator.__doc__ Tensor with shape ( samples,1 ) containing the e = K.stop_gradient ( b ) ( 'subsample_length ' , 'strides ' ) , _runner ( initializers.TruncatedNormal ( mean=0 , stddev=1 ) , tensor_shape , states_tm1 = states_t `` `` '' A backwards compatibility alias for ` on_train_batch_begin ` . '' '' '' all_dims_padding = transpose_shape ( all_dims_padding , data_format , kernel_shape = int_shape ( kernel ) x_d = np.array ( [ 0 , 7 , 2 , 3 ] , dtype=np.float32 ) for _ in output_names : # reshape in case it 's in shape ( num_samples , 1 ) instead of ( num_samples , ) self._check_trainable_weights_consistency ( ) # now model.output_shape == ( None , 30 , 30 , 64 ) def ctc_cost ( predict , Y ) : 'the best epoch ' ) def filter_top_k ( x , k ) : if 'atrous_rate ' in kwargs : shape = list ( shape ) output_tensors=output , # Handle data tensors support when no input given 'alpha_initializer ' : initializers.serialize ( self.alpha_initializer ) , def test_boston_load_does_not_affect_global_rng ( fake_downloaded_boston_path ) : out = model ( input_4 , training=True ) .numpy ( ) predictions to consider when calculating recall . str ( y_shape ) + ' . ' ) return unpack_singleton ( specs ) class GlobalMaxPooling2D ( _GlobalPooling2D ) : dtype : Default dtype of the layers 's weights . for axis in range ( 1 , ndim ( x ) ) : label_length = tf.cast ( tf.squeeze ( label_length , axis=-1 ) , tf.int32 ) # it is not needed and should be removed return self.cell.strides `` `` '' Adds a bias vector to a tensor . 'momentum ' : float ( K.get_value ( self.momentum ) ) , return squeeze ( x , spatial_start_dim ) from .np_utils import to_categorical pool_size , op = K.update_sub ( x_var , decrement ) embeddings_vars [ layer.name ] = embedding out_depth = conv_utils.deconv_length ( depth , fan_in = shape [ -2 ] * receptive_field_size ` False ` . Note that because this implementation relies on def less ( x , y ) : def poisson ( y_true , y_pred ) : shuffle=True , rnn_inputs = C.to_sequence ( rnn_inputs ) initial_state = K.expand_dims ( initial_state ) # ( samples , 1 ) for i in sequence : the size of the cell output . epochs=1 , y_pred : The predicted values . 'tensorflow ' prec_slope = K.switch ( ` batch_input_shape= ( ... ) ` to the first layer in your model . conda info -a learning rate decay , and Nesterov momentum . axis=self.axis , with pytest.raises ( AttributeError ) : output = K.dot ( x , self.W ) for w , new_w in zip ( new_model.get_weights ( ) , new_weights ) : def step ( x , _ ) : # dense.output if interpolation == 'nearest ' : sample_weight * = mask A tuple length of 3 , ` ( normalized_tensor , mean , variance ) ` . return decorated initial_states = [ K.variable ( initial_state_vals ) ] callbacks = cbks.CallbackList ( callbacks ) C.variables.Parameter , def __call__ ( self , inputs , * * kwargs ) : class LambdaCallback ( Callback ) : ` value ` should be a Numpy array . `` `` '' Rectified Linear Unit . including 1D iterables such as np.ndarray . probs = np.hstack ( [ 1 - probs , probs ] ) strides = ( 1 , ) + strides + ( 1 , ) _LOCAL_DEVICES = [ x.name for x in devices ] return np.pad ( x , all_dims_padding , mode='constant ' ) self.stateful = True # All metric layers are stateful . self.recurrent_dropout = min ( 1. , max ( 0. , recurrent_dropout ) ) i = 0 x = training_utils.standardize_input_data ( self.input_spec = InputSpec ( ndim=2 ) # determine if we 're loading a CuDNNLSTM layer dtype=np.float32 ) WITH_NP = [ KTF , KNP ] for i , j in enumerate ( indices ) : legacy_upsampling1d_support = generate_legacy_interface ( x_rep._keras_shape = tuple ( x_shape ) if layer.__class__.__name__ == 'Conv2D ' : arrays have shapes that match the network 's expectations . initial_state : list of tensors or None assert len ( state ) == num_states return K.temporal_padding ( inputs , padding=self.padding [ 0 ] ) from tensorflow.keras.activations import serialize kernel_initializer='normal ' , # Relocate the model definition under CPU device scope if needed if x_squashed : array ( [ 2 , 4 , 5 ] , dtype=int32 ) batch_size=32 , initializer : The first value used ( ` elems [ 0 ] ` in case of None ) ( step , int ( embedding_size ) ) ) raise ValueError ( ' ` cropping ` should be either an int , ' self._delta_t_batch = 0 . # right to left we have no such problem and the result is larger def simple_rnn_add_constant ( inputs , states_and_constants ) : batch_sizes -= set ( [ None ] ) new_model_gcs = load_model ( gcs_filepath ) 'layers ' : copy.deepcopy ( layer_configs ) `` `` '' Filters top-k values in the last dim of x and set the rest to NEG_INF . A tuple , ` ( last_output , outputs , new_states ) ` . if constants is not None : from .load_backend import normalize_batch_in_training Constrains the weights incident to each hidden unit assert ( X_train [ [ 0 , 1 ] ] == X_train [ :2 ] ) .all ( ) Returns ` 0. ` if ` x < -2.5 ` , ` 1. ` if ` x > 2.5 ` . value = image_data_format ( ) return K.cast ( K.equal ( y_true , y_pred ) , K.floatx ( ) ) def ask_to_proceed_with_overwrite ( filepath ) : def is_symbolic ( x ) : uninitialized_vars.append ( v ) from .pooling import AvgPool3D from tensorflow.keras.callbacks import * * args : Variable length list of dictionaries of name , @ states.setter reversed sequence . SELU is equal to : ` scale * elu ( x , alpha ) ` , where alpha and scale for inputs/kernels/outputs . class_weight , feed_output_names ) y_pred = y_pred [ ... , class_id ] for axis in _axes : print ( ' A local file was found , but it seems to be incomplete ' dirname = os.path.join ( 'datasets ' , 'fashion-mnist ' ) def equal ( x , y ) : # ( num_samples * timesteps , ... ) from .load_backend import abs with pytest.raises ( ValueError ) : 'bias_constraint ' : constraints.serialize ( self.bias_constraint ) } conv_out = squeeze ( conv_out , spatial_start_dim ) # placeholders and variables in CNTK backend use_multiprocessing=use_multiprocessing , where ` x ` is a numpy array containing a batch # This test checks the consistency of the stop_gradient backend API . Object of type ` CustomObjectScope ` . for i in range ( len ( self._output_layers ) ) : i = num_dynamic_axis if v not in unique_variables_to_update : from .pooling import GlobalMaxPooling1D other one is based on original 1406.1078v1 and has the order reversed . from tensorflow.keras.datasets.reuters import load_data def transform ( kernel ) : of sharing the weights of the existing layers . info += ' - % s : ' % k thresholds = [ ( i + 1 ) * 1.0 / ( num_thresholds - 1 ) 'b_regularizer ' : regularizers.serialize ( self.b_regularizer ) , if axis in reduction_axes : subsample_length=2 , 'CNTK Backend : tensor with keras shape : ` % s ` has ' result = x ms = [ K.zeros ( K.int_shape ( p ) , if not isinstance ( lr , ( float , np.float32 , np.float64 ) ) : elif isinstance ( identifier , six.string_types ) : Note : This is not intended to be a generic wrapper . assert len ( layer.losses ) == 8 input_dim : dimensionality of the input ( integer ) . This argument import zipfile ` false_positives ` and ` false_negatives ` that are used to compute the inputs = inputs [ 0 ] with K.name_scope ( metric_name ) : val_enqueuer = OrderedEnqueuer ( for i , batch_out in enumerate ( batch_outs ) : return x * top_k_mask + NEG_INF * ( 1 - top_k_mask ) class Reduce ( Metric ) : reduce_result , if isinstance ( self.data , type ( out ) ) : if hasattr ( self , 'clipnorm ' ) and self.clipnorm > 0 : the last dimension squeezed , ` sample_weight ` could be extended by one MINORING = 'minoring ' for i in range ( output_row ) : config [ 'interpolation ' ] = self.interpolation # If inputs have been transposed , koh = K.eval ( K.one_hot ( K.variable ( indices , dtype='int32 ' ) , num_classes ) ) from .common import set_image_data_format # now : model.output_shape == ( None , 3 , 32 ) seed : random seed for sample shuffling . 'divided by the batch size . Found : ' raise ImportError ( 'You need the TensorFlow ( v1 ) module installed to ' class Nadam ( Optimizer ) : if isinstance ( self.data , h5py.Group ) and attr in self.data : if layer.__class__.__name__ == 'Conv1D ' : batch_size=None ) : from tensorflow.keras.optimizers import * verbose=verbose , from tensorflow.keras.constraints import serialize old_layer = keras.layers.GlobalAveragePooling3D ( dim_ordering='default ' , idx = 2 sample_weight = K.expand_dims ( sample_weight , -1 ) self.rank = len ( padding ) loss='mse ' , for i , gpu_id in enumerate ( target_gpu_ids ) : if doc is None : data_format='channels_first ' , config = yaml.load ( yaml_string , Loader=yaml.FullLoader ) rows = input_shape [ 3 ] will then be the sum of all individual losses . if batch_input_shape is not None : pool_out = pool.pool_3d ( x , ws=pool_size , stride=strides , if shape is None : K.zeros_like ( p [ : self.num_thresholds - 1 ] ) ) , metrics= [ keras.metrics.CategoricalAccuracy ( ) ] ) grads = self.get_gradients ( loss , params ) global _LEARNING_PHASE_PLACEHOLDER 3D tensor with shape ` ( batch_size , timesteps , input_dim ) ` . # Legacy methods BaseMeanIoU = tf.keras.metrics.MeanIoU self.log_values.append ( ( k , logs [ k ] ) ) ` f ( x ) = alpha * x for x < 0 ` , def ones_like ( x , dtype=floatx ( ) , name=None ) : return x._keras_shape y_pred_rank = K.ndim ( y_pred ) if w.name.split ( '/ ' ) [ -1 ] == 'variable ' : class_mode='categorical ' ) < tf.Tensor 'Shape_9:0 ' shape= ( 3 , ) dtype=int32 > allowed_positional_args= [ 'function ' , 'output_shape ' ] ) # [ output_a_np , output_b_np ] , input_tensors = [ ] close to 0 and the activation standard deviation close to 1 . assert tuple ( merged.shape ) == ( None , 16 * 2 ) base_config = super ( RMSprop , self ) .get_config ( ) # as the first layer in a Sequential model each with shape ` ( batch_size , units ) ` . For example , the number of seed=None ) : return self.mask # to match the value shapes . if shape is None or shape [ 0 ] is None : from .losses import categorical_hinge if not num_words : Note that ` alt ` should have the * same shape * as ` x ` . ' '' . This layer has no information ' data : Attributes data to store . # In case of nested models : recover the first layer use_multiprocessing : if True , use process based threading . new_space = [ ] from .generic_utils import custom_object_scope 'passed to optimizer : ' + str ( k ) ) del model # deletes the existing model overwrite : Whether we should overwrite an existing file/object at the target if data_format='channels_first ' chunk = self.data.attrs [ chunk_attr ] closure = tuple ( c.cell_contents for c in func.__closure__ ) self._set_sublayers ( layer ) K.variable ( mask ) ) ) of random values to generate . shape = shapes [ i ] padding , # Same labels assumed . constraints.serialize ( self.depthwise_constraint ) ) return ( input_shape [ 0 ] , length , self.filters ) for element in list_of_classes : epochs=1 , batch_size=4 , self.schedule_decay = kwargs.pop ( 'schedule_decay ' , 0.004 ) output_shapes = unpack_singleton ( output_shapes ) non_dyn_shape = [ ] nones = _get_dynamic_axis_num ( x ) initial_state = self.get_initial_state ( inputs ) # when loading and converting saved weights . ` y_pred ` . `` `` '' Creates a 1-D tensor containing a sequence of integers . steps=steps , is_loss_wrapper = isinstance ( loss , losses.LossFunctionWrapper ) max_num_labels_tns , current_input ) A sliced tensor : build ( input_shape ) def test_check_last_is_one ( ) : # arbitrary data as ` embeddings_data ` and results from the fact zero = _to_tensor ( 0. , x.dtype.base_dtype ) if isinstance ( value , Layer ) : [ 0.92629528 , 0.28055015 , 1.70484698 ] ] , dtype=float32 ) output_shape = tf.stack ( output_shape ) outputs , bias_regularizer=bias_regularizer , strides : a tuple of 2 integers , specifying the strides return _SHARED_SEQUENCES [ uid ] [ i ] raise ValueError ( `` { } function does n't have any documentation '' .format ( name ) , m_t_prime = m_t / ( 1 . - m_schedule_next ) if mode not in [ 'auto ' , 'min ' , 'max ' ] : stride_d , 'Found : ' + str ( x ) ) # Raise an error if the corresponding layer node def __getitem__ ( self , key ) : num_samples = check_num_samples ( ins , self.recurrent_kernel_h = self.recurrent_kernel [ : , self.units * 2 : ] The dictionary containing the mapping from class names to class common_factor = T.max ( log_probs ) shape = [ None , None ] label = ' % s\n| { input : |output : } | { { % s } | { % s } } ' % ( label , if dim_ordering == 'th ' : width and height of the 2D convolution window . axes = [ self.axes % len ( shape1 ) , self.axes % len ( shape2 ) ] when using multiprocessing . custom_opt = optimizers.RMSprop updated_per_output_metrics = [ ] class Add ( _Merge ) : return cell assert_allclose ( zero_list [ i ] , z_list [ i ] , atol=1e-05 ) def square ( x ) : if isinstance ( value , metrics_module.Metric ) : kwargs [ 'utils ' ] = utils # shape ( batch , input_width , char_count ) param_dset [ : ] = val cropping [ 0 ] , 2 , dot.set ( 'rankdir ' , rankdir ) printable_module_name='constraint ' ) mean = squeeze ( mean , _axes ) from .convolutional import Cropping2D new_layer = keras.layers.Conv2D ( 5 , ( 3 , 3 ) , name='conv ' ) stateful_metrics=model.metrics_names [ 1 : ] ) ] dtype=dtype ) if hasattr ( x , '_is_metric ' ) : `` `` '' Densely connected highway network . 'as the output . ' ) return outputs [ -1 ] , np.stack ( outputs , axis=1 ) , states_tm1 assert np.mean ( np.abs ( samples [ 0 ] - samples [ 1 ] ) ) > 0 . _extract_archive ( fpath , datadir , archive_format ) layer.compute_mask ( computed_tensor , check_single_tensor_operation ( 'tile ' , ( 3 , 4 , 5 ) , WITH_NP , n= ( 1 , 2 ) ) strides , Returns : from tensorflow.keras.activations import softsign input_shape [ 1 ] + padding [ 0 ] + padding [ 1 ] , level : fraction of the entries in the tensor ( without its trained weights ) from this configuration . self.interpolation = interpolation def get_input_shape_at ( self , node_index ) : self.stateful = layer.stateful metric , output_shape=output_shapes [ i ] , loss_fn=loss_fns [ i ] ) args = ( args [ 0 ] , ) + args [ 2 : ] # faster than this y = np.array ( y , dtype='int ' ) if ( hasattr ( self.layer , 'activity_regularizer ' ) and positions = [ int ( line_length * p ) for p in positions ] self.keys = sorted ( logs.keys ( ) ) list/tuple of float threshold values in [ 0 , 1 ] . A threshold is > > > K.dtype ( K.placeholder ( shape= ( 2,4,5 ) , dtype='float64 ' ) ) def _preprocess_conv3d_kernel ( kernel , data_format ) : ( i.e . the number of output filters in the convolution ) . total_log_prob = total_log_prob + common_factor weight_values ) ) : return tf.less_equal ( x , y ) global _EPSILON from ` x ` ) . ] , name='nested ' ) `` `` '' Callback that accumulates epoch averages of metrics . batch of 64 processed samples . def get_index ( uid , i ) : parallel_model.fit ( [ a_x , b_x ] , [ a_y , b_y ] , epochs=epochs ) 'Found : ' + str ( cropping ) ) allowed_positional_args=None , go_backwards : Boolean ( default False ) . # hidden state projected by all gate matrices at once def _runner ( init , shape , target_mean=None , target_std=None , Determines the type of label arrays that are returned : super ( ConvRNN2D , self ) .__init__ ( cell , 'Error when checking model ' + exception_prefix batch_size = 64 op , input_shape , kernel_shape , WITH_NP , output_shape=output_shape , cell : A RNN cell instance . A RNN cell is a class that has : go_backwards=go_backwards , 2D tensor with shape : ` ( nb_samples , input_dim ) ` . WITH_NP , cntk_dynamicity=True , import six n_s = [ ] sample_weight=sample_weight , for k , v in six.iteritems ( self._fn_kwargs ) : NAME_SCOPE_STACK = [ ] if self.baseline is not None : from .. layers import Conv1D # install TensorFlow ( CPU version ) . None or a tensor ( or list of tensors , assert ( out2.max ( ) ! = out3.max ( ) ) return layer.name + '_ib- ' + str ( node_index ) prefix = self.__class__.__name__ if reshape and layer_weights_shape ! = weights [ 0 ] .shape : return obj.__name__ return self.layer.non_trainable_weights followed by a pointwise convolution which mixes together the resulting _x = x [ : , rep : ( rep + 1 ) ] if training is None : from .core import SpatialDropout3D All others will be averaged over time ( e.g . loss , etc ) . x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) states list are the hidden state and the cell state , respectively . ( tensor or callable that returns a tensor ) . y = np.stack ( y , axis=-1 ) if not dtype : each batch item in y_true assert o1._keras_shape == ( None , 3 , 2 , 1 ) base_config = super ( LossFunctionWrapper , self ) .get_config ( ) shape = tuple ( [ None for _ in range ( ndim ) ] ) z = f ( [ x_val ] ) [ 0 ] dynamic_axes=x.dynamic_axes ) for _ in states ] self.padding = padding self.recurrent_bias_z = None return np.cumprod ( x , axis=axis ) if not os.path.exists ( datadir ) : from tensorflow.keras.applications.densenet import preprocess_input 'in recurrent layers are deprecated . ' gamma = tf.cast ( gamma , tf.float32 ) this implementation relies on multiprocessing , In this case there will be soft-placing on the GPU device . `` `` '' Loads the Boston Housing dataset . 'optimizer attributes or optimizer state ' if instance is None : if 0 . < self.rate < 1. : self._is_compiled = True Dictionary of parameter names mapped to their values . x_r = matrix_x [ : , self.units : 2 * self.units ] mask=mask , if isinstance ( args [ 1 ] , ( list , tuple ) ) : sample_weight : User-provided ` sample_weight ` argument . def call ( self , inputs , mask=None ) : result_t._is_metric = True # Backwards compatibility This is the expected shape of your inputs dictionary with config . dim3_padding = conv_utils.normalize_tuple ( padding [ 2 ] , 2 , `` `` '' Multiplies the values in a tensor , alongside the specified axis . sequence_length=input_length ) return nasnet.NASNetLarge ( * args , * * kwargs ) preds3 = model3.predict ( [ np.random.random ( ( 1 , 32 ) ) ] ) > > > xy return x.dtype `` `` '' Sets entries in ` x ` to zero at random , alpha_initializer : initializer function for the weights . kernel_constraint='max_norm ' , name='d ' ) if name in group.attrs : return self._output_tensor_cache [ cache_key ] assert len ( model.updates ) == 2 ' , '.join ( [ x for x in bad_attributes ] ) ) ) index = axis if axis > = 0 else len ( shape ) + 1 axes over which to normalize . @ interfaces.legacy_upsampling3d_support name=name ) return node.input_tensors assert dense_layer.get_losses_for ( None ) == [ 1 ] if self.save_weights_only : http : //www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015 ) new_model_disk = load_model ( fname , * * load_kwargs ) with shape ( samples , sequence_length ) , NOTE that only part of the module is mocked and that the same Exceptions ` Dense ` implements the operation : self.activity_regularizer = regularizers.get ( activity_regularizer ) # Update self._per_input_updates `` `` '' Input layer code ( ` Input ` and ` InputLayer ` ) . axes [ 1 ] += y.ndim shape2 = list ( input_shapes [ 1 ] ) } ] ` layer_test ( layers.AveragePooling1D , func = K.function ( [ inputs ] , mask_outputs ) K.batch_set_value ( [ ( v , 0 ) for v in self.weights ] ) if len ( return_in_sub ) < len ( has_return ) : raise ValueError ( 'max_value of ReLU layer ' 'of the optimizer ( ' + str ( len ( params ) ) + ' ) ' ) from tensorflow.keras.applications.inception_v3 import decode_predictions from six.moves.urllib.request import urlopen # This also updates the layer history of the output tensor ( s ) . uid = object_list_uid ( inputs ) K.is_keras_tensor ( x ) else : return np.arange ( start , stop , step , dtype ) for sw , w in zip ( cell.weights , weights ) : target_size= ( height , width ) , kwargs.pop ( 'kernel_dim3 ' ) ) dropout=dropout , For instance , ` ( 2 , 1 ) ` permutes the first and second dimension sample_weight = 1.0 state_shape = self.compute_output_shape ( input_shape ) Subclasses should override for any actions to run . self.beta_regularizer = regularizers.get ( beta_regularizer ) filter_shape [ 0 ] , filter_shape [ 1 ] ) elif ( k == KC ) & ( cntk_two_dynamicity ) : padding = True retain_prob = 1 . - level super ( _ConfusionMatrixConditionCount , self ) .__init__ ( name=name , dtype=dtype ) In subsequent layers , there is no need for the ` input_shape ` : return tf.nn.softplus ( x ) k_inputs = K.variable ( inputs , dtype= '' float32 '' ) `` `` '' Instantiates an all-ones variable . h_pad = pool_size [ 1 ] - 2 if odd_pad_h else pool_size [ 1 ] - 1 A Numpy array of probability predictions . 'true_negatives ' , # The node is relevant to the model : return inputs * K.random_normal ( shape=K.shape ( inputs ) , x_shape = [ ] recurrent_kernel_shape = self.kernel_size + ( self.filters , self.filters * 4 ) super ( StackedRNNCells , self ) .__init__ ( * * kwargs ) build_fn : callable function or class instance input_shapes = [ ] output_shape += [ base [ : ] for _ in range ( 2 ) ] ` Sequence ` instance . on_train_begin : called at the beginning of model training . scope_name = 'lambda ' if self.name == ' < lambda > ' else self.name result = squeeze ( result , 1 ) `` `` '' Initializer capable of adapting its scale to the shape of weights . ( single state ) in which case it is specify ` shuffle=False ` when calling fit ( ) . # but increments states +1 per timestep batch_size=batch_size ) matches = self._fn ( y_true , y_pred , * * self._fn_kwargs ) axes = { } check_single_tensor_operation ( 'abs ' , ( 4 , 2 ) , WITH_NP ) and validation metrics values ( if applicable ) . f_stats.append ( assert K.get_session ( ) .run ( fetches= [ x , y ] ) == [ 11. , 5 . ] string , path to the saved model def greater_equal ( x , y ) : to another . object_type='class ' ) : An epoch is an iteration over the entire data provided , super ( CategoricalCrossentropy , self ) .__init__ ( dot.set_node_defaults ( shape='record ' ) pos = K.sum ( y_true * y_pred , axis=-1 ) ' ` cropping ` should be either an int , a tuple of 3 ints ' outputs = K.separable_conv1d ( y_permute_dim = [ y_permute_dim.pop ( -2 ) ] + y_permute_dim from .base_layer import Layer x_shape = ( 20 , 6 , 10 ) # behaves like tf.batch_matmul as default val = { '_is_group ' : True } return self._trainable ` constants ` . Such constants can be used to condition the cell scale : Whether to rescale image values layer : The layer to be wrapped . if time_per_unit > = 1 : index = 1 - _get_dynamic_axis_num ( x ) del base_config [ 'cell ' ] 'make it possible to access ' calling ` self.add_weight ( ) ` like : ` self.var = self.add_weight ( ... ) ` `` `` '' Saves an image stored as a Numpy array to a path or file object . def resize_images ( x , def predict_generator ( self , generator , > > > K.epsilon ( ) line = line [ : positions [ i ] ] validation_freq=validation_freq , # to stack recurrent layers , you must use return_sequences=True raise TypeError ( 'There are no layers in the model . ' ) optimizer_weight_names = [ zeros_like ( reduce_result ) ) threshold = K.cast ( threshold , y_pred.dtype ) elif K.backend ( ) == 'tensorflow ' : class_weight=class_weight , # List of tensors . 1:1 mapping with inbound_layers . from keras.backend import numpy_backend as KNP f , self.layers , skip_mismatch=skip_mismatch , if input_shape and input_shape [ -1 ] == 1 and len ( input_shape ) > 1 : isinstance ( path , six.string_types ) or legacy_recurrent_support = generate_legacy_interface ( ' has no inbound nodes . ' ) workers=workers , ( n + 1 ) D one hot representation of the input # We type-check that ` x ` and ` y ` are either single arrays This argument ( or alternatively , self.input_spec = input_spec if num_weights > 0 : else x for x in data use_multiprocessing : Boolean . Used for generator or feed_dict.update ( { self.batch_id : i , self.step : step } ) from .load_backend import any steps=None , end_axis=index + 1 ) return base_config ( 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='maxpool2d ' ) self.momentum = momentum `` `` '' Adamax optimizer from Adam paper 's Section 7 . x_o = K.bias_add ( x_o , self.bias_o ) backward compatible reason , if this attribute is not available sed -i `` \/keras\/backend\/ $ { KERAS_BACKEND } _backend.py/d '' .coveragerc to generate . filepath : string , path to save the model file . } ) [ WaveNet : A Generative Model for Raw Audio , section 2.1 ] ( steps_per_epoch=steps_per_epoch , if ( len ( v ) > 1 ) or ( len ( v ) == 1 and len ( v [ 0 ] .inbound_layers ) > 1 ) : class Hinge ( MeanMetricWrapper ) : from keras import layers if axis_without_batch ! = -1 and axis_without_batch ! = output_dimensions [ -1 ] : WITH_NP , cntk_two_dynamicity=True , assert merged._keras_shape == ( None , 16 * 2 ) h_i = self.recurrent_conv ( h_tm1_i , https : //www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf ) padding = 'VALID ' from .load_backend import separable_conv2d ref = [ 1.73308 ] from .merge import subtract shape= ( self.input_dim , self.output_dim ) , Note : that if hasattr ( x [ 0 ] , 'shape ' ) : that are n't inputs to this model ) . pool_size= ( 2 , 2 ) , border_mode='valid ' , name='maxpool2d ' ) # reset gate training . If instead you would like to use your own of images with shape ` ( batch_size , * target_size , channels ) ` indices = K.placeholder ( shape= ( 5 , 6 ) , dtype='int32 ' ) if input_length is None : score_sum = keras.layers.Add ( name='add ' ) ( [ score1 , score2 ] ) 'CuDNNGRU is not compatible with GRU ( reset_after=False ) ' ) A tensor , result of transposed 3D convolution . validation_data= ( [ input_a_np , input_b_np ] , ) ) string , path to the file to save the model to ` ( batch_size , channels , rows , cols ) ` input_layer = InputLayer ( batch_input_shape=batch_shape , ' a previous non-Input layer . ' warnings.warn ( ' ` implementation=0 ` has been deprecated , ' `` `` '' Print the message & the tensor when evaluated & return the same tensor . return K.cast ( K.equal ( K.argmax ( y_true , axis=-1 ) , from tensorflow.keras.datasets.reuters import get_word_index `` `` '' Computes the logarithm of the hyperbolic cosine of the prediction error . `` `` '' Computes cos of x element-wise . return proba.argmax ( axis=-1 ) for ( input_shape , pool_size ) in zip ( [ ( 5 , 10 , 12 , 3 ) , ( 5 , 10 , 12 , 6 , 3 ) ] , section `` Note on passing external constants '' below . `` `` '' Computes the squared hinge loss between ` y_true ` and ` y_pred ` . [ Adaptive Subgradient Methods for Online Learning and Stochastic @ contextmanager Assume x = [ [ 1 , 2 ] , [ 3 , 4 ] ] and y = [ [ 5 , 6 ] , [ 7 , 8 ] ] for w , new_w in zip ( new_model.get_weights ( ) , new_weights ) : meaning the confidence on label values are relaxed . e.g . computed_tensor , computed_mask = computed_data [ 0 ] accepted_name = [ 'from_config ' ] raise ValueError ( 'Unexpectedly found an instance of type ` ' install : # updated value in feed_dict will be modified within the K.function ( ) > > > kvar_zeros = K.zeros_like ( kvar ) FC_SHAPE = ( 200 , 100 ) index : position of the batch in the Sequence . name : ( Optional ) name for the loss . output , if self.padding ! = 'valid ' : y = K.reshape ( y , ( -1 , batch_size ) ) control_inputs : A list of Operation or Tensor objects a tensor . def test_depthwise_conv ( self , shape = K.int_shape ( self.outputs [ i ] ) output = concatenate ( output , axis=0 ) loss_name = loss.name for d_rec , d in zip ( datas_rec , datas ) : num_batches_tns = tf.stack ( [ label_shape [ 0 ] ] ) self._per_output_metrics = updated_per_output_metrics keywords=full_arg_spec.varkw , h = K.dot ( inputs , self.kernel ) name , ndim_diff = expr_ndim - cond_ndim # since it does not include m_schedule at head of the weight list . Set from tensorflow.keras import __version__ for dim in state_size ] from .pooling import GlobalAveragePooling3D return normed , mean , var input_dim = 5 loss == 'categorical_crossentropy ' ) > > > # A placeholder is not a Keras tensor . def test_random_normal ( self ) : the batch axis of the arrays matches the expected of ` directory ` for it to work correctly . return tf_keras_backend.constant ( from .convolutional import Convolution2D old_layer = keras.layers.AtrousConvolution1D ( 5 , 3 , if current : `` ` python from .recurrent import LSTMCell ' { } '.format ( self.state_spec , self.cell.state_size ) ) raise TypeError ( 'Layer ' + layer.name ` output_shape = ( None , ) + output_shape ` elif isinstance ( x , dict ) : return tf_state_ops.assign_add ( x , increment ) ` output_shape = ( input_shape [ 0 ] , ) + output_shape ` b_skip_idxs , zeros , b_active , log_b_curr , log_b_prev ) weights = np.ones ( ( 3 , ) ) return tf_keras_backend.random_binomial ( super ( Sequential , self ) .__init__ ( name=name ) inputs : 3D tensor with shape : ( batch_size , steps , input_dim ) num_time_step = inputs.shape [ 0 ] x , tf_data_format = _preprocess_conv3d_input ( x , data_format ) just by knowing the inputs and outputs of the model . batch_size=32 , class Lambda ( Layer ) : backward_updates = self.backward_layer.get_updates_for ( inputs ) indices = concatenate ( [ batch_ind , label_ind ] , axis=0 ) def flow_from_dataframe ( self , if shape [ i ] is None and dynamic_axis_index < nones : def test_on_epoch_end_threads_sequence_change_length ( ) : num_states = 2 if layer_class is keras.layers.CuDNNLSTM else 1 config = { 'strides ' : self.strides , fitting ( predicting ) parameters are selected in the following order : `` `` '' Functional interface to the ` Concatenate ` layer . norms = K.sqrt ( K.sum ( K.square ( w ) , axis=self.axis , keepdims=True ) ) self._input_dtypes = K.dtype ( inputs ) class DirectoryIterator ( image.DirectoryIterator , Iterator ) : `` `` '' Normalization layers . for cbk in callbacks : 'CNTK backend : The placeholder has been resolved ' batch_size=batch_size ) # biases : bias_z_i , bias_r_i , bias_h_i def DISABLED_test_CallbackValData ( ) : filtered_layers.append ( layer ) K.zeros_like ( numer ) ) ) inner_input_shape = self._get_shape_tuple ( ( -1 , ) , inputs , 2 ) [ Exact solutions to the nonlinear dynamics of learning in deep ' outputs , but you passed metrics= ' + str ( metrics ) ) output_names , workers=1 , np.random.seed ( random_seed + ident ) dim_ordering='th ' , # Verify that the metrics added using ` compile ` and ` add_metric ` API are return categorical_crossentropy ( target , output , from_logits , axis=-1 ) value_conversions= { 'dim_ordering ' : { 'tf ' : 'channels_last ' , x_train = np.empty ( ( num_train_samples , 3 , 32 , 32 ) , dtype='uint8 ' ) self.input_spec = layer.input_spec targets=self.targets , for i in range ( 1 , len ( outs ) ) : if sequential model : 1 . Squeezes last dim of ` y_pred ` or ` y_true ` if their rank differs by 1 . from .load_backend import batch_set_value if self.initial_weights is not None : '1st entry of padding ' ) time_per_unit = ( now - self._start ) / current `` `` '' Normalization layers . '' '' '' return paths value = output_values [ o ] if bias_dims == 1 : or computed before running the operations defined in the context . beta = tf.constant ( 0.0 , # Clone layer . if embeddings_freq ! = 0 : for i in range ( len ( fields ) ) : file_like.write ( f.read ( ) ) patience=5 , min_lr=0.001 ) from .. engine.base_layer import Layer , InputSpec x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) set_x = set_of_lengths ( inputs ) def load_function ( h5file ) : inputs_c = inputs def _prepare_total_loss ( self , masks=None ) : self.output_padding , 3 , 'output_padding ' ) ' but got shape ' + str ( x_shape ) ) x._keras_shape [ 1 ] , constraint=constraint ) raise Exception ( `` `` '' Sequential model class . '' '' '' self.on_batch_begin = lambda batch , logs : None epoch = initial_epoch metrics_utils.Reduction.WEIGHTED_MEAN ] : layer.bias_c_i , w = np.transpose ( w , ( 0 , 1 , 2 , 4 , 3 ) ) return np.all ( x , axis=axis , keepdims=keepdims ) logs [ k ] = self.totals [ k ] / self.seen samplewise_center=samplewise_center , self.l1 = K.cast_to_floatx ( l1 ) reduction=Reduction .SUM , name='mape_1 ' ) import warnings generate_legacy_method_interface = functools.partial ( generate_legacy_interface , [ On the Convergence of Adam and Beyond ] ( class _Merge ( Layer ) : from .merge import add from .. layers import deserialize as deserialize_layer from .merge import Multiply kept_states.append ( T.switch ( state_mask , new_state , state ) ) or 4D tensor with shape : ( 'dropout_U ' , 'recurrent_dropout ' ) , # We will compile after the first self.function = theano.function ( inputs , outputs , updates=updates , # We can go with reshape-based implementation for performance . super ( TopKCategoricalAccuracy , self ) .__init__ ( outputs = K.conv3d_transpose ( inputs , def val_generator ( ) : batch_size , ) + tuple ( kwargs [ 'input_shape ' ] ) x = np.random.randn ( 2 , 2 ) [ 0.0 , 0.0 , 0.0 , 0.0 ] , # t=4 ( ignored ) reduction_axes : iterable of integers , be used to directly access ` _GLOBAL_CUSTOM_OBJECTS ` . conv_out = conv_out [ : , : , : , : , : i ] sparse=False , `` `` '' Zero-padding layer for 3D data ( spatial or spatio-temporal ) . def test_fit_generator_with_class_weight ( self ) : nested_model = keras.models.Sequential ( [ pushd ~/mpi ' does not support masking , ' self.embeddings_constraint = constraints.get ( embeddings_constraint ) for _ in new_shape : strides=strides , dot.add_subgraph ( submodel_wrapper ) > > > keras_input = Input ( [ 10 ] ) stride=cell.strides [ 1 ] , Whether to use Theano or TensorFlow/CNTK data format # Update dimensions of weights to match with values if possible . new_shape = tf.shape ( x ) [ rows : cols + 1 ] in a single file . from theano.sandbox.softsign import softsign as T_softsign `` `` '' Runs CTC loss algorithm on each batch element . do_validation = False # and for each layer , which node and which params = self.weights [ 3. , 4 . ] ] ) go_backwards=go_backwards , return config A generator or ` keras.utils.Sequence ` returning regularization = 0 . layer : ` Recurrent ` instance . self.recurrent_kernel [ : , : , : , self.filters * 2 : self.filters * 3 ] ) super ( _Merge , self ) .__init__ ( * * kwargs ) if not isinstance ( layer , Layer ) : value = eval ( value ) ` ( batch , conv_dim1 , conv_dim2 , conv_dim3 , channels ) ` workers=4 ) inputs = keras.Input ( shape= ( 3 , ) ) # accepted in scope name . weight._tracked = True # and labels , from each line in the file if merge_mode not in [ 'sum ' , 'mul ' , 'ave ' , 'concat ' , None ] : assert _floatx in { 'float16 ' , 'float32 ' , 'float64 ' } str ( validation_data ) ) node_indices= [ ] , archive.extractall ( path ) callback.on_test_begin ( logs ) regularizer=self.bias_regularizer , def __init__ ( self , rate , noise_shape=None , seed=None , * * kwargs ) : 'argument to your first layer.\n ' def on_batch_end ( self , batch , logs=None ) : raise NotImplementedError config = projector.ProjectorConfig ( ) for i , output in enumerate ( layer.output ) : error_sq , sample_weight=sample_weight ) processes = ... print_fn ( 'Non-trainable params : { : , } '.format ( non_trainable_count ) ) mock_module.delete_file = self.delete_file self.csv_file = io.open ( self.filename , 'positional arguments : ' input_length : Length of input sequences , when it is constant . assert len ( states ) == 0 ` validation_data ` could be : for flag , v in zip ( is_initialized , candidate_vars ) : if self.model.uses_learning_phase : Both factors should be positive integers . from . import normalization greedy : perform much faster best-path search if ` True ` . or ` K.in_test_phase ( ) ` . assert np.abs ( np.mean ( rand ) - mean ) < 0.016 < tensorflow.python.ops.variables.Variable object at 0x10ab12dd0 > # # included epsilon=epsilon , def sqrt ( x ) : keras.layers.LSTMCell ( output_dim ) , reference_input_tensors = node.input_tensors # Do affine transformation etc ... @ pytest.mark.parametrize ( 'shape ' , [ ( 3 , ) , ( 1 , 3 ) , ( 2 , 1 ) , ( 4 , 2 ) , ( 4 , 2 , 3 ) ] ) Default parameters follow those provided in the original paper . fn : Callable to inspect . If set to False , sorts the data in alphanumeric order . str ( pv.shape ) inbound_layers.append ( None ) metrics_utils.update_state_wrapper ( obj.update_state ) , obj ) 'curve ' : self.curve.value , dtype = self.dtype list of GPU IDs on which to create model replicas . assert_allclose ( w , org_w ) Can be a tuple or function . v_t_prime = v_t / ( 1 . - K.pow ( self.beta_2 , t ) ) ` batch_shape= ( ... ) ` to all the first layers in your model . self.kernel_i = self.kernel [ : , : self.units ] if b_any ( _ == C.InferredDimension for _ in x.shape ) or b_any ( new_a = self.rho * a + ( 1 . - self.rho ) * K.square ( g ) if isinstance ( self.function , python_types.LambdaType ) : output = act + ( 1 - transform_weight ) * x # Support for RandomStreams ( ) .normal ( ) , .uniform ( ) . or a mismatch in the shape of the weight class Bidirectional ( Wrapper ) : m = SomeMetric ( ... ) compute accuracy . You should pass ` metrics= [ `` accuracy '' ] ` to def test_model_with_input_feed_tensor ( ) : from .wrappers import TimeDistributed the padding normal operation after lr has been reduced . supported . If PIL version 3.4.0 or newer is installed , A node from layer A to layer B is added to : if not self.file_exists ( filepath ) : from tensorflow.keras.layers import Conv3DTranspose # first column is probability of class 0 and second is of class 1 zero_list = [ ] [ ] , self._metrics , return K.all ( concatenated , axis=-1 , keepdims=False ) from tensorflow.keras.applications.xception import preprocess_input if inputs is not None : new_layer = keras.layers.Embedding ( 1 , 1 , name='d ' ) ( 0.1 , 5.0 , 0.8 ) , # set all a None shape is compatible with any shape . step_input_shape = ( input_shape [ 0 ] , ) + input_shape [ 2 : ] common_factor = T.max ( log_p_prev [ : active ] ) center : If True , add offset of ` beta ` to normalized tensor . for i in range ( num_samples ) : def foldr ( fn , elems , initializer=None , name=None ) : class_id=None , def preprocess_input ( * args , * * kwargs ) : val_outs = to_list ( val_outs ) model._make_train_function ( ) def wrapper ( * args , * * kwargs ) : assert outputs.op.name.lower ( ) .endswith ( '/relu6 ' ) denom = self.true_positives [ min_index ] + self.false_negatives [ min_index ] def test_save_load_weights_gcs ( ) : def build ( self , input_shape=None ) : def __init__ ( self , padding= ( 1 , 1 , 1 ) , data_format=None , * * kwargs ) : splits = tf.split ( value=x , num_or_size_splits=x_shape [ axis ] , axis=axis ) # [ output_a_np , output_b_np [ :2 ] ] , x = tf.transpose ( x , ( 0 , 3 , 1 , 2 ) ) # NHWC - > NCHW if isinstance ( a , C.Axis ) \ metrics_dict , h_tm1_c = h_tm1 return args , kwargs , converted + _converted used for the linear transformation of the recurrent state y_shape = tuple ( y_shape ) output_shape : Shape tuple . See ` input_shape ` . 'call ` multi_gpu_model ` with ` gpus > = 2 ` . ' `` `` '' Returns a tensor with the same content as the input tensor . node_index=node_index , def test_relu_tf_ops ( ) : This will not include the ` compile ` metrics of a model layer . self.amsgrad = amsgrad elif self.merge_mode is None : The file hash compress ( args , args_not_in_doc ) ) ) , member.__module__ ) 'correspond to layer ' + name target_gpu_id = [ 0 , 2 , 4 ] kernel_regularizer=kernel_regularizer , model.compile ( optimizer , loss= [ 'mse ' , 'mae ' , 'mape ' ] ) initial_state = [ initial_state ] data_format : Image data format , either `` channels_first '' or `` channels_last '' . return self.layers [ index ] existing_class_weight = set ( class_weight.keys ( ) ) class MeanSquaredError ( Loss ) : if len ( successive_outputs ) == 0 : criterion = ( cell_losses = cell.losses kernel_regularizer='l1 ' , layer = Lambda ( hadamard_product_sum , hadamard_product_sum_output_shape ) func : the function to serialize . def _postprocess_conv2d_output ( x , data_format ) : target_size=target_size , set_inputs = False # In stack , each array must have the same shape . inputs = self._feed_inputs does not exist . Also raises ValueError when ` steps ` is not ` None ` # Compute total loss . import json from keras_applications import mobilenet_v2 return np.prod ( int_shape ( x ) ) if self.restore_best_weights : step = min ( self.batch_size , val_size - i ) recursively . # Instantiate the base model ( or `` template '' model ) . The purpose of this argument is to preserve weight x_h = K.bias_add ( x_h , self.input_bias_h ) self.input_spec = full_input_spec # sequence axis is removed by default , so do n't need reshape on it filter_flip=not flip_filters , def test_func ( ) : list ( compress ( args , styles ) ) ) , bar += ( ' . ' * ( self.width - prog_width ) ) def test_function_tf_feed_dict ( self ) : `` `` '' Displays a progress bar . from pathlib import Path In this case ` call ` just reapplies if data_format ! = 'channels_last ' : The new int_shape with the first part from init_tuple # we assume a 1:1 mapping from tensor to mask target , from .load_backend import random_binomial import cntk as C shape : The shape tuple of the weight . self.bias = bias outputs , states = step_function ( inputs [ i ] , states + constants ) target : Total number of steps expected , None if unknown . conversions= [ ( 'pool_length ' , 'pool_size ' ) , name='bias ' , p_prev = T.exp ( log_p_prev [ : active ] - common_factor ) pattern = [ of the output shape return count_params ( self.weights ) input_shapes = unpack_singleton ( input_length , check_single_tensor_operation ( 'batch_flatten ' , ( 20 , 2 , 5 ) , WITH_NP , 'l2 ' : self.l2 } low=minval , * ` x.shape [ 1 ] ` : 20 : do not append to output shape , state_size = self.cell.state_size self , image_shape = _preprocess_conv2d_image_shape ( int_shape ( x ) , data_format ) _IMAGE_DATA_FORMAT = str ( data_format ) raise ValueError ( 'Expected learning phase to be ' from .load_backend import get_value return np.sum ( target * -np.log ( output ) , axis=-1 , keepdims=False ) if not isinstance ( y , np.ndarray ) and not K.is_tensor ( y ) : return unpack_singleton ( outputs ) Everything gets normalized to a single sample-wise ( or timestep-wise ) if is_chunked : # return a set with the variation between ` ( batch , channels , upsampled_rows , upsampled_cols ) ` initial_state=forward_state , * * kwargs ) return x.shape dtype=dtype , raise ValueError ( 'Unknown ` count_mode ` : ' + str ( count_mode ) ) activations and will otherwise just result in an effective learning rate state_mask = [ None for _ in self.states ] for i in range ( len ( axes ) ) : layer : Layer from which ` tensor ` comes from . If not provided , reason='Theano behaves differently ' targets_values = predictions [ T.arange ( targets.shape [ 0 ] ) , targets ] ( 2 , 2 , 2 ) , ( 2 , 2 , 2 ) , 'valid ' , name='avgpooling3d ' ) class Metric ( Layer ) : y = keras.layers.Dense ( 2 ) ( x ) h_tm1_c = h_tm1 show_layer_names , rankdir , if self._is_graph_network : output_dim : int > = 0 . Dimension of the dense embedding . 'return_state ' : self.return_state , gamma = tf.reshape ( gamma , [ -1 ] ) def test_check_not_failing ( ) : after each block read thereafter . AtrousConv2D = AtrousConvolution2D 'You should pass ` metrics= [ `` accuracy '' ] ` to ' return ( self.forward_layer.trainable_weights def _add_unique_metric_name ( self , metric_name , output_index ) : improvement . fetches the required object from GCS if ` filepath ` starts with `` gs : // '' . initializer : The first value used ( elems [ 0 ] in case of None ) new_shape = list ( shape ) Use weights of 0 to mask values . color_mode='rgb ' , # # ` loss_weights ` does not match output_names . layers will be saved . If set to 0 , embeddings wo n't be computed . self.trainer_output = tuple ( [ f.output for f in criterion ] ) def test_doc_multiple_sections_code ( ) : max_value = min_value def call ( self , x ) : padding = 'SAME ' ` metrics= [ 'accuracy ' ] ` . To specify different metrics for different def built ( self ) : axis=axis_without_batch ) beta = zeros_like ( gamma ) `` `` '' Pads the 2nd and 3rd dimensions of a 4D tensor . if 'acc ' in self.monitor or self.monitor.startswith ( 'fmeasure ' ) : break x = tf.transpose ( x , ( 0 , 2 , 1 ) ) # NCW - > NWC ' weights . Provided weights : ' from .utils import losses_utils epsilon ) padding , data_format , pool_mode='max ' ) assert tuple ( n.shape ) == ( None , 5 ) logs : dict , currently no data is passed to this argument for this method super ( SimpleRNNCell , self ) .__init__ ( * * kwargs ) def call ( self , y_true , y_pred ) : 'an instance of class Layer . ' 0 = silent , 1 = progress bar . class Callback ( object ) : update_op = update_state_fn ( * args , * * kwargs ) beta = zeros_like ( x ) elif hasattr ( padding , '__len__ ' ) : while self.is_running ( ) : old_layer = keras.layers.GRU ( input_shape= [ 3 , 5 ] , output_dim=2 , name='d ' ) This tuple ( a single output of the generator ) makes a single def DISABLED_test_fit_generator_dynamic_size_sequence_main_thread ( ) : num_col = 6 layer.bias_r , # expand mask so that ` mask [ : , t ] .ndim == x.ndim ` ( which should be the same as the size of the cell output ) . order : Normalization order ( e.g . 2 for L2 norm ) . If necessary , we ` build ` the layer to match while _is_graph_model ( layer ) : sample_weight_mode , output_name , data : User-provided input data ( polymorphic ) . self.targets.append ( target ) class TensorBoard ( tf.keras.callbacks.TensorBoard ) : global uses_learning_phase name='sd3d ' ) # Replicates the model on 8 GPUs . super ( CosineSimilarity , self ) .__init__ ( 'Conv2D ' , expected_state [ 1 ] += ( num_timesteps - mask_last_num_timesteps ) # Otherwise , linearly interpolate ( num_thresholds - 2 ) thresholds in from .base_layer import Node self.model.save ( filepath , overwrite=True ) return { 'class_name ' : layer.__class__.__name__ , > > > K.set_floatx ( 'float16 ' ) maxval : A float , upper boundary of the uniform distribution return 'InputSpec ( % s ) ' % ' , '.join ( x for x in spec if x ) There are two variants . The default one is based on 1406.1078v3 and x , ` ( batch , height , width , channels ) ` while ` `` channels_first '' ` The metric name . from keras.losses import Reduction `` `` '' This test may cause Travis to hang . '' '' '' value = value.value depthwise_kernel_shape = ( self.kernel_size [ 0 ] , first = K.get_uid ( ) source = 'LSTM ' this model could have been built on CPU , for instance similarity is computed . pydot = None for p , g , m , v in zip ( params , grads , ms , vs ) : categorical = np.reshape ( categorical , output_shape ) by sample_weight or class_weight during training and testing . the axes to sum over . If ` None ` ( default ) , sums over all `` `` '' Save a model to a HDF5 file . reps [ auxiliary_axis ] = rep self.bias = self.add_weight ( shape= ( self.filters , ) , import pytest test_cases.append ( [ ( None , 3 , 4 ) , ( None , 2 , 3 , 4 ) , ( 2 , 3 ) ] ) return nasnet.NASNetMobile ( * args , * * kwargs ) containing consecutive data points ( timesteps ) . axis : Axis along which to concatenate . output_tensors = to_list ( output_tensors ) if len ( match ) > 1 : 'dimension . ' ) ` ( batch , channels , height , width ) ` . `` `` '' Compute the moving average of a variable . # now model.output_shape == ( None , 6 , 32 ) `` be provided . '' ) y_true : Ground truth values . return self.cell.activation mape = keras.losses.MeanAbsolutePercentageError ( ) if self.verbose > 0 : callbacks : List of callbacks or an instance of # If obj is any numpy type print ( ' % d gpus inference : ' % i , total_time ) def test_cropping2d_legacy_interface ( ) : for state in initial_state ] dtype=dtype , return self.data.ndim ` ( batch_size , features , steps ) ` verbose=0 ) import keras.backend as K http : //www.cs.toronto.edu/~fritz/absps/momentum.pdf ) target_mean=0. , target_max=1 , target_min=-1 ) def update_sub ( x , decrement ) : env : KERAS_BACKEND=cntk PYTHONWARNINGS=ignore RUN_ONLY_BACKEND_TESTS=1 ( self.mode == 'auto ' and 'acc ' not in self.monitor ) ) : check_two_tensor_operation ( if image_shape is not None : subsample= ( 1 , 1 ) , 'input ' ) self.curve = curve shape = ( bias.shape [ 2 ] , ) + bias.shape [ :2 ] def test_multi_gpu_simple_model ( ) : `` `` '' Built-in activation functions . self.cooldown = cooldown output = _reduce_on_axis ( x , axis , 'reduce_prod ' ) { self._confusion_matrix_cond : self.accumulator } , of ` x.dot ( y.T ) ` , although we never have to calculate the off-diagonal will be considered to be the temporal dimension . The value of Arbitrary . Use the keyword argument input_shape out_depth , out_height , out_width ) If all outputs in the model are named , ret = ret.dimshuffle ( ' x ' , 0 ) with h5py.File ( file_id , * * h5_file_args ) as h5_file : return vgg16.preprocess_input ( * args , * * kwargs ) 'Identity matrix initializer ' validate_filenames=validate_filenames ) return tf_keras_backend.batch_get_value ( ops ) def linear ( x ) : self._output_shape = tuple ( output_shape ) the user should handle the initialization . < tf.Tensor 'MatMul_9:0 ' shape= ( 32 , 28 , 4 ) dtype=float32 > ValueError : If ` unroll ` is ` True ` op , 'recurrent_dropout ' : self.recurrent_dropout } y = permute_dimensions ( y , permute_pattern ) raise ValueError ( 'All cells must have a ` call ` method . ' def __new__ ( cls , * args , * * kwargs ) : if cache_key in self._output_tensor_cache : } if not metrics : # install pyux def __init__ ( self , rate , * * kwargs ) : This is useful when using [ recurrent layers ] ( recurrent.md ) return self [ key ] legacy_input_support = generate_legacy_interface ( def v1_variable_initialization ( ) : losses = K.cast ( losses , K.floatx ( ) ) env : KERAS_BACKEND=theano THEANO_FLAGS=optimizer=fast_compile MKL= '' mkl mkl-service '' RUN_ONLY_BACKEND_TESTS=1 `` `` '' CIFAR100 small images classification dataset . '' '' '' model.compile ( optimizer , loss='mse ' , loss_weights= { 'lstm ' : 0.5 } ) return_sequences=True ) , A tensor , result of transposed 2D convolution . write_graph=write_graph , ' : you are passing a list as input to your model , ' * ` __init__ ( ) ` : All state variables should be created in this method by legal_params_fns.append ( self.__call__ ) return result_t input_c = tf.expand_dims ( input_c , axis=0 ) total_loss += loss_tensor 'CNTK only supports float32 , float64 , and ' [ group.attrs [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) class StackedRNNCells ( Layer ) : from .. utils.generic_utils import func_dump paths_tensors ) ` fn ( y_true , y_pred , * * kwargs ) ` . self.sensitivity = sensitivity K.binary_crossentropy ( y_true , y_pred , from_logits=from_logits ) , axis=-1 ) import keras_preprocessing momentum_cache_t_1 = self.beta_1 * ( 1 . - 0.5 * ( label_shape ) output = [ ] return inputs [ 0 ] - inputs [ 1 ] bar = ' % 7d/Unknown ' % current target_shape.append ( x_shape [ axis ] ) nw = len ( weights ) num_words : max number of words to include . Words are ranked get_input_shape_at 'and Theano has some dependency issues for sparse . ' ) if ndim is not None : return y wget https : //repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh -O miniconda.sh ; from .data_utils import OrderedEnqueuer # ( k == 0 or k > num_classes ) does not raise an error axis=-1 ) ) , noise_shape : shape for randomly generated keep/drop flags , model : A Keras model instance . To avoid OOM errors , elif dims == 3 : positions = positions or [ .45 , .85 , 1 . ] super ( Activation , self ) .__init__ ( * * kwargs ) You can also post * * bug reports and feature requests * * ( only ) in [ GitHub issues ] ( https : //github.com/keras-team/keras/issues ) . Make sure to read [ our guidelines ] ( https : //github.com/keras-team/keras/blob/master/CONTRIBUTING.md ) first . > > > input_transposed val_enqueuer.start ( workers=workers , raise ValueError ( 'Can not perform batch_dot over axis 0 . ' return AUCSummationMethod.MINORING or K.int_shape ( tensor ) , where every ` None ` is replaced by momentum : float > = 0 . Parameter that accelerates SGD for i in range ( x.shape [ 0 ] ) : def _reset_batch_timing ( self ) : parallel_model.compile ( loss='mean_squared_error ' , optimizer='adam ' ) epsilon : Small float added to variance to avoid dividing by zero . binary_targets = np.array ( [ [ .3 , .7 ] , [ .2 , .8 ] , [ .4 , .6 ] , [ .1 , .9 ] ] , n.decode ( 'utf8 ' ) for n in def start ( self , workers=1 , max_queue_size=10 ) : print_fn ( line ) return T.patternbroadcast ( x , broadcastable ) from .utils.generic_utils import to_list class Minimum ( _Merge ) : inner_mask_shape = self._get_shape_tuple ( ( -1 , ) , mask , 2 ) sub_w_last_node = { } model.fit ( x , y , epochs=1 , batch_size=1 ) # Check for redundancy in inputs . assert_list_pairwise ( new_val_list ) [ str ( node.output_shapes ) for node in self._inbound_nodes ] ) hasher = 'sha256 ' bias_regularizer=bias_regularizer , self.model = self.build_fn ( * * self.filter_sk_params ( self.build_fn ) ) return tf_keras_backend.random_uniform ( `` `` '' Exponential linear unit node_index = node.node_indices [ j ] ' ` type not understood : ' first_connection = `` x , kernel , return unpack_singleton ( input_shapes ) and you wish to share parameters across space assert_blank_before ( name , member , doc , [ ' # Arguments ' , ' # Raises ' , ' # Returns ' ] ) tensor_index = self._output_coordinates [ i ] [ 2 ] model.add ( Masking ( mask_value=0. , input_shape= ( timesteps , features ) ) ) kwargs [ 'dilations ' ] = dilation_rate For instance , if your inputs have shape `` `` '' Converts input first axis to CNTK static axis . if [ [ `` $ MODE '' == `` INTEGRATION_TESTS '' ] ] ; then layer = layer_class ( 2 , recurrent_initializer='identity ' ) def normalize ( x , axis=-1 , order=2 ) : ( 'pool3d ' , ( 2 , 3 , 7 , 7 , 7 ) , ( 3 , 3 , 3 ) , ( 1 , 1 , 1 ) , from .training_utils import is_sequence spatial_axes= ( 1 , 2 , 3 ) ) UserWarning ) dtype : String , data type of returned Keras variable _y = [ ] y._keras_shape = tuple ( keras_shape_list ) fpath = os.path.join ( path , 'test ' ) with K.name_scope ( 'num_elements ' ) as scope : class _ConfusionMatrixConditionCount ( Metric ) : assert json.dumps ( old_layer.get_config ( ) ) == json.dumps ( new_layer_2.get_config ( ) ) # non-trainable weights target_tensors = [ v for v in y if K.is_tensor ( v ) ] new_layer = keras.layers.GlobalAvgPool2D ( data_format='channels_first ' , as they are instantiated ( default ) , or if ValueError : in case the layer is missing shape information original_backend ) kernel = reshape ( kernel , kernel_shape ) # Verify input_map has one mapping from inputs to reshaped inputs . ( ie . `` linear '' activation : a ( x ) = x ) . val_x , val_y = validation_data mape = MAPE = mean_absolute_percentage_error cells output return C.sqrt ( var ( x , axis=axis , keepdims=keepdims ) ) CTC loss of each element . from .cudnn_recurrent import CuDNNGRU projector.visualize_embeddings ( self.writer , config ) kwargs [ 'models ' ] = models [ 0. , 0. , 0. , 0 . ] , recurrent_z = K.dot ( h_tm1_z , self.recurrent_kernel_z ) if accept_all : try : if K.backend ( ) in uses_correlation : class UpSampling2D ( _UpSampling ) : 'bias_regularizer ' : 'l2 ' , return root_gradients `` `` '' The ` Model ` class adds training & evaluation routines to a ` Network ` . nodes = [ ] def test_update ( self ) : def DISABLED_test_trainable_weights_count_consistency ( ) : period : Interval ( number of epochs ) between checkpoints . itself=True ) self.mask_zero = mask_zero slices = [ ] assert len ( layer.non_trainable_weights ) == 3 regularizer=None , dot = model_to_dot ( model , show_shapes , show_layer_names , rankdir , from .merge import dot @ pytest.mark.skipif ( ( K.backend ( ) == 'cntk ' ) , return _old_batch_normalization ( x , mean , var , beta , gamma , epsilon ) from tensorflow.keras.layers import SeparableConv1D # Add arbitrary offset - this is fine if 'mask ' not in kwargs : all_outputs [ o ] .append ( outputs [ o ] ) 'which is not available . Available metrics are : % s ' % if self.seed is not None : y = f [ ' y ' ] self.loss_weights = loss_weights be stopped . if sys.version_info [ 0 ] == 3 : computed_masks ) ) x = Input ( shape= ( 32 , ) ) uses_lp = getattr ( self , 'uses_learning_phase ' , False ) or uses_lp option can lead to speed-up in the execution of this function . _LEARNING_PHASE = T.scalar ( dtype='uint8 ' , name='keras_learning_phase ' ) new_shape.insert ( index , 1 ) if hasattr ( self.cell.state_size , '__len__ ' ) : if a0 ! = x_ndim - 1 : x = np.random.randn ( 1 , 2 ) keepdims ) axes = [ self.axes ] * 2 target_mean=0 . ) inputs_hash = None # delete and recreate model with ` use_bias=False ` def test_cudnnrnn_bidirectional ( ) : self.recurrent_initializer = recurrent_identity init='normal ' , x_ndim += 1 # KTF.function ( ) these do not have control dependency on ` outputs ` , so def test_multi_gpu_with_siamese ( ) : # Raise error if ndim of weights is > values . if K.backend ( ) == 'theano ' : array ( [ 0 , 2 , 1 , 2 , 0 ] ) input_row , input_col = input_shape [ 1 : -1 ] If a single int is provided , return pool_out the augmented pictures being generated def test_random_uniform ( self ) : _gcs_copy ( filepath , tmp_filepath ) cell_config = self.cell.get_config ( ) allowed_positional_args= [ 'generator ' , 'steps_per_epoch ' , 'epochs ' ] , ` ( batch_size , ) + target_shape ` above_threshold = x * ( x > = threshold ) from .noise import AlphaDropout # and output arrays of shape ( * , 32 ) are_different = K.concatenate ( [ pMin , pMax ] , axis=0 ) % ( epoch + 1 , self.monitor , self.best , epochs=100 , tmp = C.ops.slice ( x , axis , i , i + 1 ) axis = [ axis ] would be .3 . You can provide logits of classes as ` y_pred ` , since argmax of if seed is None : xt = tf.reshape ( x , [ -1 , x_shape [ -1 ] ] ) self._built = False raise ValueError ( 'Masking is not supported for CuDNN RNNs . ' ) 3D tensor with shape : output_shape += [ 1 ] from .io_utils import H5Dict return self.forward_layer.losses + self.backward_layer.losses constraint=self.W_constraint ) value = kwargs.pop ( old_name ) outs [ 0 ] += float ( batch_outs ) * len ( batch_ids ) # set up keras backend base = transpose_shape ( base , cell.data_format , spatial_axes= ( 1 , 2 ) ) x = to_list ( input_tensors ) [ 0 ] C.variables.Variable , import sys labels = labels [ indices ] 'only with TensorFlow backend . ' ) with context.eager_mode ( ) : are_different = K.any ( are_different , axis=0 ) kernel = C.transpose ( kernel , ( 4 , 3 , 0 , 1 , 2 ) ) if isinstance ( args [ 1 ] , ( tuple , list ) ) : def add ( self , layer ) : self.dilation_rate [ 0 ] ) For example : if ` filepath ` is ` weights . { epoch:02d } - { val_loss : .2f } .hdf5 ` , test loading model weights by name on : batch . Therefore , all arrays in this tuple must have the same class Accuracy ( MeanMetricWrapper ) : subset=subset , return variable ( value=p.value + low + scale ) def update ( x , new_x ) : The second variant is compatible with CuDNNGRU ( GPU-only ) and allows del shape [ _ ] return layer ( getattr ( losses , loss_fn.fn.__name__ , None ) is None ) ) ) : for l , o in zip ( model.metrics_names , batch_outs ) : all_outs [ i ] .append ( out ) if input_shape [ 0 ] is None : # Get or create layer . output_shape = list ( input_shape ) # This is always a single layer , never a list . self.sess = K.get_session ( ) model.add ( TimeDistributed ( Conv2D ( 64 , ( 3 , 3 ) ) , if not embeddings_layer_names : info += ' - % s : ' % k self.save_weights_only = save_weights_only name='maxpool3d ' ) unique_tensors_ids.add ( id ( x ) ) # time the model gets called on training data . [ [ 1.0 , 0.0 , 0.0 , 0.0 ] , # t=0 > > > var = K.variable ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] ] ) return K.elu ( inputs , self.alpha ) m = keras.metrics.Sum ( ) 'channels_last ' dirname = 'cifar-100-python ' def ctc_update_log_p ( skip_idxs , zeros , active , log_p_curr , log_p_prev ) : if ndim ( gamma ) > axis : ` false_negatives ` , that are used to compute the recall . This value is you should not pass non-picklable arguments to the generator @ disable_tracking You can cast a Keras variable but it still returns a Keras tensor . @ wraps ( save_function ) model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.BinaryAccuracy ( ) ] ) confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES , A pair of list of sample weights and sample weight modes `` `` '' Contains the base Layer class , from which all layers inherit . conversions= [ ( 'output_dim ' , 'units ' ) , shape.insert ( 1 , n ) weights : The concatenation of the lists trainable_weights and from .advanced_activations import Softmax output_shapes = None config [ k ] = K.eval ( v ) if K.is_tensor ( v ) or K.is_variable ( v ) else v preprocessor=conv1d_args_preprocessor ) x : Tensor or variable to resize . The same Numpy array , cast to its new type . # List of tensors , created by outbound_layer.compute_mask ( ) . def test_multi_gpu_with_multi_input_layers ( ) : model.save ( 'my_model.h5 ' ) labels = np.asarray ( [ [ 0 , 1 , 2 , 1 , 0 ] ] ) if isinstance ( layer , InputLayer ) : return input_shape return ( getattr ( seq , 'use_sequence_api ' , False ) input_size = 10 A tensor with expanded dimensions . in each line . If not provided , weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 2 , 3 , 4 , 1 , 0 ) ) layer.__class__.__name__ == 'Sequential ' ) The default 'auto ' is [ 'tar ' , 'zip ' ] . assert K.int_shape ( y ) == ( 1 , None , None , None , 1 ) dilation=self.dilation_rate [ i ] ) when using process-based threading . elif self.mode == 'fan_out ' : len_dim3 = input_shape [ 3 ] callbacks.on_epoch_begin ( epoch ) make_sampling_table = sequence.make_sampling_table If a file object was used instead of a filename , this val = np.random.random ( ( 100 , 100 ) ) def test_ordered_enqueuer_timeout_threads ( ) : sys.stderr.write ( 'Using TensorFlow backend.\n ' ) quadratic = K.minimum ( abs_error , delta ) * ` y.shape [ 2 ] ` : 20 : do not append to output shape , Dropout consists in randomly setting and makes sense only in the context of model serialization/ for p , g , a in zip ( params , grads , accumulators ) : # We do n't transpose inputs if they are class ProgressTracker ( object ) : # a = activations.get ( layer ) assert 0 < = test_split < 1 # Poor man 's truncated normal : we literally clip the tensor val_data = self.validation_data # if generator is instance of Sequence and steps_per_epoch are not provided [ 1. , 1. , 1. , 1 . ] , if i not in self.shared_axes : if masks is None : def lecun_normal ( seed=None ) : self._feed_inputs = [ ] kwargs [ 'dilation_rate ' ] = dilation_rate check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis= [ 1 , -1 ] ) layer.recurrent_kernel_z , if len ( all_output_shapes ) == 1 : # Additional checks to avoid users mistakenly Python dictionary . datapath : string , path to a HDF5 file x : Input samples , as a Numpy array . z = f ( [ x_val ] ) [ 0 ] 'GRU ( reset_after=False ) ' ) if converted : '= ' + str ( ndim_expr ) ) with keras.utils.CustomObjectScope ( Alpha Dropout fits well to Scaled Exponential Linear Units x = np.random.random ( ( num_samples , timesteps , input_size ) ) if getattr ( p , 'constraint ' , None ) is not None : from tensorflow.keras.layers import UpSampling3D layer = node.outbound_layer return super ( ConvRNN2D , self ) .__call__ ( inputs , * * kwargs ) min_value = -np.inf JSON-serializable structure representing ` obj ` . base_config = super ( Activation , self ) .get_config ( ) # It would be unreliable to build a dictionary # print ( 'val_seq.logs ' , val_seq.logs ) y , val_y = ( slice_arrays ( y , 0 , split_at ) , self.add_loss ( regularization_loss , inputs ) def print_tensor ( x , message= '' ) : if isinstance ( loss , collections.Mapping ) : strides : An integer or tuple/list of 2 integers , dot.write ( to_file , format=extension ) with warnings.catch_warnings ( ) : from .load_backend import identity if hasattr ( x , 'dynamic_axes ' ) : input_shape = shape [ 1 : ] key for key in variables_to_update if key not in list ( ConfusionMatrix ) def _call_batch_hook ( self , mode , hook , batch , logs=None ) : filters=filters , A layer with ` n ` input tensors must have seed = np.random.randint ( 1 , 10e6 ) HTTP POST , with a ` data ` argument which is a padding : string , ` `` same '' ` or ` `` valid '' ` . warnings.warn ( 'Could not import the CNTK backend ' ) self.data_format ) input_shapes.append ( x_elem._keras_shape ) out2 = model.predict ( np.ones ( ( num_samples , timesteps ) ) ) additional_inputs += constants return len ( self.data ) grad_parameter_dict [ g ] = v a 2D input with shape ` ( batch_size , input_dim ) ` . return np.reshape ( x , ( x.shape [ 0 ] , -1 ) ) for chunk in chunk_read ( response , reporthook=reporthook ) : The next value of generator ` uid ` . nb_worker=1 , pickle_safe=True , max_q_size=3 ) val_data += [ 0 . ] channel_axis = 1 if self.data_format == 'channels_first ' else -1 six.reraise ( * sys.exc_info ( ) ) if not ins or any ( K.is_tensor ( x ) for x in ins ) : update_tn = ConfusionMatrix.TRUE_NEGATIVES in variables_to_update model.add ( Dense ( units=64 , activation='relu ' , input_dim=100 ) ) for name in target_tensors : def evaluate_generator ( model , generator , return self.forward_layer.get_weights ( ) + self.backward_layer.get_weights ( ) load from . batch_shape = tuple ( input_shape ) layer_names = load_attributes_from_hdf5_group ( f , 'layer_names ' ) def test_avgpooling2d_legacy_interface ( ) : mean , variant = _moments ( x , _normalize_axis ( reduction_axes , x ) ) validation_steps=None , Specifying any ` strides ! =1 ` is incompatible with specifying kwargs = { } check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=-1 , % ( ndim ( bias ) , ndim ( x ) - 1 ) ) initializer=self.moving_variance_initializer , rnn_constants = constants shape = int_shape ( x ) def test_training_and_eval_methods_on_symbolic_tensors_single_io ( ) : data_format=tf_data_format ) List of callbacks to apply during prediction . model.compile ( optimizer , loss='mse ' , loss_weights= [ 0.5 ] ) model._make_train_function ( ) unroll , if os.path.exists ( self.filename ) : callbacks=callbacks , verbose=0 ) weights = weights [ num_weights : ] ( 'max_q_size ' , 'max_queue_size ' ) ] , assert K.int_shape ( o ) == ( None , 1 ) config : Optimizer configuration dictionary . batch_logs = { 'batch ' : step_index , 'size ' : 1 } # Update _keras_shape . will be done by the next layer . ask.assert_called_once ( ) k : An ` int ` , number of top elements to consider . inds = [ 1 , 3 , 7 , 9 ] skip_target_weighing_indices , sample_weight_mode [ i ] , name , i ) An instance of ` Model ` reproducing the behavior ( you should n't need to -- they are universal function approximators ) , elif x_ndim > 1 : val_enqueuer_gen = val_enqueuer.get ( ) implementing an efficient stacked RNN . 'Use keyword arguments instead . ' ) kwargs [ 'dilation_rate ' ] = rate hashing_trick = text.hashing_trick return K.in_train_phase ( noised , inputs , training=training ) export MKL_THREADING_LAYER= '' GNU '' shuffle=shuffle , `` `` '' Transpose and cast the input before the conv2d . return any ( [ x._uses_learning_phase for x in self.outputs ] ) return y / np.sum ( y , axis , keepdims=True ) data_format='channels_last ' , output_shape [ -1 : ] ] , 0 ) initial_state = inputs [ 1 : ] and is a generator . Total number of steps ( batches of samples ) the keyword argument ` input_shape ` ) `` `` '' Uniquify list of strings . return any ( [ ( hasattr ( layer , 'stateful ' ) and inner_init='glorot_uniform ' , axis = axis [ num_dynamic_axis : ] import keras out = th_sparse_module.basic.structured_dot ( x , y ) for stride , out_pad in zip ( self.strides , self.output_padding ) : function_type = 'lambda ' return T.arange ( start , stop=stop , step=step , dtype=dtype ) Either ` x ` or ` alt ` based on ` K.learning_phase ` . ValueError : if ` data_format ` is neither if isinstance ( model.optimizer , optimizers.TFOptimizer ) : output = np.clip ( output , 1e-7 , 1 - 1e-7 ) if histogram_freq ! = 0 : self._reset ( ) assert_doc_style ( name , member , doc ) # sample_weight=sample_weight ) send_as_json=False ) : kernel [ i * output_col + j , : , : ] ) ) def DISABLED_test_model_loading_from_binary_stream ( ) : for i , ( w , val ) in enumerate ( zip ( symbolic_weights , weight_values ) ) : interval : Minimum visual progress update interval ( in seconds ) . x_o = self.input_conv ( inputs_o , self.kernel_o , self.bias_o , on_epoch_end=lambda epoch , logs : json_log.write ( with pytest.raises ( ValueError ) : tuples.append ( ( sw , w ) ) # For use by subclasses # self.best = current mask_cache_key = object_list_uid ( inputs ) batch_size = 1 elif isinstance ( sample_weight_mode , list ) : return x.numpy ( ) x = tf.cond ( condition , 3.6 interpolation='nearest ' ) : # Examples output_shape = None recurrent_r = matrix_inner [ : , self.units : 2 * self.units ] def _get_cntk_version ( ) : e.g . `` 2 '' for 2D convolution . assert h._keras_shape == d._keras_shape return isinstance ( filepath , string_types ) and filepath.startswith ( 'gs : // ' ) `` `` '' Pad the middle dimension of a 3D tensor > > > input = K.cast ( input , dtype='float16 ' ) for l in getattr ( self , '_layers ' , [ ] ) : weight_tensor_td_conv_new = saving.preprocess_weights_for_loading ( return decoded_dense [ : , : np.max ( decoded_length ) ] , log_prob The ` LocallyConnected2D ` layer works similarly dot = pydot.Dot ( ) def batch_shuffle ( index_array , batch_size ) : Can use either greedy search ( also known as best path ) The model will set apart this fraction of the training data , if len ( computed_data ) == len ( reference_input_tensors ) : `` `` '' Utilities for preprocessing sequence data . '' '' '' sub_n_first_node [ layer.name ] .get_name ( ) ) h_tm1_z = h_tm1 reduction_axes , epsilon=1e-3 ) : if maxlen : function_name ) ) 'keyword arguments : ' return self.cell.bias_constraint config = identifier if all ( [ m is None for m in mask ] ) : `` `` '' Validates a file against a sha256 or md5 hash . mask=mask , subsample=strides , if filepath not in self.local_objects : reduction=losses_utils.Reduction.SUM , name='mape_1 ' ) self.dropout = 0 ( e.g . 'float16 ' , 'float32 ' , 'float64 ' ) . def random_normal_variable ( parallel_model = keras.models.load_model ( fname ) The input samples are processed batch by batch . for grad in grads ] 'and thus has no defined ' + attr_name + ' . ' ) def test_convolution_2d_channels_last ( ) : `` `` '' Convolutional LSTM . mode='high_mem ' ) # Likewise for the test set required_entries = [ 'placeholder ' , 'variable ' , 'function ' ] be set , as in most other Keras optimizers . wrapper._original_function = func 'Input layers to a ` Model ` must be ` InputLayer ` objects . ' super ( AveragePooling3D , self ) .__init__ ( pool_size , strides , padding , def save_to_binary_h5py ( save_function , stream ) : args_not_in_doc = [ arg not in doc for arg in args ] def tanh ( x ) : shape = ( 1 , 1 , 1 , bias.shape [ 0 ] ) if go_backwards and need_convert is False : self.headers = headers if node.arguments : 'but got : ' , model ) the model 's optimizer 's state ( if any ) self.pool_size = conv_utils.normalize_tuple ( pool_size , 1 , 'pool_size ' ) h5dict [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) xs = [ ] class_weight=None , self.padding , self.strides [ 0 ] ) computed_tensor , computed_mask = computed_data [ 0 ] return self._gcs_prefix + self.bucket_name assert tuple ( a.shape ) == ( None , 32 ) nested_states.append ( [ states [ 0 ] ] ) output_mask = [ mask , mask ] def __init__ ( self , theta=1.0 , * * kwargs ) : elif len ( args ) == 3 and isinstance ( args [ 2 ] , int ) : dilation=dilation_rate ) elif axis == 2 : legacy_dropout_support = generate_legacy_interface ( mask=mask , seed : Integer , random seed . return ( high - low ) * np.random.random ( shape ) .astype ( dtype ) + low return log ( sum ( exp ( x ) , axis=axis , keepdims=keepdims ) ) if self.embeddings_data is None and self.embeddings_freq : from tensorflow.keras.models import model_from_json from .. layers.wrappers import Wrapper tmp_target_tensors.append ( target_tensors.get ( name , None ) ) `` `` '' Send current Iterable to all workers . '' '' '' For additional information about specificity and sensitivity , see the TypeError : if an incorrect type is passed for the ` metrics ` argument . A zero padding is used such that assert_list_pairwise ( function_outputs_list ) sub_n_last_node = { } idx = int ( len ( xs ) * ( 1 - test_split ) ) return x name='input_wrapper_for_ ' + str ( x.name ) ) max_val = 1 . final_output = expand_dims ( outputs [ 0 ] , 1 ) super ( Recall , self ) .__init__ ( name=name , dtype=dtype ) # numeric formatting . def __init__ ( self , pool_size=2 , strides=None , def conv ( x , w , padding , data_format ) : add at the start and end of dim 1 . # delete and recreate model self.epochs_since_last_save = 0 tensor , mask = tensor_map [ str ( id ( x ) ) ] cell.kernel_size [ 1 ] , # so just flatten all the dim in x.shape input_row , input_col = input_shape [ 2 : ] adamax = Adamax a = activations.get ( layer ) lr_t = lr * ( K.sqrt ( 1 . - K.pow ( self.beta_2 , t ) ) / output = repeat_elements ( output , height_factor , axis=2 ) if ( len ( self.trainable_weights ) ! = if samples_seen_since > = self.update_freq : dtype=None , slices = [ slice ( None ) ] + slices_dims + [ slice ( None ) ] ' times in the model . ' The generator is run in parallel to the model , for efficiency . strides=2 , y_true = K.squeeze ( y_true , -1 ) > > > input axes [ i ] = x.ndim - 1 if ndim ( bias ) == 1 : if data_format is None : return prefix + '/ ' + default 'should be either a list or a dict . ' `` `` '' Retrieves input shape and input dtype of layer if applicable . update_state_fn : function that accumulates metric statistics . * ` NONE ` : Un-reduced weighted losses with the same shape as input . When this from .common import cast_to_floatx ` ( batch , time , ... , channels ) ` def reset_after ( self ) : return sp.special.logsumexp ( x , axis=axis , keepdims=keepdims ) def pattern_broadcast ( x , broadcastable ) : if rnn_type == 'lstm ' : 'pip install git+git : //github.com/Theano/Theano.git ' with pytest.raises ( ValueError ) as ex : version = version [ : -1 ] print_fn ( 'Trainable params : { : , } '.format ( trainable_count ) ) from tensorflow.keras.models import model_from_config ` ( ( left_dim1_pad , right_dim1_pad ) , self.patched_file_io.start ( ) raise ValueError ( 'The initial state of a Bidirectional ' # of all layers it is connected to . outputs = self.call ( unpack_singleton ( self.inputs ) , training=training ) `` `` '' Built-in weight initializers . '' '' '' self.min_lr = min_lr of the data by size [ 0 ] , size [ 1 ] and size [ 2 ] respectively . exception_prefix='input ' ) model.add_metric ( K.mean ( y ) , name='metric_2 ' ) name not in sample_weight_mode ) : sample_weight , values_shape [ i ] , axis=i ) output_shape , cache_subdir='datasets ' , then_expression , def test_save_to_binary_h5py_to_bytes_io ( ) : old_floatx = K.floatx ( ) This allows you to save the entirety of the state of a model y = np.repeat ( y , n , axis=1 ) return T.nnet.binary_crossentropy ( output , target ) if value not in { 0 , 1 } : self.recurrent_kernel [ : , : , : , self.filters : self.filters * 2 ] ) verbose : Integer . 0 , 1 , or 2 . Verbosity mode . added = keras.layers.Add ( ) ( [ x1 , x2 ] ) `` `` '' Converts layers nested in ` Bidirectional ` wrapper . 'kernel_constraint ' : 'max_norm ' , if ` data_format= '' channels_last '' ` . dim_ordering='th ' , inputs = K.variable ( inputs.transpose ( ( 1 , 0 , 2 ) ) ) progbar.update ( step + 1 ) base_config = super ( RNN , self ) .get_config ( ) input_shape [ 4 ] ) If unspecified , ` use_multiprocessing ` will default to ` False ` . ` f ( x ) = max_value ` for ` x > = max_value ` , brightness_range=None , elif 'input_shape ' in kwargs : return isinstance ( x , tf_ops._TensorLike ) or tf_ops.is_dense_tensor_like ( x ) return len ( shape ) 'doc ' : `` '' '' Base class for recurrent layers . batch_logs = { 'batch ' : batch_index , 'size ' : len ( batch_ids ) } class_weight=class_weight , which is an idempotent operation that simply divides ` total ` by ` count ` . self.bias_c = None ( _LEARNING_PHASE_PLACEHOLDER.value == 1.0 or print ( '\nEpoch % 05d : LearningRateScheduler setting learning ' ` `` temporal '' ` indicated that we expect 2D weight data ImageNet Classification ] ( http : //arxiv.org/abs/1502.01852 ) from tensorflow.keras.utils import multi_gpu_model num_param = len ( cell.weights ) session = get_session ( ) be 0 and 1 ) . For each example , there should be a single floating-point value len_dim2 = input_shape [ 2 ] outputs = input_layer._inbound_nodes [ 0 ] .output_tensors y_pred , y_true = losses_utils.squeeze_or_expand_dimensions ( y_pred , y_true ) ( and the order of the classes , which will map to the label from tensorflow.keras.layers import GRUCell ( isinstance ( loss_fn , losses.LossFunctionWrapper ) and dilation=self.cell.dilation_rate [ i ] ) initializer=self.kernel_initializer , updates = [ ] env : KERAS_BACKEND=tensorflow MODE=API 'Output missing from sample_weight_modes dictionary ' ) num_classes = reference.shape [ 0 ] from tensorflow.python.keras.utils import tf_utils h_f = self.recurrent_conv ( h_tm1_f , initargs= ( seqs , self.random_seed ) ) class NonNeg ( Constraint ) : if name is None or name == `` : dummy_docstring = `` '' '' Multiplies 2 tensors ( and/or variables ) and returns a * tensor * . output_length , feature_dim , filters = kernel_shape target_class = layer.__class__.__name__ if self.unroll and timesteps in [ None , 1 ] : outputs check_single_tensor_operation ( 'log ' , ( 4 , 2 ) , WITH_NP ) str ( weights [ 0 ] .shape ) + ' and size ' Model ( [ j , k ] , [ m , n , 0 ] ) return inception_resnet_v2.InceptionResNetV2 ( * args , * * kwargs ) determined via tensor._keras_history if not provided . 'because of the broadcast . ' ) for line in f : `` `` '' Depthwise 2D convolution . return else_expression def DISABLED_test_check_array_length_consistency ( ) : Note : that self.wait = 0 from .load_backend import not_equal # Add regularization penalties and other layer-specific losses . ProgressTracker.progbar.update ( count * block_size ) rank = ndim ( tensors [ 0 ] ) return fn label = class_name 'keras_version ' : keras_version , return oh output_shape = tuple ( output_shape ) permutation += output_dimensions [ axis + 1 : ] + [ axis ] def _smooth_labels ( ) : flag = True if hasattr ( cell.state_size , '__len__ ' ) : if isinstance ( data , dict ) : if start < 0 : target_dim = input_shape [ dim ] input_shape= ( num_samples , num_row , num_col , stack_size ) ) at time ` t ` for sample ` s ` . expected_width = ( x.shape [ 2 ] + strides [ 0 ] - 1 ) // strides [ 0 ] x = getattr ( C.sequence , reduce_fun_name ) ( x , a ) training : Optional scalar tensor if key in list ( ConfusionMatrix ) ) : ' ` collections.Container ` ( e.g . list , tuple , etc . ) ' ) if archive_type == 'tar ' : bias_shape = int_shape ( bias ) ] ( http : //www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf ) 'Common ops without gradient : ' val = self.data.attrs [ attr ] previous_depth = layers_depths.get ( node.outbound_layer , 0 ) def trainable_weights ( self , weights ) : for axis , shape in enumerate ( self.noise_shape ) ] ( 'separable_conv1d ' , ( 2 , 8 , 2 ) , ( 3 , ) , 1 , 'same ' , 'channels_last ' ) , sample_weight = K.cast ( sample_weight , dtype=K.floatx ( ) ) if g in grad_parameter_dict : str ( len ( allowed_positional_args ) ) # i.e . ( rows , 1 , input_depth , depth ) # for subsequent layers , no need to specify the input size : return resnet_v2.ResNet50V2 ( * args , * * kwargs ) class CustomObjectScope ( object ) : 'should have a single output tensor . ' ( -1 , filters , output_row , output_col ) ) mask=mask , If ` class_id ` is specified , we calculate precision by considering only the old_layer = keras.layers.Dense ( input_dim=3 , output_dim=2 , name='d ' ) 'Dimension incompatibility ' n_gates = 4 `` `` '' Creates a ` MeanMetricWrapper ` instance . self.bias_r , from tensorflow.keras.layers import SpatialDropout2D x : The data passed as ` x ` to fit/evaluate/predict . used for model definition or training . alpha : float > = 0 . Negative slope coefficient . counts = { } thresholds , default_threshold=0.5 ) for i in range ( output_row ) : weights will be rescaled at each step to slowly move The provided generator can be finite in which case the class will throw sys.stderr.write ( 'Using Theano backend.\n ' ) # carry over the input mask x._keras_shape = s workers=0 ) regularizers.serialize ( self.pointwise_regularizer ) ) therefore computed using the height of the recall values by the false positive elif validation_steps : if isinstance ( value , np.ndarray ) is False : from_config ( config ) It keeps the shape , but changes between the layout ( Fortran/C ) . Eg . : sample_weight : Optional weighting of each example . Defaults to 1 . tuple ( s if s ! = -1 else None for s in self.target_shape ) ) ( if the model has multiple inputs ) . config = { 'cells ' : cells } from .. utils.data_utils import get_file assert history.history [ 'val_accuracy ' ] [ -1 ] > 0.6 initializer=bias_initializer , raise ValueError ( 'If evaluating from data tensors , ' `` `` '' Retrieves an attribute ( e.g . input_tensors ) from a node . separator : string used to separate elements in the csv file . return metrics_module.sparse_categorical_crossentropy num_time_step = shape [ 1 ] threshold : ( Optional ) Float representing the threshold for deciding str ( x.shape ) + ' and ' + str ( y.shape ) is required when using this layer as the first layer in a model . With the keyword argument ` states ` . ones_like ( reduce_result ) , _ , min_val = parse_shape_or_val ( shape ) ` ( batch , steps , features ) ` while ` `` channels_first '' ` with shape ( output_items , feature_dim , filters ) Also called at the end of a validation batch in the ` fit ` methods , specifying the length of the 1D convolution window if input_tensors : write_images=False , elif len ( names ) > 1 : kernel_size = ( kwargs.pop ( 'kernel_dim1 ' ) , return tf_keras_backend.cast_to_floatx ( x ) ' { } is not a legal parameter'.format ( params_name ) ) 'Connected to ' ] return lp printable_module_name='layer ' ) raise ValueError ( 'Could not interpret initializer identifier : ' initializer=self.alpha_initializer , name='binary_crossentropy ' ) : x = K.identity ( x ) padding=padding , data_format=data_format ) `` `` '' Element-wise square . @ K.symbolic using newly instantiated weights . def test_pickling_without_compilation ( ) : 'has been renamed ` epochs ` . ' , stacklevel=2 ) ValueError : if ` data_format ` is return_states.append ( Update ops . self.on_train_end = on_train_end beta = beta.dimshuffle ( shuffle_pattern ) `` `` '' Rectified linear unit . return self.layer.activity_regularizer class Average ( _Merge ) : # to process batches of any size . if not hasattr ( self , '_metrics ' ) : # Theano gets confused by broadcasting patterns in the scan op elif len ( shape1 ) < len ( shape2 ) : add_edge ( dot , name , output_name ) def check_rnn_operation ( step_function_k , [ 1 , 2 , 3 , 4 , 5 , 6 ] , For example , if ` y_true ` is [ 0 , 1 , 0 , 0 ] and ` y_pred ` is [ 0 , 0 , 1 , 1 ] if keepdims : 'gamma_constraint ' : constraints.serialize ( self.gamma_constraint ) permute_pattern = list ( range ( x_ndim ) ) self._set_per_output_metric_attributes ( layer = filtered_layers [ k ] model.compile ( 'sgd ' , loss=keras.losses.CategoricalHinge ( ) ) constraint=self.beta_constraint ) self.kernel_shape = ( # Create an Enqueuer that can be reused _SHARED_SEQUENCES = { } def max ( x , axis=None , keepdims=False ) : # What follows is input validation and standardization to list format , # implemented as a wrapper for ` Model ` which maintained assert ask.call_count == 2 axis=C.Axis.all_axes ( ) ) mock.return_value = ' n ' [ 'metric_1 ' , 'metric_2 ' ] ) : numer / denom , `` `` '' Returns the value of more than one tensor variable , self.outputs , InputLayer . If so , this method checks the provided ` batch_size ` and ` x ` if shape is not None and not batch_shape : return metric.name if hasattr ( metric , 'name ' ) else metric.__name__ class ReshapeBatch ( C.ops.functions.UserFunction ) : 'please run with GPU to get better performance . ' ) if layer.name == name : # see ` extras_require ` in ` setup.py ` . data_format=self.data_format ) last_output_k = K.eval ( last_output_k ) if not use_sequence_api and use_multiprocessing and workers > 1 : model.compile ( optimizer='rmsprop ' , arguments = self.arguments dim1_cropping = conv_utils.normalize_tuple ( cropping [ 0 ] , 2 , # In this case the OS does not allow us to use file_like.seek ( 0 ) callbacks=None , output_t = np.where ( output_mask [ : , t ] , output_t , output_tm1 ) output_shape = self._output_shape.__name__ with h5py.File ( 'in-memory-h5py ' , driver='core ' , backing_store=False ) as h5file : dirname = 'cifar-10-batches-py ' assert output == [ 21 . ] def has_seq_axis ( x ) : out._keras_shape = tuple ( x_shape + y_shape ) ' : expected ' + names [ i ] + ' to have ' logits and probabilities are same . auto_padding= [ False , padding , padding ] , output_names : List of model output names . trainable=True , raise TypeError ( 'TypeError while preparing batch . ' return None , None the squared hinge metric value is 2.6 . str ( spec.dtype ) + ' , found dtype= ' # Test ` get_losses_for ` from .load_backend import permute_dimensions original_backend=None , ValueError : In case of invalid ` count_mode ` . if layer.__class__.__name__ == 'TimeDistributed ' : padding=self.padding , sample_weight = K.expand_dims ( sample_weight , axis=i ) ( 'conv2d ' , ( 2 , 3 , 9 , 8 ) , ( 4 , 3 , 3 , 4 ) , loss_weights : Optional list or dictionary specifying scalar g = tf.cond ( condition , def DISABLED_test_model_saving_to_pre_created_h5py_file ( ) : self.supports_masking = True training_updates = self.optimizer.get_updates ( super ( RMSprop , self ) .__init__ ( * * kwargs ) weights : a list of Numpy arrays . The number if self.activation is not None : Models often benefit from reducing the learning rate by a factor return ( name in arg_spec.args or validation_data : Data on which to evaluate assert msle_obj.reduction == Reduction .SUM # we will return a constant as place holder , the cntk learner will apply x , tf_data_format = _preprocess_conv2d_input ( x , data_format ) # node is not part of the current network write_grads : whether to visualize gradient histograms in TensorBoard . config = { 'cropping ' : self.cropping , # Test shape inference when input from .load_backend import less_equal if self.amsgrad : return C.round ( x ) def prepare_loss_weights ( output_names , loss_weights=None ) : name='siamese ' ) self._states = states # Check that for stateful networks , number of samples is a multiple y._keras_shape = indices._keras_shape + reference._keras_shape [ 1 : ] name='categorical_crossentropy ' ) : values , sample_weight=sample_weight ) if callable ( obj ) : elif isinstance ( path , six.string_types ) or _is_path_instance ( path ) : from .load_backend import floatx dtype = kwargs.get ( 'input_dtype ' ) `` `` '' Hard sigmoid activation function . if x is None and y is None and steps_per_epoch is None : # TODO : remove the conversion when cntk supports int32 , int64 strides , data_format ) `` `` '' Spatial 1D version of Dropout . child_class_name = layer.layer.__class__.__name__ def arange ( start , stop=None , step=1 , dtype='int32 ' ) : for t in tuples : width = 150 In the snippet below , there is a single floating point value per example for This is useful for separating training updates and It is recommended to leave it at the default value . metrics_utils.ConfusionMatrix.TRUE_POSITIVES : self.true_positives , 'TensorFlow is executing eagerly . ' ) def pred_generator ( ) : converted.append ( ( 'mode ' , None ) ) self.queue_length = queue_length val_x , val_y , list of losses . The loss value that will be minimized by the model keras_var = K.variable ( np_var ) if callable ( else_expression ) : return tf.reduce_any ( x , axis , keepdims ) # if inbound_layer is wrapped Model x = C.convolution_transpose ( dtype=self.dtype if dtype is None else dtype , def DISABLED_test_regression_predict_shape_correct_num_test_1 ( ) : super ( MinimalRNNCell , self ) .__init__ ( * * kwargs ) # In this case , we run extensive shape validation checks . num_samples = 3 # Must wait for tf.keras to support sparse ops . if gpus < = 1 : rmsprop = RMSprop super ( Bidirectional , self ) .__init__ ( layer , * * kwargs ) 'input shape ' ) update_op = self.update_state ( * args , * * kwargs ) from .load_backend import ctc_decode input_spec : List of InputSpec class instances f = K.function ( [ x , y ] , [ z ] ) incompatible with specifying any ` strides ` value ! = 1 . 3D tensor with shape : ` ( batch , steps , channels ) ` ' '' Tests for functions in io_utils.py . def test_func ( ) : y_shape.append ( s ) # Check if ` initial_state ` can be splitted into half x = repeat_elements ( x , depth_factor , axis=2 ) [ Empirical Evaluation of Gated Recurrent Neural Networks on cval=cval , weights [ : num_weights_per_layer ] , node_indices=node_indices , with shape ( batch_size , output_length , filters ) if callable ( then_expression ) : raise ValueError ( 'Output tensors to a ' + cls_name activity_regularizer : instance of [ ActivityRegularizer ] ( .. /regularizers.md ) , from collections import namedtuple return self.normalizer ( self.data [ idx ] ) > > > K.ndim ( kvar ) to draw samples . if num_thresholds < = 0 : model.fit ( x_train , y_train , epochs=5 , batch_size=32 ) min_value : the minimum norm for the incoming weights . def get_updates_arg_preprocessing ( args , kwargs ) : gpus = len ( ( x for x in available_devices if '/gpu : ' in x ) ) from keras.legacy import layers as legacy_layers batch_outs = f ( ins ) ` `` valid '' ` means `` no padding '' . @ pytest.fixture ' , found shape= ' + str ( value.shape ) ) data = d [ 'data ' ] super ( Mean , self ) .__init__ ( 'num_thresholds ' : self.num_thresholds , last_output_np , output_np , last_states_np = KNP.rnn ( if not params : # Connect nodes with edges . inbound_layer_name = inbound_layer.layer.name data_rec = h5file [ 'data ' ] [ : ] if workers > 0 : bucket_name = bucket_name [ len ( self._gcs_prefix ) : ] assert_list_pairwise ( z_list ) def __init__ ( self , n , * * kwargs ) : weighted_metrics=self._compile_weighted_metrics , while ( ' % s % d ' % ( name , chunk_id ) ) in group.attrs : self.input_spec = InputSpec ( ndim=3 ) z_list = [ ] return True if 'unroll ' in kwargs : if isinstance ( pattern , list ) : `` `` '' Repeats the elements of a tensor along an axis , like ` np.repeat ` . config : Model config dictionary . new_shape_temp.append ( C.InferredDimension ) progbar = Progbar ( target=steps ) updates += l.get_updates_for ( inputs ) num_classes : total number of classes . The output of the generator must be either # Creating dataset to store labels def _remove_repeats ( inds ) : self._init_subclassed_network ( * * kwargs ) if not expand_nested or not isinstance ( layer , Model ) : input_spec = to_list ( self.input_spec ) > > > print ( K.is_sparse ( b ) ) name = `` loss_config = training_config [ 'loss ' ] # Deserialize loss class . if content_type is not None : filter_dilation=dilation_rate ) from tensorflow.keras.applications.resnet_v2 import ResNet152V2 verbose=0 ) : seed = np.random.randint ( 10e8 ) # `` forward '' method to let cntk know we want to evaluate them.from from tensorflow.keras.datasets.imdb import load_data Requires the ` requests ` library . callbacks._call_batch_hook ( 'train ' , 'begin ' , step_index , batch_logs ) assert len ( model2.losses ) == 1 , model2.layers [ 1 ] ._per_input_losses outputs = model ( inputs ) which should be used in the following code . Otherwise the use_multiprocessing=False , cosine_similarity , name , dtype=dtype , axis=axis ) # Legacy support . if not th_sparse_module : model.compile ( 'sgd ' , metrics= [ keras.metrics.Hinge ( ) ] ) return self.cell.recurrent_activation nodes = self._nodes_by_depth [ depth ] from .network import Network from multiprocessing.pool import ThreadPool self._is_started = True # since CuDNNGRU always has biases . # - * - coding : utf-8 - * elif max_value == 6 : applied to the network output . shape_set.add ( tuple ( reduced_inputs_shapes [ i ] ) ) W_regularizer='l1 ' , weights [ 11 ] ] , axis=-1 ) last_output , outputs , last_states = K.rnn ( for i in range ( 1 , len ( connections ) ) : Setting it to true will also force ` bias_initializer= '' zeros '' ` . for chunk_id , chunk_data in enumerate ( chunked_data ) : return None # Edge case where ins == [ static_learning_phase ] elif padding == 'full ' : metrics=None , from .load_backend import temporal_padding def transpose_input ( from_cudnn ) : batch_size=batch_size , return python_types.FunctionType ( code , globs , baseline : Baseline value for the monitored quantity to reach . ids= [ 'model_plain ' , 'model_nested ' ] ) `` `` '' implementation : one of { 0 , 1 , or 2 } . name=name + '_target ' , `` `` '' Element-wise equality between two tensors . if ndim is not None and ndim < spec.min_ndim : steps_per_epoch=None , 'operation on a placeholder and a variable , ' shape_key = layer.name + '_0_0 ' all_input_shapes = set ( `` `` '' Fast LSTM implementation with [ CuDNN ] ( https : //developer.nvidia.com/cudnn ) . return tf.where ( x > 0 , res , alpha * res ) ' ` top_k ` argument for ` Recall ` metric is currently supported only ' ` ( batch , filters , new_depth , new_rows , new_cols ) ` broadcast_mean = C.reshape ( mean , target_shape ) return normed , mean , T.inv ( stdinv * * 2 ) `` `` '' Dumps all layer weights to a HDF5 file . def serialize_keras_object ( instance ) : input2 = keras.Input ( input_shape ) x_r = K.dot ( inputs_r , self.kernel_r ) return metrics_utils.update_confusion_matrix_variables ( i ) verbose=1 , # In this case we will later create an input layer self.forward_layer._num_constants = self._num_constants monitor_value = logs.get ( self.monitor ) raise ValueError ( 'The channel dimension of the inputs ' p = C.parameter ( def conv3d_args_preprocessor ( args , kwargs ) : we want to compute `` binary_accuracy '' and `` binary_crossentropy '' , self.rate = rate return nasnet.preprocess_input ( * args , * * kwargs ) return output_shape specifying the strides of the convolution along each spatial dimension . from .load_backend import sum def test_return_state ( ) : if self._num_constants is not None and constants is None : def test_truncated_normal ( self ) : file_hash='8a61469f7ea1b51cbae51d4f78837e45 ' ) I.e . returns : `` `` '' Adds a weight variable to the layer . 1. invalid ` data_format ` argument . from .. engine.training_utils import standardize_input_data bias=True , class MSE_MAE_loss ( object ) : of timesteps . To introduce masks to your data , for i in range ( predictions.shape [ 0 ] ) ] image_shape = tuple ( int_or_none ( v ) for v in image_shape ) weight array . If both ` sample_weights ` and ` class_weights ` are provided , model : Instance of ` Model ` . > > > inputs = keras.backend.placeholder ( shape= ( 2 , 4 , 5 ) ) def __init__ ( self , cell , __go_backwards__ : Boolean ( default False ) . if not kernel_shape : def softsign ( x ) : datas_rec = load_from_binary_h5py ( load_function , file_like ) from .utils.generic_utils import deserialize_keras_object super ( AUC , self ) .__init__ ( name=name , dtype=dtype ) have 1 , 3 , or 4 channels . from .load_backend import is_placeholder 'Please set ` dilation_rate ` to ( 1 , 1 , 1 ) . ' if ndim ( out ) == 1 : def __init__ ( self , minval=-0.05 , maxval=0.05 , seed=None ) : file_hash : The expected hash string of the file . self._metrics.append ( value ) [ Efficient Object Localization Using Convolutional Networks ] ( https : //cloud.google.com/video-intelligence/docs/common/auth ` states ` should be a numpy array or list of numpy arrays representing skip_target_indices : A list of indices of model outputs where loss training = learning_phase ( ) x = tf.nn.conv2d_transpose ( x , kernel , output_shape , strides , y , def softmax ( x , axis=-1 ) : metrics.extend ( _get_metrics_from_layers ( layer.layers ) ) Tuple of ` y_pred ` , ` y_true ` and ` sample_weight ` . Each of them possibly has range ( 2 , K.ndim ( inputs ) ) ] ) raise ValueError ( 'Could not interpret ' def result ( self ) : c = f * c_tm1 + i * self.activation ( x_c + K.dot ( h_tm1_c , self.tensor_indices = tensor_indices # recompute steps_per_epoch after each epoch def print_layer_summary ( layer ) : # Make deserialization case-insensitive for built-in optimizers . if argument in feed_dict : 'padding ' : padding , x = K.placeholder ( shape= ( None , 4 ) ) if ( losses.is_categorical_crossentropy ( self.model.loss ) and callbacks._call_batch_hook ( 'train ' , 'begin ' , batch_index , batch_logs ) conv_out = conv_out [ : , : , : i , : ] is meant to be sparse . You can specify the initial state of RNN layers symbolically by arrays ( same order as ` names ` ) , while checking that the provided # communicate on_epoch_end to the main thread x_aggregate = concatenate ( xs , axis=0 ) metrics_dict = OrderedDict ( ) 'you should specify the ` steps ` ' # https : //github.com/shawntan/ target_tensors ) def load_data ( path='mnist.npz ' ) : assert_allclose ( group4 [ ' z ' ] , array ) label = ' { } : { } '.format ( layer_name , class_name ) mode : one of { auto , min , max } . In ` min ` mode , inputs = inputs [ : ] shape : Integer shape tuple . layer = keras.layers.CuDNNLSTM ( units ) 'top_k ' : self.top_k , [ [ [ 0.633766 , 0.221185 , 0.0917319 , 0.0129757 , 0.0142857 , 0.0260553 ] , ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='maxpool3d ' ) # on any recurrent layer that feeds into another recurrent layer . def _set_metric_attributes ( self ) : y = np.random.random ( ( 10 , 2 ) ) shape = ( self.embeddings_data [ 0 ] .shape [ 0 ] , int ( embedding_size ) ) if sample_weight is not None and len ( sample_weight.shape ) ! = 1 : return x.dynamic_axes [ 1 ] == C.Axis.default_dynamic_axis ( ) @ allow_write_to_gcs for k in self._values : y_shape , y_val = parse_shape_or_val ( y_shape_or_val ) info += ' % .0fus/step ' % ( time_per_unit * 1e6 ) if filename.startswith ( self._gcs_prefix ) : if callable ( alt ) and isinstance ( alt , C.cntk_py.Function ) is False : if self.use_multiprocessing : Providing ` start ` and ` end ` allows use of a slice of the dataset . if self.read_only : `` `` '' Instantiates an all-ones variable of the same shape as another tensor . return _postprocess_conv3d_output ( x , data_format ) x = permute_dimensions ( x , [ 0 , 3 , 1 , 2 ] ) import h5py output = T.zeros ( output_shape ) def decode_predictions ( * args , * * kwargs ) : If the output mask at each time step is not ` None ` : feed_output_names , # No specific assumptions . model = saving.unpickle_model ( state ) return np.asarray ( x , dtype=_FLOATX ) top_k=None , if ProgressTracker.progbar is None : def test_update_add ( self ) : samplewise_std_normalization=samplewise_std_normalization , of weight arrays is present but their shape does not match . To load a network from a JSON save file , use slices_dims.append ( slice ( start , end ) ) `` sparse '' will be 1D integer labels , cells : List of RNN cell instances . from tensorflow.contrib.tensorboard.plugins import projector case ` output ` is expected to be the logits ) . target , output , from_logits=from_logits ) if self.unit_forget_bias : Important : blank labels are returned as ` -1 ` . Input shape tuple class FalseNegatives ( _ConfusionMatrixConditionCount ) : broadcast_beta = K.reshape ( self.beta , broadcast_shape ) def reverse ( x , axes ) : target_dim = 2 * output_dim if mode == 'concat ' else output_dim If int : the same symmetric padding assert node.input_masks == [ None ] This layer can only be used as the first layer in a model . [ TensorBoard ] ( https : //www.tensorflow.org/guide/summaries_and_tensorboard ) original_backend = f.attrs [ 'backend ' ] .decode ( 'utf8 ' ) if step == 0 : then_expression = then_expression ( ) rate : float between 0 and 1 . Fraction of the input units to drop . width_shift_range=0. , if param.kind == inspect.Parameter.VAR_KEYWORD : def batch_normalization ( x , mean , var , beta , gamma , axis=-1 , epsilon=0.001 ) : elif isinstance ( x , dict ) : A list of loss weights of python floats . metrics.logcosh , The same tensor ` x ` , unchanged . layer.kernel_f , # Keep track of unconditional losses # via ` add ` , and omits the auto-generated ` InputLayer ` reason='Specific to Tensorflow . ' ) # zero init of 1st moment mean_squared_error , name=name , reduction=reduction ) base_config = super ( ConvRecurrent2D , self ) .get_config ( ) with tf.device ( '/cpu:0 ' if cpu_merge else '/gpu : % d ' % target_gpu_ids [ 0 ] ) : name = _prepare_name ( name , 'placeholder ' ) warnings.warn ( 'The ` implementation ` argument ' precision ( see Davis & Goadrich 2006 for details ) . flip_filters = False placeholder_shape = placeholder.shape The object returned by ` load_function ` . except TypeError : # old API for backward compatibility axis=-1 ) : new_layer_1 = keras.layers.SpatialDropout3D ( rate=0.5 , label_key : key for label data in the retrieve return ( super ( Wrapper , self ) .get_losses_for ( inputs ) ( see [ constraints ] ( .. /constraints.md ) ) . return self.layer.get_weights ( ) # Masking def custom_object_scope ( * args ) : 1 . Values passed to the dictionary arguments of for state in initial_state ] `` `` '' Calculates the number of the given confusion matrix condition . from keras.layers import pooling check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis= [ 1 , -1 ] , assert ( normalized_dtype_X_train.dtype == np.uint8 ) ( e.g . via the ` input_shape ` argument ) self.input_masks = input_masks low : Float , lower boundary of the output interval . permute_pattern [ i ] = permute_pattern [ i + 1 ] return obj.item ( ) if custom_objects and function_name in custom_objects : self._values [ k ] [ 0 ] / max ( 1 , self._values [ k ] [ 1 ] ) ) if initial_state is None and constants is None : the epoch with the best value of the monitored quantity . for dim in range ( len ( output_shape ) ) : # the loss per batch should be proportional input_shape = layers_to_output_shapes [ shape_key ] if isinstance ( args [ 2 ] , int ) and isinstance ( args [ 3 ] , int ) : If True , process the input sequence backwards . additional_inputs += initial_state if self.shared_axes is not None : `` `` '' Computes the mean of absolute difference between labels and predictions . # outputs expected to be same as inputs for the first sample ( output_row , output_col , -1 , filters ) ) This argument is not supported when ` x ` is a generator or if show_layer_names : `` `` '' Sets the global TensorFlow session . reduction_axes , elif axis == 3 or axis == -1 : If x has shape ( s1 , s2 , s3 ) and axis=1 , the output weights [ 1 ] = np.transpose ( weights [ 1 ] , ( 3 , 2 , 0 , 1 ) ) def _num_elements ( losses ) : self.use_steps = False if not any ( key ultimately returned as ` recall ` , an idempotent operation that simply divides `` `` '' Parses a yaml model configuration file and returns a model instance . # Test that ` leakyrelu ` op gets used . # and building the layer if needed . contains the decoded sequence . from tensorflow.keras.regularizers import * sample_weight [ 1 ] [ :2 ] ] ) ( E.g. , inner layer is Masking or RNN ) import tensorflow as tf order = ' F ' if from_cudnn else ' C ' if isinstance ( self.axes , int ) : ValueError : if ` validation_freq ` is an Integer and less than 1 , or if This value must be provided , since a confusion matrix of dimension = name=None , y = to_categorical ( y ) # Using all visible GPUs when not specifying ` gpus ` feed_dict = { y_placeholder : 3 . } _ , temp_fname = tempfile.mkstemp ( suffix=suffix ) ` f ( x ) = negative_slope * ( x - threshold ) ` otherwise . ` ( batch , rows , cols , channels ) ` `` `` '' Reuters topic classification dataset . among the top-k classes with the highest predicted values of a batch entry is model_config [ 'config ' ] = model.get_config ( ) raise RuntimeError ( 'The following attributes can not be saved to HDF5 ' class UpSampling3D ( _UpSampling ) : def build_map ( tensor , check_single_tensor_operation ( 'tile ' , ( 3 , 4 ) , WITH_NP , n= ( 2 , 1 ) ) old_layer = keras.layers.SpatialDropout3D ( p=0.5 , Only use for 2D matrices . def is_placeholder ( x ) : ( -1 , 1 , feature_dim ) ) ) value.dtype ! = np.float32 and str ( len ( input_spec ) ) + ' inputs , ' 'the original value of { } so we will re-download the ' Does not include the batch axis . Recurrent Neural Networks ] ( http : //arxiv.org/abs/1512.05287 ) raise ValueError ( 'Invalid input_shape argument ' top_pad , bottom_pad = padding [ 0 ] Divide the model 's input ( s ) into multiple sub-batches . if self._is_graph_network : to lists of layer instances . weights [ 5 ] , The upsampling factors for dim1 , dim2 and dim3 . xs = xs [ indices ] X_dset [ : ] = X with open ( fpath , 'rb ' ) as fpath_file : 'minval ' : self.minval , 'automatically inferred . ' inputs = np.asarray ( inputs ) .transpose ( ( 1 , 0 , 2 ) ) if output_mask is None : 'activation ' : activations.serialize ( self.activation ) , show_shapes=True ) return # Check ` batch_size ` argument is consistent with InputLayer . indices , will be alphanumeric ) . for item in seq : return shape m.update_state ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) layer.layer , weights , original_keras_version , original_backend ) result = stack ( inner_products ) return Model ( input_tensors , output_tensors , name=model.name ) with ` stddev = sqrt ( 2 / fan_in ) ` from .load_backend import stack If ` keepdims ` is ` False ` , the rank of the tensor is reduced self.data [ attr ] = pickle.dumps ( val ) saved . If an absolute path ` /path/to/folder ` is ` 1 ` if ` x > 2.5 ` from tensorflow.keras.layers import Dropout kernel_size=kernel_size , for i in range ( axes [ 0 ] , x_ndim - 1 ) : assert tensor_index == 0 export LD_LIBRARY_PATH= $ HOME/miniconda/envs/test-environment/lib/ : $ LD_LIBRARY_PATH elif self.cell.data_format == 'channels_last ' : * ` SUM_OVER_BATCH_SIZE ` : Scalar ` SUM ` divided by number of elements in losses . def test_convert_weights ( ) : tensor , _ = tensor_map [ id ( x ) ] check_single_tensor_operation ( 'eye ' , 3 , WITH_NP , shape_or_val=False ) if config [ 'class_name ' ] .lower ( ) in all_classes : inputs_h = inputs self.stateful = stateful momentum=0.99 , v = tf.ones ( shape=shape , dtype=dtype , name=name ) # for m1 , m2 in zip ( [ m.name for m in model._compile_metrics ] , [ 'metric_1 ' ] ) : return self.cell.call ( inputs , states , constants=constants , y_true = K.l2_normalize ( y_true , axis=axis ) from tensorflow.keras.utils import custom_object_scope filepath : one of the following : from .load_backend import dot decode_pred_tf , log_prob_pred_tf = K.ctc_decode ( inputs , idxs = T.arange ( L.shape [ 1 ] ) .dimshuffle ( ' x ' , 0 ) return xception.Xception ( * args , * * kwargs ) if current < self.target : try : return layer_class ( 2 , input_shape= [ 3 , 5 ] , * * kwargs ) use_bias=use_bias , `` ` self._output_shape = None if data_format is None : def _standardize_user_data ( self , x , `` `` '' Types of loss reduction . from .load_backend import conv3d_transpose global _UID_PREFIXES dtype ( 'float64 ' ) import tensorflow as tf return - K.sum ( y_true * y_pred , axis=axis ) if dtype == np.float32 : K.is_keras_tensor ( np_var ) # x_train and y_train are Numpy arrays -- just like in the Scikit-Learn API . # self.rank is 1 for Cropping1D , 2 for Cropping2D ... from keras_applications import vgg16 'W_constraint ' : constraints.MaxNorm ( 1 ) , if len ( bias_shape ) == 1 : < tensorflow.python.ops.variables.Variable object at 0x10ab40b10 > check_single_tensor_operation ( 'max ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) output of some convolution to something that has the shape of its input yt = tf.reshape ( tf.transpose ( y , perm=y_permute_dim ) , [ y_shape [ -2 ] , -1 ] ) old_layer = keras.layers.GlobalMaxPooling3D ( dim_ordering='default ' , additional_specs += state_specs reason='cntk only supports dilated conv transpose on GPU ' ) trainable_weights = self.trainable_weights progbar = None elif axis == 1 : axis=axis , concat_args=True ) if isinstance ( variables , ( list , tuple ) ) : 'name ' : self.name } optimizer='sgd ' , if type ( obj ) .__name__ == type.__name__ : dtype=K.dtype ( p ) , x_squashed_dim = tf.reduce_prod ( x_mid_dims ) the Numpy arrays . We expect Numpy data to be fed for these [ 0.280884 , 0.429522 , 0.0326593 , 0.0339046 , 0.0326856 , 0.190345 ] , ordering when switching a model from one data format bad_attributes = [ x for x in val if len ( x ) > HDF5_OBJECT_HEADER_LIMIT ] return v return last_output , final_output , f_stats weight_names.append ( name.encode ( 'utf8 ' ) ) # learning_phase_scope = tf_keras_backend.learning_phase_scope # TODO return np.clip ( y , 0 , 1 ) return Average ( * * kwargs ) ( inputs ) if weights_rank ! = 0 : additional_specs += constants_spec model.compile ( 'sgd ' , loss=keras.losses.SparseCategoricalCrossentropy ( ) ) self.shared_axes = to_list ( shared_axes , allow_tuple=True ) scale , python setup.py install def random_normal_variable ( shape , mean , scale , dtype=None , name=None , seed=None ) : sliced_shape [ axis ] = rep + 1 broadcast_gamma = None ` `` same '' ` will be supported in future . self.b_constraint = constraints.get ( b_constraint ) pr_auc : an approximation of the area under the P-R curve . # ` to_categorical ` converts this into a matrix with as many self._feed_output_shapes.append ( shape ) def eager_fn_wrapper ( * args , * * kwargs ) : K.reset_uids ( ) for depth in depth_keys : self.is_placeholder = False if layer : input_a = keras.Input ( ( input_dim_a , ) ) x = t [ 0 ] ' ` MeanIoU ` metric is currently supported only ' PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/keras/backend/ ; label_smoothing : Float in [ 0 , 1 ] . When > 0 , label values are smoothed , Will only include updates that are either strides = ( 2 , 2 ) super ( GaussianNoise , self ) .__init__ ( * * kwargs ) out = x * ` update_state ( ) ` : Has all updates to the state variables like : `` `` '' Model saving utilities . if initial_state is not None : `` `` '' See docstring for ` Model.predict_generator ` . '' '' '' if c < = 0 : # if clipnorm == 0 no need to add ops to the graph return arg , args , kwargs for b in WITH_NP ] x = np.ones ( ( 3 , 4 ) ) return C.dropout ( x , level ) cudnn_rnn_layer_class = CuDNNLSTM v._keras_shape = v.shape axis : concatenation axis . # ELEMENT-WISE OPERATIONS enqueuer = OrderedEnqueuer ( base_config = super ( Bidirectional , self ) .get_config ( ) weight_col=None , and ` y ` is a numpy array of corresponding labels . self.wait = 0 decay = tf_ops.convert_to_tensor ( 1.0 - momentum ) m = tf.reduce_mean ( x , axis , True ) try : assert len ( model.losses ) == 6 'If the expected output shape is different , ' be equal to ` ceil ( num_samples / batch_size ) ` `` `` '' Computes the number of input and output units for a weight shape . from keras import constraints When attempting to multiply a nD tensor if logs is not None : x : input data , as a Numpy array or list of Numpy arrays weights_bi_conv_new = preprocess_weights_for_loading ( Ignored with the default value of ` None ` . # to specify custom placeholders if the need arises . def standardize_sample_weights ( sample_weight , output_names ) : x = C.swapaxes ( x , 0 , 1 ) stateful=stateful , old_layer = keras.layers.Convolution2D ( 5 , 3 , 3 , name='conv ' ) if not _has_nchw_support ( ) : broadcast_gamma = K.reshape ( self.gamma , finished_nodes = set ( ) recurrent_activation='hard_sigmoid ' , continue from .. import __version__ as keras_version def DISABLED_test_regression_predict_shape_correct_num_test_0 ( ) : init = initializer ( seed=1337 ) data_format=self.data_format ) inputs_o = inputs * dp_mask [ 3 ] # Construct epoch logs . original_input_spec = self.input_spec `` `` '' if x.ndim < 2 or y.ndim < 2 : from tensorflow.keras.utils import CustomObjectScope should * not * be averaged over an epoch . if self.stop_signal.is_set ( ) : archive_format = [ 'tar ' , 'zip ' ] # # all input/output/weight arrays should have the same number of samples . if m is not None : # zero init of exponentially weighted infinity norm total_time = time.time ( ) - start_time shape for randomly generated keep/drop flags . raise ImportError ( ' ` save_model ` requires h5py . ' ) return C.reduce_sum ( any_matrix ) input_shapes , output_shapes , bias_initializer=bias_initializer , if hasattr ( w , 'name ' ) and w.name : def test_Bidirectional_with_constants ( ) : dtype=K.dtype ( p ) , weight_tensor_td_conv_new = preprocess_weights_for_loading ( then the accuracy is 3/4 or .75 . If the weights were specified as * * self.filter_sk_params ( self.build_fn.__call__ ) ) c , d = K.stop_gradient ( [ a , b ] ) permutation += output_dimensions [ axis_without_batch + 1 : ] inputs : List of tensors p : A float , ` 0 . < = p < = 1 ` , probability of binomial distribution . scale = 1.0507009873554804934193349852946 def kernel_initializer ( self ) : return x.ndim super ( _GlobalPooling2D , self ) .__init__ ( * * kwargs ) # it does not know its number of inputs def shape ( self ) : inputs_i = inputs ' ( named `` ' + layer.name def call ( self , inputs , * * kwargs ) : nD tensor with shape : ` ( batch_size , ... , units ) ` . inputs = inputs [ 0 ] self._feed_input_names.append ( name ) class Function ( object ) : self.bias_initializer = initializers.get ( bias_initializer ) # Use cases : model.add_metric ( K.sum ( y ) , name='metric_1 ' ) h_tm1_r = h_tm1 elif [ [ `` $ RUN_ONLY_BACKEND_TESTS '' == `` 1 '' ] ] ; then int_A^B { Precision.dP } = int_A^B { ( TP_A + slope * ( P - P_A ) ) * dP / P } model.add ( Dense ( 32 , input_dim=32 ) ) raise ValueError ( 'Invalid padding : ' , padding ) file_access_property_list = h5py.h5p.create ( h5py.h5p.FILE_ACCESS ) A TensorFlow session . for x in node.input_tensors : # retrieve it from there instead of recomputing it . from .losses import hinge z = f ( [ x_val , y_val ] ) [ 0 ] keras_shape = list ( K.int_shape ( inputs ) ) self.input_spec = InputSpec ( min_ndim=2 , axes= { -1 : input_dim } ) def use_bias ( self ) : cache_key = object_list_uid ( inputs ) # it 's possible to callback a different model than itself losses += l.losses _add_inbound_node ( layer , index=0 ) callbacks=None , if self.target is not None and current < self.target : # Obtain symbolic outputs by calling the model . all_inputs = [ ] return K.in_train_phase ( indices = 2 , 3 return deserialize ( str ( identifier ) ) from .embeddings import Embedding If tuple of 2 tuples of 2 ints : return ( constraint : Optional projection function to be `` `` '' Global average pooling operation for spatial data . self._outbound_nodes = [ ] return { } if epoch % self.embeddings_freq == 0 : x = K.l2_normalize ( x , axis=1 ) x , kernel , output_shape , dilation_rate [ 0 ] , padding ) It is also possible for ` cell ` to be a list of RNN cell instances , import functools name='GRU ' ) for i in range ( len ( self.axes ) ) : self.states = [ K.zeros ( ( batch_size , dim ) ) ` initial_state ` should be a tensor or list of tensors representing self._values = collections.OrderedDict ( ) super ( TimeDistributed , self ) .__init__ ( layer , * * kwargs ) new_height = original_shape [ rows ] * height_factor input_length ) # Networks start with a pre-existing node 4D tensor with shape : data_shape = data_shape [ 1 : ] 'loss_weights ' : model.loss_weights , base_config = super ( SGD , self ) .get_config ( ) with pytest.warns ( UserWarning ) : def compile ( self , optimizer , from keras import metrics } , return [ ( i * batch_size , min ( size , ( i + 1 ) * batch_size ) ) max_val = min_val + 1 . input_shape= ( 28 , 28 , 3 ) ) ) layer = Bidirectional ( layer ) if elems is not None : ' class . Please specify ` steps_per_epoch ` ' def test_map ( self ) : for name , value in self.sk_params.items ( ) : from .. utils.io_utils import load_from_binary_h5py conversions= [ ( 'dim_ordering ' , 'data_format ' ) ] , data_format = normalize_data_format ( data_format ) output_shape = ( list ( batch_sizes ) [ 0 ] , ) + output_shape x2 = keras.layers.Dense ( 8 , activation='relu ' ) ( input2 ) `` `` '' Abstract nD depthwise separable convolution layer ( private ) . config = { 'layer ' : { 'class_name ' : self.layer.__class__.__name__ , spatial_start_dim = 2 If a default TensorFlow session is available , we will return it . if len ( class_sample_weight ) ! = len ( y_classes ) : from tensorflow.keras import Model as Network % ( hook_name , delta_t_median ) , RuntimeWarning ) except requests.exceptions.RequestException : model = model_cls ( weights=None , for depth in depth_keys : inputs with shape ` ( batch , channels , ... ) ` . whether we should overwrite an existing file/object at the target pointwise_kernel_shape , data_format ) metrics = self._get_training_eval_metrics ( ) def backward ( self , state , root_gradients ) : m = keras.metrics.MeanAbsolutePercentageError ( ) output_shape = shape1 + shape2 y_classes = np.argmax ( y , axis=1 ) from .recurrent import LSTM save_model ( model , fname , overwrite=False ) or in case a stateful model receives a number of samples 'is not a list . We expect a list . ' global grad_parameter_dict # Collect input tensor ( s ) coordinates . activity_regularizer=None , layer.kernel_z , def _extract_archive ( file_path , path= ' . ' , archive_format='auto ' ) : `` `` '' Returns ` variables ` but with zero gradient w.r.t . every other variable . Mode for yielding the targets : version = version [ :2 ] + version [ 2 : ] .replace ( ' . ' , `` ) if self.wait > = self.patience : rank of input due to usage of TimeDistributed . Issue : # 10356 . return self.cell.bias_regularizer rows = conv_utils.conv_output_length ( rows , self.pool_size [ 0 ] , # the input tensors come from : which layers , model.add ( keras.layers.Dense ( 1 , activation='sigmoid ' ) ) def check_single_tensor_operation ( function_name , from tensorflow.keras.applications.resnet_v2 import decode_predictions K.zeros_like ( self.thresholds ) ) initializers.normal , return inputs * K.cast ( K.greater ( inputs , self.theta ) , K.floatx ( ) ) and that with Theano , only ` size= ( 2 , 2 ) ` is possible . output = sum ( output , axis=3 ) shape=shape , 'beta_regularizer ' : regularizers.serialize ( self.beta_regularizer ) , `` `` '' Get the value from the Sequence ` uid ` at index ` i ` . y = np.random.random ( ( 10 , ) ) x_shape.append ( i ) non_trainable_weights : List of variables . # dimensions are time x depth the model 's configuration ( topology ) raise ValueError ( 'CNTK Backend : non-square dilation_rate is ' Note that because super ( UpSampling3D , self ) .__init__ ( normalized_size , data_format , * * kwargs ) # ` sample_weight ` is neither a dict nor a list . if _ is None : _runner ( initializers.identity ( ) , tensor_shape , Options are 'auto ' , 'tar ' , 'zip ' , and None . from_logits : ( Optional ) Whether output is expected to be a logits tensor . def on_test_end ( self , logs=None ) : import Queue as queue return loss if keyword in doc_lines : mode='fan_in ' , offset=16 ) .reshape ( len ( y_train ) , 28 , 28 ) `` `` '' Abstract class for different pooling 3D layers . super ( _Cropping , self ) .__init__ ( * * kwargs ) where ` fan_in ` is the number of input units in the weight tensor . random_seed : Initial seed for workers , For missing biases in ` LSTM ` / ` GRU ` ( ` use_bias=False ` ) , If batch size is specified : `` `` '' Retrieves the weights of the model . `` `` '' Functional interface to the ` Add ` layer . def get_config ( self ) : ` Flatten ` then ` Dense ` layers upstream # which provides a speedup in TensorFlow . delta_accumulators = [ K.zeros ( shape , name='delta_accumulator_ ' + str ( i ) ) from .pooling import GlobalAveragePooling1D conversions= [ ( 'sigma ' , 'stddev ' ) ] ) @ interfaces.legacy_generator_methods_support for ( i , p ) in enumerate ( params ) ] def test_saving_overwrite_option ( ) : for param in signature.parameters.values ( ) : if isinstance ( output_shapes , list ) : return last_output , outputs , new_states return self.cells [ -1 ] .state_size def multi_gpu_application_np_array_benchmark ( ) : class KerasRegressor ( BaseWrapper ) : > > > input_ph._keras_shape raise TypeError ( 'Layer can receive at most 3 positional arguments . ' ) u_list = [ ] True labels for ` x ` . [ 1 , 0 , 0 , 1 ] then the binary accuracy would be 1/2 or .5 . summation_method='interpolation ' , cache_dir=None ) : for x , value in tuples : return np.sum ( x , axis=axis , keepdims=keepdims ) self.embeddings_initializer = initializers.get ( embeddings_initializer ) def __init__ ( self , k=5 , name='sparse_top_k_categorical_accuracy ' , dtype=None ) : the dilation rate to use for dilated convolution . ValueError : if a loss function or target array # add a layer that returns the concatenation from .common import epsilon reshape=False ) : from .. layers.wrappers import Wrapper 'kernel_regularizer ' : 'l2 ' , elif isinstance ( n , tuple ) and len ( n ) < len ( shape ) : # Padding the axis input_masks=previous_mask , kept_nodes = 0 uses_learning_phase = False output_masks ) : index_array = index_array.flatten ( ) this should be ` max ` , for ` val_loss ` this should 'keras_embedding.ckpt ' ) , identifier=identifier.__class__.__name__ ) ) raise ValueError ( `` { } needs a ' # Arguments ' section '' .format ( name ) , model.add ( Conv2D ( 64 , ( 3 , 3 ) , padding='same ' ) ) broadcast_moving_variance = K.reshape ( self.moving_variance , out_pad_h , def DISABLED_test_model_save_load_binary_in_memory ( ) : inside each of the subdirectories directory tree If omitted ( ` None ` ) , then ` backend.floatx ( ) ` or ` float32 ` are used . ` fit ` , ` predict ` , ` predict_proba ` and ` score ` methods data_format=tf_data_format , if kernel_shape [ 2 ] % 2 == 0 : layer_output_tensors = layer._inbound_nodes [ node_index ] .output_tensors z += K.dot ( h_tm1 , self.recurrent_kernel ) if not self.reset_after : if x_batch_size ! = y_batch_size : if hasattr ( x , '_keras_shape ' ) : from tensorflow.keras.utils import get_source_inputs logs : dict , metric results for this training epoch , and for the self.output_size = self.units output_shape = transpose_shape ( output_shape , cell.data_format , # Gather inputs to call the new layer . 'class_name ' : self.__class__.__name__ , 'kernel_size ' : self.kernel_size , '2nd entry of padding ' ) class_mode='categorical ' ) x._theano_placeholder = True self.best = np.Inf def load_data ( path='boston_housing.npz ' , test_split=0.2 , seed=113 ) : in which case the number of samples is set to ` None ` . hash_algorithm='auto ' , return categorical if self.normalizer is not None : filtered_inbound_nodes = [ ] samples_per_epoch=1 , elif len ( args ) == 2 and isinstance ( args [ 1 ] , dict ) : name , # Done with the current epoch , waiting for the final batches def to_list_or_none ( x ) : # be compiled lazily when required . ( loss.fn in key_loss_fns ) ) ) : weights = convert_weights ( weights , from_cudnn=source == 'CuDNNLSTM ' ) 'config ' : layer.get_config ( ) } ' a ` batch_input_shape ` or an ` input_shape ` . ' ) self._call_batch_hook ( _TRAIN , 'end ' , batch , logs=logs ) x = C.convolution ( conv_out = T.nnet.conv2d ( conv_out , pointwise_kernel , lp = tf_keras_backend.learning_phase ( ) constants=constants_k , # cntk only support calculate on float , do auto cast here dtype=dtype , def _broadcast_normalize_batch_in_training ( x , gamma , beta , [ 0 , 0 , 1 , 0 ] then the false positives value would be 1 . filters , output_shape = ( input_shape [ 0 ] , self.filters , rows , cols ) * * kwargs ) : self.add_loss ( layer.get_losses_for ( computed_tensors ) , inputs ) beta = tf.cast ( beta , tf.float32 ) class Mean ( Reduce ) : def range_less_than ( _ , current_input ) : x , y , validation_steps : Only relevant if ` steps_per_epoch ` result = [ C.classification_error ( predictions [ i ] , _targets [ i ] , topN=k ) output_tensors [ i ] ._uses_learning_phase = getattr ( w_img = tf.reshape ( w_img , [ 1 , layer.backward_layer.add_loss ( 1 , inputs=None ) keras_shape.pop ( 1 ) # Inferring the output shape is only relevant for Theano . `` `` '' Element-wise sign . pad = ( 0 , 0 , 0 ) merge_repeated=merge_repeated ) dot.add_edge ( pydot.Edge ( src , dst ) ) isinstance ( loss_fn , losses.SparseCategoricalCrossentropy ) or if 0 in axes : index_from : index actual words with this index and higher . from .. utils import losses_utils neural network weights and filters . indices = T.flatten ( indices ) The next element in the queue , i.e . a tuple if not os.path.exists ( _keras_dir ) : self.bias = self.add_weight ( shape=bias_shape , self.fn = fn ( e.g . build a new computational graph from the provided inputs ) . if len ( u.arguments ) == 0 : for batch_index , ( batch_start , batch_end ) in enumerate ( batches ) : It will be called on each line of the summary . x = K.variable ( x_np ) [ here ] ( https : //github.com/keras-team/keras/pull/9473 # issuecomment-372166860 ) the corresponding dimension from K.shape ( tensor ) original_backend=original_backend ) ) batch_size = x.shape [ 0 ] with C.default_options ( axis_offset=1 ) : kernel_size : a tuple of 2 integers , specifying the nested_metrics = [ metrics ] from .load_backend import variable `` `` '' Masks a sequence by using a mask value to skip timesteps . return self.cell.implementation padding : One of ` `` valid '' ` , ` `` causal '' ` or ` `` same '' ` ( case-insensitive ) . the reported loss will be a scalar value . output_shape [ -1 ] * = 2 if type ( obj ) .__module__ == np.__name__ : Stacking layers is as easy as ` .add ( ) ` : num_weights = len ( sublayer.trainable_weights ) from tensorflow.keras.layers import GlobalAveragePooling3D embeddings_layer_names = self.embeddings_layer_names go_backwards , rnn_constants.append ( C.sequence.broadcast_as ( recurrent_h = K.dot ( r * h_tm1_h , self.recurrent_kernel_h ) losses += self.get_losses_for ( None ) layer_test ( convolutional.AveragePooling2D , mean : Mean of batch . return self.cell.recurrent_constraint initializer='uniform ' , elif K.ndim ( mask_i ) < K.ndim ( input_i ) : layer.bias_h_i , f = H5Dict ( path , mode= ' w ' ) super ( LambdaFunc , self ) .__init__ ( [ arg ] , name=name ) ( 0.1 , None , 0.0 ) , # set alpha only zoom_range=0. , negative_part = C.relu ( -x ) moving_variance_initializer='ones ' , [ False , True ] , check_positional_args = True one of ` 'channels_last ' ` ( default ) or ` 'channels_first ' ` . name_scope = tf.name_scope if layer not in layer_map : bash miniconda.sh -b -f -p $ MINICONDA i = ( x.shape [ 3 ] + strides [ 1 ] - 1 ) // strides [ 1 ] first_layer = layer return { 'axis ' : self.axis } for i , s in zip ( int_shape ( y ) , tf.unstack ( tf.shape ( y ) ) ) : ' '' input_length '' is % s , but received input has shape % s ' % elif len ( args ) == 3 : model.add ( Cropping2D ( cropping= ( ( 2 , 2 ) , ( 4 , 4 ) ) , for test_value in ( -20 , 0 , 1 , 10 ) : y += self.b 'recurrent_constraint ' : the logsumexp over all dimensions . and ` axis=1 ` corresponds to data format self._trainable = True input_length = K.variable ( np.array ( [ seq_len_0 ] , dtype=np.int32 ) ) weights = layer.weights active_skip_idxs = skip_idxs [ ( skip_idxs < active ) .nonzero ( ) ] input_shape = K.int_shape ( inputs ) self._output_loss_metrics [ i ] .result ( ) units = weights [ 1 ] .shape [ 0 ] Used in ` fit_generator ` , ` evaluate_generator ` , ` predict_generator ` . def ResNet152V2 ( * args , * * kwargs ) : def load_weights ( self , filepath , by_name=False , if first_line.strip ( ) [ -1 ] ! = ' . ' : self.writer.writeheader ( ) self.loss = loss or { } pool_mode='max ' ) : group.attrs [ name ] = data init=C.initializer.normal ( and ` fan_out ` is the number of output units in the weight tensor . assert ( mse_obj.reduction == losses_utils.Reduction.SUM_OVER_BATCH_SIZE ) if hasattr ( x_elem , '_keras_shape ' ) : # Flag to check if a dict is user defined data or a sub group : self._compile_weighted_metrics = weighted_metrics def unpickle_model ( state ) : return tf.reduce_min ( x , axis , keepdims ) [ 0.155251 , 0.164444 , 0.173517 , 0.176138 , 0.169979 , 0.160671 ] ] , num_states = 1 depthwise_kernel = tf.expand_dims ( depthwise_kernel , 0 ) batch_size : Number of timeseries samples in each batch input_length , greedy=True ) def test_loss_masking ( ) : 'nb_epoch ' , 'nb_val_samples ' , 'nb_worker ' } if self.monitor_op ( current - self.min_delta , self.best ) : ) from .. import activations biases= [ # Not needed to change the device scope for model definition : for i in range ( len ( self._output_layers ) ) : skip_target_weighing_indices.append ( i ) for objects in self.custom_objects : def test_embedding_invalid ( input_shape ) : kwargs.pop ( 'transform_bias ' ) cell = MinimalRNNCell ( 32 ) submodel_wrapper = model_to_dot ( layer.layer , show_shapes , self.input_spec = InputSpec ( ndim=4 , axes= { 1 : input_filter } ) from the v1 version is n't currently supported ( but will return hasattr ( x , '_cntk_placeholder ' ) and x._cntk_placeholder `` `` '' Retrieves the elements of indices ` indices ` in the tensor ` reference ` . output_loss = loss_fn ( ' ` keras.utils.Sequence ` class . ' ) optimizer=optimizers.Adam ( ) , 2D tensor of shape ` ( num_samples , features ) ` . output_dim = self.cell.output_size mask = K.reshape ( mask , broadcast_shape ) def __init__ ( self , min_value=0.0 , max_value=1.0 , rate=1.0 , axis=0 ) : # avoid any explicit version checks transposed = False nodes += v inputs = [ np.vstack ( [ input_prob_matrix_0 [ t , : ] , conv_out = T.nnet.conv3d ( x , kernel , self.normalizer = normalizer ' '' Tests the HDF5Matrix code using the sample from @ jfsantos at return e / s # If the metric function is not stateful , we create a stateful version . if not K.is_keras_tensor ( x ) : Convenience wrapper for ` CustomObjectScope ` . from .. utils.data_utils import GeneratorEnqueuer # Both transformation should be ran for both Keras 1- > 2 conversion output_shape [ i + 1 ] = target_dim pool_size : integer or tuple of 2 integers , x : Input data . Numpy array of rank 4 or a tuple . # but for the second sample all outputs in masked region should be the same # Bad luck , we have to run the graph manually . def test_upsampling3d_legacy_interface ( ) : trainable = getattr ( self , 'trainable ' , True ) kwargs = self.filter_sk_params ( Sequential.evaluate , kwargs ) initial_states_np= [ h0 , h1 ] , self._write_logs ( logs , index ) self.updates.append ( K.update ( p , new_p ) ) def _remove_dims ( x , axis , keepdims=False ) : if initializer is not None and not is_tensor ( initializer ) : `` `` '' Instantiates a Keras model from its config . positions = positions or [ .33 , .55 , .67 , 1 . ] weights += l.non_trainable_weights from .load_backend import batch_dot assert s._keras_shape == ( None , 64 + 5 ) metrics_utils.ConfusionMatrix.TRUE_NEGATIVES : self.true_negatives , assert np.all ( koh == oh ) def _process_batch ( self , inputs , initial_state ) : dilation_rate= ( 1 , 1 , 1 ) , for xi , yi in zip ( x , y ) : last_states_k = [ K.eval ( s ) for s in last_states_k ] # if the output_mask does not have a static shape , spec = [ ( 'dtype= ' + str ( self.dtype ) ) if self.dtype else `` , x_o = K.dot ( inputs_o , self.kernel_o ) num_thresholds = len ( metrics_utils.to_list ( self.thresholds ) ) from .losses import poisson return inputs * K.cast ( boolean_mask , K.dtype ( inputs ) ) where ` x ` is a numpy array of image data from functools import wraps with tf_ops.colocate_with ( x ) : model : instance of ` keras.models.Model ` . slices = transpose_shape ( slices , self.data_format , spatial_axes ) x , None , None , reduction_axes= [ 0 , 1 , 2 , 3 ] ) deserialized = { } return key in self.data return np.argmin ( x , axis=axis ) if i in axes : `` `` '' Calls the model on new inputs . origin = 'https : //www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz ' return variable ( value , dtype=dtype , name=name ) outputs_info=None , # of the call and of all new variables created during the call . if K.is_variable ( value ) and not getattr ( value , '_tracked ' , False ) : # Validate configurations . mask_np=None , data_format='NHWC ' ) y = T.set_subtensor ( output [ indices ] , x ) inbound_layer = node.inbound_layers [ i ] ' does not support masking , ' self.progbar = Progbar ( target=self.target , layer : Target layer instance . ` ( batch , first_padded_axis , second_padded_axis , third_axis_to_pad , self.bias_f = self.bias [ self.units : self.units * 2 ] axis : ( Optional ) Defaults to -1 . The dimension along which the metric is For Python 2 , checks if there is an argument with the given name . stateful=True , 'Please remove it from your layer call . ' ) model.add ( layer ) feed_dict [ tensor ] = value axis , self.progbar.update ( self.seen , self.log_values ) self.outbound_layer = outbound_layer path = get_file ( path , ` bytes ` data ( e.g . ` io.BytesIO ` ) that represents a valid h5py file image . stride = strides [ 0 ] config.pop ( 'filters ' ) K.argmax ( y_pred , axis=-1 ) ) , of HDF5 file which is not able to store with patch ( 'keras.engine.saving.ask_to_proceed_with_overwrite ' ) as ask : z2 = z [ : , 2 * self.units : 3 * self.units ] ValueError : In case of improperly formatted ` layer_data ` dict . A tensor , result of 2D pooling . if ( ( isinstance ( loss_fn , losses.LossFunctionWrapper ) and Users may pass data as a list of arrays , dictionary of arrays , return ( cls.NONE , cls.SUM , cls.SUM_OVER_BATCH_SIZE ) ` TimeDistributed ` can be used with arbitrary layers , not just ` Dense ` , y_pred_labels = K.cast ( y_pred_labels , K.floatx ( ) ) # Dictionary mapping layer instances to return self.optimizer.get_config def DISABLED_test_model_with_partial_loss ( ) : dtype : String , either ( ` 'float16 ' ` , ` 'float32 ' ` , or ` 'float64 ' ` ) . 'normal ' , specifying the length of the 1D convolution window . # Process all nodes in layer , if not yet processed def DISABLED_test_Bidirectional_with_constants ( ) : def test_CallbackValData ( ) : or 4D tensor with shape : result = arguments.data ( ) .as_shape ( ( num_batch , ) + self.target_shape ) # ` loss ` does not match outputs . for ( i , d ) in enumerate ( dilation_rate ) : callbacks=None , top_k_categorical_accuracy , name , dtype=dtype , k=k ) model.compile ( loss='categorical_crossentropy ' , if spec.axes : `` `` '' Adds losses to the layer . def DISABLED_test_eval_generator_with_sample_weight ( self ) : # we can handle dynamic shapes ( that include None ) . elif isinstance ( identifier , tf.keras.optimizers.Optimizer ) : def __call__ ( self , inputs , initial_state=None , constants=None , * * kwargs ) : In case of None value is provided during the call raise ValueError ( 'All layers in a Sequential model ' `` `` '' Returns the config of the layer . ids= [ 'FC ' , 'RNN ' , 'RNN_INVALID ' , 'CONV ' ] ) `` `` '' Model-related utilities . data_format=None , dilation_rate= ( 1 , 1 ) ) : return np.argmax ( x , axis=axis ) constraints.update ( self.forward_layer.constraints ) return self.cell.non_trainable_weights dtype : String , dtype of returned tensor . old_layer = keras.layers.AveragePooling2D ( return K.mean ( math_ops.square ( y_pred - y_true ) , axis=-1 ) initial_states , first_padded_axis , second_padded_axis , third_axis_to_pad ) ` return np.eye ( n , m , dtype=dtype ) kernel_constraint='max_norm ' , You can set it to a custom function if name in kwargs : constants=constants , raise TypeError ( 'Layer ' + self.name if shape [ 0 ] > shape [ 1 ] : return standardize_sample_or_class_weights ( sample_weight , return np.mean ( x , axis=axis , keepdims=keepdims ) if axes [ 0 ] < 0 : T.maximum ( sys.stdout.flush ( ) 'way , so can\'t test as this . ' ) use_multiprocessing=False , if on_epoch_begin is not None : decoded_dense = -np.ones_like ( y_pred [ ... , 0 ] ) self.output_row = output_row in the ` x ` and ` y ` data provided , before shuffling . `` `` '' Parametric Rectified Linear Unit . if reset_metrics : self.recurrent_regularizer = regularizers.get ( recurrent_regularizer ) Y_ = T.alloc ( -1 , Y.shape [ 0 ] * 2 + 1 ) if 'batch_input_shape ' in kwargs : Should only be called after computing the gradients indices = list ( range ( input_length ) ) check_single_tensor_operation ( 'reshape ' , ( 4 , 2 ) , WITH_NP , shape= ( 8 , 1 ) ) name=None , config.pop ( 'kernel_constraint ' ) nodes_in_progress , A flat list of Numpy arrays . validate_filenames=validate_filenames , if py_any ( list ( is_symbolic ( x ) for x in ( shape , minval , maxval ) ) ) : # Unsafe deserialization from bytecode channel_shift_range=0. , th_sparse_module = None dilation=self.dilation_rate [ 1 ] ) for i in range ( top_paths ) : super ( MinimalRNNCell , self ) .__init__ ( * * kwargs ) steps_name='steps_per_epoch ' ) old_layer = keras.layers.SpatialDropout2D ( p=0.5 , # TODO : support it in subclassed networks after inputs are set . super ( MeanAbsolutePercentageError , self ) .__init__ ( new_a = a + K.square ( g ) # update accumulator For example , if ` y_true ` is [ 1 , 1 , 0 , 0 ] and ` y_pred ` is [ 0.98 , 1 , 0 , 0.6 ] # so that the model 's weights are hosted on CPU memory . 'Total params : { : , } '.format ( trainable_count + non_trainable_count ) ) connections.append ( inbound_layer ( typically the features axis ) . 'However ` cell.state_size ` is ' stateful=stateful , # Useful for debugging any issues with conda output = super ( Recurrent , self ) .__call__ ( inputs , * * kwargs ) pointwise_kernel_shape = pointwise_kernel.eval ( ) .shape 'kernel_size ' : self.kernel_size , `` `` '' Utilities for text input preprocessing . '' '' '' # assert len ( model.losses ) == 6 if len ( computed_data ) == 1 : `` `` '' Computes the recall of the predictions with respect to the labels . build_map ( x , finished_nodes , nodes_in_progress , bias_shape = weights [ 2 ] .shape for t in time_index : class Constant ( Initializer ) : `` raw '' , sparse '' or None . Default : `` categorical '' . `` `` '' Switches between two operations depending on a scalar value . output_shape = [ output_shape ] self.stopped_epoch = 0 # and one tensor output . return 'tf ' output = K.concatenate ( [ y , y_rev ] ) updates=self.state_updates + metrics_updates , from tensorflow.keras.applications.resnet import decode_predictions callback_params = { y_pred = tf_math_ops.log ( tf.transpose ( y_pred , perm= [ 1 , 0 , 2 ] ) + epsilon ( ) ) # Note : the case ( 'PR ' , 'interpolation ' ) has been handled above . alpha : A scalar , slope of negative section . model_weights_group [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) trainable_weights = [ ] for index in sorted ( _axis , reverse=True ) : for device in target_devices : if preprocessor : old_layer = keras.layers.GlobalMaxPooling2D ( dim_ordering='default ' , # This is for performance optimization when calling the Network on new if uses_learning_phase : from .layer_utils import convert_all_kernels_in_model like the mean squared error , but will not be so strongly affected by the pip install keras # CNTK expects ` ( depth , input_depth , steps ) ` . x_shape = int_shape ( x ) print ( 'Batch % d : Invalid loss , terminating training ' % ( batch ) ) if ( self.mode == 'min ' or which helps prevent overfitting . 'specify the time dimension by passing ' to match the scikit-learn API , 'but got ' , distribution ) 'Increase maxlen . ' ) def add ( self , n , values=None ) : if self.output_padding is not None : def update_state ( self , values , sample_weight=None ) : X_test = HDF5Matrix ( h5_path , 'my_data ' , start=150 , end=200 ) def in_tmpdir ( tmpdir ) : return K.mean ( total_loss ) additional_specs += self.state_spec raise ValueError ( ' ` padding ` should have two elements . ' def _run ( self ) : 'y_binary = to_categorical ( y_int ) \n ' noise_shape = x.shape for fn in legal_params_fns : kernel_constraint=kernel_constraint , initializer=self.init , str ( value ) ) return C.log ( x ) X_train [ np.array ( [ 1000 ] ) ] if key in ( 'pr ' , 'PR ' ) : output_shape = self.compute_output_shape ( input_shape ) from .load_backend import elu else for functional model with 1 or more Input layers : shape = ( bias.shape [ 0 ] , 1 ) new_layer = keras.layers.Conv1D ( 5 , 3 , name='conv ' , input_shape= ( 4 , 3 ) ) underflows caused by taking the log of small inputs . > > > K.eval ( kvar_tile ) self.W_constraint = constraints.get ( W_constraint ) lines = doc.split ( `` \n '' ) # Afterwards , we do automatic shape inference : self.input_bias , self.recurrent_bias = self.bias , None updated.append ( outputs [ o ] ) data_format : Image data format to use for convolution kernels . regularizers.serialize ( self.recurrent_regularizer ) , raise ValueError ( 'Layer weight shape ' # Subclassed Models may not have been built so ca n't be checked . batch_sizes = set ( batch_sizes ) y_true = K.cast ( y_true , dtype=K.floatx ( ) ) raise ValueError ( 'Graph disconnected : ' initial_state = K.zeros_like ( inputs ) return variable ( np.random.uniform ( low=low , high=high , size=shape ) , KeyboardInterrupt ) : if len ( params ) == len ( weights ) + 1 : kernel_dim2=3 , K.pool3d ( x , pool_size=pool_size , pool_mode='median ' ) for st in decoded : raise ValueError ( 'Input ' + str ( input_index ) def __init__ ( self , filters , if epoch % self.histogram_freq == 0 : config = { 'units ' : self.units , x.set_shape ( transpose_shape ( output_shape , data_format , future.idx = i return np.append ( index_array , last_batch ) raise ValueError ( 'Please do not pass a dictionary ' i.e . applies a transformation that maintains the mean activation # list . We need to recover the ndarray set_epsilon ( _epsilon ) self.__dict__.update ( model.__dict__ ) self.input_spec = InputSpec ( ndim=len ( self.dims ) + 1 ) # TODO : raise exception when a ` .compute_mask ( ) ` call 'input_dtype ' , # legacy model.compile ( optimizer , loss='mse ' , filtered_layers = [ ] a warning will be displayed . When ` compile ` is set [ padding [ 2 ] [ 0 ] , padding [ 2 ] [ 1 ] ] , and ( output_dim , ) for weights and biases respectively . x_c = K.dot ( inputs_c , self.kernel_c ) return Multiply ( * * kwargs ) ( inputs ) # Except permission denied and potential race conditions enqueuer.stop ( ) layer_test ( layers.MaxPooling1D , categorical_crossentropy , warnings.warn ( 'The ` MaxoutDense ` layer is deprecated ' assert_allclose ( y1 , y2 , atol=1e-05 ) with h5py.File ( filepath , ' w ' ) as f : # output masks and output shapes in one pass , x : input tensor for dim in state_size ] height_shift_range=0. , # test dtype changing normalizer If ` by_name ` is True , weights are loaded into layers if y.shape [ : sample_weight.ndim ] ! = sample_weight.shape : loss = self.model.evaluate ( x , y , * * kwargs ) @ six.add_metaclass ( abc.ABCMeta ) class _Pooling2D ( Layer ) : metrics=self._compile_metrics , str ( [ w.shape for w in weights ] ) ) assert_allclose ( out , new_out , atol=1e-05 ) if len ( self._inbound_nodes ) ! = 1 : for s , n_s in zip ( states , new_states ) : sample_weight : Optional weight scalar or ` Tensor ` whose dimensions match model : A Keras model instance # assert config [ 'config ' ] [ k ] == new_config [ 'config ' ] [ k ] self.best = self.baseline padding='valid ' , data_format=None , dilation_rate=1 ) : # Sample-based predictions . self.local_objects = { } ImportError : if h5py is not available . if len ( initial_state ) ! = len ( self.states ) : layer = layer_class ( units ) seed=seed , inputs_z = inputs def assert_thresholds_range ( thresholds ) : if training is True : if ( steps_done > = steps_per_epoch and return_sequences=False , elif x.ndim == 4 : or an instance of Sequence ( keras.utils.Sequence ) depthwise_kernel = _preprocess_conv2d_kernel ( depthwise_kernel , data_format ) `` `` '' Calls the ` on_predict_batch_end ` methods of its callbacks . dropped_inputs , if hasattr ( model , '_collected_trainable_weights ' ) : def next_sample ( uid ) : Check if optimization works . base_config = super ( Adadelta , self ) .get_config ( ) weight_values [ i ] .shape ) ) # This use case is different and is handled separately . return rng.normal ( size=shape , avg=mean , std=stddev , dtype=dtype ) # then call node.inbound_layer on them . def chunk_read ( response , chunk_size=8192 , reporthook=None ) : overwrite the training Sequence . for tensor in initial_state : from tensorflow.keras.layers import UpSampling1D return np_value z_np = KNP.batch_dot ( x_np , y_np , axes ) class Conv1D ( _Conv ) : metrics = self._metrics [ : ] the size of the recurrent state return losses + super ( Wrapper , self ) .get_losses_for ( None ) refs = defaultdict ( int ) summary_value = summary.value.add ( ) return weights from .losses import sparse_categorical_crossentropy self.kernel_initializer = initializers.get ( kernel_initializer ) biases = np.array ( weights [ 2 ] ) .reshape ( ( 2 , -1 ) if from_cudnn else -1 ) from tensorflow.keras.layers.experimental.preprocessing import * untar : Deprecated in favor of 'extract ' . # Iterated over every node in the reference model , in depth order . base_config = super ( _Cropping , self ) .get_config ( ) PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/ -- ignore=tests/integration_tests -- ignore=tests/docs -- ignore=tests/keras/legacy/layers_test.py -- ignore=tests/test_api.py -- cov-config .coveragerc -- cov=keras tests/ ; are used for create a sample sequence . raise ValueError ( 'If your data is in the form of a Python generator , ' specified as we expect users to provide batched datasets . inputs_c = inputs precision = K.switch ( stack_size = 3 _SYMBOLIC_SCOPE.value = prev_value # A model is needed to initialize weights . self.on_train_begin = lambda logs : None except ValueError : @ pytest.mark.skipif ( K.backend ( ) == 'theano ' , reason='Not supported ' ) and given the filename ` fname ` . The final location of a file f.file.close ( ) class Recurrent ( Layer ) : `` `` '' Element-wise square root . def test_highway ( ) : the beginning of each epoch . Only used with instances return cls ( inputs=input_tensors , outputs=output_tensors , name=name ) Scalar tensor . # Update the name on the metric class to be the unique generated name . if model.uses_learning_phase and not isinstance ( K.learning_phase ( ) , constraints = { } return isinstance ( x , tf.Variable ) For stride ` s ` , consecutive output samples would old_layer = keras.layers.Convolution2D ( 5 , nb_row=3 , nb_col=3 , name='conv ' ) ( less the batch dimension and the dimension that was summed over ) . list ( custom_objects.items ( ) ) ) ) assert id ( x ) in tensor_map , 'Could not compute output ' + str ( x ) ` ( batch , new_depth , new_rows , new_cols , filters ) ` reduction=reduction , an ` Activation ` layer must be added after . self.end_of_epoch_signal.set ( ) printable_module_name='function in Lambda layer ' ) outputs = [ ] for layer , depth in layers_depths.items ( ) : SUM = 'sum ' specificities = K.switch ( assert K.floatx ( ) == dtype `` `` '' Training-related utilities . return tf.transpose ( x ) ValueError : in case of invalid constructor arguments . using ` use_multiprocessing=True ` . if isinstance ( n , int ) : # [ output_a_np , output_b_np ] , > > > from keras import backend as K os.remove ( fname ) elif dtype == 'float16 ' : def __init__ ( self , units , WITH_NP , cntk_two_dynamicity=True , axes= ( 2 , 1 ) ) y_pred = K.squeeze ( y_pred , -1 ) return model be either ` 1 ` , or the same as the corresponding ` y_true ` dimension ) . def get_params ( self , * * params ) : config = { 'return_sequences ' : self.return_sequences , self._metrics.append ( value._metric_obj ) if positions [ -1 ] < = 1 : def else_expression_fn ( ) : You can also pass a list ( len = len ( outputs ) ) of lists of metrics embeddings_metadata = self.embeddings_metadata from keras.backend import tensorflow_backend as KTF return ( variable , variable * momentum + value * ( 1 . - momentum ) ) then_expression_fn = then_expression output_tm1 = np.zeros ( output_sample.shape ) non_trainable_weights ( list of variables ) # 0.879 ... = scipy.stats.truncnorm.std ( a=-2 , b=2 , loc=0. , scale=1 . ) example . The shape of both ` y_pred ` and ` y_true ` are output = super ( RNN , self ) .__call__ ( full_input , * * kwargs ) if k not in self._values : check_single_tensor_operation ( 'flatten ' , ( 4 , 1 ) , WITH_NP ) index_array = batch_shuffle ( index_array , batch_size ) while metric_name in self.metrics_names : self.axes = axes or { } from six.moves.urllib.request import urlretrieve dense.output_mask a ` state_size ` attribute . This can be a single integer ( single state ) shape = tuple ( [ -1 for _ in range ( num_dynamic_axis - i ) ] ) + shape ' ` categorical_crossentropy ` expects ' y = np.transpose ( y , ( 0 , 2 , 1 ) ) 'output2 ' : [ 'mse ' , 'binary_accuracy ' ] ' If your inputs are not batched , ' run_metadata=run_metadata ) the input mask is not ` None ` : 'Input ' + str ( input_index ) # which is a slice of the first 150 elements def maximum ( x , y ) : batch : integer , index of batch within the current epoch . outputs : List of outputs tensors . # apply sample weighting noise_shape = self._get_noise_shape ( inputs ) self.data.file.flush ( ) or list/tuple/set . If an integer , specifies how many training if not self.layer.built : `` `` '' Class for capturing the TF device scope . '' '' '' output_shapes = [ ] z = K.function ( [ x ] , [ z ] ) ( [ x_val ] ) [ 0 ] def ones_like ( x , dtype=None , name=None ) : greedy=False , beam_width=1 , top_paths=1 , if shape1 [ axes [ 0 ] ] ! = shape2 [ axes [ 1 ] ] : self._compile_metric_functions = [ ] 'sample_weight_mode ' : model.sample_weight_mode , model.add ( TimeDistributed ( Dense ( 8 ) , input_shape= ( 10 , 16 ) ) ) output_tensors [ i ] ._keras_shape = output_shapes [ i ] dimension 2 of y has been summed over . ( dot_axes [ 1 ] = 2 ) For every pair of values in y_true and y_pred : mask=mask_np , from tensorflow.keras.utils import get_custom_objects if all ( [ hasattr ( x , '_keras_shape ' ) for x in computed_tensors ] ) : if data_format not in { 'channels_last ' , 'channels_first ' } : false positives . This metric creates one local variable , ` accumulator ` input_length , greedy=True ) and between ` CuDNNGRU ` and ` GRU ( reset_after=True ) ` . Default ` GRU ` is not `` `` '' Deserializes a user defined function . return updated from tensorflow.keras.layers import GlobalMaxPooling3D if callable ( alt ) : def test_ctc ( self ) : from .load_backend import l2_normalize # columns as there are classes . The number of rows from .. engine.base_layer import Layer while self.queue.qsize ( ) > 0 : y_true = y_true [ ... , class_id ] def save ( self , filepath , overwrite=True , include_optimizer=True ) : if initial_state is None : raise ValueError ( 'Inputs to ` DepthwiseConv2D ` should have rank 4 . ' input_length=timesteps ) Output tensor or list of output tensors . makes sure the arguments are separated and that ` initial_state ` and # We keep it for compatibility reasons . if type ( obj ) .__name__ == type.__name__ : sys.stderr.write ( 'Using ' + _BACKEND + ' backend.\n ' ) y = 0 return AUCSummationMethod.MAJORING 'class_name ' : model.optimizer.__class__.__name__ , legacy_pooling3d_support = generate_legacy_interface ( _TEST = 'test ' write_graph is set to True . input_dict , self.trainer_output ) Default value is ` SUM_OVER_BATCH_SIZE ` . than the input . If the number of dimensions is reduced to 1 , `` `` '' Filters ` sk_params ` and returns those in ` fn ` 's arguments . receptive_field_size = np.prod ( shape [ : -2 ] ) stateful : Boolean indicating whether the layer carries 'embeddings display when using TensorFlow 2.0 . ' 'An input could not be retrieved . ' name = 'input_ % d ' % ( i + 1 ) kept_states = [ ] # We need a second forward-pass here because we 're passing input_tensors.append ( layer_output_tensors [ tensor_index ] ) reason='Uses the ` options ` and ` run_metadata ` arguments . ' ) from .. utils import conv_utils # Following 2 properties : input and output shapes . self.local_objects [ filepath ] .seek ( 0 ) data_format , class_id : ( Optional ) Integer class ID for which we want binary metrics . return x.eval ( ) tuple , or set , specifies the epochs on which to run validation , def nullcontextmanager ( ) : ids= [ 'single ' , 'bidirectional ' ] ) sequences= [ inputs , mask ] , if len ( to_list ( input_tensors ) ) ! = 1 : x = C.convolution ( depthwise_kernel , x , # 10 timesteps , with 64 output filters output_dim = cell.output_size # reverse_state_order determines whether the state size will be in a self.num_thresholds = num_thresholds finally : # or if the nodes have multiple inbound_layers sequence = list ( range ( len ( self.sequence ) ) ) 'should be fully-defined , but layer received ' top_paths=top_paths , merge_repeated=merge_repeated ) `` `` '' Exponential linear unit . update_op = metric_obj.update_state ( * args , * * kwargs ) such as ` metrics= [ [ 'accuracy ' ] , [ 'accuracy ' , 'mse ' ] ] ` or `` `` '' Spatial 3D version of Dropout . timesteps = 6 # Store in cache . from tensorflow.keras.layers import Flatten loop_vars [ ConfusionMatrix.TRUE_NEGATIVES ] = ( label_is_neg , pred_is_neg ) height = input_shape [ h_axis ] assert ( history.history [ 'accuracy ' ] [ -1 ] > = 0.75 ) if archive_format is None : check_single_tensor_operation ( 'cumprod ' , ( 4 , 2 ) , WITH_NP , axis=1 ) from .core import SpatialDropout2D new_layer = keras.layers.LSTM ( 2 , input_shape= [ 3 , 5 ] , name= 'd ' , implementation=2 ) outputs = K.depthwise_conv2d ( y : Numpy array of model targets to be weighted . def name_scope ( name ) : def random_uniform_variable ( shape , low , high , dtype=None , name=None , seed=None ) : if shapes [ i ] is not None and not K.is_tensor ( data [ i ] ) : # E501 line too long ( 82 > 79 characters ) height_cropping = conv_utils.normalize_tuple ( def random_uniform ( shape , minval=0.0 , maxval=1.0 , dtype=None , seed=None ) : # equivalent to added = keras.layers.add ( [ x1 , x2 ] ) shear_range=shear_range , if self.monitor_op ( current , self.best ) : `` `` '' MNIST handwritten digits dataset . '' '' '' self.assert_input_compatibility ( inputs ) n = 2 return ( target * -np.log ( sigmoid ( output ) ) test_cases.append ( [ ( None , 3 , 4 , 5 ) , ( None , 2 , 4 ) , 2 ] ) kwargs.pop ( 'initial_state ' ) # Take exponential as we directly apply ctc_decode_beam_search return [ kernels , recurrent_kernels , biases ] thresholds = sorted ( thresholds ) y_batch_size = y_shape [ 0 ] weight_names = load_attributes_from_hdf5_group ( g , 'weight_names ' ) cache_dir ` ~/.keras ` , placed in the cache_subdir ` datasets ` , Optimization ] ( http : //www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf ) more than one rnn inputs is required but not supported in Keras yet . ) x = tf_keras_backend.placeholder ( # depth levels in the graph . self.weights = [ self.iterations ] + accumulators + delta_accumulators output_shape = layer.output_shape def load_batch ( fpath , label_key='labels ' ) : m.update_state ( input ) feed_input_shapes = None ' can not be negative value : % s ' % str ( max_value ) ) ValueError : In case the network is not valid ( e.g . disconnected graph ) . output_shape = ( batch_size , out_depth , output_shape= ( 6 , 7 , 5 ) , name='deconv ' ) states = tuple ( initial ) if num_time_step is None and not has_seq_axis ( inputs ) : K.variable ( `` , dtype='unsupported ' ) y_rev = y_rev [ 0 ] if self.epochs_since_last_save > = self.period : new_layer = keras.layers.GlobalAvgPool2D ( data_format='channels_last ' , if strides is None : recurrent_kernels = transform_kernels ( weights [ 1 ] , lambda k : k.T , n_gates ) if isinstance ( v , ( np.ndarray ) ) : class Wrapper ( Layer ) : from tensorflow.keras.layers import AveragePooling3D def _raise_invalid_arg ( key ) : from . import np_utils def InceptionResNetV2 ( * args , * * kwargs ) : from tensorflow.keras.layers import BatchNormalization on_epoch_begin=None , super ( AlphaDropout , self ) .__init__ ( * * kwargs ) th_padding = 'valid ' } , loss = K.cast ( loss , input_dtype ) get_graph = tf_keras_backend.get_graph different patch of the input . @ abstractmethod seed=None , end = None # Add an inbound node to the layer , so that it keeps track padding='valid ' , x : A candidate placeholder . encodes a probability distribution . current = squeeze ( current , time_axis ) x = k.switch ( k.greater_equal ( x , 0.5 ) , x * 0.1 , x * 0.2 ) def __init__ ( self , output_dim , out_filters = input_shape [ 1 ] * self.depth_multiplier in both ` sk_params ` and ` fn ` 's arguments . closure = None `` `` '' Contains the base Layer class , from which all layers inherit . '' '' '' `` `` '' Abstract class for different pooling 1D layers . 3D tensor with shape : # build batch logs # add the time_step axis back A tensor , the element-wise maximum of the inputs . b_regularizer : instance of [ WeightRegularizer ] ( .. /regularizers.md ) , ndim = K.ndim ( x ) self.backup = None kernel_regularizer='l1 ' , def test_arange ( self ) : for state , dim in zip ( self.states , self.cell.state_size ) : return ( input_shape [ 0 ] , ) + tuple ( in_lens ) + ( self.output_dim , ) self.input_bias_h = self.input_bias [ self.units * 2 : ] `` `` '' Test pickling model without compiling . x = keras.Input ( ( None , 5 ) ) except TypeError : for i , rep in enumerate ( n ) : A batch class MeanAbsoluteError ( MeanMetricWrapper ) : entries in the batch for which ` class_id ` is in the label , and computing the pool_size=2 , padding='valid ' , data_format='channels_last ' , the device ( ` CPU ` or ` GPU ` ) . If the scope is not explicitly set , it will Reduce the input_mask to 2 dimensions and return it . elif shuffle : x_shape = tuple ( x_shape ) in the output sequences . This is useful to reserve part of the _ = o ( i ) 'recurrent_initializer ' : z_list += [ z ] class HDF5Matrix ( object ) : if not overwrite and os.path.isfile ( filepath ) : node_conversion_map = { } if x_expanded : original_keras_version = model_weights_group [ 'keras_version ' ] .decode ( 'utf8 ' ) layer.forward_layer.add_loss ( lambda : 0 ) `` `` '' Zero-padding layer for 2D input ( e.g . picture ) . for _ in sorted ( _axis , reverse=True ) : elif x_batch_size is not None and y_batch_size is not None : return K.constant ( self.value , shape=shape , dtype=dtype ) devices = get_session ( ) .list_devices ( ) if cache_key in self._output_shape_cache : self.cooldown_counter = 0 if ( hasattr ( self , 'activity_regularizer ' ) and # apply a unshared weight convolution 1d of length 3 to a sequence with self.weights = [ self.iterations ] + accumulators from tensorflow.keras.applications.nasnet import NASNetLarge `` `` '' A backwards compatibility alias for ` on_train_batch_end ` . '' '' '' alpha : A scalar , slope of negative section ( default= ` 0. ` ) . code = inspect.getsource ( member ) loss_weights=None , If layer is not built : `` `` '' Returns a clone of the metric if stateful , otherwise returns it as is . '' '' '' base_config = super ( GRUCell , self ) .get_config ( ) # rather than input-dependent 'implementation ' : self.implementation } if isinstance ( val , h5py.Dataset ) : return variable ( value=np.ones ( shape , ctype ) , dtype=dtype , name=name ) # where d is the dimension to reduce . if shape is not None : x = expand_dims ( x , spatial_start_dim ) y_pred=None , m = keras.metrics.CategoricalHinge ( ) layers : list of Layer instances . `` `` '' Pads the middle dimension of a 3D tensor . # dropout patterns are different , only check mean constraint=self.bias_constraint ) self.execute = execute fn each with shape ` ( batch_size , units ) ` . 'but got ' , mode ) self.shared_axes = None conversions = conversions or [ ] config [ 'config ' ] , # if the model has multiple nodes return const if ` class_mode ` is ` `` binary '' ` or ` `` sparse '' ` it must include super ( BinaryCrossentropy , self ) .__init__ ( bias_regularizer=None , for k in kwargs : # 0 ( padding ) , 1 ( start ) , 2 ( OOV ) ` embeddings_layer_names ` . Numpy array ( if the model has a single super ( GeneratorEnqueuer , self ) .__init__ ( sequence , use_multiprocessing ) def __init__ ( self , rate , data_format=None , * * kwargs ) : self.sample_weight_modes = sample_weight_modes It is also possible for ` cell ` to be a list of RNN cell instances , def test_func_dump_and_load ( test_function_type ) : max_value=0.6 ) to be within ` [ 0 , 255 ] ` . if os.path.exists ( temp_fname ) : Scalar training loss # copy over break ` depth ` and ` rows ` and ` cols ` values might have changed due to padding . the padding dimension ( axis 1 ) . def __init__ ( self , mask_value=0. , * * kwargs ) : ` f ( x ) = x for x > theta ` , `` `` '' oh = T.reshape ( oh , input_shape + ( num_classes , ) ) input_dim = int ( input_shape [ channel_axis ] ) name=name , dtype=dtype ) base_config = super ( ReLU , self ) .get_config ( ) 'Provided ` ' + weight_type `` `` '' Adds a layer instance on top of the layer stack . if isinstance ( value , ( float , int ) ) : yield inputs Path to the downloaded file rate = 1 height_factor=2 , that is , a different set of filters is applied at each from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams A tensor ( or list of tensors if the layer has multiple outputs ) . def filter_sk_params ( self , fn , override=None ) : output_shapes.append ( shape ) def zeros_like ( x , dtype=None , name=None ) : from .load_backend import all output : A tensor . from .vis_utils import plot_model z_list.append ( k.eval ( x ) ) border_mode='valid ' , the axes to compute the logical or . If ` None ` ( default ) , computes model.add ( pooling.GlobalAveragePooling1D ( ) ) > > > kvar_ones = K.ones_like ( kvar ) tf_data_format = 'NCW ' `` `` '' Computes the hinge metric between ` y_true ` and ` y_pred ` . layers_by_depth [ depth ] = [ ] layer ( computed_tensor , * * kwargs ) ) name='global_avgpool3d ' ) metrics_utils.AUCSummationMethod ) and summation_method not in list ( self._output_coordinates.append ( ( layer , node_index , tensor_index ) ) averages = [ float ( outs_per_batch [ -1 ] [ 0 ] ) ] # index 0 = 'loss ' 'are no longer supported . ' ) batch_input_shape=None , node_index , `` `` '' Computes sin of x element-wise . else a symbolic loop will be used . 'summation_method ' : self.summation_method.value , base_config = super ( LeakyReLU , self ) .get_config ( ) shape.pop ( axis ) name : String , the name for the weight variable . `` ` `` `` '' Element-wise truth value of ( x < = y ) . tf.summary.histogram ( ' { } _grad'.format ( mapped_weight_name ) , [ 3. , 4 . ] ] , dtype=float32 ) # Find the index of the threshold where the sensitivity is closest to the from . import experimental new_layer2 = keras.layers.GaussianDropout ( 0.6 , name='drop ' ) def test_cudnn_rnn_canonical_to_params_gru ( ) : mode='fan_in ' , x_r = K.bias_add ( x_r , self.input_bias_r ) unprocessed_nodes [ layer ] .append ( node_data ) raise ValueError ( 'In ` Conv3DTranspose ` , with padding mode ` same ` , ' height_factor , if invalid_thresholds : 'convolutional RNNs . ' ) if hasattr ( obj , 'get_config ' ) : of the 1st , 2nd , and 10th epochs . `` `` '' Transforms kernel for each gate separately using given function . ValueError if unknown identifier . ` _collected_trainable_weights ` are inconsistent ( i.e . have different unconcatenated_outs = [ ] # with pytest.raises ( AttributeError ) : https : //gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d ) val = np.random.random ( ( 4 , 2 ) ) unroll=self.unroll , print ( 'Epoch % d/ % d ' % ( epoch + 1 , self.epochs ) ) dilation_rate : An integer or tuple/list of a single integer , specifying model = _deserialize_model ( h5dict , custom_objects , compile ) fn : Callable that will be called upon each element in elems and an for input_data in node_data : class Ones ( Initializer ) : x = C.splice ( x , C.constant ( value=0 , shape=postfix_shape ) , axis=axis ) from .wrappers import Bidirectional output_shape [ dim ] += sum ( padding_all_dims [ dim ] ) [ 0.9 , 0.1 , 0.0 , 0.0 ] , # t=4 * * kwargs : dictionary arguments if parameter is None : outputs = T.squeeze ( outputs ) if i is None or j is None : name ) ) if spec is None : weight_names = _uniquify ( weight_names ) parallel_model.fit ( x , y , epochs=2 ) alpha = 1.6732632423543772848170429916717 input_shapes = [ ] nodes_depths [ inbound_node ] = max ( depth + 1 , previous_depth ) ' weights as the third element of the generator . ' ) # ( TODO : fix in tf.keras ) return np.reshape ( x , shape ) added = keras.layers.add ( [ x1 , x2 ] ) def hinge ( y_true , y_pred ) : Must be specified if using ` unroll ` . self.kernel_c = self.kernel [ : , self.units * 2 : self.units * 3 ] def update_add ( x , increment ) : mask , and generates batches of augmented/normalized data . from .pooling import GlobalMaxPool2D It should have same length as ` data ` . ( may include None for unchecked axes ) . raise ValueError ( 'CNTK Backend : randomness op with ' from .convolutional import UpSampling3D K.pool2d ( x , pool_size=pool_size , pool_mode='median ' ) elif do_validation : prob = y_pred [ i ] # if the state is not reset , output should be different weights [ 7 ] ] , axis=-1 ) self.sequence = sequence follow_links : whether to follow symlinks inside class subdirectories def test_resize_images_bilinear ( self , data_format ) : momentum : Momentum for the moving mean and the moving variance . model.load_weights ( fname ) pattern.insert ( axis , ' x ' ) ( layers.CuDNNLSTM , { 'units ' : 2 , 'input_shape ' : [ 3 , 5 ] } ) , ValueError : In case the ` layer ` argument does not embeddings_data : data to be embedded at layers specified in for i in range ( len ( feed ) ) : 'dilation_rate ' ) padding='valid ' , data_format=None , identifier : Optimizer identifier , one of kernel_initializer=kernel_initializer , activation : name of activation function to use This allows you to optionally specify a directory n = tuple ( n ) b_constraint='unitnorm ' , assert dense._inbound_nodes [ 1 ] .inbound_layers == b_layer if dim < 0 : 'epsilon ' : epsilon ( ) , from .common import floatx , epsilon , image_data_format recurrent_dropout : Float between 0 and 1 . error_sq = K.square ( y_pred - y_true ) `` `` '' Returns the ` updates ` from all layers that are stateful . return tf.tile ( x , n ) 'Error when checking ' + exception_prefix output_names : a list of the names ( strings ) of model outputs . bias_constraint=bias_constraint , return T.clip ( x , min_value , max_value ) to be created is sparse . A tensor with the variance of elements of ` x ` . constraints.serialize ( self.embeddings_constraint ) , name='depthwise_kernel ' , outputs = T.switch ( output_mask , outputs , output_tm1 ) if layer.__class__.__name__ == 'Conv2DTranspose ' : labels : dense CTC labels . > > > kvar = K.variable ( np.random.random ( ( 2 , 3 ) ) ) if self.trainer is not None : custom_objects : Optional dictionary mapping unroll=unroll , func = K.function ( [ model.input ] , mask_outputs ) pytestmark = pytest.mark.skip ` ( samples , rows , cols , channels ) ` if ` data_format='channels_last ' ` . mask tensors . losses : ` Tensor ` of shape ` [ batch_size , d1 , ... dN ] ` . line += ' ' * ( positions [ i ] - len ( line ) ) merge_mode=mode ) ( inputs ) > > > _hash_file ( '/path/to/file.zip ' ) for i , v in enumerate ( inputs ) : metric_name = ' % s_ % s ' % ( self.output_names [ output_index ] , metric_name ) shape = list ( int_shape ( x ) ) xval = np.random.random ( x_shape ) # create numpy arrays of input data or as an extreme version of an Inception block . rnn_layer_kwargs [ 'reset_after ' ] = True return tf.transpose ( x , perm=pattern ) def _preprocess_conv3d_volume_shape ( volume_shape , data_format ) : params : dict . Training parameters num_samples = 4 ] , from .data_utils import get_file 'same ' , 'channels_last ' ) , constants , batch_size : Integer batch size or ` None ` if not defined . def get ( identifier ) : reduction : ( Optional ) Type of loss Reduction to apply to loss . shape : Tuple of integers , shape of returned Keras variable odd_pad_w = pool_size [ 0 ] > 2 and pool_size [ 0 ] % 2 == 1 `` `` '' Returns op to update the given confusion matrix variables . metrics_utils.ConfusionMatrix.TRUE_POSITIVES : self.true_positives , sample_weight_mode= { 'dense_1 ' : 'temporal ' } ) > > > K.count_params ( kvar ) ` y_true ` = [ [ 0 , 0 , 1 ] , [ 1 , 0 , 0 ] , [ 0 , 1 , 0 ] ] . self.input_shapes = input_shapes kwargs [ 'mask ' ] = previous_mask w = x._keras_shape [ 3 ] + left_pad + right_pad raise ValueError ( 'Invalid AUC summation method value `` % s '' . ' % key ) model = keras.models.Model ( [ inputs ] + initial_state , output ) metrics = { 'output1 ' : [ 'mse ' , 'binary_accuracy ' ] , # RANDOMNESS 10000 samples . Note that writing too frequently to TensorBoard # Update tensor history , _keras_shape and _uses_learning_phase . of every epoch . input_prob_matrix_1 [ t , : ] ] ) self.build_fn = build_fn Consider a custom object ` MyObject ` seed=seed ) return metric dtype=dtype , output_generator = generator Under Python 2 , ` urlretrieve ` relies on ` FancyURLopener ` from legacy def in_cooldown ( self ) : return K.mean ( inputs , axis= [ 2 , 3 , 4 ] ) _SEQUENCE_COUNTER.value += 1 pattern [ 1 ] = a1 pool_size : tuple of 3 integers . assert 'loss ' in kwargs `` `` '' Input : nD integer tensor of shape ( batch_size , dim1 , dim2 , ... dim ( n-1 ) ) y_true = K.squeeze ( y_true , -1 ) if 'name ' in config : raise ValueError ( ' ` cropping ` should have 3 elements . ' from .. import backend if archive_format == 'auto ' : y = y [ 0 ] ValueError : In case of invalid user-provided arguments . ValueError : If the provided weights list does not match the from . import data_utils while i < len ( shape ) : array ( [ [ 1. , 1. , 1. , 1 . ] , cls = module_objects.get ( class_name ) if steps_done == 1 : # We assume it is sparse categorical only if loss is explicitly given return [ output_shape ] + state_shape + copy.copy ( state_shape ) preprocessor=zeropadding2d_args_preprocessor ) reshape=False ) : self.input_spec = InputSpec ( ndim=4 ) training : Boolean or None . Only relevant in symbolic mode . if parameter is None : def __init__ ( self , name='poisson ' , dtype=None ) : data_format = 'channels_last ' if len ( sample_weight.shape ) > len ( y.shape ) : while i < val_size : assert K.dtype ( K.variable ( False , dtype='bool ' ) ) == 'bool ' outputs , new_states = step_function ( inputs , states ) 'incompatible with the specified batch ' n = ( 4 , 3 ) `` `` '' dense_layer.add_loss ( 1 , inputs=None ) the ` kernel ` weights matrix raise ValueError ( 'Layer was passed constants ' e.g . ` input_shape= ( 128 , 128 , 128 , 1 ) ` for 128x128x128 volumes def separable_conv2d ( x , depthwise_kernel , pointwise_kernel , strides= ( 1 , 1 ) , _ , output_values = self.metrics_func.forward ( self.add_update ( layer.get_updates_for ( computed_tensors ) , inputs ) kernel_initializer='normal ' , The activation function , ` linear ` if ` identifier ` is None . indicating the start indices of the slice # non scalar used to manually specify thresholds which split the predictions more evenly . n = tuple ( [ 1 for _ in range ( len ( shape ) - len ( n ) ) ] ) + n node_data = [ ] h5dict [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) stride_d , kernel_d , but smaller ones , thus running slower reshape=False ) 'embeddings_freq was set to 0 ' ) ` ( inputs , targets ) ` or go_backwards=self.go_backwards , str ( weight_values [ i ] .shape ) + ' . ' ) indexes = [ words.index ( arg + `` : '' ) for arg in args ] model_nest_level , model_type ) # We do n't process nodes ( i.e . make layer calls ) warnings.warn ( return [ x_weight ] pool_size= ( 2 , 2 , 2 ) , padding='valid ' , data_format='channels_first ' , width_factor : Positive integer . # add to filtered_inbound_nodes . [ h , c ] = states_and_constants return x m_schedule_new = self.m_schedule * momentum_cache_t batch_size : integer batch size . # Overwrite the argument with its numpy translation return activations.softmax ( inputs , axis=self.axis ) self.false_negatives = self.add_weight ( hasher = hashlib.md5 ( ) def _handle_per_output_metrics ( self , cell_value = dummy_fn.__closure__ [ 0 ] return isinstance ( layer , Model ) `` `` '' Base object for fitting to a sequence of data , such as a dataset . elif layer_weights_shape ! = weights [ 0 ] .shape : # Make sure that changes to the global floatx are effectively from tensorflow.keras.datasets.imdb import get_word_index output_shape_type = 'function ' conv_out = conv_out [ : , : , : , : i ] `` `` '' Cropping layer for 1D input ( e.g . temporal sequence ) . initializers.Ones ( ) ( ( self.units , ) , * args , * * kwargs ) , shuffle_pattern [ axis ] = 1 ( 'W_regularizer ' , 'embeddings_regularizer ' ) , self._recurrent_dropout_mask = _generate_dropout_mask ( class ConvLSTM2DCell ( Layer ) : old_layer = keras.layers.GlobalMaxPooling2D ( dim_ordering='th ' , # To do that , we need an auxiliary axis to repeat elements along ndim = K.ndim ( score_array ) None or an empty list will return no matches found . initial_states : Tensor with shape ( samples , ... ) ( no time dimension ) , of images with shape ` ( batch_size , * target_size , channels ) ` it could be either Numpy array ( s ) , framework-native tensor ( s ) , class LSTM ( RNN ) : [ 0. , 1. , 0 . ] , v = value.asarray ( ) use_bias=use_bias , if None not in input_shape and len ( set ( map ( len , input_shape ) ) ) == 1 : on_batch_begin : logs include ` size ` , def reset_uids ( ) : x = tf.nn.atrous_conv2d_transpose ( for the samples in the next batch . This assumes a one-to-one mapping sub_w_last_node [ layer.layer.name ] = sub_w_nodes [ -1 ] mean : Mean of the values . strides=strides , # ` build ( batch_input_shape ) ` : self.schedule = schedule raise ValueError ( 'Weights must be of equal size to ' tuple or None . def _prepare_name ( name , default ) : stacklevel=3 ) def test_remove_long_seq ( ) : obj.result = types.MethodType ( # TensorFlow example i = self.recurrent_activation ( x_i + h_i ) length : Length of the output sequences ( in number of timesteps ) . skip_target_weighing_indices : Indices of output for which sample weights by size [ 0 ] and size [ 1 ] respectively . ` Flatten ` then ` Dense ` layers upstream grad.values if is_indexed_slices ( grad ) else grad absolute paths if ` directory ` is ` None ` ) . self.samples_seen = 0 v = C.parameter ( shape=shape , y_true = math_ops.cast ( y_true , y_pred.dtype ) inputs = np.exp ( inputs ) @ pytest.mark.skipif ( ( K.backend ( ) == 'cntk ' and load_backend.dev.type ( ) == 0 ) , from tensorflow.keras.layers import Reshape 'array per model output . ' ) def __init__ ( self , name='cosine_similarity ' , dtype=None , axis=-1 ) : take as argument will contain keys for quantities relevant to nodes_in_progress = set ( ) h_tm1_c = h_tm1 * rec_dp_mask [ 2 ] out_labels = self.metrics_names conversions= [ ( 'input_dtype ' , 'dtype ' ) ] ) raise ValueError ( 'Invalid ` mode ` argument : ' 'Received : ` gpus= % d ` ' % gpus ) elif K.is_tensor ( x ) : 'batches of input data ) . ' ) # return the same x to take cntk broadcast feature input_dim = input_shape [ channel_axis ] self.optimizer = optimizer from tensorflow.python.framework import ops as tf_ops reps [ axis ] = rep raise IOError ( ' { } does not exist'.format ( filepath ) ) loss_weights : Optional list or dictionary specifying scalar coefficients return 0 . self.patched_file_io.stop ( ) then we will divide the input into 2 sub-batches of 32 samples , and must be broadcastable to ` y_true ` . enqueuer.close ( ) for k in range ( w.shape [ 1 ] ) : from keras.preprocessing.sequence import TimeseriesGenerator # HDF5Matrix behave more or less like Numpy matrices with regards to indexing y.append ( x [ : , : , k : k1 : strides [ 0 ] , l : l1 : strides [ 1 ] ] ) info += ' % .0fs/step ' % time_per_unit # constants are appended to states in K.rnn model.compile ( optimizer , loss= [ ] ) shape = value.shape if hasattr ( value , 'shape ' ) else ( ) target_f.write ( source_f.read ( ) ) 'test_function_type ' , if floatx not in { 'float16 ' , 'float32 ' , 'float64 ' } : from .utils.generic_utils import has_arg def serialize ( metric ) : global custom objects are reverted to state elif len ( u_ops ) > 0 : d = { } name=layer.name + '_input ' ) 'ce ' : binary_crossentropy ( ) , model.fit ( x , y , epochs=4 , batch_size=32 ) assert len ( outputs ) > 0 result._keras_shape = shape b_regularizer=None , [ 0 , 0 , 1 , 0 ] then the recall value would be 1 . pad = [ ( 0 , 0 ) , ( 0 , 0 ) ] + [ ( s // 2 , s // 2 ) for s in pool_size ] min_index = K.argmin ( output = _reduce_on_axis ( x , axis , 'reduce_min ' ) input2_depth = -1 A Keras variable , an identity matrix . old_layer = keras.layers.LSTM ( 2 , init='normal ' , # Arguments elif is_wrapped_model ( layer ) : # Theano expects ` ( input_depth * depth , 1 , rows , cols ) ` reason='Requires TensorFlow backend and a GPU ' ) assert mse_obj.reduction == losses_utils.Reduction.SUM is_initialized = session.run ( # every kernel as a separate image def sparse_categorical_accuracy ( y_true , y_pred ) : from .convolutional import DepthwiseConv2D target tensors ( in turn , Keras will not expect external if ( now - self._last_update < self.interval and return tf.expand_dims ( tf.range ( label_shape [ 1 ] ) , 0 ) < tf.fill ( for x in inputs : y_pred = K.cast ( y_pred , dtype=K.floatx ( ) ) # with pytest.warns ( UserWarning ) : new_seq , new_label = _remove_long_seq ( maxlen , seq , label ) state_spec = self.state_spec layer_test ( convolutional.MaxPooling2D , def binary_accuracy ( y_true , y_pred , threshold=0.5 ) : target_mean = ( 1 . * min ( tensor_shape ) ) / ( tensor_shape [ 0 ] * tensor_shape [ 1 ] ) 'As a result , we can not save the optimizer ' config [ 'depthwise_regularizer ' ] = ( > > > arr = numpy.array ( [ 1.0 , 2.0 ] , dtype='float64 ' ) 'Batch size : ' + str ( batch_size ) + ' . ' ) return K.concatenate ( inputs , axis=self.axis ) def _set_device_from_string ( self , device_str ) : if not hasattr ( T.nnet.bn , 'batch_normalization_train ' ) : for shape in input_shapes [ 1 : ] : x1 = Dense ( 32 ) ( input_1 ) # test with custom optimizer , loss from .load_backend import log raise ValueError ( 'Unknown data_format : ' , data_format ) follow_links : Whether to follow symlinks inside check_two_tensor_operation ( 'concatenate ' , shape , shape2 , WITH_NP , heights = K.minimum ( y [ : self.num_thresholds - 1 ] , y [ 1 : ] ) target_shape.append ( 1 ) fields = [ name + ' ( ' + cls_name + ' ) ' , to input images ( mainly used to work with autoencoders ) . base_config = super ( Concatenate , self ) .get_config ( ) `` `` '' Resets graph identifiers . '' '' '' # Theano-like behavior example data_npy = np.asarray ( data ) dilation_rate=dilation_rate , sparse=layer.sparse , if not os.access ( _keras_base_dir , os.W_OK ) : out = model.evaluate ( input_a_np , output_a_np ) assert K.int_shape ( o ) == ( None , None , None ) sample_weight_mode : If you need to do timestep-wise return 1 . / ( 1 . + np.exp ( -x ) ) if 'generator already executing ' in str ( e ) : def _reshape_dummy_dim ( x , axis ) : 'output_shape_type ' : output_shape_type , return np.prod ( f ( ) ) if hasattr ( yaml , 'FullLoader ' ) : for x in inputs : constraints.serialize ( self.recurrent_constraint ) , 'If you have an expression instead , use ` eval ( ) ` . ' ) if not target_tensors : if num_train_samples is not None : assert p1 < p2 `` `` '' Convert the input ` x ` to a tensor of type ` dtype ` . raise TypeError ( 'expects constants shape ' ) 'skipping . ' % ( self.monitor ) , RuntimeWarning ) ValueError : In case of invalid ` sample_weight_mode ` input . output = K.minimum ( output , inputs [ i ] ) sample_weight=sample_weight ) self.trainable = kwargs.get ( 'trainable ' , True ) if None in state_shape : file_io_proxy.assert_exists ( gcs_filepath ) execute the generator on the main thread . Use ` sample_weight ` of 0 to mask values . 'and will be removed after 06/2017 . ' ) p2 = K.eval ( K.foldr ( lambda a , b : a * b , vx ) ) import csv assert ( out1.max ( ) ! = out2.max ( ) ) self.custom_objects = args rank is 1 . padding = False # Apply weights to losses . 'same ' , 'channels_last ' , 'max ' ) , from .load_backend import var if isinstance ( identifier , TFOpt ) : if len ( value.shape ) == 0 : if None in x._keras_shape : 'max_value ' : self.max_value , A tensor with the mean of elements of ` x ` . raise ValueError ( ' ` specificity ` must be in the range [ 0 , 1 ] . ' ) # convert the weights between CuDNNGRU and GRU ( reset_after=True ) Example 3 - Training models with weights merge on GPU ( recommended for NV-link ) # Keep track of unconditional updates ( e.g . a counter ) . gamma = ones_like ( mean ) self.ndim = len ( shape ) model.train_on_batch ( x_batch , y_batch ) false_fn=lambda : start ) 'the same number of samples as target arrays . Got ' 'Expected sample_weight with rank ' def test_clip_supports_tensor_arguments ( self , shape ) : `` `` '' Helper function for on_ { train|test|predict } _begin methods . '' '' '' def test_batchnorm_th ( self , x_shape ) : if type ( self.recurrent_initializer ) .__name__ == 'Identity ' : for j in range ( w.shape [ 0 ] ) : _input_tensors.append ( x ) return self.layer.trainable optimizer_weight_values = [ optimizer_weights_group [ n ] for n in print ( ' # # # # # # # test simple model ' ) def forward ( self , argument , device=None , outputs_to_retain=None ) : if updates is None : # train once so that the states change reason='cntk only supports dilated conv on GPU ' ) if len ( mask ) ! = len ( inputs ) : def input_spec ( self ) : printable_module_name='optimizer ' ) ( 'nb_epoch ' , 'epochs ' ) , initializer=self.bias_initializer , if not num_classes : from tensorflow.keras.applications.resnet_v2 import ResNet101V2 def image_data_format ( ) : ` ( batch , steps , channels ) ` str ( list ( set_w ) [ 0 ] ) + ' target samples . ' ) inputs , m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1 class PReLU ( Layer ) : def on_train_end ( self , logs=None ) : self.stop ( ) # new : ( kernel_rows , kernel_cols , filters , stack_size ) # test_gradient test ) . self.padding , file_hash : The expected hash string of the file after download . ( e.g . L2 weight regularization , which only depends inputs_i = inputs * dp_mask [ 0 ] return self.interpolate_pr_auc ( ) 'sample_weight can not be broadcast . ' ) y : Numpy array of target data , def test_with_list_as_targets ( ) : unrelated_updates = [ ] if loss_name == 'categorical_crossentropy ' and len ( y.shape ) ! = 2 : with shape ( batch_size , dim1 , dim2 , ... dim ( n-1 ) , num_classes ) auto_padding= [ verbose=0 ) : IOU = true_positive / ( true_positive + false_positive + false_negative ) . def test_separable_conv ( self , import tarfile axis=axis ) outputs = self.function ( * inputs ) FALSE_NEGATIVES = 'fn ' self.embeddings_layer_names = embeddings_layer_names if fn is None : import marshal computed_masks = [ computed_mask ] elif isinstance ( key , np.ndarray ) : def test_temporal_padding ( self ) : 'dtype ' , return_state=return_state , keepdims=True ) depth_keys = list ( nodes_by_depth.keys ( ) ) tensor_index = node.tensor_indices [ j ] if hasattr ( layer , 'reset_states ' ) and getattr ( layer , 'stateful ' , False ) : __Input shape__ super ( ReduceLROnPlateau , self ) .__init__ ( ) spatial_start_dim = 1 warnings.warn ( 'The ` nb_words ` argument in ` load_data ` ' copy to . ( 'conv1d ' , ( 2 , 8 , 3 ) , ( 4 , 3 , 2 ) , 'valid ' , 'channels_last ' , 2 ) , model : Instance of ` Sequential ` . fn : Callable that will be called upon each element in elems raise ValueError ( 'All sample_weight arrays should have ' ' ` keras.layers.Input ` ' `` `` '' Sets the weights of the model . old_layer = keras.layers.UpSampling1D ( length=3 , name='us1d ' ) class TFOptimizer ( Optimizer ) : if tf_data_format == 'NDHWC ' : The output will then have shape ` ( 32 , 10 , 8 ) ` . : expected_depth ] that no input sequence could be kept . handle_function ( name , member ) ( epoch + 1 , self.monitor , self.best ) ) K.abs ( specificities - self.value ) , axis=0 ) bias_regularizer='l1 ' , new_states = [ ] categorical = np.zeros ( ( n , num_classes ) , dtype=dtype ) def get ( self , key , default=None ) : pool_mode ) : reduction : ( Optional ) Type of reduction to apply to loss . from .pooling import AveragePooling2D k : k1 : strides [ 0 ] , values_shape [ i ] is not None and # If all previous input tensors are available in tensor_map , # It is the same as writing as frequently as possible . from .. models import Model from tensorflow.keras.applications.imagenet_utils import preprocess_input class Sequence ( object ) : if num_thresholds == 1 : axes [ 0 ] += x_ndim set_weights ( weights ) ( self.true_positives + self.false_negatives ) ) , and corresponding variables to update as values . If the final rank is 1 , we reshape it to ` ( batch_size , 1 ) ` . new_width = original_shape [ cols ] * width_factor ` f ( x ) = x ` for ` threshold < = x < max_value ` , int_lp = tf.cast ( lp , 'int32 ' , name='learning_phase ' ) x=None , super ( DepthwiseConv2D , self ) .__init__ ( def preprocess_weights_for_loading ( layer , weights , ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='avgpooling3d ' ) # causal ( dilated ) convolution : A tensor with sum of ` x ` . from tensorflow.keras.layers import SimpleRNN y = np.max ( np.sum ( x * * 2 , axis , keepdims=True ) , axis , keepdims=True ) elif ndim ( beta ) > 1 : import random name : name of this node return out weights_tiled = K.tile ( assert_list_keras_shape ( output_tensors , output_arrays ) preds = self.predict ( x , batch_size , verbose ) return np.float32 computed_tensors = [ computed_tensor ] from an under-represented class . for i , metrics in enumerate ( nested_metrics ) : self.supports_masking = True assert len ( layer.get_updates_for ( x ) ) == 0 from keras.utils import to_categorical `` `` '' Convolutional-recurrent layers . model.compile ( 'sgd ' , loss=keras.losses.LogCosh ( ) ) 'activity_regularizer ' : node_index = layer._inbound_nodes.index ( node ) str ( np.prod ( layer_weights_shape ) ) + ' . ' K.set_value ( state , np.zeros ( get_tuple_shape ( dim ) ) ) 'go_backwards ' : self.go_backwards , if sample_weight is None : For 'channels_first ' data_format , 'expected one of { `` normal '' , `` uniform '' } ' if learning_phase ( ) : K.greater ( denom , 0 ) , input_length = inputs.shape [ 1 ] def test_cropping3d_legacy_interface ( ) : last_batch = index_array [ batch_count * batch_size : ] self._network_nodes = nodes The function arguments use the same convention as y_true : Target output . # ( unless serialization is implemented by assert loss [ 0 ] == np.inf or np.isnan ( loss [ 0 ] ) self.executor_fn = self._get_executor_init ( workers ) return g self._build_input_shape = None self.cell._recurrent_dropout_mask = None [ self.total_loss ] + metrics_tensors , y = np.random.random ( ( num_samples , units ) ) return output , [ output ] xs = [ ] return state_updates raise ValueError ( 'CNTK Backend : Invalid data_format : % s ' % data_format ) return conv_out if probs.shape [ 1 ] == 1 : if 'padding ' in kwargs and isinstance ( kwargs [ 'padding ' ] , dict ) : Only applicable if the layer has one inbound node , if identifier is None : def __init__ ( self , name='mean_absolute_error ' , dtype=None ) : # identical to the previous one batch_size = input_shape [ 0 ] if K.backend ( ) ! = 'cntk ' : reason='cntk does not support it yet ' ) model , input_tensors= [ input_a , input_b ] ) tf_keras_backend.reset_uids ( ) xs = [ [ w if ( skip_top < = w < num_words ) else oov_char for w in x ] arguments : optional dictionary of keyword arguments to be passed if on_train_begin is not None : return C.swapaxes ( x , 0 , 1 ) def top_k_categorical_accuracy ( y_true , y_pred , k=5 ) : return sample_weights , sample_weight_modes if isinstance ( summation_method , metrics_utils.AUCSummationMethod ) : With the keyword argument ` states ` . generic_utils.check_for_unexpected_keys ( 'metrics ' , metrics , output_names ) for j in range ( len ( new_weights ) ) : if not self.built : the display labels for the scalar outputs . uses_learning_phase = ( output._uses_learning_phase or that the layer itself is also trainable ) . init_state = keras.Input ( ( 3 , ) ) that is used to keep track of the number of true positives . `` `` '' , model.compile ( 'sgd ' , loss=keras.losses.MeanSquaredError ( ) ) self.classes_ = np.arange ( y.shape [ 1 ] ) before matrix multiplication ) . False = `` before '' ( default ) , raise ValueError ( 'Layer ' + self.name + ' was called with ' if len ( shape ) == 2 : # dense layer kernel case self.depthwise_kernel , array ( [ [ 1. , 4 . ] , > > > keras.backend.image_data_format ( ) updated_log_p_prev = T.log ( _p_prev ) + common_factor model.compile ( 'sgd ' , metrics= [ keras.metrics.TopKCategoricalAccuracy ( ) ] ) if you do n't pass a ` weights ` argument . compile : Boolean , whether to compile the model param_values = K.batch_get_value ( params ) 'GRU ( reset_after=False ) is not compatible with ' # Network_nodes : set of nodes included in the graph of layers # Check that x is an input tensor . left_pad = dilation_rate * ( kernel_shape [ 0 ] - 1 ) file_like = io.BytesIO ( ) length = conv_utils.conv_output_length ( steps , `` `` '' Called at the beginning of evaluation or validation . return { 'outbound_layer ' : outbound_layer , nodes_by_depth : dict mapping ints ( depth ) to lists of node instances . initializer=self.embeddings_initializer , self._metrics_function = theano.function ( p.terminate ( ) for p in processes if p.is_alive ( ) ] ) tuple ` ( x_val , y_val , val_sample_weights ) ` name='input_wrapper_for_ ' + name ) if loss is None or isinstance ( loss , losses.Loss ) : for input_index , ( x , spec ) in enumerate ( zip ( inputs , input_spec ) ) : class_mode : One of `` categorical '' , `` binary '' , `` sparse '' , `` `` '' Calculates the number of false positives . ( https : //en.wikipedia.org/wiki/Riemann_sum ) : 'interpolation ' [ default ] , raise TypeError ( 'The ` BatchNormalization ` layer ' dtype=layer.dtype , return serialize_keras_object ( initializer ) mean=mean , for w , org_w in zip ( model.get_weights ( ) , org_weights ) : `` `` '' Calls the ` on_predict_batch_begin ` methods of its callbacks . e.g . ` input_shape= ( 128 , 128 , 128 , 3 ) ` for a 128x128x128 volume with 3 channels 'negative number , not nan , so can\'t ' matrix : raise ValueError ( 'Invalid bias shape : ' + str ( bias_shape ) ) class Sequential ( Model ) : self.recurrent_kernel_z , model.load_weights ( fname , by_name=False , reshape=False ) elif len ( data ) == 1 and not hasattr ( data [ 0 ] , 'shape ' ) : class LSTMCell ( Layer ) : old_layer = keras.layers.AvgPool1D ( pool_length=2 , val_samples=2 , original_keras_version , `` `` '' Layer that adds a list of inputs . KC = None rank=2 , else : # curve == 'PR ' . where each entry ` i ` will be the dot product between reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE , raise ValueError ( ' ` num_thresholds ` must be > 0 . ' ) steps=steps , shape = input_shape [ i ] [ 1 : ] slice_row = py_slice ( i * stride_row , assert input_shape and len ( input_shape ) > = 2 from this class during training . False cbks.ProgbarLogger ( count_mode , stateful_metrics=model.metrics_names [ 1 : ] ) ) str ( layer_weights_shape ) + ' and size ' if len ( tensor_shape ) > 2 : t_list += [ t ] the output would have shape ` ( batch_size , units ) ` . self.layer.set_weights ( weights ) # Recover the corresponding layer . self.weights = [ ] regularizer=self.gamma_regularizer , ( 'output ' , 'outputs ' ) ] ) assert dtype in str ( var.dtype.name ) if is_sparse ( x ) : name=name , `` `` '' Called at the end of an epoch . self.update ( self._seen_so_far + n , values ) in the FAQ for instructions on how to install ` h5py ` . for cell_config in config.pop ( 'cells ' ) : return { 'reduction ' : self.reduction , 'name ' : self.name } initial_state=backward_state , * * kwargs ) for batch_out in batch_outs : if data_format='channels_last ' A tensor , result of 3D convolution . ValueError : if ` value ` or the global ` data_format ` invalid . h_tm1_f = h_tm1 outputs [ i ] = v is_match_fn = zipfile.is_zipfile if custom_objects : 'or ( x , y ) . Found : ' assert pool_size [ 0 ] > = 1 and pool_size [ 1 ] > = 1 output_shape = func_dump ( self._output_shape ) from .convolutional import Conv2D ' ` data_format ` must be `` channels_last '' ' ` 0.2 * x + 0.5 ` if ` -2.5 < = x < = 2.5 ` . if factor > = 1.0 : * * kwargs ) : 'batch_input_shape ' , ( i.e . make the layer affine rather than linear ) . arguments=user_kwargs ) reduction=Reduction.SUM , name='scc ' ) layers.GlobalAveragePooling2D ] ] filtered_layer_names.append ( name ) return T.le ( x , y ) if ndim ( bias ) ! = 1 and ndim ( bias ) ! = ndim ( x ) - 1 : x = _padding ( x , p , # note that numpy reference implementation is independent of ` unroll ` argument ( see [ activations ] ( .. /activations.md ) ) . self.weights = [ self.iterations ] + moments decay = tf_math_ops.cast ( decay , x.dtype.base_dtype ) layers_with_complete_input = [ ] # To provide a better error msg . check_single_tensor_operation ( 'all ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) entire 1D feature maps instead of individual elements . If adjacent frames apply_channel_shift = image.apply_channel_shift return tf_keras_backend.get_uid ( prefix ) _BACKEND = _backend super ( TrueNegatives , self ) .__init__ ( if len ( shape ) > 1 : on_train_begin=None , weights=None , return ( parameter.kind in ( inspect.Parameter.POSITIONAL_OR_KEYWORD , training=training , broadcast_beta , broadcast_gamma , print ( '\nEpoch % 05d : saving model to % s ' % ( epoch + 1 , filepath ) ) def ResNet50V2 ( * args , * * kwargs ) : from .load_backend import batch_get_value 'config ' : self.layer.get_config ( ) } } start = tf.cond ( start < 0 , strides=self.strides , val = np.random.random ( ) for each input channel . nodes_in_progress : Set of nodes that are currently active on the that simply divides ` total ` by ` count ` . input_shape [ 4 ] ) def test_batchnorm_tf ( self , x_shape ) : 'GRU ( reset_after=True ) is not compatible with ' check_two_tensor_operation ( 'bias_add ' , x_shape , ( 10 , 6 ) , 'but it received ' + str ( len ( inputs ) ) one per output tensor of the layer ) . K.greater ( self.true_positives + self.false_negatives , 0 ) , string , one of ` 'th ' ` , ` 'tf ' ` specify a fixed batch size for your model , by passing # ` pydot ` is an optional dependency , return isinstance ( layer , Wrapper ) and isinstance ( layer.layer , Model ) return T.maximum ( x , y ) layer : Layer instance . # complicate weight sharing . ( strings ) to custom classes or functions to be i = ( x.shape [ 2 ] + strides [ 0 ] - 1 ) // strides [ 0 ] output_shape = ( input_shape [ 0 ] , input_shape [ 1 ] , self.units ) if weight_names : session : A TF Session . Note : RNN dropout must be shared for all gates , if len ( initial_state ) == 1 : updates += layer.get_updates_for ( inputs ) def test_batchnorm_legacy_interface ( ) : metrics_updates = [ ] input_ndims = list ( map ( K.ndim , inputs ) ) # in succession results in different values . if num_dynamic_axis == 0 : `` `` '' Upsampling layer for 3D inputs . # disable run_options . # model.compile ( optimizer , loss= [ ] ) 'or ` batch_shape ` argument to your Input layer . ' ) test_cases.append ( [ ( None , 4 ) , ( None , 4 ) , None ] ) inputs * = mask _axis = list ( axis ) [ 3 , 4 , 5 ] ] [ 1 , 3 , 5 ] ] batch_hook = getattr ( callback , hook_name ) try : # new API self._function_kwargs = kwargs return metrics_utils.update_confusion_matrix_variables ( { 'Output { 0 } missing from loss dictionary . We assume ' preprocessed_input = self.preprocess_input ( inputs , training=None ) # End : For use by subclasses # # # f = self.recurrent_activation ( z1 ) from_logits=True , reduction=Reduction.NONE ) confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES , else , 4D tensor with shape : [ 0.0 , 0.9 , 0.1 , 0.1 ] , # t=3 y_pred_shape = K.int_shape ( y_pred ) def ctc_decode ( y_pred , input_length , greedy=True , beam_width=100 , return linear if axis ! = axes [ 1 ] : def cumsum ( x , axis=0 ) : config [ 'output_shape ' ] , from_logits : Whether ` output ` is expected to be a logits tensor . or alternatively , Theano function to use for weights del f condition : tensor ( ` int ` or ` bool ` ) . sample_weight : Optional Numpy array of weights for for cell , states in zip ( self.cells , nested_states ) : for layer in self.model.layers : base_config = super ( CuDNNLSTM , self ) .get_config ( ) elif self.data_format == 'channels_first ' : label = [ ' a ' , ' b ' ] return tf.reshape ( tf.matmul ( xt , yt ) , kernels , [ biases ] ) ( Numpy arrays ) . unit_forget_bias=True , modules = [ 'keras.layers ' , 'keras.models ' , 'keras ' , if self.outbound_layer : def random_normal_variable ( shape , mean , scale , dtype=None , # if loss function is None , then this output will be skipped during total elif [ [ `` $ MODE '' == `` API '' ] ] ; then # execution [ [ 0.1 , 0.9 , 0.0 , 0.0 ] , # t=0 datagen = ImageDataGenerator ( return np.repeat ( x , rep , axis=axis ) shape [ i ] , f : Keras function returning a list of tensors . x = tf.transpose ( x , ( 0 , 2 , 3 , 4 , 1 ) ) is the cosine proximity between the two samples . assert len ( layer.cell.get_losses_for ( x ) ) == 0 `` `` '' Converts layers nested in ` Model ` or ` Sequential ` . if total_loss is None : [ 0.0 , 0.9 , 0.1 , 0.0 ] , # t=1 if mask is not None : self.embeddings = self.add_weight ( symbolic_weights [ i ] .shape , return decoded_dense , log_prob self.reset_states ( ) reps = tf.constant ( reps , dtype='int32 ' ) def assert_args_presence ( args , doc , member , name ) : return args , kwargs , converted logs = dict ( [ ( k , logs [ k ] if k in logs else 'NA ' ) for k in self.keys ] ) shuffle=True , # Skip progbar update for the last batch ; # Build train function ( to get weight updates ) . Specifying any stride value ! = 1 is incompatible with specifying 'gradient defined ( i.e . are differentiable ) . ' return ( input_shape [ 0 ] , loss = loss / _num_elements ( weighted_losses ) if condition.dtype ! = tf.bool : for sw , w in zip ( layer.weights , layer_weights ) : if uid in self._input_map : self.bias_initializer ( ( self.filters , ) , * args , * * kwargs ) , for depth , layers in model._layers_by_depth.items ( ) : enqueuer = None axis : ( Optional ) Defaults to -1 . The dimension along which the cosine check_batch_axis=False , # Do n't enforce the batch size . def test_global_maxpooling2d_legacy_interface ( ) : return self.cells [ -1 ] .output_size between feature maps and should be used instead . def DISABLED_test_model_with_input_feed_tensor ( ) : sequential_like = False x = K.random_normal ( shape , self.mean , self.stddev , ndim = len ( shape ) self.rate = min ( 1. , max ( 0. , rate ) ) strides : An integer or tuple/list of a single integer , generator : A generator or an instance of ` Sequence ` name : A string name for the foldr node in the graph as defined by ` steps_per_epoch ` . variance_mean = C.square ( C.minus ( x , shift ) ) a directory to which to save dataset [ ( ) ] = val of the output tensor . load_function , 'filepath ' , args , kwargs ) matrix_inner = K.dot ( h_tm1 , self.recurrent_kernel ) verbose=self.verbose , `` `` '' Called at the beginning of a batch in ` predict ` methods . nesterov=False , * * kwargs ) : unique_name = name + ' _ ' + str ( idx ) legacy_add_weight_support = generate_legacy_interface ( Input kernels for each gate are transposed and converted between Fortran weights = convert_nested_model ( weights ) devs_squared = C.square ( x - m ) self._helper_bilinear ( data_format , 4 , 4 ) 'negative_slope ' : self.negative_slope , `` `` '' Get the Pool initializer for multiprocessing . len ( output_ls ) > 1 ) : one of ` `` channels_last '' ` ( default ) or ` `` channels_first '' ` . assert_allclose ( out , out2 , atol=1e-05 ) dim_ordering='tf ' , arg_spec = inspect.ArgSpec ( `` `` '' Resets all of the metric state variables . return self.cell.trainable_weights return K.softsign ( x ) arg_spec = inspect.getargspec ( fn ) * * kwargs ) hasher.update ( chunk ) if 'KERAS_HOME ' in os.environ : with prediction values to determine the truth value of predictions raise TypeError ( 'Unexpected keyword argument ' for i , layer in enumerate ( layers ) : from theano.tensor.nnet.nnet import softsign as T_softsign normalizer_rs = lambda x : x [ : , : :2 ] old_layer = keras.layers.PReLU ( init='zero ' , name= ' p ' ) old_layer = keras.layers.Convolution3D ( 5 , 3 , 3 , kernel_dim3=4 , name='conv ' ) # Calculate sensitivities at all the thresholds . def eval ( x ) : finished_nodes : Set of nodes whose subgraphs have been traversed losses : loss tensor or list of loss tensors Tensor , the gradient clipped if required . expected_height = ( x.shape [ 3 ] + strides [ 1 ] - 1 ) // strides [ 1 ] if len ( self.thresholds ) == 1 : h5dict : ` keras.utils.hdf5_utils.HFDict ` instance . known , unknown = 1 , None def test_target_tensors ( ) : then the true negatives value is 2 . If the weights were specified as raise ValueError ( 'Unknown dim_ordering : ' , dim_ordering ) data_format=None , return resnet_v2.preprocess_input ( * args , * * kwargs ) 'rnn with non-static axis , please try ' strides=strides , padding=padding , new_states = new_states_temp # Do n't repeat work for shared subgraphs # for logging purposes . np.concatenate ( [ np.identity ( units ) ] * num_kernels , axis=1 ) ) self.recurrent_bias = K.flatten ( self.bias [ 1 ] ) from tensorflow.keras.layers import ActivityRegularization 'Early stopping conditioned on metric ` % s ` ' kwargs [ 'unit_forget_bias ' ] = True loss=self.loss , x = tf.nn.bias_add ( x , bias ) > > > input = K.placeholder ( ( 2 , 3 ) , dtype='float32 ' ) Note that if the recurrent layer is not the first layer if not is_tensor ( x ) : ' ( thus holding past layer metadata ) . ' 'even kernel sizes are not supported with Theano . ' layer = keras.layers.LSTM ( units ) target_size : tuple of integers ` ( height , width ) ` , default : ` ( 256 , 256 ) ` . out_labels=None , init='glorot_uniform ' , 'were accessed without issue : ' * * Alternatively : install Keras from the GitHub source : * * batch_sizes.append ( batch_size ) if ` return_sequences ` return ( input_shape [ 0 ] , rows , cols , self.filters ) # We set NA so that csv parsers do not fail for this last epoch . super ( SpatialDropout3D , self ) .__init__ ( rate , * * kwargs ) # test both HDF5 and dict implementations y._keras_shape = ( None , ) self.inputs [ 0 ] .shape [ 1 : ] , output_shape [ 2 ] , An array-like HDF5 dataset . and returns a new learning rate as output ( float ) . and C layout , recurrent kernels are transposed . For LSTM biases are summed/ return _deserialize_model ( h5dict ) elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING : match=r'Skipping loading . * due to mismatch . * ' ) : with pytest.warns ( UserWarning , assert len ( layer.losses ) == 4 shape = shape_or_val broadcast_var = tf.reshape ( var , target_shape ) if ` return_state ` : a list of tensors . The first tensor is # now model.output_shape == ( None , 10 , 32 ) inputs = self.sequence [ idx ] super ( SparseCategoricalCrossentropy , self ) .__init__ ( go_backwards=go_backwards , return ( input_shape [ 0 ] , length , features ) kld = KLD = kullback_leibler_divergence if wait_time is not None : d = x._keras_shape [ 3 ] + padding [ 2 ] [ 0 ] + padding [ 2 ] [ 1 ] moving_mean_initializer : Initializer for the moving mean . skip_target_weighing_indices = [ ] neg = K.max ( ( 1 . - y_true ) * y_pred , axis=-1 ) config = { 'dims ' : self.dims } ndim = len ( input_shape ) self.recurrent_kernel = self.add_weight ( 'or `` batch_input_shape '' argument to the first ' self.outputs = [ self.layers [ -1 ] .output ] if data_format == 'channels_first ' : # tf require using a special op to multiply IndexedSliced by scalar tensor_indices : a list of integers , for node in layer._inbound_nodes : y = concatenate ( [ ones ( sliced_shape , dtype=x.dtype ) , self.end = self.data.shape [ 0 ] ' ( left_dim2_pad , right_dim2_pad ) , ' # if tuple , convert to list . inner_activation='relu ' , 'We do not have any information on the lost sample . ' , `` `` '' Reduce learning rate when a metric has stopped improving . variables should be initialized x._keras_shape = shape interpolation_order=1 , # note that you only need to specify the input size on the first layer . callback_metrics += [ 'val_ ' + n for n in model.metrics_names ] model.add ( Conv2D ( 64 , ( 3 , 3 ) , optionally include ` val_loss ` _SHARED_SEQUENCES = seqs If you pass ` None ` , no activation is applied layer.stateful ) for layer in self.layers ] ) def run_internal_graph ( self , inputs , masks=None ) : base_config = super ( Wrapper , self ) .get_config ( ) _SEQUENCE_COUNTER = 0 axes = [ self.axes % K.ndim ( x1 ) , self.axes % K.ndim ( x2 ) ] Modeling all of TP ( true positive ) , FP ( false positive ) and their sum result = [ ] update_freq : ` 'batch ' ` or ` 'epoch ' ` or integer . When using ` 'batch ' ` , writes coefficients ( Python floats ) to weight the loss contributions # here we order them by traversal order . TensorFlow does not support NCHW on CPU . categorical_accuracy , name , dtype=dtype ) batch_size=None , 'Discrepancy between trainable weights and collected trainable ' from mock import patch , Mock , MagicMock recurrent_h = K.dot ( h_tm1_h , self.recurrent_kernel_h ) import tensorflow as tf class TerminateOnNaN ( Callback ) : `` `` '' Part of the training engine related to plain array data ( e.g . Numpy ) . model = model_cls ( weights=None , padding='valid ' , x : What to return in test phase `` `` '' Computes the crossentropy loss between the labels and predictions . __y.append ( signal.convolve ( x [ i , j ] , w [ j , k ] , mode=padding ) ) array ( [ [ 0.10940075 , 0.10047495 , 0.476143 ] , for x in reference_input_tensors : layer_name = ' { } ( { } ) '.format ( layer_name , layer.layer.name ) arg_spec = inspect.getfullargspec ( fn ) log_p_curr [ : active_next ] + updated_log_p_prev # We need to use ` x ` to set the model inputs . tf_file_io.delete_file ( filename ) ` ( samples , filters , new_rows , new_cols ) ` if data_format='channels_first ' ` ( symmetric_height_pad , symmetric_width_pad ) ` . the weight tensor has shape def test_resize_volumes ( self ) : 'Dilated convolution on CPU is not supported by CNTK backend . ' `` `` '' Pad the 2nd and 3rd dimensions of a 4D tensor # Corner case where the user passes an InputLayer via ` add ` . if len ( batch_sizes ) == 1 : if key in kwargs : padding=cell.padding , y = np.random.randint ( 0 , 2 , size= [ 2 , 1 ] ) layer_name = layer.name raise TypeError ( 'Could not interpret loss_weights argument : ' follow_links=False , name = str ( w.name ) + ' _ ' + str ( i ) num_thresholds=200 , elif isinstance ( axes , tuple ) : def cos ( x ) : 'axis ' : self.axis } image_data_format ( ) ) inputs = node.input_tensors raise ValueError ( ' ` validation_freq ` must be an Integer or ' def __init__ ( self , function , output_shape=None , target = T.cast ( T.flatten ( target ) , 'int32 ' ) ( input_depth , depth_multiplier ) ) return self.cell.units op , input_shape , kernel_shape , WITH_NP , member.__module__ ) Output : ( n + 1 ) D one hot representation of the input of corresponding labels . If 'sample_weight ' is not None , self.n = n removing layers is fine as long as they do n't have weights . C.user_function ( ConvertToStatic ( l_s , batch_size=i_s.shape [ 0 ] ) ) ) y = K.spatial_2d_padding ( x , padding=padding , data_format='channels_last ' ) [ initializers.orthogonal , def interpolate_pr_auc ( self ) : gamma = tf.constant ( 1.0 , h = None return [ x for x in _LOCAL_DEVICES if 'device : gpu ' in x.lower ( ) ] model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.FalseNegatives ( ) ] ) self.skip_target_indices = [ ] if len ( set_x ) > 1 : 'left_pad ' , 'right_pad ' } : padding= ( 1 , 1 ) , thresh_tiled = K.tile ( batch_shape=batch_shape , return then_expression 'which is not yet supported on the CNTK backend . ' ) # reduce score_array to same ndim as weight array model = pickle.loads ( pickle.dumps ( model ) ) def DISABLED_test_imdb_load_does_not_affect_global_rng ( fake_downloaded_imdb_path ) : layer = RNN ( cell ) assert tuple ( s.shape ) == ( None , 64 + 5 ) class Subtract ( _Merge ) : input_shape= ( timesteps , input_size ) , ( ordered names of model layers ) . original_backend : Keras backend the weights were trained with , Example subclass implementation : `` `` '' CIFAR10 small images classification dataset . '' '' '' from .. utils.layer_utils import print_summary as print_layer_summary `` `` '' Training-related part of the Keras engine . '' '' '' # tensor inputs and outputs of outbound_layer . super ( SparseTopKCategoricalAccuracy , self ) .__init__ ( start= [ 1 , 0 , 0 , 0 ] , size=size ) 2. invalid bias shape . def exponential ( x ) : samples = 2 'The tensor that caused the issue was : ' List of callbacks to apply during training and validation # Functions for train , test and predict will every Keras model . The ` History ` object super ( Metric , self ) .__init__ ( name=name , dtype=dtype , * * kwargs ) if bucket_name.startswith ( self._gcs_prefix ) : name = sub_n_last_node [ inbound_layer.name ] .get_name ( ) `` `` '' Checks that we can have code blocks in multiple sections . '' '' '' callbacks=callbacks , original version of Adadelta you do n't have to set an initial learning self.states = [ K.zeros ( ( batch_size , self.cell.state_size ) ) ] dtype : Expected datatype of the input . def test_regularizer ( ) : skip_idxs = T.arange ( ( Y.shape [ 0 ] - 3 ) // 2 ) * 2 + 1 # Network.layers needs to have a deterministic order : beta = zeros_like ( mean ) # score_array has ndim > = 2 return T.argmin ( x , axis=axis , keepdims=False ) Alpha Dropout is a ` Dropout ` that keeps mean and variance of inputs input_prob_tensor = K.placeholder ( shape= ( None , None , None ) , ' input tensors . Input received : ' ( the depth ) is at index 1 , dilation=cell.dilation_rate [ 0 ] ) which are adapted relative to how frequently a parameter gets self.target_shape = ( batch_size , ) + input.shape def pickle_model ( model ) : `` `` '' Casts a tensor to a different dtype and returns it . file_hash='bfafd718b763782e994055a2d397834f ' ) text_to_word_sequence = text.text_to_word_sequence will return an array of shape ` ( n_samples , 2 ) ` self._total_width += len ( info ) `` `` '' Constrains the weights incident to each hidden unit to have unit norm . self.bias_z_i = self.bias [ : self.units ] def built ( self , value ) : # Iterate over nodes , by depth level . set_inputs = True self.forward_layer.name = 'forward_ ' + self.forward_layer.name grad_array_view = root_gradients.data ( ) # ` sample_weight_mode ` does not match output_names . if len ( shape_set ) > 1 : 1 ] - self.true_positives [ 1 : ] self.input_spec [ 0 ] = InputSpec ( shape= ( batch_size , None , input_dim ) ) For example , if values is [ 1 , 3 , 5 , 7 ] then the mean is 4 . The created weight variable . rnn_inputs = reverse ( rnn_inputs , 1 ) y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) def clip ( x , min_value , max_value ) : data_format : Image data format . as well as pickling . This is achieved via a ` sample_weight ` and the metric is then calculated from it . If only one integer is specified , the same window length the cropping dimension ( axis 1 ) . check_single_tensor_operation ( 'std ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) 'Alternatively , you can use the loss function ' if 'forget_bias_init ' in kwargs : return ( input_shape [ 0 ] , features , length ) mask = K.cast ( mask , y_pred.dtype ) running statistics over callback execution time . return unpack_singleton ( outputs ) `` `` '' Instantiate a layer from a config dictionary . assert np.abs ( np.mean ( rand ) - p ) < 0.015 MAJORING = 'majoring ' the top k predictions . def test_conv_transpose ( self , ValueError : If loss is a dict with keys not in model output names , shape=shape , mean=mean , with get_graph ( ) .as_default ( ) : # VALUE MANIPULATION `` `` '' Instantiates a variable with values drawn from a normal distribution . ' class . ' ) ) `` distribution '' arguments . return K.update_add ( var , K.sum ( label_and_pred , 1 ) ) py_slice ( padding [ 2 ] [ 0 ] , input_shape [ 3 ] + padding [ 2 ] [ 0 ] ) , # CuDNNLSTM has ( units * 8 ) weights ; while LSTM has ( units * 4 ) return K.sum ( y_true * y_pred , axis=axis ) node_index : Origin node index of the tensor . callbacks._call_end_hook ( 'test ' ) self._losses += losses initial_states [ 0 ] = T.unbroadcast ( initial_states [ 0 ] , 0 , 1 ) return mobilenet.MobileNet ( * args , * * kwargs ) callbacks.on_epoch_begin ( epoch ) assert tuple ( q.shape ) == ( None , 64 ) h_tm1_i = h_tm1 * rec_dp_mask [ 0 ] x : Numpy array . step_function_np=get_step_function ( KNP , wi , wh ) , result.append ( np.tensordot ( xi , yi , axes ) ) 'call ` multi_gpu_model ` with ` len ( gpus ) > = 2 ` . ' input_tensors=input_tensors , A tensor , the concatenation of the inputs alongside axis ` axis ` . printable_module_name='metric function ' ) a.k.a . an attention mechanism . x = tf.clip_by_value ( x , zero , inf ) trainable_weights = self._trainable_weights [ : ] output_masks = layer.compute_mask ( computed_tensors , name='global_avgpool2d ' ) pool_size= ( 2 , 2 , 2 ) , padding='valid ' , data_format='channels_last ' , x_sparse = sparse.csr_matrix ( ( x_d , ( x_r , x_c ) ) , shape= ( 4 , 5 ) ) 'Unexpected channels axis { } . '.format ( axis ) , If set to ` None ` ( default ) , the output shape is inferred . fpath : path to the file being validated recurrent_constraint=recurrent_constraint , raise ValueError ( 'When passing validation_data , ' # The `` depth '' of a node is the max of the depths raise ValueError ( 'Can not perform batch dot on inputs ' name='avgpooling3d ' ) `` `` '' Sets entries in ` x ` to zero at random , while scaling the entire tensor . info += ' % .0fms/step ' % ( time_per_unit * 1e3 ) return np.ones_like ( x , dtype=dtype ) output_mask = self.compute_mask ( inputs , previous_mask ) if input_shape [ 0 ] : cond_shape = tf.concat ( [ tf.shape ( condition ) , [ 1 ] * ndim_diff ] , axis=0 ) check_single_tensor_operation ( 'l2_normalize ' , ( 4 , 3 ) , WITH_NP , axis=-1 ) assert len ( layer.get_losses_for ( None ) ) == 4 true negatives . This metric creates one local variable , ` accumulator ` # not updated last timestep : with open ( self.filename , ' r ' + self.file_flags ) as f : 'inputs with matching shapes ' raise TypeError ( 'Invalid type for ` axes ` - ' ` ( batch_size , channels , pooled_dim1 , pooled_dim2 , pooled_dim3 ) ` depthwise_kernel = _preprocess_conv2d_depthwise_kernel ( # Do not slice the training phase flag . `` `` '' Wrapper function . invalid_thresholds = [ t for t in thresholds if t is None or t < 0 or t > 1 ] # Process nodes in order if i > = num_dynamic_axis and shape [ i ] is not None : batch_input_shape = tuple ( kwargs [ 'batch_input_shape ' ] ) summation_method , for input_i , mask_i in zip ( inputs , mask ) : if ( algorithm == 'sha256 ' ) or ( algorithm == 'auto ' and len ( hash ) == 64 ) : ( e.g . TensorFlow data tensors ) . def deconv2d_args_preprocessor ( args , kwargs ) : 'Build the model first by calling build ( ) ' `` `` '' Computes output tensors for new inputs . ( ordered names of weights tensor of the layer ) . return tf.gradients ( loss , variables ) # Prepare broadcasting shape . ` Sensitivity ` measures the proportion of actual positives that are correctly # It 's an input layer : compute_output_shape is identity , unique_name = name + '_1 ' d = cPickle.load ( f , encoding='bytes ' ) from keras.utils.io_utils import load_from_binary_h5py for output_shape , loss_fn in zip ( self._feed_output_shapes , We update the _keras_shape of every input tensor with losses thresholds : ( Optional ) Defaults to 0.5 . A float value or a python 'of your input tensors : \n ' raise ValueError ( 'CNTK backend : creating placeholder with ' for more details . self.backup = _GLOBAL_CUSTOM_OBJECTS.copy ( ) ( unless ` from_logits ` is True , in which def input_conv ( self , x , w , b=None , padding='valid ' ) : super ( DirectoryIterator , self ) .__init__ ( layer = layer_class ( units , if not hasattr ( module , func ) : layer.kernel_h , for chunk in iter ( lambda : fpath_file.read ( chunk_size ) , b '' ) : depthwise_kernel_shape = self.kernel_size + depthwise_kernel_shape if len ( all_outs ) == 1 : If ` sample_weight ` is ` None ` , weights default to 1 . Use ` sample_weight ` of 0 ` ( batch_size , channels , spatial_dim1 , spatial_dim2 , spatial_dim3 ) ` placeholders when calling ` fit ` /etc . denom = ( self.true_positives + self.false_negatives ) @ pytest.mark.skipif ( not supports_sparse , for dim in shape [ : -1 ] : conv_layers = [ 'Conv1D ' , self._cudnn_lstm = cudnn_rnn_ops.CudnnLSTM ( def on_predict_end ( self , logs=None ) : for l , o in zip ( out_labels , val_outs ) : inbound_layer = created_layers [ inbound_layer_name ] class RootMeanSquaredError ( Mean ) : This is the crossentropy metric class to be used when there are only two except IOError : label_and_pred = K.all ( are_different , axis=0 ) # Update network_nodes . kwargs = self.filter_sk_params ( Sequential.predict , kwargs ) 'Received type : ' # Note : you have to use shuffle='batch ' or False with HDF5Matrix the axes to find minimum values . If ` None ` ( default ) , finds the concatenated = K.concatenate ( masks , axis=self.axis ) if callbacks.model.stop_training : ROC = 'ROC ' if type ( self.model.input ) == list : divides ` total ` by ` count ` . `` `` '' Private base class for CuDNNGRU and CuDNNLSTM . C.variables.Parameter ) : 'init ' : initializers.serialize ( self.init ) , if len ( batch_sizes ) > 1 : def test_batch_dot_shape ( self ) : test_split : fraction of the data to reserve as test set . It will be autogenerated if it is n't provided . kernel_constraint : Constraint function applied to legacy_convlstm2d_support = generate_legacy_interface ( Model config with Keras version information added . n_samples = embeddings_data [ 0 ] .shape [ 0 ] # No network-level masking for now . _ = Sequential ( [ layer ] ) return 'float64 ' base_config = super ( AlphaDropout , self ) .get_config ( ) # Check specific shape axes . if isinstance ( mask , list ) : return k pool_out = pool_out.dimshuffle ( ( 0 , 2 , 3 , 1 ) ) self._expects_training_arg = False class ReduceLROnPlateau ( Callback ) : `` `` '' 3D Pooling . seed=None ) : def test_load_from_binary_h5py_from_bytes_io ( ) : self.b = self.add_weight ( shape= ( input_dim , ) , if self.scale : yield None class SparseCategoricalCrossentropy ( MeanMetricWrapper ) : def test_model_with_crossentropy_losses_channels_first ( ) : _raise_invalid_arg ( key ) axis = list ( pattern ) name=None ) : 'epochs ' : epochs , # TODO : consider supporting it . self.moving_variance = self.add_weight ( def l1_l2 ( l1=0.01 , l2=0.01 ) : start_char=1 , oov_char=2 , index_from=3 , * * kwargs ) : If all features for a given sample timestep are equal to ` mask_value ` , # shape : batch , row , col , filters def get_updates_for ( self , inputs ) : min_val_k = K.variable ( min_val ) if getattr ( self.cell , 'output_size ' , None ) is not None : str ( len ( output_names ) ) + ' outputs . ' return Maximum ( * * kwargs ) ( inputs ) ' a list of tensors , or dict of tensors , but got : ' , from .load_backend import placeholder # us to add pathlib2 to the Python 2 dependencies . volume_shape [ 1 ] , volume_shape [ 2 ] , volume_shape [ 3 ] ) if inspect.isclass ( mem ) : from .load_backend import local_conv2d if index < 0 or index > 1 : # the opposite of the negative part from __future__ import absolute_import ` batch_shape= ( None , 32 ) ` indicates batches of an arbitrary number final_output = _reshape_sequence ( final_output , num_time_step ) nested_metrics = [ ] ValueError width and height of the 2D convolution window . self.initial_decay = kwargs.pop ( 'decay ' , 0.0 ) output_names : List of output names ( strings ) in the model . self.run_thread = None return tf_keras_backend.categorical_crossentropy ( `` `` '' Upsampling layer for 1D inputs . np.asarray ( self.target_shape ) ) ' is incompatible with layer ' callbacks._call_batch_hook ( 'predict ' , 'end ' , steps_done , batch_logs ) 'Expected to be -1 or one of the axes of ` output ` , ' , self._base_shape = first_val.shape [ 1 : ] * * kwargs ) self._layers.append ( layer ) class MeanSquaredError ( LossFunctionWrapper ) : class Concatenate ( _Merge ) : _LEARNING_PHASE_CACHE = { } allowed_positional_args= [ 'alpha_initializer ' ] , `` `` '' Legacy getter for ` image_data_format ` . data = np.random.random ( ( 3 , 5 ) ) x_ndim = len ( x_shape ) from tensorflow.keras.utils import GeneratorEnqueuer save_prefix= '' , from keras.layers import Bidirectional , GRU , LSTM , CuDNNGRU , CuDNNLSTM archive_format = [ archive_format ] elif isinstance ( x , list ) : return np.any ( targets == top_k , axis=-1 ) for path in paths : return T.var ( x , axis=axis , keepdims=keepdims ) # Broadcast weights if possible . use_multiprocessing=use_multiprocessing , yield ( { 'input_1 ' : x1 , 'input_2 ' : x2 } , { 'output ' : y } ) return open ( filepath , mode ) def test_function_tf_fetches ( self ) : dictionary or a list of modes . distribution='uniform ' , if isinstance ( config [ 'arguments ' ] [ key ] , dict ) : loss = mape ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) class Poisson ( MeanMetricWrapper ) : elif len ( names ) == 1 : new_states_temp = [ ] break # new : ( ... , stack_size , filters ) tensor_map [ str ( id ( x ) ) ] = ( y , mask ) model_type ) : out = C.swapaxes ( out , -1 , axis ) def predict_classes ( self , x , batch_size=32 , verbose=0 ) : # 3 . Callables return themselves for some reason return training_generator.predict_generator ( return tf.sin ( x ) callbacks._call_begin_hook ( 'train ' ) name=_prepare_name ( name , 'variable ' ) , output = K.get_value ( variable ) 'as integer positional arguments . For safety reasons , ' self.bias_i , [ C.InferredDimension if _ is None else _ for _ in new_shape ] ) inputs * = dp_mask [ 0 ] use_cudnn = ( ndim ( x ) < 5 and self.best = 0 self.bucket_name = bucket_name K.cast ( y_true , dtype='bool ' ) , [ 1 , -1 ] ) from .cudnn_recurrent import CuDNNLSTM ' y.shape [ % d ] ( % d ! = % d ) . ' % ( axes [ 0 ] , axes [ 1 ] , d1 , d2 ) ) cell_losses = cell.get_losses_for ( inputs ) args , kwargs , converted = preprocessor ( args , kwargs ) return C.cast ( C.zeros_like ( x , name ) , dtype ) sensitivity at the given specificity . The threshold for the given specificity are passed into ` K.function ` . cropping : int , or tuple of 2 ints , or tuple of 2 tuples of 2 ints . if output_shapes is not None : weighted_metrics=weighted_metrics , so that each filter only has one set of parameters , def _node_key ( layer , node_index ) : List of callbacks to apply during evaluation . All recurrent layers ( ` LSTM ` , ` GRU ` , ` SimpleRNN ` ) also kwargs [ 'padding ' ] = ( ( top_pad , bottom_pad ) , ( left_pad , right_pad ) ) seq_len_1 = 5 Use the keyword argument ` input_shape ` `` `` '' Base class for recurrent layers . return K.softplus ( x ) assert ( out1.shape == ( num_samples , units ) ) wi_k = K.variable ( wi ) from .. utils.generic_utils import deserialize_keras_object # its shape must be the same as mask 's return metric_name # dense.input_mask y = C.transpose ( y , perm=permutation ) handle_class_init ( name , member ) KTH = None loss=None , # for creating scopes . We prefix the name with `` private '' in this case . return self.mask ( inputs , mask ) return x / ( 1 + C.abs ( x ) ) str ( shape1 ) + ' ' + str ( shape2 ) ) def get_uid ( prefix= '' ) : shape : A tuple of integers , the shape of tensor to create . if self.restore_best_weights : return self.trainable_weights + self.non_trainable_weights masks=masks ) even after this dropout . if not isinstance ( axes [ 0 ] , int ) or not isinstance ( axes [ 1 ] , int ) : if input_dim is None : or 4D tensor with shape : y = K.variable ( 0 . ) min_value : Python float , integer or tensor . timesteps = K.int_shape ( inputs ) [ 1 ] updated_per_output_metrics.append ( self._per_output_metrics [ i ] ) # after the first layer , you do n't need to specify xi = x [ i ] self.model = self.build_fn ( 'data_format ' : self.data_format , shape [ -1 ] = self.cell.filters assert np.isclose ( normalized_X_train [ 0 ] [ 0 ] , X_train [ 0 ] [ 0 ] + 1 ) name : Name of the layer ( string ) . # Configuration of py.test def _init_graph_network ( self , inputs , outputs , name=None , * * kwargs ) : second positional argument should indicate the location to save to . if not layer._inbound_nodes : or ` batch_input_shape ` as well as ` dtype ` ) . t = getattr ( k , function_name ) ( if ndim : * ` x.shape [ 0 ] ` : 100 : append to output shape dilation_rate = ( dilation_rate , ) * ( x.ndim - 2 ) first input Numpy array . When ` steps ` is not ` None ` and ` ( batch , channels , steps ) ` . to_file='model.png ' , spatial_axes= ( 1 , 2 ) ) name : An optional name string for the layer . raise ValueError ( msg ) export PATH= '' $ HOME/miniconda/bin : $ PATH '' raise ValueError ( ' ` validation_data ` should be a tuple ' # be changed at any point by the user if isinstance ( attr , bytes ) : tf_keras_backend.set_floatx ( floatx ( ) ) h5file_ [ 'data1 ' ] = data1 if needs_broadcasting : y_pred , y_true , sample_weight = ( `` `` '' Nesterov Adam optimizer . if dilation_rate ! = ( 1 , ) : outputs = T.stack ( * successive_outputs ) from tensorflow.keras.layers import Cropping2D condition = tf.cast ( condition , 'bool ' ) entire 3D feature maps instead of individual elements . If adjacent voxels elif s1 is None : use_multiprocessing : If ` True ` , use process based threading . input_tensors : list of input tensors . if val_enqueuer is not None : # W503 line break occurred before a binary operator self.gamma_regularizer = regularizers.get ( gamma_regularizer ) ( as long as they support masking ) . # Optional keyword arguments to layer 's ` call ` . from .local import LocallyConnected1D return _regular_normalize_batch_in_training ( x , gamma , beta , from tensorflow.keras.constraints import UnitNorm from tensorflow.keras.layers import average 'th ' : 'channels_first ' , def conv3d ( x , kernel , strides= ( 1 , 1 , 1 ) , padding='valid ' , if hasattr ( value , 'tocoo ' ) : warnings.warn ( recurrent_dropout=0.1 , dpi : dot DPI . input_size = 1000 merge_mode : Mode by which outputs of the mode ( False ) , training mode ( True ) , or using the Keras self.seed += 1 max_ndim : Integer , maximum rank of the input . batch_size : size of batch of inputs to feed to the network unzip cats_and_dogs_small.zip def simple_no_states ( inputs , states ) : return int ( np.ceil ( len ( self.x ) / float ( self.batch_size ) ) ) result = x if training == 1 or training is True else alt return [ x ] if target_mean is not None : If ` output_padding ` is specified : : # Terminate some processes after having finished model training . for weight shape computations . computed for the samples in one batch will be reused as initial states 'or use the ` keras.utils.Sequence ` class . ' ) return self.layer.updates CONV_SHAPE = ( 25 , 25 , 20 , 20 ) shape_set = set ( ) return super ( LSTM , self ) .call ( inputs , match TensorFlow 's default . import theano weights [ 3 ] , reduction=Reduction.SUM , name='mae_1 ' ) elif isinstance ( path , dict ) : # On-the-fly compilation of the model . return VarianceScaling ( scale=2. , input_shape = ( 3 , ) 'GRU ( reset_after=True ) ' ) [ Highway Networks ] ( http : //arxiv.org/abs/1505.00387v2 ) target = C.one_hot ( target , output.shape [ axis_without_batch ] , state_shape = state_shape [ :1 ] + state_shape [ 2 : ] use_sequence_api = True average of the numbers of input and output units , if mode = `` fan_avg '' targets = targets.reshape ( -1 , 1 ) def DISABLED_test_model_custom_target_tensors ( ) : self.bias = self.add_weight ( shape= ( self.units * 8 , ) , shape1 = list ( input_shape [ 0 ] ) # self.rank is 1 for UpSampling1D , 2 for UpSampling2D . dilation_rate = dilation_rate + ( 1 , ) target_mean=target_mean , target_max=1 . ) return _postprocess_conv2d_output ( x , data_format ) K.pool3d ( x , pool_size=pool_size , data_format='channels_middle ' ) return K.dropout ( inputs , self.rate , noise_shape , class AUCSummationMethod ( Enum ) : towards a value inside the desired interval . dim = 2 if hasattr ( tensor , '_keras_history ' ) ! = is_keras_tensor : def predict_proba ( self , x , * * kwargs ) : num_weights_per_layer = len ( weights ) // 2 from .load_backend import random_normal_variable max_queue_size : Integer . Used for generator or ` keras.utils.Sequence ` base_config = super ( _ConfusionMatrixConditionCount , self ) .get_config ( ) shape : Optional dimensions of resulting tensor . https : //arxiv.org/abs/1412.6980v8 ) data1 = np.random.random ( ( 3 , 5 ) ) return float ( version ) raise ValueError ( 'CNTK Backend : Invalid dropout level % s , ' 'image_data_format ' : image_data_format ( ) ( 0.0 , None , 0.0 ) , # standard relu for i in range ( len ( layers ) ) : training=training ) model_weights_group [ 'layer_names ' ] = [ layer.name.encode ( 'utf8 ' ) if expand_nested and isinstance ( layer.layer , Model ) : if max_value is None : raise ValueError ( ' A merge layer should be called ' 'to be a ` Sequential ` model instance , ' layer = layer_module.deserialize ( conf , # Wrap TF optimizer instances if set ( args [ 1 ] .keys ( ) ) < = { 'top_pad ' , 'bottom_pad ' , val_outs = to_list ( val_outs ) for start , end in self.cropping : json_logging_callback , self.dilation_rate [ 1 ] ) # Deserialize loss configuration , if needed . if len ( unrelated_updates ) > 0 : The saved model contains : metric_obj._call_result = result_t `` `` '' Abstract base class for recurrent layers . batch_size : Integer , batch size . batch_size=None , model.fit ( x , y , epochs=epochs ) n_gates = 3 broadcast_moving_variance , self._open_args = { } to_display = [ 'Layer ( type ) ' , if kwargs [ 'forget_bias_init ' ] == 'one ' : `` `` '' Returns the shape of a variable . def serialize ( optimizer ) : parallel_model.fit ( x , y , epochs=epochs ) input_shape=None , If there are multiple outputs for which the metrics are calculated , the from .recurrent import _standardize_args 'Please install ` pydot ` . ' self.input_bias_z = None args = list ( inspect.signature ( member ) .parameters.keys ( ) ) name : A name for the operation ( optional ) . c_tm1 = states [ 1 ] # previous carry state initializers.serialize ( self.pointwise_initializer ) ) self._input_coordinates.append ( ( layer , node_index , tensor_index ) ) apply_brightness_shift = image.apply_brightness_shift assert len ( states ) == 1 The batch input shape of the layer is then ` ( 32 , 10 , 16 ) ` , metrics_utils.ConfusionMatrix.TRUE_NEGATIVES : self.true_negatives , state = model.predict ( inputs ) The ` index_array ` array , shuffled in a batch-wise fashion . raise ValueError ( 'If a RNN is stateful , it needs to know ' if isinstance ( axis , list ) : x , y = generator_output if self.data_format == 'channels_first ' : w = np.flip ( np.fliplr ( np.flipud ( w ) ) , axis=2 ) self._input_dtypes = [ K.dtype ( x ) for x in inputs ] model.load_weights ( fname , by_name=True , skip_mismatch=True ) A tensor with miminum values of ` x ` . assert np.array_equal ( y , return any_matrix initializer : ( optional ) Tensor , the initial value for the accumulator . Wrapping result in identity so that control dependency between return nasnet.decode_predictions ( * args , * * kwargs ) raise ValueError ( 'The name `` ' + name + ' '' is used ' default is to calculate recall with ` thresholds=0.5 ` . if negative_slope < 0. : name='maxpool1d ' ) ( 'pool2d ' , ( 2 , 3 , 7 , 7 ) , ( 3 , 3 ) , ( 1 , 1 ) , # Perform the call if len ( self.outputs ) > 1 : > > > K.size ( inputs ) def sign ( x ) : dims = len ( int_shape ( x ) ) top_k = np.argsort ( -predictions ) [ : , : k ] raise ValueError ( 'InputLayer was provided ' inference on CPU . Thus it has separate biases for ` kernel ` and set_inputs = True return tf_keras_backend.floatx ( ) if weights_shape [ i ] ! = 1 : from .. import metrics as metrics_module check_two_tensor_operation ( 'categorical_crossentropy ' , label , ( 4 , 2 ) , data = np.array ( [ [ i ] for i in range ( 50 ) ] ) 'an input that isn\'t a symbolic tensor . ' converted.append ( ( 'output_shape ' , None ) ) kernel_shape = self.kernel_size + ( input_dim , self.filters * 4 ) class InputSpec ( object ) : sys.stdout.write ( info ) This helps prevent users from using loss functions incorrectly . This check first time the layer was called . from distutils.version import StrictVersion # we stash extra items and reappend them after shuffling def test_in_top_k ( self ) : return instance.__name__ mapping output names to Numpy arrays . return T.nnet.elu ( x , alpha ) warnings.warn ( 'Warning : could not reach RemoteMonitor ' output_dim : int > 0 . 'decay ' : float ( K.get_value ( self.decay ) ) , input_dim = 8 `` `` '' Saves attributes ( data ) of the specified name into the HDF5 group . str ( len ( shape ) ) + ' dimensions , but got array ' model.compile ( 'sgd ' , loss=keras.losses.MeanAbsolutePercentageError ( ) ) def test_none_shape_operations ( self ) : ' ` steps_per_epoch ` is the number of batches ' momentum_cache_t_1 * m_t_prime ) return C.assign ( x , new_x ) with h5py.File ( filepath , mode= ' r ' ) as f : high : Float , upper boundary of the output interval . ` ( batch , features , steps ) ` . model.compile ( 'sgd ' , metrics= [ keras.metrics.MeanAbsolutePercentageError ( ) ] ) return output ( the generator will only yield batches of image data , ValueError : In case the generator yields accumulated value calculated from the preceding invocation of ` fn ` . `` `` '' Types of metrics reduction . callbacks=callbacks , Sequence Modeling ] ( https : //arxiv.org/abs/1412.3555v1 ) d1 = x.shape [ axes [ 0 ] ] ' . They will not be included ' A ` Loss ` instance . ( 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='maxpool2d ' ) h_axis , w_axis = 1 , 2 self.input_spec = InputSpec ( ndim=3 , axes= { 2 : input_dim } ) First , clone Keras using ` git ` : or 3D tensor with shape : if not hasattr ( cell , 'call ' ) : assert ask_to_proceed_with_overwrite ( '/tmp/not_exists ' ) self.build ( ) # list < bytes > # cntk only running with float , bias_regularizer='l2 ' , axes = [ x.ndim - 1 , y.ndim - 2 ] For example , if ` values ` is [ 1 , 3 , 5 , 7 ] and reduction=SUM_OVER_BATCH_SIZE , return ( input_shape [ 0 ] , rows , cols , input_shape [ 3 ] ) # Layer instance ( NOT a list ) . ` batch_size ` or ` epochs ` as well as the model parameters . The default is -1 which indicates the last dimension . new_layer = keras.layers.Conv2DTranspose ( 5 , ( 3 , 3 ) , name='deconv ' ) ' ` batch_shape ` argument to your Input layer . ' ) # inputs projected by all gate matrices at once is placed last . result = x - decrement implementation=implementation , ` loss = y_true * log ( y_true / y_pred ) ` y_shape.append ( i ) ( 'U_regularizer ' , 'recurrent_regularizer ' ) , if not names : for i in range ( len ( states ) ) : def test_tile ( self ) : base_config = super ( Highway , self ) .get_config ( ) defaults=full_arg_spec.defaults ) sparse_categorical_crossentropy , if padding == 'causal ' : instead of creating a placeholder . raise RuntimeError ( 'tf_file_io_proxy is not started ' ) # Let 's use this cell in a RNN layer : for v , nv in updates : model.reset_states ( ) model_cls = keras.applications.Xception class MSE_MAE_loss ( losses.Loss ) : This method is the reverse of ` get_config ` , h_tm1 = states [ 0 ] # previous memory state 'as part of ` input_tensors ` . ' ) name='sparse_categorical_crossentropy ' ) : enqueuer.start ( ) def _postprocess_conv2d_output ( conv_out , x , 'You can either set ` dropout ` and ` recurrent_dropout ` to 0 , ' new_config = json.dumps ( new_layer.get_config ( ) ) for i in range ( len ( model.outputs ) ) : ' initial states . ' ) except ImportError : _TRAIN = 'train ' kernel_shape , u_list.append ( g ) str ( ndim_cond ) + ' , ndim ( then_expression ) ' if len ( output_masks ) ! = len ( output_tensors ) : summary_str = result [ 0 ] # constructor but we can modify the values in the dictionary . Through from .noise import GaussianNoise [ # blank , A , B _ , pointwise = parse_shape_or_val ( ( 1 , ) * len ( kernel_shape ) from .load_backend import tanh def _get_available_devices ( ) : .. # Theano has a built-in optimization for logsumexp class TimeseriesGenerator ( sequence.TimeseriesGenerator , utils.Sequence ) : A list of ` InputSpec ` instances ( one per input to the model ) ` ( batch , depth , rows , cols , channels ) ` batch_size = x_shape [ 0 ] ' containing { } layers into a model with { } layers ' multiprocessing , you should not pass non-picklable arguments to return self._base_dtype [ output_a_np , output_b_np [ :2 ] ] , def bias_initializer ( self ) : available_devices = [ keras.utils.multi_gpu_utils._normalize_device_name ( name ) elif isinstance ( loss , six.string_types ) : dimension 1 of ` x ` has been summed over . ( ` dot_axes [ 0 ] ` = 1 ) timesteps = 60 The task of an Enqueuer is to use parallelism to speed up preprocessing . config = { 'size ' : self.size , z_shape = K.int_shape ( z ) verbose=0 ) : True provide labels as integers , please use ` SparseCategoricalCrossentropy ` loss . def floatx ( ) : metrics.extend ( layer.metrics ) axes= { channel_axis : input_dim } ) * including the batch size * . self.name `` `` '' Long Short-Term Memory layer - Hochreiter 1997 . return _old_normalize_batch_in_training ( from . import convolutional return ( input_shape [ 0 ] , input_shape [ 4 ] ) 'op , input_shape , pool_size , strides , padding , data_format , pool_mode ' , [ [ 0 , 0 ] return vgg16.decode_predictions ( * args , * * kwargs ) weights = self._non_trainable_weights [ : ] reference_input_tensors = node.input_tensors A variable instance ( with Keras metadata included ) . return tf_keras_backend.random_uniform ( output_shape [ w_axis ] = conv_utils.deconv_length ( output_shape [ w_axis ] , cls = custom_objects [ class_name ] # Theano expects ` ( depth , input_depth , rows , cols ) ` . optimizer=optimizers.RMSprop ( lr=0.0001 ) , # Following 2 properties : input and output masks . cls_name = self.__class__.__name__ kernel_regularizer=kernel_regularizer , * ` result ( ) ` : Computes and returns a value for the metric the yielded tuples are of the form ` ( x , y , sample_weight ) ` . how many of the most probable paths will be returned . if gamma is None : if 0 . < self.recurrent_dropout < 1. : fetches= [ K.update ( y , 5 . ) ] ) condition = expand_dims ( condition ) ` batch ` , ` logs ` _targets = C.one_hot ( targets , predictions.shape [ -1 ] ) # Can not be broadcasted . if sample_weight is not None and class_sample_weight is not None : def reshape ( x , shape ) : A list of update ops . threshold is ` true ` , below is ` false ` ) . One metric value is generated output = [ y , y_rev ] padding = 'valid ' # visible signature of __init__ . Tensor ` ( top_paths , ) ` that contains on_batch_begin : called at the beginning of every batch . i : index _ , x = parse_shape_or_val ( ( num_samples , 1 , input_dim ) ) node_index += 1 if len ( raise_in_sub ) < len ( has_raise ) : shape = x.shape output_shape = tf.concat ( [ output_shape [ :1 ] , init='normal ' , if ndim_cond > ndim_expr : base_config = super ( Softmax , self ) .get_config ( ) kernel_initializer='normal ' , Note : if the input to the layer has a rank greater than 2 , then config [ 'class_name ' ] = config [ 'class_name ' ] .lower ( ) is a compiled model ready to be used ( unless the saved model check_two_tensor_operation ( 'stack ' , shape , shape , WITH_NP , return model sparse : Boolean , whether the placeholder created layer.bias_i_i , new_keywords = [ 'padding ' , 'strides ' , 'data_format ' ] new_p = p - lr * g / ( K.sqrt ( new_a ) + self.epsilon ) if validation_data : 'Found : ' + str ( cropping ) ) except HTTPError as e : def _convert_string_dtype ( dtype ) : inner_mask = K.reshape ( inner_mask , inner_mask_shape ) ` ( batch_size , features , downsampled_steps ) ` the output has the same length as the original input . not isinstance ( K.learning_phase ( ) , int ) ) for i , batch_out in enumerate ( batch_outs ) : prefix = ' _'.join ( NAME_SCOPE_STACK ) from keras.utils import losses_utils # Check that x has appropriate ` _keras_history ` metadata . return ( input_shape [ 0 ] , self.filters , rows , cols ) def wrapper ( * args , * * kwargs ) : def _reduce_on_axis ( x , axis , reduce_fun_name ) : model.weights # returns [ ] class_weight : User-provided ` class_weight ` argument . if input_length : any file-like object implementing the method ` read ` that returns y = self.forward_layer.call ( inputs , * * kwargs ) ` `` box '' ` and ` `` hamming '' ` are also supported . identifier = str ( identifier ) or list of shape tuples ( one per output tensor of the layer ) . def test_function_tf_string_input ( self ) : reason='Uses the ` feed_dict ` argument . ' ) # We do this conditionally because it saves us some downloading if the from keras import backend as K max_queue_size=max_queue_size , ` sk_params ` could also accept parameters for calling ` fit ` , ` predict ` , legacy_conv2d_support = generate_legacy_interface ( tf_keras_backend.set_epsilon ( epsilon ( ) ) ( i.e. , above the threshold is ` true ` , below is ` false ` ) . One metric def __repr__ ( self ) : assert abs ( output.mean ( ) - target_mean ) < lim super ( CuDNNLSTM , self ) .__init__ ( width_shift_range=0.2 , def infer_outputs ( self ) : ` _keras_shape ` : Integer shape tuple propagated def minimum ( inputs , * * kwargs ) : `` `` '' Returns the class predictions for the given test data . 'The same thing goes for the number of rows ' `` ` python x = expand_dims ( x , 2 ) output_shape = list ( output_shape ) self.state_spec = [ ] from .losses import categorical_crossentropy both of the same shape , and returns a single tensor , ( inputs [ 0 ] - inputs [ 1 ] ) , E.g . ` node_index=0 ` will correspond to the from tensorflow.keras.layers import SpatialDropout3D np_value.astype ( dtype ) x += reshape ( bias , ( 1 , bias_shape [ 1 ] , bias_shape [ 0 ] ) ) label_shape = tf.cast ( label_shape , tf.int64 ) all of the same shape , and returns `` `` '' Reduce ` elems ` by ` fn ` combined them from right to left on dimension 0 . ndim = len ( shape ) weights [ 4 ] , path : Either a string ( path on disk ) , a Path , a dict , or a HDF5 Group . # this is a workaround for recurrent layer kernel_size : An integer or tuple/list of 2 integers , specifying the def mean_absolute_error ( y_true , y_pred ) : else : if pool_mode == 'max ' : class MaxPooling3D ( _Pooling3D ) : conv_out , x , padding , depthwise_kernel_shape , strides , data_format ) T.sharedvar.TensorSharedVariable , 'sgd ' : SGD , if num_dynamic_axis > = len ( shape ) : self.pointwise_regularizer = regularizers.get ( pointwise_regularizer ) feed_output_shapes , unroll=unroll , _runner ( initializers.ones ( ) , tensor_shape , output_dim = 2 num_rows * = dim out_height = conv_utils.deconv_length ( height , def save_img ( path , 'config ' : model.optimizer.get_config ( ) str_val = 'array ' ( 'conv1d ' , ( 1 , 2 , 8 ) , ( 3 , 2 , 3 ) , 'valid ' , 'channels_first ' ) , return cell_value negative_slope : float > = 0 . Negative slope coefficient . recurrent_z = matrix_inner [ : , : self.units ] weights [ 1 ] = conv_utils.convert_kernel ( weights [ 1 ] ) To save the multi-gpu model , use ` .save ( fname ) ` or ` .save_weights ( fname ) ` from .load_backend import normalize_data_format output_shapes = to_list ( output_shapes ) inner_mask = mask # Reached an Input layer , stop recursion . # is repeated until all nodes are processed . callbacks._call_batch_hook ( 'test ' , 'end ' , steps_done , batch_logs ) `` `` '' Removes a 1-dimension from the tensor at index `` axis '' . y = 2 * x x = K.placeholder ( shape= ( 1 , None , None , 1 ) ) variables = tf.global_variables ( ) if K.backend ( ) == 'theano ' or K.backend ( ) == 'cntk ' : # to make them broadcastable . biases = np.tile ( 0.5 * weights [ 2 ] , 2 ) return inputs , [ s + 1 for s in states ] `` `` '' Sets the weights of the layer , from Numpy arrays . data_format=tf_data_format ) It is recommended to leave the parameters of this optimizer steps = 6 if self.inputs : if not isinstance ( layer , InputLayer ) : grads = [ clip_norm ( g , self.clipnorm , norm ) for g in grads ] match = self._get_existing_metric ( name ) if len ( inputs ) ! = len ( input_spec ) : controls the degree of discretization with larger numbers of thresholds more list/tuple of float threshold values in [ 0 , 1 ] . A threshold is compared axis=axis ) `` `` '' Embedding layer . '' '' '' layer , assert y_train.size == 150 , 'HDF5Matrix ndim should match input array ' new_shape = shape [ nones : ] outputs = result [ 1 ] output_shape = tf.stack ( list ( output_shape ) ) on images on CPU in parallel to training your model on GPU . if bidirectional : raise ValueError ( ' Can not apply softmax to a tensor that is 1D . ' self.recurrent_kernel_f = ( if not hasattr ( self , '_layers ' ) : # sample_weight_mode= { 'dense_1 ' : 'temporal ' } ) return self.cell.dropout random_brightness = image.random_brightness x = np.random.random ( ( 10 , 3 ) ) input_shape= ( input_dim , ) ) ) use_multiprocessing=False , label_smoothing=label_smoothing ) def __init__ ( self , name='mean_absolute_percentage_error ' , dtype=None ) : global _LEARNING_PHASE_CACHE if input_tensor is None : super ( LogCoshError , self ) .__init__ ( logcosh , name , dtype=dtype ) normed , mean , stdinv = T.nnet.bn.batch_normalization_train ( patience : number of epochs that produced the monitored ) state_size = [ self.cell.state_size ] 'with different batch sizes . ' A tensor or list/tuple of tensors . In the snippet below , each of the four examples has only a single fit_args.update ( kwargs ) bar += ' ] ' from six.moves.urllib.error import HTTPError ` y ` should not be specified ( since targets will be obtained h = o * self.activation ( c ) rnn_layer_class = LSTM # possible . greedy=False , condition = tf.reshape ( condition , cond_shape ) def __init__ ( self , name='sparse_categorical_accuracy ' , dtype=None ) : if tf_file_io is None : loss_weights=self.loss_weights , if name in accepted_name or member.__module__ in accepted_module : return K.mean ( inputs , axis= [ 1 , 2 , 3 ] ) `` `` '' Sets the values of many tensor variables at once . Same as the input shape , but with the dimensions re-ordered according ndim : Integer , expected rank of the input . delimiter = self.sep [ Fast and Accurate Deep Network Learning by Exponential Linear Units the validation loss in the filename . # eg. , total_loss = loss_weight_1 * output_1_loss_fn ( ... ) shape_temp.append ( C.InferredDimension ) return K.switch ( assert rand.shape == ( 200 , 200 ) return x * ( x > 0 ) + alpha * ( np.exp ( x ) - 1 . ) * ( x < 0 ) or the same rank as ` y_true ` , number of input units in the weight tensor , if mode = `` fan_in '' weight_names = [ ] out_pad_h , if axis in reduction_axes : self.data [ ' _ { } _pickled'.format ( attr ) ] = True output_dimensions = list ( range ( len ( output.shape ) ) ) embeddings_vars = { } right_pad = args [ 1 ] .get ( 'right_pad ' , 0 ) return None , argument batch_size : Int ( default : 32 ) . self.input_bias = K.flatten ( self.bias [ 0 ] ) base_config = super ( MeanMetricWrapper , self ) .get_config ( ) `` `` '' See docstring for ` Model.evaluate_generator ` . '' '' '' return normed , T.flatten ( mean ) , T.flatten ( var ) sub_n_first_node = { } # write as binary stream x_hadamard , x_sum = layer ( [ x1 , x2 ] ) x = C.pooling ( x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) return { 0 } layer.add_loss ( regularization_losses , _test_optimizer ( optimizers.Adamax ( decay=1e-3 ) ) weights_list = [ loss_weights.get ( name , 1 . ) for name in output_names ] If ` data_format='channels_first ' ` : stride_h , kwargs= { 'output_dim ' : 3 , x_shape = K.int_shape ( x ) # Initializers saved from ` tf.keras ` all_outputs = [ ] self.bias_h , to their original values , in order to ensure the self-normalizing property old_layer = keras.layers.Convolution1D ( 5 , 3 , mean=0. , scale=1. , outbound_layer : the layer that takes # as last output before masked region child_output_shape = self.layer.compute_output_shape ( child_input_shape ) array ( [ 1. , 2 . ] , dtype=float32 ) if losses == [ ] : data_format=self.data_format ) pointwise_kernel_shape = _preprocess_conv2d_filter_shape ( [ 0.66137183 , 0.00869417 , 0.89220798 ] ] , dtype=float32 ) 'backend ' : _BACKEND , # Pre-allocate the results arrays . if layer in self._input_layers : grads = [ K.clip ( g , -self.clipvalue , self.clipvalue ) for g in grads ] # reverse order of the cells ' state . User might want to set this to True # Build a dict { depth : list of nodes with this depth } raise ValueError ( 'All input arrays ( x ) should have ' if not hasattr ( self , '_t_enter_batch ' ) : reference_output_tensors = node.output_tensors > > > a = K.placeholder ( ( 2 , 2 ) , sparse=False ) self._collected_trainable_weights = trainable_weights To reset the states of your model , call ` .reset_states ( ) ` on either variables_to_update : Dictionary with 'tp ' , 'fn ' , 'tn ' , 'fp ' as valid keys elif isinstance ( axis , int ) : ` tanh ( x ) = ( exp ( x ) - exp ( -x ) ) / ( exp ( x ) + exp ( -x ) ) ` if ndim ( x ) == 4 : self.bias_initializer ( ( self.filters * 2 , ) , * args , * * kwargs ) , The generated values follow a normal distribution words = doc.replace ( ' * ' , `` ) .split ( ) reason='cntk currently not support function in this ' ref_params = layer._cudnn_gru.canonical_to_params ( sample_weight_mode=sample_weight_mode ) with `` padding '' zeros left and right . def test_pickling_multiple_metrics_outputs ( ) : if isinstance ( identifier , six.string_types ) : if to_cudnn : def __init__ ( self , * args ) : ` Sequence ` are a safer way to do multiprocessing . This structure guarantees if input_shape and not self.inputs : from tensorflow.keras.layers import Conv1D resnet_v2 = None def test_global_avgpooling3d_legacy_interface ( ) : kernel , return self.cell.kernel_initializer from .common import image_data_format return C.ops.reduce_mean ( self.sk_params = sk_params indices = ( py_slice ( None ) , return ( x , new_x ) cleanup_callback = LambdaCallback ( [ ( v , np.zeros ( ( num_thresholds , ) ) ) for v in self.weights ] ) return to_dense ( x ) .eval ( session=get_session ( ) ) subsample= ( 2 , 2 ) , # output_mask is not None . We need to reshape it def count_params ( self ) : labels_train = labels_train [ indices ] data = data.reshape ( data.shape [ 0 ] , 3 , 32 , 32 ) shape_key = layer.name + ' _ % s_ % s ' % ( node_index , j ) * * kwargs : Additional keyword arguments passed to ` PIL.Image.save ( ) ` . raise ValueError ( 'Could not interpret serialized ' 'ConvLSTM2D ' ] if initial_state is not None : return vgg16.VGG16 ( * args , * * kwargs ) _BACKEND = 'tensorflow ' directories : [ 0.423286 , 0.315517 , 0.0338439 , 0.0393744 , 0.0339315 , 0.154046 ] ] ] , x2 = inputs [ 1 ] metrics = self._get_training_eval_metrics ( ) h5file_ [ 'subgroup/data2 ' ] = data2 model.compile ( 'sgd ' , loss=keras.losses.KLDivergence ( ) ) inbound_node = inbound_layer._inbound_nodes [ inbound_node_index ] self.built = True base_config = super ( ThresholdedReLU , self ) .get_config ( ) ( layer._inbound_nodes and If True , the network will be unrolled , new_weights.extend ( preprocess_weights_for_loading ( from tensorflow.keras.activations import softmax ` channels_last ` ordering ( even when inputs are set output_padding : An integer or tuple/list of 3 integers , ( i.e . the number output of filters in the convolution ) . Decorated function that wraps ` result ( ) ` with identity op . K.switch ( sparse : A boolean specifying whether the placeholder if b is not None : for ( i , shape ) in enumerate ( shapes ) ] start : Start value . class CuDNNLSTM ( _CuDNNRNN ) : arguments=None ) : def test_crossentropy ( self ) : from keras_applications import resnet_v2 # two-tensor ops inputs , return variable ( value=np.zeros ( shape , ctype ) , dtype=dtype , name=name ) ` call ` method of the layer at the call that created the node . return eval ( x ) def test_ReduceLROnPlateau_backwards_compatibility ( ) : vals_sparse = tf.gather_nd ( labels , indices ) # States are a flat list of the individual cell state size . The value at index ` i ` . This callback is constructed with anonymous functions that will be called batch_logs = { 'batch ' : step , 'size ' : 1 } weights_shape [ i ] ! = values_shape [ i ] ) : save_to_dir : None or str ( default : None ) . This enables in-line display of the model plots in notebooks . allowed_positional_args= [ 'name ' , 'shape ' ] , conda config -- set always_yes yes -- set changeps1 no # only compare layers that have weights , skipping Flatten ( ) strides=1 , recurrent_regularizer=recurrent_regularizer , check_single_tensor_operation ( 'any ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) dilation_rate : integer . steps=None , 'to a ` Sequential ` model . ' ) def __init__ ( self , path , mode= ' a ' ) : self.best_weights = None for return_sequences in [ True , False ] : # see : raise ValueError ( 'Rank of condition should be less ' scale : If True , multiply by ` gamma ` . ` 0 ` if ` x < -2.5 ` val_outs = test_loop ( model , val_function , val_inputs , def batch_dot ( x , y , axes=None ) : # update dilation rate , strides # Apply activity regularizer if any : self.data_format , norm = T.sqrt ( T.maximum ( square_sum , epsilon ( ) ) ) def reduce_weighted_loss ( weighted_losses , reduction=Reduction.SUM_OVER_BATCH_SIZE ) : generator : Generator yielding tuples ( inputs , targets ) dtype=_convert_string_dtype ( dtype ) , `` `` '' Embedding layer . constants = inputs [ -self._num_constants : ] # Random prediction test case value , dtype=dtype , shape=shape , name=name ) If ` top_k ` is set , we 'll calculate precision as how often on average a class node_key = layer.name + '_ib- ' + str ( i ) include : self.input_spec = InputSpec ( ndim=2 + self.rank ) units = 2 loss = mse_obj ( y_true , y_pred , sample_weight=sample_weight ) node_index : Integer index of the node from which # if no threshold , then can use nn.relu6 native TF op for performance variables = [ variables ] grad_eval_fn = k.function ( [ x ] , [ grad [ 0 ] ] ) constants = [ constants ] mask_zero=False , self.batch_input_shape = batch_input_shape return tf.pow ( x , a ) `` `` '' Check if ` x ` is a Keras generator type . '' '' '' shape.append ( y._keras_shape [ axis ] ) input_length = input_shape [ 1 ] # as they could be copied to multiple GPU . preprocessing_function=preprocessing_function , for o in self.metrics_outputs : if sparse : self.class_id = class_id when a metric is evaluated during training . A tensor if there is a single output , or A tuple of integers ( or None entries ) . model.compile ( loss='binary_crossentropy ' , optimizer='sgd ' ) dilation_rate=self.dilation_rate , last_states = [ C.sequence.last ( s ) for s in final_states ] inbound_layer , node_index , tensor_index = x._keras_history elif ( isinstance ( x , C.variables.Constant ) or isinstance ( dtype=dtype , name=name ) return x + reshape ( bias , shape ) for i in range ( 1 , len ( inputs ) ) : self._output_shape_cache [ cache_key ] = output_shapes return tf.greater ( x , y ) 'from layer type ` { } ` . '.format ( inputs , forward and backward RNNs will be combined . This is the crossentropy metric class to be used when there are multiple node_data_list = unprocessed_nodes [ layer ] check_two_tensor_operation ( 'less_equal ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) return _clone_functional_model ( model , input_tensors=input_tensors ) unit_forget_bias=True , py_slice = slice `` `` '' Creates a constant tensor . def func ( x ) : val = [ ] It defaults to ` print ` ( prints to stdout ) . ValueError : If the shape of ` sample_weight ` is invalid . i * stride + kernel_size [ 0 ] ) return unpack_singleton ( outs ) y = np.expand_dims ( y , 0 ) raise ValueError ( 'The spatial dimensions of the inputs to ' if 'nb_row ' in kwargs and 'nb_col ' in kwargs : # The previous layer generated a mask . self.sample_weights , masks , self.loss_weights_list ) raise TypeError ( get_layer from tensorflow.keras.applications.vgg16 import VGG16 the current batch or epoch . else for functional model with 1 or more Input layers : model = pickle.loads ( state ) if isinstance ( x , C.cntk_py.Function ) : numer = prec_slope * ( dtp + intercept * K.log ( safe_p_ratio ) ) model.add ( Activation ( 'softmax ' ) ) return tf.nn.leaky_relu ( x , alpha=alpha ) if n is C.InferredDimension or n is C.FreeDimension : def save_attributes_to_hdf5_group ( group , name , data ) : This decorator parses the ` filepath ` argument of the ` save_function ` and for axis in range ( ndim ( x ) ) : x = layers.wrappers.TimeDistributed ( s ) ( x ) `` `` '' Softsign of a tensor . if trainable : if k in self.stateful_metrics : pip install tensorflow==2.0.0 -- progress-bar off ; self.backward_layer.trainable = value res = tf.nn.elu ( x ) from . import training_generator validation_steps : Number of steps to run validation for result = stack ( result , 0 ) def test_dilated_conv_transpose ( self , super ( SensitivityAtSpecificity , self ) .__init__ ( _SYMBOLIC_SCOPE.value = False assert f [ ' y ' ] == [ b'efg ' , b'hij ' , b'klmn ' ] > > > inputs = K.placeholder ( ( 2 , 3 ) ) `` `` '' Prints a string summary of the network . conv_out = _postprocess_conv2d_output ( conv_out , x , padding , y_pred : tensor of predicted targets . parallel_model = multi_gpu_model ( model , cpu_merge=False ) 'negative value : % s ' % str ( negative_slope ) ) model.save_weights ( gcs_filepath ) steps=steps_per_epoch , inputs = inputs.dimshuffle ( axes ) from .convolutional_recurrent import ConvLSTM2DCell __doc__ = image.DirectoryIterator.__doc__ WITH_NP , cntk_two_dynamicity=True , axes= ( 2 , 2 ) ) return unpack_singleton ( output_shapes ) ValueError : If the ` device_type ` string indicates an unsupported device . if inputs_hash in self._per_input_losses : for _ in range ( ndim_diff ) : raw_code = marshal.dumps ( func.__code__ ) initial = [ ] inputs = keras.Input ( ( timesteps , input_dim ) ) output_t , states_t = step_function ( inputs [ : , t ] , states_tm1 + constants ) from .load_backend import cumsum dtype=None ) : from .load_backend import update_add device_type = device_type.lower ( ) negative_part = T.nnet.relu ( -x ) norm = C.sqrt ( C.reduce_sum ( C.square ( x ) , axis=axis [ 0 ] ) ) if isinstance ( y , ( list , tuple ) ) : for p , g , m , u in zip ( params , grads , ms , us ) : if do_validation : if rnn_type == 'LSTM ' : hash_algorithm : Select the hash algorithm to verify the file . create_dataset ( h5_path ) from .utils import metrics_utils 'unit_forget_bias ' : self.unit_forget_bias , def _recurrence ( x , states , m ) : value = tf.random_normal_initializer ( depthwise_constraint=None , self.model.set_weights ( self.best_weights ) use_multiprocessing=use_multiprocessing ) # Inference Usage : ( 2 , 2 , 2 ) , ( 2 , 2 , 2 ) , 'valid ' , name='maxpool3d ' ) # More details in this discussion : the output . The remaining tensors are the last states , reduction=Reduction.SUM_OVER_BATCH_SIZE , `` `` '' Returns a list of batch indices ( tuples of indices ) . 'gamma_initializer ' : initializers.serialize ( self.gamma_initializer ) , b_regularizer='l1 ' , If you are on Windows , you will need to remove ` sudo ` to run the commands below . A Numpy array of target weights , one entry per sample to weight . def __init__ ( self , max_value=None , negative_slope=0. , from docs import autogen overwrite : Whether to silently overwrite any existing file at the uses_learning_phase = False def _reshape_batch ( x , shape ) : assert_list_keras_shape ( t_list , z_list ) tf_data_format = 'NHWC ' input_len = np.array ( input_prob.shape [ 0 ] * [ input_prob.shape [ 1 ] ] ) * * kwargs : The keyword arguments that are passed on to ` fn ` . for pooling_class in [ pooling.GlobalMaxPooling2D , A tensor of the same shape , type and content . source = 'GRU ( reset_after=False ) ' updates : update op or list of update ops # test config raise TypeError ( 'Required Group , str , Path or dict . ' _runner ( initializers.he_normal ( ) , tensor_shape , init = initializers.orthogonal ( seed=9876 ) self.kernel_size [ 0 ] * self.kernel_size [ 1 ] * input_filter , % ( HDF5_OBJECT_HEADER_LIMIT , else : # legacy config file @ pytest.mark.skipif ( K.backend ( ) == 'theano ' , metrics= [ keras.metrics.CosineSimilarity ( axis=1 ) ] ) d2 = h5file_ [ 'subgroup/data2 ' ] [ : ] return K.sqrt ( self.total / self.count ) self._output_mask_cache [ cache_key ] = output_masks Used for computing top-k prediction values in dense labels ( which has the same before_install : new_layer = keras.layers.GRU ( 2 , kernel_initializer='normal ' , return np.tanh ( x ) enqueuer.start ( workers=workers , max_queue_size=max_queue_size ) x = ( 0.2 * x ) + 0.5 from .. legacy.layers import AtrousConvolution2D code : bytecode of the function . if hasattr ( C , 'to_batch ' ) : check_single_tensor_operation ( 'softmax ' , ( 4 , 5 , 3 , 10 ) , WITH_NP , axis=2 ) # in ` LossFunctionWrapper ` class . * * kwargs : When using the Theano/CNTK backends , these arguments `` `` '' Returns the dtype of a Keras tensor or variable , as a string . inputs_c = inputs * dp_mask [ 2 ] super ( SquaredHinge , self ) .__init__ ( squared_hinge , name , dtype=dtype ) self._feed_input_names = [ ] self.cell.build ( step_input_shape ) name = 'param_ ' + str ( i ) if self.unroll and timesteps is None : # Earlier versions of keras did n't dump weighted_metrics properly . Use self.bias_o = self.bias [ self.units * 3 : ] # tf.nn.atrous_conv2d_transpose input only supports NHWC format cols = conv_utils.conv_output_length ( cols , epoch , if the validation frequency elif K.backend ( ) == 'cntk ' : `` `` '' Stop training when a monitored quantity has stopped improving . self.rank = len ( size ) @ pytest.mark.skipif ( K.backend ( ) == 'cntk ' , reason='Not supported . ' ) [ [ 0 , 1 , 2 ] , < -- - > [ [ 0 , 2 , 4 ] , model.compile ( optimizer='sgd ' , loss='mse ' ) The model architecture , allowing to re-instantiate the model . kernel_size=kernel_size , # In this case we are dealing with a Keras config dictionary . mean : a python scalar or a scalar tensor . Mean of the random values Arbitrary , although all dimensions in the input shaped must be fixed . axis : An integer , the axis to compute the sum . keras.layers.Dense ( 32 , input_shape=input_shape ) , base_config = super ( Dot , self ) .get_config ( ) out = C.times ( x , U ) self.moving_mean , * * At this time , we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to ` tf.keras ` in TensorFlow 2.0 * * . ` tf.keras ` is better maintained and has better integration with TensorFlow features ( eager execution , distribution support and other ) . kwargs [ 'steps_per_epoch ' ] = samples_per_epoch raise ValueError ( 'The ` data_format ` argument must be one of ' counts [ name ] += 1 class Cropping2D ( _Cropping ) : ( 'border_mode ' , 'padding ' ) , weights . Default : ` None ` . if matrix_cond in variables_to_update : Only applicable if the layer has exactly one inbound node , use_multiprocessing : use multiprocessing if True , otherwise threading x = keras.backend.variable ( np.ones ( ( num_samples , self.model = None not is_wrapped_model ( layer ) ) : if output_shape [ axis ] is None or shape [ axis ] is None : if hasattr ( tensor , '_keras_shape ' ) and output_shapes is not None : self.cooldown_counter = 0 # Cooldown counter . _SEQUENCE_COUNTER = mp.Value ( ' i ' , 0 ) self.beta_constraint = constraints.get ( beta_constraint ) last_output_k , output_k , last_states_k = K.rnn ( layer = RNN ( cells ) def __init__ ( self , value , num_thresholds=200 , name=None , dtype=None ) : copy from . the first dimension is conserved . updates += l.updates You can specify the initial state of RNN layers numerically by : if self.merge_mode == 'concat ' : output_shapes.append ( list ( output.shape ) ) shifted_mean = C.reduce_mean ( shifted_mean , axis=axis ) def conv2d_transpose ( x , kernel , output_shape , strides= ( 1 , 1 ) , recurrent_regularizer=recurrent_regularizer , grads = update_func.find_all_with_name ( 'keras_grad_placeholder ' ) x_list.append ( x ) return theano.gradient.disconnected_grad ( variables ) if k in self.totals : kwargs.pop ( 'implementation ' ) The number of batches in the Sequence . def std ( x , axis=None , keepdims=False ) : by 1 . If ` keepdims ` is ` True ` , the reduced dimension is input_uid = object_list_uid ( inputs ) Note : the file is temporarily copied to local filesystem from GCS before loaded . featurewise_center=featurewise_center , shape= ( self.num_thresholds , ) , self.input_spec = InputSpec ( min_ndim=3 ) Optional for ` Sequence ` : if unspecified , will use A `` Keras tensor '' is a tensor that was returned by a Keras layer , successive_outputs = [ ] # Update the depth of the corresponding layer x = k.variable ( val ) elif cell.data_format == 'channels_last ' : K.conv2d ( dummy_x_2d , dummy_w_2d , data_format='channels_middle ' ) broadcast_var , constant = Constant 'Expected : None , int , ( int , int ) , ' regularizers.serialize ( self.depthwise_regularizer ) ) dropout=0.1 , class AUC ( Metric ) : def handle_method ( name , member ) : 'You are passing a target array of shape ' + str ( y.shape ) raise ValueError ( 'You are trying to load a weight file ' strides= ( strides , ) , non_trainable_count = count_params ( model.non_trainable_weights ) def urlretrieve ( url , filename , reporthook=None , data=None ) : preprocessor=conv3d_args_preprocessor ) num_static_element = np.prod ( np.asarray ( self.from_shape ) ) zip ( y , sample_weights , class_weights , g = K.switch ( K.greater_equal ( n , c ) , g * c / n , g ) # cntk output_shape does not include batch axis 'Input { } ( 0-based ) originates ' reason='We only test the shape inference of the ' 'layer { } '.format ( layer.name ) + ' due to mismatch ' for node in nodes : The ` depth_multiplier ` argument controls how many base_config = super ( Recall , self ) .get_config ( ) # e.g . states of a 2-layer LSTM would be ` [ h1 , c1 , h2 , c2 ] ` . return np.random.random ( batch_shape ( shape ) ) masks = [ K.expand_dims ( m , 0 ) for m in mask if m is not None ] return ( input_shape [ 0 ] , new_space [ 0 ] , new_space [ 1 ] , out_filters ) return self.forward_layer.updates + self.backward_layer.updates for name , val in zip ( weight_names , weight_values ) : code , defaults , closure = code input_dim = input_shape [ 2 ] 'metrics ' : callback_metrics , def conv1d_args_preprocessor ( args , kwargs ) : logs [ k ] = self.totals [ k ] along each axis . with a nD tensor , it reproduces the Theano behavior . final_output = C.splice ( final_output , output_slice , axis=time_axis ) assert cce_obj.reduction == losses_utils.Reduction.SUM preds_tiled = K.tile ( predictions_2d , [ num_thresholds , 1 ] ) import os now = time.time ( ) # randomly set half of the predictions to an identical value if len ( in_lens ) ! = len ( input_shape ) - 1 : self.kernel_size [ 0 ] , dot.add_subgraph ( submodel_not_wrapper ) # Subclassed network from . import training_utils from .. preprocessing.sequence import _remove_long_seq def recurrent_dropout ( self ) : `` `` '' Returns a yaml string containing the network configuration . for layer in model._input_layers : self.bias_r = self.bias [ self.units * 4 : self.units * 5 ] elif ndim ( x ) == 3 : def then_expression_fn ( ) : ` keras.models.model_from_json ( json_string , custom_objects= { } ) ` . 'RNN dropout is no longer supported with the Theano backend ' shape_or_val=False , str ( x [ 0 ] .shape [ 0 ] ) + ' samples . ' if self.reset_after and self.use_bias : f_list = [ ] ` recurrent_kernel ` . Use ` 'reset_after'=True ` and 'Unknown entry in ' # is not well-formed JSON but rather has a JSON object per line . fpath = os.path.join ( datadir , fname ) self.dropout = min ( 1. , max ( 0. , dropout ) ) tf.summary.histogram ( mapped_weight_name , weight ) # # Configuring your Keras backend data in ` x_col ` column should be absolute paths . x_batch_size = x_shape [ 0 ] output_shape.append ( j ) > > > K.int_shape ( inputs ) if sample_weight is None : inputs_f = inputs * dp_mask [ 1 ] from . import losses_utils self.width = width inputs , states = cell.call ( inputs , states , def _deserialize_model ( h5dict , custom_objects=None , compile=True ) : if ( 0 < self.recurrent_dropout < 1 and self.interval = interval def __init__ ( self , size , data_format=None , * * kwargs ) : model.get_config ( ) losses , None , sample_weight ) abs_error = K.abs ( error ) transpose_input ( from_cudnn ) , shape ) if doc_lines [ index - 1 ] ! = `` : gamma , class TensorBoard ( Callback ) : 'Warning : CNTK backend does not support ' layer.reset_states ( ) raise ValueError ( ' Can not add an empty model ' layer_test ( convolutional.MaxPooling3D , unrelated_updates.extend ( u_ops ) label_array = tf.reshape ( tf.tile ( tf.range ( label_shape [ 1 ] ) , num_batches_tns ) , predictions = np.random.random ( ( batch_size , num_classes ) ) .astype ( 'float32 ' ) raise ValueError ( ' ` steps_per_epoch=None ` is only valid for a ' Node ( outbound_layer=self , if not self.optimizer : dtype = floatx ( ) def greater ( x , y ) : config = { 'batch_input_shape ' : self.batch_input_shape , res : dictionary containing variables return isinstance ( x , ( T.TensorVariable , y = np.random.random ( ( 1 , 3 , 3 ) ) output , state = outputs [ 0 ] , outputs [ 1 : ] pointwise_constraint=pointwise_constraint , width_factor=width_factor , self.weights = [ self.iterations ] + ms + us if isinstance ( axis , list ) : # without the network being notified of it . x = T.clip ( x , 0.0 , max_value ) for x in node.output_tensors : loss = weighted_losses pad = ( w_pad , h_pad , d_pad ) def output_shape ( self ) : dtype=None , trainable_weights += l.trainable_weights code = marshal.loads ( raw_code ) If the desired matrix is not square , it gets padded from .load_backend import foldl `` `` '' LeCun uniform initializer . value : A tensor with the same shape as ` x ` . 'theano backend . ' ) output_tensors [ i ] , '_uses_learning_phase ' , False ) or uses_lp mask_ = np.expand_dims ( mask_ , axis=-1 ) num_identical = num_classes // 2 reason='CNTK does not support float64 ' ) 'CNTK can not take variable length inputs . Please ' > > > np_var = numpy.array ( [ 1 , 2 ] ) dilation_rate= ( 1 , 2 ) ) x_val = np.random.random ( x_shape ) .astype ( np.float32 ) [ 0. , 1. , 0 . ] ] , dtype=float32 ) batch_size=None , `` `` '' Creates a ` Reduce ` instance . Since the bucket name can be provided using an environment variable , it is def _compute_elemwise_op_output_shape ( self , shape1 , shape2 ) : if x [ 0 ] .shape [ 0 ] > batch_size and x [ 0 ] .shape [ 0 ] % batch_size ! = 0 : self.cooldown_counter = self.cooldown W_regularizer='l1 ' , self._inbound_nodes = [ ] def __init__ ( self , layers=None , name=None ) : return display.Image ( filename=to_file ) `` `` '' This wrapper runs for all the tests in the legacy directory ( recursively ) . np.random.shuffle ( index_array ) return _axis shape = ( ) xs = [ [ w for w in x if skip_top < = w < num_words ] 'kernel_constraint ' : options are 'md5 ' , 'sha256 ' , and 'auto ' . Same shape as input . if isinstance ( v , list ) : bar += '= ' by randomly setting activations to the negative saturation value . _FLOATX = str ( floatx ) raise ValueError ( 'Can only use ` validation_steps ` ' if y is None or loss is None : self.name + ' : expected shape= ' raise ValueError ( `` { } class does n't have any documentation '' .format ( name ) , ' ( left_dim3_crop , right_dim2_crop ) ) . ' from .load_backend import image_data_format 'expected on of { `` fan_in '' , `` fan_out '' , `` fan_avg '' } ' `` `` '' Function decorator to support saving to Google Cloud Storage ( GCS ) . dtype=dtype , seed=self.seed ) output_mask = K.any ( K.not_equal ( inputs , self.mask_value ) , axis=-1 ) weights [ 0 ] = np.reshape ( weights [ 0 ] , layer_weights_shape ) '- If using the functional API , specify ' for x in names def __getitem__ ( self , index ) : conv_out = op ( kernel , x , output_shape [ 2 : ] ) delta : A float , the point where the Huber loss function changes from a if isinstance ( layer.output , list ) : def _send_sequence ( self ) : elif function_name in _GLOBAL_CUSTOM_OBJECTS : old_layer = keras.layers.ConvLSTM2D ( 5 , nb_row=3 , nb_col=3 , name='conv ' ) for _original , _cloned in zip ( model._input_layers , input_layers ) : # Broadcast weights if possible . with gzip.open ( paths [ 0 ] , 'rb ' ) as lbpath : def __init__ ( self , save_prefix= '' , warnings.warn ( argdefs=defaults , str ( x_shape ) + ' and ' def test_maxpooling2d_legacy_interface ( ) : save for _ in range ( ndim - 1 ) : shape = tuple ( shape_temp ) width_factor=2 , str ( len ( weights ) ) @ non_trainable_weights.setter return super ( GRU , self ) .call ( inputs , Layer instance ( may be Model , Sequential , Layer ... ) layer = node.outbound_layer self.field = field 'do_validation ' : do_validation , data_format=data_format , dim_ordering='th ' , axes = [ axes [ 0 ] - 1 , axes [ 1 ] - 1 ] # ignore batch dimension self.dilation_rate = conv_utils.normalize_tuple ( dilation_rate , 2 , # Generate sample-wise weight values given the ` sample_weight ` and from tensorflow.keras.layers import ThresholdedReLU dtype : The destination type . legacy_deconv2d_support = generate_legacy_interface ( [ 0.215136 , 0.439699 , 0.0370931 , 0.0393967 , 0.0381581 , 0.230517 ] , 'false_negatives ' , assert dense_layer.get_updates_for ( None ) == [ 1 ] assert z1.shape == z2.shape integer , specifies how many training epochs to run before a new if data_format == 'channels_first ' : self.input_spec = InputSpec ( ndim=len ( input_shape ) , axes=axes ) raise ValueError ( ' Can not unroll a RNN if the ' h5dict [ 'training_config ' ] = json.dumps ( { def epsilon ( ) : if isinstance ( metrics , list ) : # we copy them to avoid loss of tensor metadata . with pytest.raises ( TypeError ) : if not all_outs : y_pred = K.constant ( y_pred ) classes : optional list of classes ( e.g . ` [ 'dogs ' , 'cats ' ] ` ) . target_gpu_ids = gpus cropping : A tuple of tuples of 2 ints . enqueuer = SequenceEnqueuer ( ... ) self.data = h5py.File ( path , mode=mode ) original_keras_version=original_keras_version , # set or validate state_spec num_chunks = 1 self.constants_spec = [ InputSpec ( shape=K.int_shape ( constant ) ) use_multiprocessing=False ) : # https : //github.com/evhub/keras/blob/2.1.1/keras/utils/generic_utils.py # L166 # Check shapes compatibility . sequence_length=input_length , beam_width=beam_width , axis : axis along which to perform the reduction . ask.assert_called_once ( ) 'input_tensors ' , metrics=metrics , if isinstance ( axes , list ) : layers : a list of target layers . y_squashed_dim = tf.reduce_prod ( y_trail_dims ) `` `` '' Applies Alpha Dropout to the input . old_layer = keras.layers.Deconvolution2D ( 5 , 3 , 3 , ( 6 , 7 , 5 ) , name='deconv ' ) accept_all : What to return if there is no parameter called ` name ` [ Cosine Similarity ] ( https : //en.wikipedia.org/wiki/Cosine_similarity ) input_shape : original shape of array being reshaped model.add ( Flatten ( ) ) input_shape [ 3 ] + left_pad + right_pad ) super ( BatchNormalization , self ) .__init__ ( * * kwargs ) validation_steps=validation_steps , res = { } predictions_k = T.sort ( predictions ) [ : , -k ] insecure = re.sub ( ' ( [ a-z ] ) ( [ A-Z ] ) ' , r'\1_\2 ' , intermediate ) .lower ( ) lambda : then_expression , layer = node.inbound_layers [ i ] super ( MaxPooling1D , self ) .__init__ ( pool_size , strides , int_A^B { Precision.dTP } = int_A^B { slope * dTP } = slope * ( TP_B - TP_A ) By default , Keras will use TensorFlow as its tensor manipulation library . [ Follow these instructions ] ( https : //keras.io/backend/ ) to configure the Keras backend . return np.prod ( x , axis=axis , keepdims=keepdims ) ( 'conv2d_transpose ' , ( 2 , 5 , 6 , 3 ) , ( 3 , 3 , 2 , 3 ) , ( 2 , 5 , 6 , 2 ) , return [ initial_state for _ in self.cell.state_size ] A [ Sequence ] ( /utils/ # sequence ) instance . mean_np = np.random.random ( other_shape ) fraction of them for which ` class_id ` is above the threshold and/or in the get a specific one . A single generator would cause the validation to thresholds=None ) : ( ` float32 ` , ` float64 ` , ` int32 ` ... ) if tf_file_io is None : class_id : Optional int , limits the prediction and labels to the class def get_losses_for ( self , inputs=None ) : # learning phase tensor . from .load_backend import random_uniform data_format , * * kwargs ) if x._keras_shape [ 3 ] is not None : summary_value.tag = name if ( val_gen and not val_use_sequence_api and # Global variables to be shared across processes supports_sparse = True self.kernel_f = self.kernel [ : , : , : , self.filters : self.filters * 2 ] # ( e.g . activity regularizers ) . if not batch_size : only . Maximum number of processes to spin up when using name='beta ' , use when discretizing the roc curve . Values must be > 1 . shuffle=shuffle ) len_dim3 = conv_utils.conv_output_length ( len_dim3 , self.pool_size [ 2 ] , # Ensure name unicity , which will be crucial for serialization new_model = load_model ( fname ) along the height and width . 'activation argument of another layer . Instead , advanced ' num_dynamic_axis = _get_dynamic_axis_num ( x ) from .data_utils import Sequence def test_relu ( self , alpha , max_value , threshold ) : # Yields # now : model.output_shape == ( None , 65536 ) u = update ` filepath ` can contain named formatting options , axes = [ x_ndim - 1 , y_ndim - 2 ] for batch_index , ( batch_start , batch_end ) in enumerate ( batches ) : This is useful to mitigate overfitting # Only nodes in network_nodes are saved . ` ( samples , dim1 , dim2 , dim3 , channels ) ` if ` data_format='channels_last ' ` . from tensorflow.keras.layers import Cropping1D h5file.flush ( ) def deserialize_keras_object ( identifier , module_objects=None , # Keras variable num_samples = 2 from .. import backend as K inbound_layer = node.inbound_layers [ j ] `` `` '' Instantiate an identity matrix and returns it . new_layer = keras.layers.Conv2D ( 5 , ( 3 , 3 ) , sample_weight = broadcast_weights ( losses , sample_weight ) # check that bucket exists and is accessible for j in range ( output_col ) : self.output_names [ i ] + '_loss ' _SEQUENCE_COUNTER = None if getattr ( self.cells [ -1 ] , 'output_size ' , None ) is not None : def test_model_saving_to_pre_created_h5py_file ( ) : mask=None ) : np.zeros ( ( batch_size , self.cell.state_size ) ) ) # Set input spec `` ` python def model_from_config ( config , custom_objects=None ) : K.clear_session ( ) do_validation = True vs = [ K.zeros ( K.int_shape ( p ) , name='_keras_learning_phase ' ) receptive_field_size = np.prod ( shape [ 2 : ] ) ' ` ' + new_arg + ' ` . Stick to the latter ! ' ) def step ( self , inputs , states ) : ctype = _convert_string_dtype ( dtype ) padding : Tuple of 2 tuples , padding pattern . download = False will have shape ` ( s1 , s2 * rep , s3 ) ` . self.recurrent_kernel [ : , 2 * self.units : ] ) Whether the images will be converted to by the progbar before display . m.update_state ( [ 1 , 3 , 5 , 7 ] ) shape = bias.shape dtype=None , name=None , seed=None ) : ( self.true_positives / ( self.true_positives + self.false_negatives ) ) , x._uses_learning_phase = False if isinstance ( code , ( tuple , list ) ) : # unpack previous dump def initialize_weights ( layer ) : ( tuple of integers , does not include the batch axis ) , def DISABLED_test_model_with_external_loss ( ) : output = T.switch ( output_mask , output , prev_output ) if self.amsgrad : x = np.random.uniform ( 0 , 1 , size= ( 10 , 3 ) ) from .load_backend import tile def reset_metrics ( self ) : if check_positional_args : if K.backend ( ) == 'cntk ' : # now : model.output_shape == ( None , 6 , 2 ) x._keras_shape [ 4 ] ) data_format : ` 'channels_first ' ` or ` 'channels_last ' ` . batch_y = self.y [ idx * self.batch_size : ( idx + 1 ) * self.batch_size ] `` `` '' Element-wise truth value of ( x > = y ) . raise TypeError ( 'Type of ` metrics ` argument not understood . ' do_validation = True `` `` '' Multi-GPU training utilities . for o in self.trainer_output : the categorical hinge metric value is 1.0 . output_index ) : self.add ( layer ) initializers.Ones ( ) ( ( self.units , ) , * args , * * kwargs ) , elif ndim ( beta ) == ndim ( x ) and shape ( beta ) [ 0 ] == 1 : get_config # Example : model.add ( TimeDistributed ( Dense ( 32 ) ) ) 2 . An instance of a class that implements the ` __call__ ` method Loading weights between incompatible layers should fail fast with an exception . from theano.printing import Print use_bias=use_bias , constants = self.get_constants ( inputs , training=None ) from .load_backend import sigmoid model.add ( layers.Masking ( mask_value=0. , input_shape= ( 3 , 4 ) ) ) _image_data_format = _config.get ( 'image_data_format ' , batch_size = list ( x.values ( ) ) [ 0 ] .shape [ 0 ] `` `` '' Accumulates root mean squared error statistics . > > > K.dtype ( kvar ) if accept_all and arg_spec.keywords is not None : An integer denoting the number of elements in the dataset . return unpack_singleton ( outs ) `` `` '' Returns the index of the minimum value along an axis . if 0 < self.dropout < 1. : > > > K.is_keras_tensor ( keras_layer_output ) if sample_weight is None : super ( Cropping1D , self ) .__init__ ( normalized_cropping , if mode == 'min ' : if isinstance ( x_weight , list ) : assert k_s_d.shape == k_d.shape return tf.convert_to_tensor ( x , dtype=dtype ) outputs = to_list ( outputs ) metric_fn.result ( ) @ pytest.mark.parametrize ( 'shape , shape2 , axis ' , [ ( 'inner_activation ' , 'recurrent_activation ' ) , raise ValueError ( 'CNTK Backend : the input of rnn has only rank % d ' class VarianceScaling ( Initializer ) : For example , if ` y_true ` is [ 0. , 0. , 1. , 1 . ] , and ` y_pred ` is [ 1. , 1. , 1. , 0 . ] specified for histogram visualizations . # model , input_tensors= [ input_a , input_b ] ) It draws samples from a uniform distribution within [ -limit , limit ] y = self._merge_function ( reshaped_inputs ) if eta > 3600 : targets = np.random.random ( ( num_samples , units ) ) output_tensors = to_list ( self.kernel_size [ i ] , self.updates.append ( K.update ( m , m_t ) ) if self.build_fn is None : shape = K.int_shape ( state ) except ValueError : http : //www.cs.toronto.edu/~graves/preprint.pdf ) x = np.transpose ( x , ( 0 , 2 , 1 ) ) decoded = _remove_repeats ( decoded ) # With TensorFlow or CNTK , we can infer the output shape directly : input_shape = list ( input_shape ) self._non_trainable_weights = [ ] # Otherwise , we default to the input shape . dataset : string , name of the HDF5 dataset in the file specified x : input tensor . x , distribution : Random distribution to use . One of `` normal '' , `` uniform '' . self._base_dtype = first_val.dtype return deserialize_keras_object ( name , def is_supported_type ( path ) : return f_active_next , log_f_next , b_active_next , log_b_next raise ValueError ( 'Invalid format for ` axes ` - ' _p_prev = T.inc_subtensor ( import types as python_types set -e save_to_dir=save_to_dir , ( see : [ activations ] ( .. /activations.md ) ) , str ( layer.input_spec ) ) batch_size = 32 # Test that calling a same seeded random initializer if isinstance ( identifier , Layer ) : y_ndim += 1 # check that the call to ` predict ` updated the states legal_params_fns = [ Sequential.fit , Sequential.predict , del reduced_inputs_shapes [ i ] [ self.axis ] optimizer_config = training_config [ 'optimizer_config ' ] elif isinstance ( then_expression , tf.IndexedSlices ) : A PIL Image instance . expand_nested , dpi ) regularizer=self.beta_regularizer , `` `` '' Wraps arbitrary expression as a ` Layer ` object . config = { 'function ' : function , Every ` Sequence ` must implement the ` __getitem__ ` and the ` __len__ ` methods . cache_key += ' _ ' + object_list_uid ( masks ) return tf_keras_backend.sparse_categorical_crossentropy ( new_nested_states = new_nested_states [ : :-1 ] hasher = 'md5 ' if hasattr ( self.cell.state_size , '__len__ ' ) : if hasattr ( x , '_keras_history ' ) : arrays = list ( map ( np.random.random , s ) ) def concatenate ( tensors , axis=-1 ) : batch_ids = index_array [ batch_start : batch_end ] data_format=None , dilation_rate= ( 1 , 1 , 1 ) ) : use_bias=use_bias , value = shape_or_val def __init__ ( self , specificity , num_thresholds=200 , name=None , dtype=None ) : if isinstance ( archive_format , six.string_types ) : output_shape = None new_model.compile ( 'rmsprop ' , 'mse ' ) node_indices.append ( node_index ) `` `` '' Iterate indefinitely over a Sequence . @ pytest.mark.parametrize ( 'tensor_shape ' , [ FC_SHAPE , CONV_SHAPE ] , ids= [ 'FC ' , 'CONV ' ] ) first_connection = connections [ 0 ] 'in number of weights ( { } vs { } ) . '.format ( reduction=Reduction.SUM , name='mape_1 ' ) verbose=0 , # shape : batch , filters , row , col cbk.validation_data = val_data reset_metrics : If ` True ` , the metrics returned will be only for this output = k.batch_normalization ( x , mean , var , beta , gamma , axis=axis ) masks=None ) : class_weight=None , [ 0.0 , 0.0 , 0.4 , 0.6 ] , # t=1 continue each batch item in ` y_pred ` . from tensorflow.keras.layers import DepthwiseConv2D lambda : y_true ) reduction=Reduction.SUM ) if isinstance ( self.cell.state_size , int ) : Only relevant when using Theano . data_format=data_format ) num_thresholds : ( Optional ) Defaults to 200 . The number of thresholds to K.greater ( ( self.false_positives ) , 0 ) , 't10k-labels-idx1-ubyte.gz ' , 't10k-images-idx3-ubyte.gz ' ] from_logits=True , reduction=losses_utils.Reduction.NONE ) if hasattr ( self.forward_layer , 'trainable_weights ' ) : # A Network does not create weights of its own , summation_method ) out_eval = model.evaluate ( X_test , y_test , batch_size=32 , verbose=False ) if target is None or K.is_placeholder ( target ) : config [ 'output_layers ' ] = model_outputs _LOCAL_DEVICES = tf.config.experimental_list_devices ( ) y = np.mean ( np.ma.masked_invalid ( y ) , axis=-1 ) .data if _need_convert_kernel ( original_backend ) : activity_regularizer : Regularizer function applied to legacy_gaussiannoise_support = generate_legacy_interface ( ` bytes ` data ( e.g . ` io.BytesIO ` ) . assert len ( states ) == 2 num_dynamic = _get_dynamic_axis_num ( x ) return [ None for _ in output_names ] # until all of them have the same rank . if kwargs : do_validation = bool ( validation_data ) loss='sparse_categorical_crossentropy ' , ValueError : if ` data_format ` is neither ` `` channels_last '' ` or attr : Exact node attribute name . val_data = validation_data with a beam of this width . The file is subsequently written to the binary ` stream ` . 'together with shapes ' def hadamard_product_sum ( tensors ) : output_generator = iter_sequence_infinite ( generator ) [ 0. , 1. , 0 . ] , layer = layer_class ( units , return_state=True , stateful=True ) for file_name in batch_x ] ) , np.array ( batch_y ) # # ` loss ` does not match outputs . masks = [ None for _ in range ( len ( inputs ) ) ] unpack_singleton ( input_shapes ) ) if stop + self.start < = self.end : tensor_index ) : if weights is not None : List : if ` greedy ` is ` True ` , returns a list of one element that Please note that in case of class_mode None , bias_initializer='zeros ' , `` `` '' Concatenates a list of tensors alongside the specified axis . reduce_result = prod ( x , axis , keepdims=keepdims ) Supported methods are ` `` nearest '' ` , ` `` bilinear '' ` , and ` `` bicubic '' ` . ` `` hamming '' ` are also supported . By default , ` `` nearest '' ` is used . # The mean / var / beta / gamma may be processed by broadcast # Does update the network states . # here to mapping the correct axis . Will remove this tricky after we add support # add dim to x to have ( batch , length , 1 , input_dim ) the keyword arguments listed below . kwargs [ 'backend ' ] = backend [ num_classes , num_classes ] will be allocated . k.variable ( x_val ) , k.variable ( convert_kernel ( y_val ) ) , * * kwargs ) output = repeat_elements ( output , width_factor , axis=3 ) ` output [ i ] ` is ` True ` if ` predictions [ i , targets [ i ] ] ` is within top- ` k ` # imports for backwards namespace compatibility targets = np.random.randint ( num_classes , size=batch_size , dtype='int32 ' ) name = str ( w.name ) [ f.wait ( ) for f in last_ones ] start_char : The start of a sequence will be marked with this character . if hasattr ( input , 'shape ' ) and hasattr ( placeholder , 'shape ' ) : sparse=K.is_sparse ( self.outputs [ i ] ) , from .callbacks import ReduceLROnPlateau # i.e . we mark it to be saved 'numpy > =1.9.1 ' , ` states ` should be a numpy array or if has_arg ( self.layer.call , 'training ' ) : specifying the strides of the convolution along the width and height . d_decoded [ k.decode ( 'utf8 ' ) ] = v # Handle case where start is a tensor x._keras_shape [ 2 ] ) padding : Tuple of tuples of two ints . Can be a tuple of ints when initial_epoch=initial_epoch ) str ( ( batch_size , dim ) ) layer : a Layer object . Pseudocode : # Sample weighting not supported in this case . if end == 0 : def squeeze ( x , axis ) : axis=-1 , keepdims=True ) with open ( fname , 'rb ' ) as f : check_two_tensor_operation ( 'maximum ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) The current release is Keras 2.4.0 , which simply redirects to ` tf.keras ` . fit_function : Keras function returning a list of tensors if 0 . < self.dropout < 1. : if isinstance ( obj , dict ) : super ( ConvRecurrent2D , self ) .__init__ ( * * kwargs ) return layer_name + '_ib- ' + str ( node_index ) y : array-like , shape ` ( n_samples , ) ` input_shapes = input_shape from .load_backend import set_image_data_format loss_functions = [ ] from tensorflow.keras.applications.nasnet import NASNetMobile # Case 2 : Symbolic tensors or Numpy array-like . new_layer = keras.layers.BatchNormalization ( beta_initializer='ones ' , recurrent_initializer=recurrent_initializer , preds : array-like , shape ` ( n_samples , ) ` if dynamic_batch_size : weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 2 , 3 , 0 , 1 ) ) from .. import layers as layer_module * * kwargs ( 1:1 mapping between weights and samples ) , from keras.layers import Dense scale=scale , x : A ` Variable ` . sample_size = K.prod ( [ K.shape ( inputs ) [ axis ] 'nadam ' : Nadam , # layer call until it becomes possible to process it value : Value to set the tensor to , as a Numpy array y = T.reshape ( x , tuple ( shape ) ) x = np.random.random ( ( num_samples , height , width , 3 ) ) * * kwargs ) for node in reversed ( nodes_in_decreasing_depth ) : axis_1 = 1 self.state_size = ( self.units , self.units ) if not model.inputs and build_input_shape : elif self.data_format == 'channels_last ' : x : Tensor or variable . return resnet.preprocess_input ( * args , * * kwargs ) 'sparse ' : self.sparse , is a visualization tool provided with TensorFlow . # the instance to the ` metrics ` list as soon as it is created . output_tensors.append ( layer_output_tensors [ tensor_index ] ) pattern = transpose_shape ( pattern , data_format , spatial_axes= ( 1 , 2 , 3 ) ) def __init__ ( self , alpha_initializer='zeros ' , assert np.array_equal ( a , np.arange ( test_value ) ) label_lengths : length of the labels . data = [ ] output = y + y_rev super ( KLDivergence , self ) .__init__ ( output_names.append ( n ) conversions= [ ( 'beta_init ' , 'beta_initializer ' ) , return metric_obj # training/eval . 'valid ' , 'channels_last ' , ( 2 , 2 , 2 ) ) , class GRUCell ( Layer ) : def simple_rnn_with_extra_mock_state ( inputs , states ) : from .load_backend import binary_crossentropy return super ( ConvLSTM2D , self ) .call ( inputs , Generator yielding tuples ` ( inputs , targets ) ` dim = self.cell.state_size [ index ] input_shape : Shape tuple . Provided for convenience , but note # Get sorted list of layer depths . output_loss ) axis : An integer , the axis to compute the product . from keras.layers import Bidirectional , GRU , LSTM CTC loss of each element value = tf.random_uniform_initializer ( ` on_batch_begin ` and ` on_batch_end ` expect two positional arguments : input_shape = _collect_input_shape ( inputs ) def DISABLED_test_sample_weights ( ) : return np.concatenate ( all_outs [ 0 ] ) super ( _UpSampling , self ) .__init__ ( * * kwargs ) self._call_batch_hook ( _PREDICT , 'begin ' , batch , logs=logs ) process-based threading . If unspecified , ` workers ` will default ' ) does not match the number of weights ' `` `` '' Gets parameters for this estimator . uses_lp = any ( h_tm1_i = h_tm1 TRUE_POSITIVES = 'tp ' scale = ( high - low ) / 2 def handle_module ( mod ) : 'total ' , initializer='zeros ' ) This can also be a list/tuple of integers mkdir ~/mpi return ( ( x - mean ) / sqrt ( var + epsilon ) ) * gamma + beta if has_arg ( layer.call , 'mask ' ) : b = keras.layers.Dense ( hidden_dim ) ( input_b ) parallel_model.compile ( loss='mse ' , optimizer='rmsprop ' ) assert_allclose ( K.eval ( last_states [ 0 ] ) , expected_last_state ) # computable_tensors : all tensors in the graph strides= ( 1 , 1 , 1 ) , val = self.data [ attr ] return mean ( devs_squared , axis=axis , keepdims=keepdims ) for use_cudnn in [ True , False ] : name=self.name ) for loss_tensor in self.losses : assert_allclose ( model.predict ( inputs ) , cudnn_model.predict ( inputs ) , atol=1e-4 ) return unpack_singleton ( values ) return 0.5 * K.square ( quadratic ) + delta * linear output_shape = deserialize_keras_object ( shuffle : Whether to shuffle output samples , layer_test ( convolutional.AveragePooling3D , from tensorflow.keras.constraints import Constraint # keys are based on ids on input tensors and inputs masks . `` `` '' Implements name-based weight loading . assert f [ 'x2 ' ] == u'abcd ' constraint=self.bias_constraint ) if y_pred_rank == 0 and weights_rank == 1 : # old : ( filters , stack_size , kernel_rows , kernel_cols ) # User has provided a list of len = len ( outputs ) . self.recurrent_bias [ self.units : self.units * 2 ] ) def call ( self , y_true , y_pred ) : 'However this machine only has : % s . ' len_dim1 , len_dim2 , len_dim3 , from .core import RepeatVector return wrapper ( getattr ( self.data , attr ) ) kernel_regularizer=keras.regularizers.l1 ( 0.01 ) , self.implementation = implementation This class takes in a sequence of data-points gathered at for the recurrent step from keras_applications import inception_resnet_v2 self._per_output_weighted_metrics [ i ] , is `` large enough '' ( see references for more information ) . shape=None , elif ( y_true_rank - y_pred_rank == 1 ) and ( y_true_shape [ -1 ] == 1 ) : # Augment the mask to match the length of the output . x1 = K.l2_normalize ( x1 , axis=axes [ 0 ] ) class ConvertToStatic ( C.ops.functions.UserFunction ) : assert ( X_train [ 0 ] == X_train [ :1 ] [ 0 ] ) .all ( ) max_queue_size=10 , # Theano implementation of CTC total_loss = None output_shape : a tuple with ( output_row , output_col ) self.moving_mean_initializer = initializers.get ( moving_mean_initializer ) model._feed_targets > > > keras_layer_output = Dense ( 10 ) ( keras_input ) x = C.convolution ( kernel , parallel_model = multi_gpu_model ( model , gpus=8 ) os.remove ( filename ) # Note on using statefulness in RNNs if os.path.exists ( _config_path ) : return tf.size ( x ) follow_links=False , False , c_axis , h_axis , w_axis = 1 , 2 , 3 input_shapes=input_shapes , y_true : tensor ( samples , max_string_length ) containing the truth labels ' Found : ' + str ( self.inputs ) ) prefix_shape = tuple ( prefix_shape ) input1_depth = -1 self.recurrent_initializer = initializers.get ( recurrent_initializer ) [ ( 100 , 100 ) , ( 10 , 20 ) , ( 30 , 80 ) , ( 1 , 2 , 3 , 4 ) ] , if node in nodes_in_progress : accepted_module = [ 'keras.legacy.layers ' , 'keras.utils.generic_utils ' ] if x is None and y is None and steps is None : data : Attributes data . shape=False , y_pred = K.l2_normalize ( y_pred , axis=axis ) padding_all_dims = transpose_shape ( padding_all_dims , total_size = None class ReLU ( Layer ) : inputs = future.get ( timeout=30 ) from .load_backend import ctc_batch_cost # TF uses the last dimension as channel dimension , `` `` '' Computes the Poisson loss between ` y_true ` and ` y_pred ` . if hasattr ( self.layer , 'updates ' ) : Metrics in this list will be logged as-is . 'via both kwarg and inputs list ) ' ) # The mean / var / beta / gamma may be processed by broadcast self._send_sequence ( ) # Share the initial generator layer = new_layer kept_nodes += 1 grads = K.gradients ( loss , params ) while steps_done < steps : callbacks=callbacks ) legacy_upsampling3d_support = generate_legacy_interface ( if py_all ( v.shape.as_list ( ) ) : mask_vals = np.ones ( ( num_samples , num_timesteps ) ) constraint=self.embeddings_constraint , if not has_arg ( self.cell.call , 'constants ' ) : # and set output_tensors ' _keras_history . expected_state [ 0 ] += num_timesteps decoded_dense [ i , : len ( decoded ) ] = decoded if self.histogram_freq and self.merged is None : x , y , sample_weight = generator_output new_layer = keras.layers.GlobalMaxPool2D ( name='global_maxpool2d ' ) from .load_backend import expand_dims namespace = globals ( ) # ( i.e . until the input tensors to the call all exist ) . bias_regularizer='l1 ' , gamma_constraint : Optional constraint for the gamma weight . `` `` '' Returns the current value of the weights of the optimizer . if ( embeddings_freq or embeddings_layer_names or # This step is expensive , so we only run it on variables return cls ( * * config [ 'config ' ] ) ( see usage example below ) . def _postprocess_conv3d_output ( conv_out , x , Either ` x ` or ` alt ` based on the ` training ` flag . occurrences [ n ] += 1 def updates ( self ) : steps_name='steps ' ) : max_val = 2 . * * kwargs ) : will be obtained from ` tensor._keras_history ` . heights = ( y [ : self.num_thresholds - 1 ] + y [ 1 : ] ) / 2 . self.workers = 0 def shape ( x ) : `` `` '' Constructs a new model with ` build_fn ` & fit the model to ` ( x , y ) ` . w = np.flip ( np.fliplr ( np.flipud ( w ) ) , axis=2 ) for _ in range ( 2 , len ( K.int_shape ( mask ) ) ) : updates= [ ( x , x_placeholder + 1 . ) ] , threading . If unspecified , ` use_multiprocessing ` will default to inputs , self.gamma , self.beta , reduction_axes , gamma , ( 'separable_conv2d ' , ( 1 , 7 , 6 , 3 ) , ( 3 , 3 ) , 2 , 'same ' , 'channels_last ' ) , from itertools import compress old_layer = keras.layers.SpatialDropout1D ( p=0.6 , name='sd1d ' ) i = layers.Input ( shape= ( 3 , 2 , 1 ) , dtype='float16 ' ) describing the origin of the ` input_tensors ` , verifying the following : model_config = model_config.encode ( 'utf-8 ' ) def summary ( self , line_length=None , positions=None , print_fn=None ) : K.pow ( K.cast_to_floatx ( 0.96 ) , t * self.schedule_decay ) ) ) if K.is_placeholder ( v ) : result = self.sess.run ( [ self.merged ] , feed_dict=feed_dict ) sample_weight_mode= { 'lstm ' : 'temporal ' } ) [ 2. , 5 . ] , zero_loss = k.stop_gradient ( loss ) tensor_indices.append ( None ) def get_filepath ( self , filename ) : # If input_tensor is set , and batch_input_shape is not set : resample the image if the K.ones_like ( states [ 1 ] ) , [ 0.111121 , 0.588392 , 0.278779 , 0.0055756 , 0.00569609 , 0.010436 ] , ( isinstance ( loss_fn , losses.LossFunctionWrapper ) and assert z.shape == x_shape For best results , ` predictions ` should be distributed approximately uniformly for argument in self.metrics_func.arguments : return C.times ( x , y , len ( y_shape ) - 1 ) include the ` y_col ` column with the class/es of each image . raise NotImplementedError ( x_z = matrix_x [ : , : self.units ] samples from an under-represented class . shape = list ( x._keras_shape ) gamma = k.variable ( gamma_np ) callbacks._call_end_hook ( 'test ' ) reduction=losses_utils.Reduction.SUM , name='mae_1 ' ) should_run_validation ( validation_freq , epoch ) ) : subgraph : whether to return a pydot.Cluster instance . for w , org_w in zip ( new_model.get_weights ( ) , org_weights ) : curve='ROC ' , rate : rate for enforcing the constraint : weights will be metrics += l.metrics x = keras.Input ( ( None , 5 ) ) result._uses_learning_phase = uses_learning_phase On the [ Keras Google group ] ( https : //groups.google.com/forum/ # ! forum/keras-users ) . ( one size per state ) . In this case , the first entry ( ` state_size [ 0 ] ` ) shape : A shape tuple ( integer ) , not including the batch size . class Reshape ( Layer ) : predictions : A tensor of shape ` ( batch_size , classes ) ` and type ` float32 ` . [ 0.0 , 0.0 , 0.4 , 0.6 ] , # t=2 # new gate ( self.true_positives / ( self.true_positives + self.false_positives ) ) , _GLOBAL_CUSTOM_OBJECTS = { } filepath : the path to the file to be overwritten . self.metrics_outputs = [ f.output for f in outputs [ 2 : ] ] xs , labels = _remove_long_seq ( maxlen , xs , labels ) padding=padding , data_format=data_format ) ) raise NotImplementedError ( dev.startswith ( 'cuda ' ) or dev.startswith ( 'gpu ' ) ) ) tf_data_format = 'NCHW ' set ` axis=1 ` in ` BatchNormalization ` . > > > xy = K.dot ( x , y ) raise ValueError ( 'Length of the specified weight list ( ' from .load_backend import eval return T.std ( x , axis=axis , keepdims=keepdims ) save_model ( model , gcs_filepath , overwrite=False ) self.layer.activity_regularizer is not None ) : kernel = C.transpose ( kernel , ( 3 , 2 , 0 , 1 ) ) dim = input_shape [ self.axis ] if go_backwards : or list/tuple/set . If an integer , specifies how many training from keras_applications import xception if data_format='channels_last ' . except : assert ( 10 , ) == kx2.shape A 4d tensor with shape : dummy_x_3d = K.variable ( np.ones ( ( 2 , 3 , 4 , 5 , 4 ) ) ) if len ( y_shape ) > 2 : with open ( fname , 'wb ' ) as f : If you want to modify your dataset between epochs you may implement # We batch weight value assignments in a single backend call # now model.output_shape == ( None , 20 , 16 , 64 ) self._metrics = [ x for x in outputs if hasattr ( x , '_is_metric ' ) ] assert np.array_equal ( x , batch_size=batch_size , # add a new conv1d on top print ( x , message ) This boolean flag determines whether nested_metrics.append ( output_metrics ) warnings.filterwarnings ( 'ignore ' , message=r ' ( .+ ) Keras 2 ' , values of ` predictions [ i ] ` . if self._dynamic_display : cudnn_rnn_layer_class = CuDNNGRU self.best_weights = self.model.get_weights ( ) def __init__ ( self , count_mode='samples ' , ( during training only ) . You can either pass a flat ( 1D ) > > > inputs raise ValueError ( 'Model inputs are already set . ' ) y_pred = K.expand_dims ( y_pred , -1 ) the updates as conditional on these inputs . metrics= [ keras.metrics.BinaryCrossentropy ( ) ] ) input_tensor = Input ( batch_shape=layer.batch_input_shape , Before installing Keras , please install one of its backend engines : TensorFlow , Theano , or CNTK . We recommend the TensorFlow backend . a string specifying the format of the plot : if 'nb_epoch ' in kwargs : from . import training_arrays K.dtype ( self.decay ) ) ) ) if K.backend ( ) == 'theano ' : before declaring the prediction round finished . except : if os.path.exists ( fpath ) : from .training_utils import iter_sequence_infinite ValueError : In case of invalid shape for ` y ` argument . returns any object . output_shape=output_shape , from tensorflow.keras.backend import * # open mpi is needed for cntk Size of the tensor . out_pad_w , smoothing = K.cast_to_floatx ( label_smoothing ) ' elements , but the model has ' auto_padding= [ padding ] ) epoch += 1 x : Numpy array of training data , 'padding ' : self.padding , active_next = T.cast ( T.minimum ( avg = np.mean ( fit/evaluate/predict . for layer in layers : LambdaFunc ( x , incompatible with specifying any stride value ! = 1 . `` `` '' Adds a 1-sized dimension at index `` axis '' . log_probs , mask = ctc_path_probs ( predict , ctc_interleave_blanks ( Y ) ) output_mask = mask if self.return_sequences else None target = T.extra_ops.to_one_hot ( target , nb_class=output.shape [ -1 ] ) model.reset_metrics ( ) # If ranks of all inputs are available , state for the sample of index i in the following batch . 'tensor_indices ' : self.tensor_indices } reason='cntk only supports dilated conv on GPU ' ) variable = K.variable ( init ( shape ) ) if len ( args ) == 5 : v = np.expand_dims ( v , 1 ) is not divisible by the batch size . strides : a tuple of a single integer , 'use the functional API . ' ) label_length : tensor ` ( samples , 1 ) ` containing the sequence length for # on the format ` ( space , input_depth , depth ) ` , input_shape = K.shape ( inputs ) ( ( 3 , 2 , 3 ) , ( 1 , 1 , 0 ) , ( 1 , 1 , 3 ) ) , return C.splice ( * slices , axis=axis ) `` `` '' Cell class for the LSTM layer . x.value = value return variable ( np.eye ( n , m ) , dtype , name ) v = self.momentum * m - lr * g # velocity symmetric cropping values for depth , height , and width : name='sd2d ' ) init ( shape= ( 10 , 5 ) ) huber_loss , name=name , reduction=reduction , delta=delta ) sample_weight=None , def symbolic_fn_wrapper ( * args , * * kwargs ) : super ( Reshape , self ) .__init__ ( * * kwargs ) ( one size per state ) . In this case , the first entry return C.assign ( x , result ) _config = { count_mode='steps ' , config [ 'go_backwards ' ] = not config [ 'go_backwards ' ] if num_constants is not None : There should be ` # classes ` floating point values per feature . x = np.random.random ( ( 1 , 3 ) ) model = Sequential ( ) initial_epoch : Epoch at which to start training if len ( version ) > 2 and version [ 1 ] == ' . ' : alpha_regularizer=None , ` `` channels_last '' ` nor ` `` channels_first '' ` . time.sleep ( 0.1 ) filter_shape [ 0 ] , filter_shape [ 1 ] , filter_shape [ 2 ] ) i , feed_input_shapes = self._feed_input_shapes shape.append ( x._keras_shape [ axis ] ) from tensorflow.keras.applications.mobilenet import preprocess_input init = kwargs.pop ( 'init ' ) if hasattr ( x , '_keras_shape ' ) and hasattr ( y , '_keras_shape ' ) : model.add ( layers.GlobalAveragePooling1D ( ) ) import contextlib if self.append_header : ndim=None , > > > new_arr.dtype return thresholds new_states = return_states def test_global_avgpooling2d_legacy_interface ( ) : write_images : whether to write model weights to visualize as json_log = open ( 'loss_log.json ' , mode='wt ' , buffering=1 ) assert K.get_uid ( ) == first get_output_shape_at ( node_index ) model = _make_nested_model ( input_shape , layer ) from tensorflow.python.keras.saving.hdf5_format import preprocess_weights_for_loading 'optimizer_config ' : { y = T.any ( x , axis=axis , keepdims=keepdims ) from .load_backend import random_uniform_variable 'it should have one entry per model output . ' y_true_step = y_true_step [ 0 : label_length_step [ 0 ] ] # could be chunked input_length = tf.cast ( tf.squeeze ( input_length , axis=-1 ) , tf.int32 ) 'rho ' : float ( K.get_value ( self.rho ) ) , over the baseline . > > > kvar self._is_file = True output , new_states = step_function ( inputs [ i ] , states + constants ) fields = [ `` , `` , `` , connections [ i ] ] layer.backward_layer , inputs : a cntk tensor which has batch axis ( only applicable if ` use_bias ` is ` True ` ) . ( 'stride ' , 'strides ' ) , the initial state of the RNN layer . output_keras_shape = ( x._keras_shape [ 0 ] , return deserialize ( identifier ) name=None , dtype=None , sparse=False , output_shape [ axis ] = None build_map ( x , finished_nodes , nodes_in_progress , layer , out = func ( * args , * * kwargs ) elif is_model ( inbound_layer ) : # Avoid input redundancy . def close ( self ) : if issubclass ( layer.__class__ , Network ) : cols = input_shape [ 4 ] 'class_id ' : self.class_id initializer=init_pool , `` `` '' Generic merge layer for elementwise merge functions . output = self.layer.call ( x , * * kwargs ) cpu_merge : A boolean value to identify whether to force def dropout ( x , level , noise_shape=None , seed=None ) : identified as such ( tn / ( tn + fp ) ) . .format ( batch_size , static_batch_size ) ) from .load_backend import symbolic x.values if x.__class__.__name__ == 'DataFrame ' Total number of steps ( batches of samples ) if any ( [ isinstance ( a , ( list , tuple ) ) for a in axes ] ) : thresholds , default_threshold=default_threshold ) from .callbacks import Callback config = super ( UpSampling2D , self ) .get_config ( ) initial_epoch=0 , while unique_name in weight_names : return ( parameter.kind in ( inspect.Parameter.POSITIONAL_OR_KEYWORD , from tensorflow.keras.activations import sigmoid uid = object_list_uid ( model.inputs ) return last_output , outputs , states shape = ( 1 , bias.shape [ 0 ] ) if len ( y_shape ) == 1 : initializers.Ones ( ) ( ( self.filters , ) , * args , * * kwargs ) , rank=3 , elif callable ( self._output_shape ) : as_numpy=False ) shape = np.arange ( 2 , 2 + ndims ) warnings.warn ( 'EarlyStopping mode % s is unknown , ' fan_out = shape [ 1 ] os.remove ( path ) mode : One of `` fan_in '' , `` fan_out '' , `` fan_avg '' . if weights_rank > values_rank : name = kwargs.get ( 'name ' ) ` predict_proba ` , and ` score ` methods ( e.g. , ` epochs ` , ` batch_size ` ) . `` `` '' Categorical crossentropy between an output tensor and a target tensor . config : dict of the form { 'class_name ' : str , 'config ' : dict } def add_metric ( self , value , name=None ) : x = layers.wrappers.TimeDistributed ( layers.Dense ( 3 , activation='softmax ' ) ) ( x ) specs.append ( None ) 'as model inputs . ' ) top_pad = args [ 1 ] .get ( 'top_pad ' , 0 ) parallel_model = multi_gpu_model ( model , gpus=10 ) class LambdaFunc ( C.ops.functions.UserFunction ) : raise ValueError ( 'The lists ` inputs ` and ` mask ` ' if use_sequence_api : name=_prepare_name ( name , 'constant ' ) ) weight_names = layer_weights [ 'weight_names ' ] By default , we assume that ` y_pred ` encodes a probability distribution . return preds raise AssertionError ( ' { } does not exist'.format ( filepath ) ) raise ValueError ( ' ` cell ` should have a ` call ` method . ' 'the TensorFlow backend . ' ) # User-provided argument validation . batch_size = x.shape [ 0 ] `` `` '' Abstract nD ZeroPadding layer ( private , used as implementation base ) . % ( len ( bias_shape ) , ndim ( x ) ) ) v = metrics [ j ] if isinstance ( x.shape , tuple ) : x = _postprocess_conv2d_output ( x , data_format ) _is_tf_1 ( ) ) : outs_per_batch = [ ] moments = [ K.zeros ( shape , name='moment_ ' + str ( i ) ) y_pred = K.clip ( y_pred , K.epsilon ( ) , 1 ) predictions ( i.e. , above the threshold is ` true ` , below is ` false ` ) . list ( padding [ 0 ] ) , node_data : List of layer configs label_smoothing : ( Optional ) Float in [ 0 , 1 ] . When > 0 , label values are ' not compatible with ' state_size = [ ] a depthwise spatial convolution initial_state=initial_state ) tensor_index , A 3D Numpy array . state_shape = ( input_shape [ 0 ] , self.filters , rows , cols ) ( self.true_positives / nones = _get_dynamic_axis_num ( inputs ) for i in range ( len ( unconcatenated_outs ) ) ] shape = int_shape ( inputs ) def print_row ( fields , positions ) : output = permute_dimensions ( x , [ 0 , 3 , 1 , 2 ] ) padding : int , or tuple of 3 ints , or tuple of 3 tuples of 2 ints . shape= ( self.units , self.units * 3 ) , input_length : tensor ( samples,1 ) containing the sequence length for self.writer = csv.DictWriter ( self.csv_file , loss_functions = [ get_loss_function ( loss ) for _ in range ( len ( output_names ) ) ] Note : the file is temporarily writen to local filesystem before copied to GSC . y_squashed = True name='stack_ ' + str ( np.random.randint ( 1e4 ) ) ) ) tensor_index = self._output_coordinates [ i ] [ 2 ] # Numpy implementation data_format : `` channels_last '' or `` channels_first '' . steps : Total number of steps ( batches of samples ) dummy_x_2d = K.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) # https : //github.com/keras-team/keras/pull/7766 # issuecomment-329195622 if self.nesterov : # add dim to x to have ( batch , input_dim , length , 1 ) h5file [ 'data ' ] = data print_fn ( '= ' * line_length ) n , m = size tuple , or set , specifies the epochs on which to run validation , 'activation layers should be used just like any other ' mask_slice = squeeze ( mask_slice , time_axis ) def DISABLED_test_layer_sharing_at_heterogeneous_depth_order ( ) : ` f ( x ) = alpha * ( x - threshold ) ` otherwise . input_dim , ` on_train_begin ` and ` on_train_end ` expect one positional argument : `` `` '' Returns the index of the maximum value along an axis . name : String metric name . from .merge import Concatenate ` ( batch , channels , padded_rows , padded_cols ) ` base_config = super ( Nadam , self ) .get_config ( ) sample_weight , feed_output_names ) global _LEARNING_PHASE converted = [ ] def eager ( func ) : bias_constraint='max_norm ' , name='d ' ) computed_mask ) 5.811451 , # output beam 0 from .. import utils str ( len ( data ) ) + ' arrays : ' + str ( data ) [ :200 ] + ' ... ' ) dataset [ : ] = val self.state_size = self.units subsample= ( 2 , 2 , 2 ) , truncated_normal = TruncatedNormal def _convert_binary_labels ( ) : dense_layer.add_loss ( lambda : 1 , inputs=None ) pointwise_constraint=None , # Shape : ( num_samples * timesteps , ... ) . And track the if hasattr ( y , '_keras_shape ' ) : For every weight in the layer , a dataset outs = to_list ( outs ) initializers.serialize ( self.recurrent_initializer ) , outs.append ( np.zeros ( shape , dtype=batch_out.dtype ) ) If ` use_bias ` is True , a bias vector is created and added to the outputs . on_batch_end : logs include ` loss ` , and optionally ` acc ` are_different = K.concatenate ( [ label , pred ] , axis=0 ) y_pred = filter_top_k ( y_pred , top_k ) return x / np.sqrt ( y ) self.best = current def trainable_weights ( self ) : self._handle_metrics ( assert deserialized.__code__ == test_func.__code__ x = K.placeholder ( shape=x_shape ) # Update tensor_map . str ( [ y.shape for y in targets ] ) ) # Expand to rank 3 if needed raise ValueError ( 'Invalid merge mode . ' weight_ndim = K.ndim ( weights ) accumulator = initializer `` `` '' Layers that operate regularization via the addition of noise . elif hasattr ( value , 'shape ' ) : if np.isnan ( loss ) or np.isinf ( loss ) : base = 'http : //fashion-mnist.s3-website.eu-central-1.amazonaws.com/ ' def _get_noise_shape ( self , inputs ) : inputs_i = inputs * dp_mask [ 0 ] initial_state = [ initial_state for _ in range ( len ( self.states ) ) ] Much like Adam is essentially RMSprop with momentum , x : Input tensor . with closing ( self.executor_fn ( _SHARED_SEQUENCES ) ) as executor : reload_module ( load_backend ) self.add_loss ( regularizer ( weight ) ) x_squashed_shape = tf.stack ( [ x_shape [ 0 ] , x_squashed_dim , x_shape [ -1 ] ] ) from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2 `` `` '' TensorBoard callback for training visualization . indices can be obtained via the attribute ` class_indices ` . return mobilenet_v2.preprocess_input ( * args , * * kwargs ) use_multiprocessing=use_multiprocessing , self._feed_targets 'time dimension is undefined . \n ' except that values more than two standard deviations from the mean def moving_average_update ( variable , value , momentum ) : from keras.engine.saving import preprocess_weights_for_loading kernel_shape = self.kernel_size + ( input_dim , self.filters ) v_t = ( self.beta_2 * v ) + ( 1 . - self.beta_2 ) * K.square ( g ) `` `` '' Deserializes a layer , then call it on appropriate inputs . import cPickle as pickle 'Found input_spec = ' embeddings_layer_names = [ layer.name for layer in self.model.layers This is recommended in [ Jozefowicz et al . ( 2015 ) ] ( num_samples = input_shape [ 0 ] if input_shape else None new_p = p + self.momentum * v - lr * g config = super ( UpSampling1D , self ) .get_config ( ) self.beta = self.add_weight ( shape=shape , For instance , for a 2D input with shape ` ( batch_size , input_dim ) ` , assert_list_pairwise ( z_list , allclose=assert_value_equality ) 'Layers should have equal number of output tensors ' from tensorflow.keras.activations import linear return tuple ( state_size ) `` `` '' Computes the mean Intersection-Over-Union metric . if 'arguments ' in config : if shape_or_val : x_ndim = K.ndim ( x ) hasattr ( validation_data , '__next__ ' ) or return T.switch ( condition , then_expression , else_expression ) count_mode = 'steps ' kernel = np.transpose ( kernel , ( 2 , 3 , 1 , 0 ) ) data2 = np.random.random ( ( 2 , 3 , 5 ) ) `` `` '' Replacement for ` urlretrieve ` for Python 2 . accept_all : What to return if there is no parameter called ` name ` if pool_mode == 'avg ' : if data_format == 'channels_first ' : str ( x.name ) ) See [ callbacks ] ( /callbacks ) . # Then ` cls ` may be a function returning a class . output_shape , shape = list ( x.shape ) that are used to compute the precision . This value is ultimately returned as self.loss_functions = training_utils.prepare_loss_functions ( from .load_backend import constant output_shape = 'multiple ' 'which does expect integer targets . ' ) states : List of tensors . value : Metric tensor . node = layer._inbound_nodes [ node_index ] layer = created_layers [ layer_data [ 'name ' ] ] output_name = sub_n_first_node [ layer.name ] .get_name ( ) return prefix + '/ ' + name from .. utils.generic_utils import is_all_none X_dset = f.create_dataset ( 'my_data ' , ( 200 , 10 ) , dtype= ' f ' ) # notice that this layer will consume ( 30 * 30 ) * ( 3 * 3 * 3 * 64 ) based on the network 's topology , meaning the architecture `` `` '' Functional interface to the ` Minimum ` layer . @ interfaces.legacy_pooling3d_support def test_dtypes ( ) : node_conversion_map [ node_key ] = kept_nodes parameter = signature.parameters.get ( name ) ' , but you passed the following ' will be used for both dimensions . self.pointwise_kernel , sequential model array ( [ [ 1. , 0. , 1. , 0. , 1. , 0 . ] , optimizer=keras.optimizers.SGD ( lr=0.01 , momentum=0.9 , nesterov=True ) ) input_size = 1 [ 0.458235 , 0.396634 , 0.123377 , 0.00648837 , 0.00903441 , 0.00623107 ] ] , callback.set_model ( model ) batch_size = self.input_spec [ 0 ] .shape [ 0 ] out = model.predict ( input_4 ) value , name=_prepare_name ( name , 'variable ' ) ) from __future__ import division bool : if the current scope device placement would support nchw name = layer.name `` `` '' Type of AUC summation method . ( and on the optional ` initializer ` ) passed as a second argument . if 'GRU ( reset_after=False ) ' in types : x_data = HDF5Matrix ( 'input/file.hdf5 ' , 'data ' ) # Test functions with string inputs . if py_all ( [ hasattr ( tensor , '_keras_shape ' ) for tensor in tensors ] ) : if original_backend is None : recurrent_kernel = np.concatenate ( [ weights [ 1 ] , array ( [ [ 1. , 0. , 0 . ] , if not val.shape : 'Incompatible shapes : ` values ` { } vs ' inbound_node = inbound_layer._inbound_nodes [ node_index ] assert node_index == 0 ` y_true ` and ` # classes ` floating pointing values per example for ` y_pred ` . workers : Integer . Used for generator or ` keras.utils.Sequence ` input check_single_tensor_operation ( 'resize_volumes ' , x_shape , layer = TimeDistributed ( layer ) The amount of output padding along a given dimension must be durations=20 t = k.arange ( test_value ) # x : ( b_size , x1 , ... , d , ... , xn ) `` `` '' Returns whether a tensor is a sparse tensor . `` `` '' Abstract nD convolution layer ( private , used as implementation base ) . new_states : List of tensors , same length and shapes # with the input_spec specified in the layer constructor . accumulator , for instance lambda acc , x : acc + x return _LEARNING_PHASE_CACHE [ id ( lp ) ] `` `` '' Update the value of ` x ` by subtracting ` decrement ` . size : int , or tuple of 3 integers . initial_output = step_function ( inputs [ 0 ] , initial_states + constants ) if self.axes [ i ] < 0 : 'int_shape ' , 'get_variable_shape ' ] ) def cast ( x , dtype ) : out_pad_h = out_pad_w = None kernel , def get_custom_objects ( ) : `` `` '' Accumulates statistics for computing the reduction metric . unroll_options = [ kwargs.pop ( 'unroll ' ) ] If ` name ` and ` index ` are both provided , ` index ` will take precedence . paths.append ( get_file ( fname , def weighted ( y_true , y_pred , weights , mask=None ) : self.bias_i = self.bias [ : self.filters ] subsample=strides , Two : The value of ` initial_state ` should be a tensor or list of additional inputs ) and ` y ` is a numpy array assert loss [ 0 ] == np.inf [ padding [ 1 ] [ 0 ] , padding [ 1 ] [ 1 ] ] , { 'go_backwards ' : False , 'mask ' : None } , Given ` x = y_true - y_pred ` : ( or auto-inferred when using TensorFlow or CNTK ) . 'use TensorBoard . ' ) if enqueuer is not None : you want the dropout mask to be the same for all timesteps , y._keras_shape = ( x._keras_shape [ 0 ] , np.prod ( x._keras_shape [ 1 : ] ) ) nb_worker=1 , pickle_safe=False , max_q_size=3 ) [ ] ) ] old_layer = keras.layers.GlobalMaxPooling3D ( dim_ordering='tf ' , dilation_rate = ( 1 , dilation_rate ) e = K.exp ( x - K.max ( x , axis=axis , keepdims=True ) ) recurrent_regularizer=recurrent_regularizer , `` { } the documentation should be on the first line . `` .format ( name ) , For instance , if the model has 2 outputs , and for the first output Shape tuples can include None for free dimensions , if len ( weights ) == 9 : def function ( inputs , outputs , updates=None , * * kwargs ) : channel_shift_range=channel_shift_range , kernel_size = ( kwargs.pop ( 'nb_row ' ) , kwargs.pop ( 'nb_col ' ) ) return np.min ( x , axis=axis , keepdims=keepdims ) super ( Network , self ) .__setattr__ ( name , value ) updates = [ ] layer.forward_layer.add_loss ( 1 , inputs=None ) # Shape : ( batch , filters , output_length ) one_indexed_epoch = epoch + 1 assert o._keras_shape == ( None , 4 , 5 ) from .load_backend import conv3d filename : where to store the retrieved data locally . padding='valid ' , self.output_padding , 2 , 'output_padding ' ) def __init__ ( self , l1=0. , l2=0. , * * kwargs ) : x_train , labels_train = f [ 'x_train ' ] , f [ 'y_train ' ] output = K.pool2d ( inputs , pool_size , strides , # in reverse cell order . accumulators = [ K.zeros ( K.int_shape ( p ) , cells = [ MinimalRNNCell ( 32 ) , MinimalRNNCell ( 64 ) ] for i in range ( len ( space ) ) : ` ( batch , channels , dim1 , dim2 , dim3 ) ` A padded 3D tensor . `` `` '' Converts a PIL Image instance to a Numpy array . recurrent_initializer='glorot_uniform ' , assert dot.get_node ( inbound_layer_id ) update = g * K.sqrt ( d_a + self.epsilon ) / K.sqrt ( new_a + self.epsilon ) Repeats the rows and columns of the data self.return_state = layer.return_state ` output_shape ` = ` ( 100 , 30 ) ` save_function : A function that takes a ` h5py.File ` , writes to it and > > > new_arr = K.cast_to_floatx ( arr ) self._feed_input_names.append ( layer.name ) shape2 = input_shape [ 1 ] 'other than an ` InputLayer ` . ' self.input_spec = input_spec + state_spec from .core import Dropout return K.sparse_categorical_crossentropy ( h_tm1_o = h_tm1 x.dimshuffle ( shuffle_pattern ) , self.depthwise_kernel = self.add_weight ( name=None ) : 1 , - ( mask_last_num_timesteps + 1 ) ] for x , s in zip ( output_tensors , shapes ) : [ Understanding the difficulty of training deep feedforward neural > > > K.image_data_format ( ) A Jupyter notebook Image object if Jupyter is installed . strides : strides integer . verbose=verbose , model = Sequential ( ) has_seq = False categorical_hinge , name=name , reduction=reduction ) from .common import symbolic , eager application/json . Otherwise the serialized JSON will be send within a form save_format : One of `` png '' , `` jpeg '' self.recurrent_bias_z = self.recurrent_bias [ : self.units ] `` `` '' Retrieves the input tensor ( s ) of a layer . callbacks : List of ` keras.callbacks.Callback ` instances . layer ( unpack_singleton ( input_tensors ) , * * kwargs ) class CosineSimilarity ( MeanMetricWrapper ) : def __iter__ ( self ) : `` `` '' Returns a tensor with normal distribution of values . 'momentum ' : self.momentum , channel_axis = -1 You can now iterate on your training data in batches : set_w = set_of_lengths ( weights ) strides= ( 1 , 1 ) , tuples.append ( ( sw , w ) ) ( -1 , input_length ) , output_mask , 1 , output_mask_int_shape [ 1 : ] ) old_layer = keras.layers.Convolution3D ( 5 , 3 , 3 , 4 , name=model.name ) while ` channels_first ` corresponds to assert_allclose ( K.eval ( last_states [ 0 ] ) , expected_state ) e.g . ` input_shape= ( 128 , 128 , 3 ) ` for 128x128 RGB pictures def test_ctc_decode_beam_search ( self ) : monitored has stopped increasing ; in ` auto ` inputs = keras.Input ( ( timesteps , input_size ) ) `` `` '' 1D convolution layer ( e.g . temporal convolution ) . __version__ = ' 2.3.1 ' ( see [ activations ] ( .. /activations.md ) ) , elif self.merge_mode == 'mul ' : # we should use that depth instead of the node depth . weights2 = preprocess_weights_for_loading ( layer , weights1 ) new_layer = keras.layers.GaussianNoise ( stddev=0.5 , name='gn ' ) self.run_thread = threading.Thread ( target=self._run ) tensor_map [ str ( id ( x ) ) ] = ( y , mask ) 'adamax ' : Adamax , if end is None : return tf_state_ops.assign_sub ( x , decrement ) _runner ( initializers.orthogonal ( ) , tensor_shape , from .recurrent import GRUCell raise ValueError ( 'When passing a list as sample_weight_mode , ' [ 0.24082 , 0.397533 , 0.0557226 , 0.0546814 , 0.0557528 , 0.19549 ] , * * kwargs : Additional keyword arguments summary_value.simple_value = value cosine similarity = ( a . b ) / ||a|| ||b|| if constants : super ( SpecificityAtSensitivity , self ) .__init__ ( def squared_hinge ( y_true , y_pred ) : a_y = np.random.random ( ( num_samples , output_dim_a ) ) except ( UnicodeEncodeError , binascii.Error , ValueError ) : 'argument . ' ) batch_input_shape , _ = get_input_shape_and_dtype ( layer ) ( 'conv3d ' , ( 1 , 2 , 2 , 2 , 1 ) , ( 2 , 2 , 2 , 1 , 1 ) , 'valid ' , 'channels_last ' ) , self.writer.flush ( ) ( 1 . - K.pow ( self.beta_1 , t ) ) ) rng = np.random.RandomState ( seed ) return model_config batches have been seen by the model . For example , if the incoming feature maps if not proceed : def _pooling_function ( self , inputs , pool_size , strides , for node in layer._inbound_nodes : f [ ' y ' ] = [ b'efg ' , b'hij ' , b'klmn ' ] params_value = keras.backend.get_value ( params ) layer_name , node_index , tensor_index = layer_data `` `` '' Method called at the end of every epoch . In the case of temporal data , you can pass a 2D array x = np.random.random ( ( 100 , 4 ) ) return T.tanh ( x ) Warning : This class should not be used directly . depthwise_kernel = C.reshape ( C.transpose ( depthwise_kernel , ( 1 , 0 , 2 , 3 ) ) , if distribution not in { 'normal ' , 'uniform ' } : save_to_binary_h5py ( save_function , file_like ) rnn_inputs = inputs if input_tensor is None : Tensor with one scalar loss entry per sample . outputs = self.predict_function ( ins ) # ( even though the model itself did n't change ) return ( input_shape [ 0 ] , input_shape [ 1 ] , rows , cols ) out = model.evaluate ( input_a_np , [ output_a_np ] ) i += 1 if len ( y.shape ) == 2 and y.shape [ 1 ] > 1 : h = K.dot ( inputs * dp_mask , self.kernel ) sequence . def call ( self , inputs ) : generator = args [ 1 ] optimizer='rmsprop ' ) for i in range ( num_batches ) ] def get_losses_for ( self , inputs ) : return super ( Metric , self ) .add_weight ( model.add ( LSTM ( 16 ) ) mode it will stop when the quantity kwargs = { } depth ) ` def __init__ ( self , target , width=30 , verbose=1 , interval=0.05 , dim_ordering='tf ' , ret , _ = theano.scan ( ` batch_size ` is ` None ` , returns ` None ` . W_regularizer='l1 ' , from .load_backend import name_scope rnn_constants.append ( constant ) ` None ` , no targets are returned ( the generator will only yield strides=dilation_rate [ 0 ] , from tensorflow.keras.layers import Bidirectional for pv , p , w in zip ( param_values , params , weights ) : from .load_backend import get_session ` loss_weights ` coefficients . If a list , it is expected to have a 1:1 `` pay more attention '' to samples strides = ( strides , 1 ) args = args [ : -1 ] data_format=None , dilation_rate=1 ) : validate_filenames : Boolean , whether to validate image filenames in padding=padding , data_format=data_format ) super ( ReLU , self ) .__init__ ( * * kwargs ) output_shapes = self._output_shape_cache [ cache_key ] ignore_border=True , def preprocess_input ( self , inputs , training=None ) : result_fn : function that computes the metric result . out = func ( * args , * * kwargs ) input_shape [ 1 : ] , self.target_shape ) `` `` '' Finds non-specific dimensions in the static shapes if not self.losses : for i in range ( a0 , x_ndim - 1 ) : return TFOptimizer ( identifier ) dtype = 'float32 ' if 'int ' in str ( dtype ) else dtype model.add ( Dense ( 1 , activation='sigmoid ' ) ) from .. utils.layer_utils import count_params self.bias_z_i , from .. utils.generic_utils import transpose_shape name=name , batch_size=batch_size , current - self._seen_so_far ] target : A tensor of the same shape as ` output ` . `` `` '' Retrieves the input tensor ( s ) of a layer at a given node . log_dir=log_dir , def test_zeropadding3d_legacy_interface ( ) : self.layer.trainable = value if n not in occurrences : the shape of ` sample_weight ` matches the shape of ` y_pred ` , then the # Remove the current backend from the coverage exclusion . if axis < 0 : n , m = size , size y_true_shape = K.int_shape ( y_true ) self._trainable_weights = weights output_shape=output_shape ) non picklable arguments to the generator return self.fit_generator ( 'stateful ' : self.stateful } def metrics ( self ) : interpolation='bilinear ' ) if 'kernel_dim3 ' in kwargs : self.loss_weights_list = training_utils.prepare_loss_weights ( _check_pydot ( ) computable_tensors.append ( x ) if self.unit_forget_bias : axis_2 = 2 if _backend : outs = to_list ( outs ) self._values [ k ] [ 1 ] += ( current - self._seen_so_far ) 'its batch size . Specify the batch size ' padding , data_format , total_loss += loss_weight * output_loss x_r = np.array ( [ 0 , 2 , 2 , 3 ] , dtype=np.int64 ) return super ( RNN , self ) .__call__ ( inputs , * * kwargs ) if known == 0 or original % known ! = 0 : check_two_tensor_operation ( 'batch_dot ' , ( 4 , 2 , 3 ) , ( 4 , 3 ) , output_dim = state_size [ 0 ] x_np = random ( x_shape ) update_fp = ConfusionMatrix.FALSE_POSITIVES in variables_to_update expected_last_state = initial_state_vals.copy ( ) Encoder-Decoder Approaches ] ( https : //arxiv.org/abs/1409.1259 ) y_true = K.constant ( [ [ 1. , 9 . ] , [ 2. , 5 . ] ] ) const = C.constant ( np_value , base_config = super ( Recurrent , self ) .get_config ( ) model.build ( ( None , 500 ) ) `` `` '' Serialize any object to a JSON-serializable structure . `` `` '' Internal utility for parsing CIFAR data . return self.callback_model final_output , final_states = _recurrence ( rnn_inputs , states , mask ) datas = enqueuer.get ( ) assert y._keras_shape == K.int_shape ( y ) mock_module.FileIO = self.FileIO def generate_legacy_interface ( allowed_positional_args=None , model.load_weights ( fname , by_name=True , reshape=False ) raise ValueError ( 'Axis ' + str ( self.axis ) + ' of ' self.queue.not_full.notify ( ) # ` keras.callbacks.CallbackList ` from .merge import average print_fn ( _LOCAL_DEVICES = None def assert_doc_style ( name , member , doc ) : stream : Any file-like object implementing the method ` write ` that accepts _test_optimizer ( optimizers.Adadelta ( ) , target=0.6 ) ' ( got ' + str ( input_shape [ 1 : ] ) + ' ) . ' `` `` '' Wrapper for using the Scikit-Learn API with Keras models . # batch_size length vector of sequence_lengths [ 0.199959 , 0.489485 , 0.0233221 , 0.0251417 , 0.0233289 , 0.238763 ] , node = pydot.Node ( layer_id , label=label ) val_gen = ( hasattr ( validation_data , 'next ' ) or changed due to padding . Specified by ` output_shape ` argument will be displayed as-is . All others will be averaged ( or list of input shape tuples , one tuple per input tensor ) . # If tensor comes from an input layer : cache the input layer . # This assumes that your machine has 8 available GPUs . initial_state = K.zeros_like ( inputs ) # ( samples , timesteps , input_dim ) ' ` Parameter ` . ' % type ( x ) ) ` on_epoch_begin ` and ` on_epoch_end ` expect two positional arguments : from .. import metrics as metrics_module shape [ -1 ] * = 2 self.states = [ K.zeros ( ( batch_size , self.units ) ) decrease . In this case , SpatialDropout3D will help promote independence str ( sample_weight.shape ) + ' . ' * ` SUM_OVER_BATCH_SIZE ` : Scalar ` SUM ` of weighted values divided by if steps is not None : where intercept = TP_A - slope * P_A = TP_B - slope * P_B , resulting in if i == len ( layers ) - 1 : x_dense_2 = x_sparse_2.toarray ( ) for callback in self.callbacks : embeddings_freq : frequency ( in epochs ) at which selected embedding `` `` '' Softsign activation function . values : List of tuples : y = np.random.random ( ( num_samples , output_dim ) ) # the ` embeddings_data ` explicitly . This design allows to pass ` fn ( y_true , y_pred , weights , mask ) ` . successive_states.append ( states ) x : List of tensors . return ( input_shape [ 0 ] , np.prod ( input_shape [ 1 : ] ) ) With default values , it returns element-wise ` max ( x , 0 ) ` . K.is_tensor ( v ) for v in x ) : `` `` '' Global max pooling operation for spatial data . x = tf.nn.convolution ( def test_spatial_3d_padding ( self ) : # scale preds so that the class probas of each sample sum to 1 If mask_zero is set to True , as a consequence , index 0 can not be with tf.device ( x.device ) : signature += name + '= ' if len ( data ) ! = len ( names ) : write_graph : whether to visualize the graph in TensorBoard . if self.verbose and self.seen < self.target : cosine = cosine_similarity = cosine_proximity return T.nnet.softplus ( x ) for argument in self.loss.arguments : an ` input_spec ` of length ` n ` . dtype : Dtype to use for the returned array . converted.append ( ( 'output_shape ' , None ) ) likely be added back later ) . use an [ Embedding ] ( embeddings.md ) layer with the ` mask_zero ` parameter # Enable line length testing with maximum line length of 85 data_format=data_format , return K.relu ( inputs , alpha=self.alpha ) beta_init='one ' , return vgg19.decode_predictions ( * args , * * kwargs ) layer = self._input_layers [ i ] axes : list ( or single ) int with target dimensions x = tf.nn.max_pool ( x , pool_size , strides , elif dtype == 'float64 ' : warnings.warn ( ' ` wait_time ` is not used anymore . ' , def hard_sigmoid ( x ) : return os.path.exists ( filename ) input_size ) ) ) # so it may have extra axes with 1 , loss_and_metrics = model.evaluate ( x_test , y_test , batch_size=128 ) legacy_gaussiandropout_support = generate_legacy_interface ( 'rate to % s . ' % ( epoch + 1 , lr ) ) cache_subdir=dirname ) ) output names ( strings ) to scalar coefficients . chunk_id += 1 new_cols = ( ( cols - 1 ) * strides [ 2 ] + kernel_size [ 2 ] model.add ( Dense ( 32 ) ) config = super ( DepthwiseConv2D , self ) .get_config ( ) on each output by passing a dictionary or a list of losses . fp_rate = K.switch ( else def get_word_index ( path='reuters_word_index.json ' ) : kwargs [ 'mask ' ] = mask if hasattr ( tf , 'compat ' ) and hasattr ( tf.compat , 'v1 ' ) : inputs_o = inputs * dp_mask [ 3 ] `` `` '' Softplus activation function . from .load_backend import control_dependencies 'same ' , 'channels_first ' , ( 2 , 2 ) ) , raise ValueError ( 'Found a sample_weight array with shape ' source_tensors_ids.add ( id ( x ) ) `` `` '' Computes total loss from loss functions . return np.ones ( ( y.shape [ 0 ] , ) , dtype=K.floatx ( ) ) config = { 'rate ' : self.rate , if ( self.curve == metrics_utils.AUCCurve.PR and inputs = to_list ( inputs ) # repeat each slice the given number of reps raise ValueError ( 'Layer does not accept initial_state argument . ' ) axis or axes along which to take the dot product . x_test , y_test = load_batch ( fpath , label_key=label_mode + '_labels ' ) `` `` '' Calls the ` on_test_batch_end ` methods of its callbacks . # Case : symbolic-mode graph network . assert np.min ( rand ) == 0 @ pytest.mark.parametrize ( 'training ' , [ True , False ] ) self.on_epoch_end = on_epoch_end @ pytest.mark.skipif ( sys.version_info < ( 3 , 3 ) , reason= '' requires python3.3 '' ) y_pred : Predicted output . strides=self.strides + ( 1 , ) , output_shape= ( 6 , 7 , 5 ) , def temp_filename ( suffix ) : layer : layer object # self.updates batch_val = [ x [ i : i + step ] for x in val_data ] axis : axis along which to perform normalization . self.depthwise_constraint = constraints.get ( depthwise_constraint ) bias_regularizer=bias_regularizer , def batch_get_value ( ops ) : 'output_ % d ' % ( i + 1 ) for i in range ( len ( self.outputs ) ) ] if has_arg ( self.call , 'mask ' ) : are predefined constants . The values of ` alpha ` and ` scale ` are ' ( temporal data ) . ' ) label_smoothing=0 , # test with variables legacy_get_updates_support = generate_legacy_interface ( self.state_spec = [ InputSpec ( shape= ( None , dim ) ) if K.ndim ( y_true ) == K.ndim ( y_pred ) : if z_shape is not None : return K.exp ( x ) if K.backend ( ) == 'tensorflow ' : def parse_init_thresholds ( thresholds , default_threshold=0.5 ) : ( see [ initializers ] ( .. /initializers.md ) ) . return np.transpose ( x , pattern ) self.padding , self.strides [ 0 ] ) # cases need to call the layer.compute_mask when input_mask is None : if output_shape [ self.axis ] is None or shape [ self.axis ] is None : target_mean=0. , target_max=0 . ) into account in the topological ordering , so adding or not provided . > > > input_ph # allow cell ( if layer ) to build before we set or validate state_spec return mobilenet_v2.MobileNetV2 ( * args , * * kwargs ) initializer='uniform ' , conv3d_transpose = conv_transpose return iter ( self.callbacks ) data provided . `` `` '' Locally-connected layer for 2D inputs . weights = [ tf.reshape ( x , ( -1 , ) ) for x in weights ] return x ( ) assert q._keras_shape == ( None , 64 ) from_logits=from_logits , ' ` tensor = keras.layers.Input ( shape ) ` .\n ' custom_objects=None , kwargs [ 'input_shape ' ] = ( input_length , ) dtp / dp , new_node_index = node_conversion_map.get ( ' layer can not be specified with a mix of ' if k < 1 : or in the case of temporal data , raise ValueError ( 'If predicting from data tensors , ' if regularizer is not None : ` ( batch , depth , height , width , channels ) ` while ` `` channels_first '' ` _callbacks.append ( elif y_pred_rank - weights_rank == 1 : supports_masking : Boolean indicator of whether the layer `` ` reduction=Reduction.NONE ) This version performs the same function as Dropout , however it drops raise ValueError ( 'Only provide the input_shape OR ' y_true , y_pred , from_logits=from_logits , axis=axis ) # now model.output_shape == ( None , 32 ) return self._merge_function ( inputs ) an implementation that uses fewer , larger matrix products , As it is a regularization layer , it is only active at training time . ' ; y= ' + str ( y ) ) self.kernel_c = self.kernel [ : , : , : , self.filters * 2 : self.filters * 3 ] ' ` data_format ` must be `` channels_last '' ' self.m_schedule = K.variable ( 1. , name='m_schedule ' ) if name is not None : 'rho ' : self.rho , # We 've already covered the input layers batch_size = static_batch_size name='conv ' ) from keras_applications import mobilenet __cell__ : A RNN cell instance . A RNN cell is a class that has : about metadata files format . In case if the same metadata file is if hasattr ( self.forward_layer , 'losses ' ) : import scipy.signal as signal get a specific one . A single Sequence would cause the validation to return tf.foldr ( fn , elems , initializer=initializer , name=name ) name : Optional name for the op . `` `` '' Instantiates an all-zeros variable and returns it . # Fix shape representation kernel = tf.zeros ( tuple ( shape ) ) `` `` '' Permutes axes in a tensor . specifying the amount of padding along the height and width name='sd2d ' ) def _maybe_convert_labels ( y_true ) : metrics_updates.extend ( m.updates ) raise ValueError ( 'You called ` set_weights ( weights ) ` on layer `` ' sample_weight= [ sample_weight [ 1 ] , with patch ( 'keras.engine.saving.ask_to_proceed_with_overwrite ' ) as ask : output_shape_keys = [ ] # simplified version of TensorFlow 's test x_shape = x_shape * reps except Exception : if isinstance ( a , C.Axis ) is False : shape1 = input_shape [ 0 ] ` keras.utils.Sequence ` input only . If ` True ` , use process-based self.summation_method = metrics_utils.AUCSummationMethod.from_str ( if on_batch_begin is not None : elif isinstance ( key , ( int , np.integer ) ) : input_length=input_length ) if target_class in [ 'GRU ' , 'CuDNNGRU ' ] and len ( weights ) == 3 : def _call_end_hook ( self , mode ) : chunk_attr = ' % s % d ' % ( attr , chunk_id ) weights ( list of variables ) [ A guide to convolution arithmetic for deep learning ] ( layer.output ) each batch item in y_pred # CNTK expects ` ( depth , input_depth , rows , cols ) ` . for inbound_layer in node.inbound_layers : U = C.constant ( np.triu ( np.ones ( ( dim , dim ) ) ) .astype ( x.dtype ) ) slices = tuple ( slices ) sign = np.sign return resnet.decode_predictions ( * args , * * kwargs ) merge_repeated=False ) : def test_specify_initial_state_keras_tensor ( ) : 'should be float . ' ) # ` loss_weights ` does not match output_names . return reshape ( accumulator , shape ( initializer ) [ 1 : ] ) tensor_index = node.tensor_indices [ i ] bar += ' > ' embeddings_regularizer=None , serialized = func_dump ( test_func ) keyword : argument of ` RNN.__call__ ` ( as well as ` RNN.call ` ) method . unroll=True , self.kernel_size = conv_utils.normalize_tuple ( kernel_size , rank , class MeanSquaredLogarithmicError ( LossFunctionWrapper ) : label = K.expand_dims ( label , 0 ) check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , WITH_NP , axes= ( 0 , -1 ) ) Exponential activation : ` exp ( x ) ` . def cast_to_floatx ( x ) : session.run ( tf.variables_initializer ( uninitialized_vars ) ) A gradients tensor . boolean_mask = K.any ( K.not_equal ( inputs , self.mask_value ) , 'alpha_constraint ' : constraints.serialize ( self.alpha_constraint ) , if not isinstance ( input_shape , list ) or len ( input_shape ) ! = 2 : def make_batches ( size , batch_size ) : self.inputs [ 0 ] .dynamic_axes ) ] y = K.placeholder ( ndim=len ( y_shape ) ) y = K.reshape ( y , output_shape ) _LEARNING_PHASE = -1 config = { 'merge_mode ' : self.merge_mode } value is computed and used to evaluate the corresponding specificity . matches , sample_weight=sample_weight ) e.g . ` validation_freq= [ 1 , 2 , 10 ] ` runs validation at the end If set to 0 , the RNN will use mask_shape = int_shape ( mask ) for i in range ( len ( params ) ) ] root='http : //localhost:9000 ' , 'Conv3D ' , ins_batch = slice_arrays ( ins , batch_ids ) # subtract the sets to pick all missing classes # sub_n : submodel_not_wrapper raise ValueError ( 'Input tensors to a ' + cls_name + ' ' for dim in self.cell.state_size ] self.end_of_epoch_signal = threading.Event ( ) class Identity ( Initializer ) : getattr ( y_rev , '_uses_learning_phase ' , False ) ) : y_trail_dims = y_shape [ 2 : ] check_single_tensor_operation ( 'std ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) # ( creating a model replica on the target device ) . `` `` '' Retrieves the input shape tuple ( s ) of a layer . grads = [ ] check_single_tensor_operation ( 'cumsum ' , ( 4 , 2 ) , WITH_NP , axis=1 ) kernel = kernel.dimshuffle ( ( 4 , 3 , 0 , 1 , 2 ) ) TypeError : if ` obj ` can not be serialized . from .load_backend import set_learning_phase num_classes = 10","['.travis.yml', '.travis/install_cntk.sh', 'README.md', 'examples/variational_autoencoder.py', 'keras/__init__.py', 'keras/activations.py', 'keras/applications/__init__.py', 'keras/applications/densenet.py', 'keras/applications/imagenet_utils.py', 'keras/applications/inception_resnet_v2.py', 'keras/applications/inception_v3.py', 'keras/applications/mobilenet.py', 'keras/applications/mobilenet_v2.py', 'keras/applications/nasnet.py', 'keras/applications/resnet.py', 'keras/applications/resnet50.py', 'keras/applications/resnet_v2.py', 'keras/applications/vgg16.py', 'keras/applications/vgg19.py', 'keras/applications/xception.py', 'keras/backend.py', 'keras/backend/__init__.py', 'keras/backend/cntk_backend.py', 'keras/backend/common.py', 'keras/backend/load_backend.py', 'keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/callbacks.py', 'keras/callbacks/__init__.py', 'keras/callbacks/callbacks.py', 'keras/callbacks/tensorboard_v1.py', 'keras/callbacks/tensorboard_v2.py', 'keras/constraints.py', 'keras/datasets/__init__.py', 'keras/datasets/boston_housing.py', 'keras/datasets/cifar.py', 'keras/datasets/cifar10.py', 'keras/datasets/cifar100.py', 'keras/datasets/fashion_mnist.py', 'keras/datasets/imdb.py', 'keras/datasets/mnist.py', 'keras/datasets/reuters.py', 'keras/engine/base_layer.py', 'keras/engine/input_layer.py', 'keras/engine/network.py', 'keras/engine/saving.py', 'keras/engine/sequential.py', 'keras/engine/topology.py', 'keras/engine/training.py', 'keras/engine/training_arrays.py', 'keras/engine/training_generator.py', 'keras/engine/training_utils.py', 'keras/initializers.py', 'keras/layers/__init__.py', 'keras/layers/advanced_activations.py', 'keras/layers/convolutional.py', 'keras/layers/convolutional_recurrent.py', 'keras/layers/core.py', 'keras/layers/cudnn_recurrent.py', 'keras/layers/embeddings.py', 'keras/layers/experimental/__init__.py', 'keras/layers/experimental/preprocessing/__init__.py', 'keras/layers/local.py', 'keras/layers/merge.py', 'keras/layers/noise.py', 'keras/layers/normalization.py', 'keras/layers/pooling.py', 'keras/layers/recurrent.py', 'keras/layers/wrappers.py', 'keras/legacy/__init__.py', 'keras/legacy/interfaces.py', 'keras/legacy/layers.py', 'keras/losses.py', 'keras/metrics.py', 'keras/models.py', 'keras/optimizers.py', 'keras/optimizers/__init__.py', 'keras/optimizers/schedules/__init__.py', 'keras/preprocessing/__init__.py', 'keras/preprocessing/image.py', 'keras/preprocessing/sequence.py', 'keras/preprocessing/text.py', 'keras/regularizers.py', 'keras/utils/__init__.py', 'keras/utils/conv_utils.py', 'keras/utils/data_utils.py', 'keras/utils/generic_utils.py', 'keras/utils/io_utils.py', 'keras/utils/layer_utils.py', 'keras/utils/losses_utils.py', 'keras/utils/metrics_utils.py', 'keras/utils/multi_gpu_utils.py', 'keras/utils/np_utils.py', 'keras/utils/test_utils.py', 'keras/utils/vis_utils.py', 'keras/wrappers/__init__.py', 'keras/wrappers/scikit_learn.py', 'pytest.ini', 'setup.py', 'tests/docs/test_doc_auto_generation.py', 'tests/docs/test_documentation.py', 'tests/integration_tests/preprocessing/sequence_test.py', 'tests/integration_tests/test_image_data_tasks.py', 'tests/integration_tests/test_temporal_data_tasks.py', 'tests/keras/activations_test.py', 'tests/keras/backend/backend_test.py', 'tests/keras/callbacks/callbacks_test.py', 'tests/keras/callbacks/tensorboard_test.py', 'tests/keras/datasets/datasets_test.py', 'tests/keras/engine/test_topology.py', 'tests/keras/engine/test_training.py', 'tests/keras/initializers_test.py', 'tests/keras/layers/advanced_activations_test.py', 'tests/keras/layers/convolutional_recurrent_test.py', 'tests/keras/layers/convolutional_test.py', 'tests/keras/layers/core_test.py', 'tests/keras/layers/cudnn_recurrent_test.py', 'tests/keras/layers/embeddings_test.py', 'tests/keras/layers/merge_test.py', 'tests/keras/layers/normalization_test.py', 'tests/keras/layers/pooling_test.py', 'tests/keras/layers/recurrent_test.py', 'tests/keras/layers/wrappers_test.py', 'tests/keras/legacy/conftest.py', 'tests/keras/legacy/interface_test.py', 'tests/keras/legacy/layers_test.py', 'tests/keras/losses_test.py', 'tests/keras/metrics_confusion_matrix_test.py', 'tests/keras/metrics_correctness_test.py', 'tests/keras/metrics_functional_test.py', 'tests/keras/metrics_training_test.py', 'tests/keras/optimizers_test.py', 'tests/keras/regularizers_test.py', 'tests/keras/test_sequential_model.py', 'tests/keras/utils/conv_utils_test.py', 'tests/keras/utils/data_utils_test.py', 'tests/keras/utils/generic_utils_test.py', 'tests/keras/utils/io_utils_test.py', 'tests/keras/utils/layer_utils_test.py', 'tests/keras/utils/multi_gpu_test.py', 'tests/keras/wrappers/scikit_learn_test.py', 'tests/test_loss_masking.py', 'tests/test_model_pickling.py', 'tests/test_model_saving.py']",Redirect Keras to tf.keras ( # 14121 )
1,e8946d5240f3b18528d4d34668ee615907879953,2019-10-24 21:52:18+02:00,`` `` '' Normalizes a Numpy array . x : Numpy array to normalize . `` `` '' Normalizes a NumPy array . x : NumPy array to normalize .,['keras/utils/np_utils.py'],Update np_utils.py ( # 13481 )
2,ecac367b2372b5f2326fcd3ddd11718323427f4e,2019-10-22 16:14:39+02:00,"name x = Activation ( 'relu ' , name='conv1 ' ) ( x ) model.load_weights ( p ) # Sort model layers by layer name to ensure that group names are strictly `` `` '' model = Model ( inputs=input_layer , outputs=x ) for layer in sorted ( layers , key=lambda x : x.name ) : def test_saving_group_naming_h5py ( tmpdir ) : for layer in layers : @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , reason='requires tensorflow ' ) from keras.layers import Conv2D , Flatten # growing to avoid prefix issues . x = Conv2D ( 1 , 1 , name='conv1/conv ' ) ( input_layer ) from keras.layers import Conv2D , Flatten , Activation p = tmpdir.mkdir ( `` test '' ) .join ( `` test.h5 '' ) model.save_weights ( p ) `` `` '' Test saving model with layer which name is prefix to a previous layer input_layer = Input ( ( None , None , 3 ) , name='test_input ' )","['keras/engine/saving.py', 'tests/keras/metrics_training_test.py', 'tests/test_model_saving.py']",Fix h5py group naming while model saving ( # 13477 )
3,4d59675b65b6733e525286bbd512e9d5a42f3a22,2019-10-21 15:46:17+02:00,"` channels_last ` corresponds to inputs with shape one of ` channels_last ` ( default ) or ` channels_first ` . In 'channels_first ' mode , the channels dimension ` ( batch , ... , channels ) ` while ` 'channels_first ' ` corresponds to If you never set it , then it will be `` channels_last '' . If you never set it , then it will be ` 'channels_last ' ` . ` ( samples , dim1 , dim2 , dim3 , channels ) ` if data_format='channels_last ' . In ` 'channels_first ' ` mode , the channels dimension ( the depth ) ` 'channels_last ' ` corresponds to inputs with shape ` ( samples , channels , dim1 , dim2 , dim3 ) ` if ` data_format='channels_first ' ` ` ( samples , dim1 , dim2 , dim3 , channels ) ` if ` data_format='channels_last ' ` . is at index 1 , in ` 'channels_last ' ` mode is it at index 4 . ` ( samples , rows , cols , channels ) ` if ` data_format='channels_last ' ` . data_format : 'channels_first ' or 'channels_last ' . ` ( samples , channels , rows , cols ) ` if ` data_format='channels_first ' ` in 'channels_last ' mode is it at index 3 . ` ( samples , channels , dim1 , dim2 , dim3 ) ` if data_format='channels_first ' one of ` 'channels_last ' ` ( default ) or ` 'channels_first ' ` . In 'channels_first ' mode , the channels dimension ( the depth ) is at index 1 , in 'channels_last ' mode is it at index 4 . In ` 'channels_first ' ` mode , the channels dimension ` ( batch , ... , channels ) ` while ` channels_first ` corresponds to in ` 'channels_last ' ` mode is it at index 3 . ` ( samples , rows , cols , channels ) ` if data_format='channels_last ' . data_format : ` 'channels_first ' ` or ` 'channels_last ' ` . ` ( samples , channels , rows , cols ) ` if data_format='channels_first '",['keras/layers/core.py'],Update core.py ( # 13472 )
4,afff7b4326f380a54c73400d1e2ae03890162bdf,2019-10-21 15:43:46+02:00,"If None , it will default to ` pool_size ` . strides : tuple of 3 integers , or None . Strides values . while ` channels_first ` corresponds to inputs with shape ` channels_last ` corresponds to inputs with shape one of ` channels_last ` ( default ) or ` channels_first ` . ` ( batch , height , width , channels ) ` while ` `` channels_first '' ` pool_size : tuple of 3 integers , If you never set it , then it will be `` channels_last '' . If you never set it , then it will be ` `` channels_last '' ` . strides : Integer , tuple of 3 integers , or None . Strides values . pool_size : Integer or tuple of 2 integers , ` ( batch , height , width , channels ) ` while ` channels_first ` while ` `` channels_first '' ` corresponds to inputs with shape ` ( batch , steps , features ) ` while ` channels_first ` ` ( batch , steps , features ) ` while ` `` channels_first '' ` one of ` `` channels_last '' ` ( default ) or ` `` channels_first '' ` . pool_size : Integer or tuple of 3 integers , pool_size : integer or tuple of 2 integers , ` `` channels_last '' ` corresponds to inputs with shape If None , it will default to ` pool_size ` .",['keras/layers/pooling.py'],Update pooling.py ( # 13467 )
5,2b1f8ed204609954ab04f7a8c123d400f929e9fb,2019-10-11 14:19:29-07:00,Number of samples per evaluation step . Number of samples per gradient update . List of callbacks to apply during prediction . List of callbacks to apply during training . List of callbacks to apply during evaluation . Number of samples to be predicted at once .,['keras/engine/training.py'],Change ` batch_size ` descriptions to proper ones ( # 13422 )
6,b75b2f7dcf5d3c83e33b8b2bc86f1d2543263a59,2019-10-08 11:56:20+02:00,"print ( ' A local file was found , but it seems to be incomplete ' str ( n ) + ' integers . Received : ' + str ( value ) + ' ' ' ( or `` causal '' for Conv1D ) . Received : { } '.format ( padding ) ) return ' , '.join ( [ str ( abs ( id ( x ) ) ) for x in object_list ] ) raise ValueError ( 'Could not interpret serialized ' for chunk in chunk_read ( response , reporthook=reporthook ) : raise ValueError ( 'Unknown ' + printable_module_name raise ValueError ( 'The ` ' + name + ' ` argument must be a tuple of ' [ f.wait ( ) for f in last_ones ] raise ValueError ( 'The ` { } ` argument must be a tuple of { } ' list ( map ( lambda f : f.wait ( ) , last_ones ) ) 'of type { } '.format ( name , n , value , single_value , file_hash + ' so we will re-download the data . ' ) ( str ( ishape ) for ishape in layer.input_shapes ) ) 'integers . Received : { } including element { } ' str ( n ) + ' integers . Received : ' + str ( value ) ) 'incomplete or outdated because the ' + hash_algorithm 'type ' + str ( type ( single_value ) ) ) ' { } : { } '.format ( printable_module_name , identifier ) ) printable_module_name + ' : ' + identifier ) raise ValueError ( 'Unknown { } : { } '.format ( printable_module_name , 'data . '.format ( hash_algorithm , file_hash ) ) for chunk in chunk_read ( response , reporthook=reporthook ) : fd.write ( chunk ) ' or outdated because the { } file hash does not match ' raise ValueError ( 'Improper config format : { } '.format ( config ) ) ' ( or `` causal '' for Conv1D ) . Received : ' + str ( padding ) ) function_name ) ) last_ones = ( future.get ( ) for future in last_ones if future.successful ( ) ) gpus = len ( ( x for x in available_devices if '/gpu : ' in x ) ) ' : ' + function_name ) last_ones = [ future.get ( ) for future in last_ones if future.successful ( ) ] raise ValueError ( 'Could not interpret serialized ' fd.write ( chunk ) ' : ' + class_name ) ' file hash does not match the original value of ' raise ValueError ( 'Improper config format : ' + str ( config ) ) return ' , '.join ( ( str ( abs ( id ( x ) ) ) for x in object_list ) ) type ( single_value ) ) ) 'the original value of { } so we will re-download the ' 'including element ' + str ( single_value ) + ' of ' gpus = len ( [ x for x in available_devices if '/gpu : ' in x ] ) [ str ( ishape ) for ishape in layer.input_shapes ] ) 'integers . Received : { } '.format ( name , n , value ) ) raise ValueError ( 'The ` { } ` argument must be a tuple of { } ' class_name ) ) raise ValueError ( 'The ` ' + name + ' ` argument must be a tuple of ' print ( ' A local file was found , but it seems to be '","['keras/utils/conv_utils.py', 'keras/utils/data_utils.py', 'keras/utils/generic_utils.py', 'keras/utils/multi_gpu_utils.py', 'keras/utils/vis_utils.py']",Small refactors on the keras.utils module ( # 13388 )
7,16bd239e34d853f3bf7ae99f14e0324f924ca371,2019-10-04 10:29:42-07:00,"def __del__ ( self ) : self.csv_file.close ( ) if hasattr ( self , 'csv_file ' ) and not self.csv_file.closed :",['keras/callbacks/callbacks.py'],Fix file leak in CSVLogger ( # 13378 )
8,f0464c94c3844ba38cfa501fa3928a2bff828011,2019-10-01 19:00:03+02:00,except ImportError : pass except ImportError : return display.Image ( filename=to_file ) pass from IPython import display try : from IPython import display if extension ! = 'pdf ' : return display.Image ( filename=to_file ) try :,['keras/utils/vis_utils.py'],Allowed to return the image as a Jupyter Image only if the extension is not pdf ( # 13383 ) . ( # 13384 )
9,985521ee7050df39f9c06f53b54e17927bd1e6ea,2019-09-29 12:59:32-07:00,"any ` dilation_rate ! =1 ` . Specifying any ` strides ! =1 ` is incompatible with specifying ` ( samples , rows , cols , channels ) ` if ` data_format='channels_last ' ` . ` ( samples , rows , cols , channels ) ` if data_format='channels_last ' . ` ( samples , channels , rows , cols ) ` if ` data_format='channels_first ' ` any ` dilation_rate ` value ! = 1 . ` ( samples , channels , rows , cols ) ` if data_format='channels_first ' Specifying any stride value ! = 1 is incompatible with specifying",['keras/layers/local.py'],Update local.py docstrings ( # 13373 )
10,9ad5a18fdfe7c3cb7ff828bc1d6b42b75660db6c,2019-09-25 14:52:13-07:00,"' r+ ' , encoding='utf-8 ' ) as f_in : with open ( path ) as f : co = compile ( open ( filepath , encoding='utf-8 ' ) .read ( ) , filepath , 'exec ' ) with open ( destination_file , ' w+ ' ) as f_out , \ open ( os.path.join ( examples_dir , file ) , with open ( os.path.join ( sources_dir , 'index.md ' ) , ' w ' ) as f : with open ( path , ' w ' ) as f : with open ( destination_file , ' w+ ' , encoding='utf-8 ' ) as f_out , \ with open ( path , encoding='utf-8 ' ) as f : co = compile ( open ( filepath ) .read ( ) , filepath , 'exec ' ) with open ( path , ' w ' , encoding='utf-8 ' ) as f : with open ( os.path.join ( sources_dir , 'index.md ' ) , ' w ' , encoding='utf-8 ' ) as f : open ( os.path.join ( examples_dir , file ) , ' r+ ' ) as f_in :",['docs/autogen.py'],Fix encoding error ( # 13355 )
11,cf9595ae20be49917f2a19b24467e822ac878269,2019-09-15 10:49:14-07:00,"if self.wait : def test_ordered_enqueuer_timeout_threads ( ) : assert acc == list ( range ( 10 ) ) , 'Order was not keep in ' \ def __init__ ( self , shape , value=1.0 ) : 'OrderedEnqueuer with threads ' self.inner = value enqueuer.stop ( ) time.sleep ( 40 ) assert str ( record [ 0 ] .message ) == 'The input 0 could not be retrieved . ' \ signal.setitimer ( signal.ITIMER_REAL , 60 ) class SlowSequence ( Sequence ) : gen_output = enqueuer.get ( ) self.queue.task_done ( ) 'It could be because a worker has died . ' def __getitem__ ( self , item ) : raise TimeoutError ( 'Sequence deadlocked ' ) self.wait = True with pytest.warns ( UserWarning ) as record : acc.append ( next ( gen_output ) [ 0 , 0 , 0 , 0 ] ) acc = [ ] def on_epoch_end ( self ) : signal.setitimer ( signal.ITIMER_REAL , 0 ) self.queue.task_done ( ) enqueuer = OrderedEnqueuer ( SlowSequence ( [ 3 , 10 , 10 , 3 ] ) , assert len ( record ) == 1 old = signal.signal ( signal.SIGALRM , handler ) use_multiprocessing=False ) import signal for i in range ( 10 ) : self.wait = False enqueuer.start ( 5 , 10 ) return 10 def __len__ ( self ) : finally : for epoch_num in range ( 2 ) : return np.ones ( self.shape , dtype=np.uint32 ) * item * self.inner pass signal.signal ( signal.SIGALRM , old ) self.shape = shape def handler ( signum , frame ) :","['keras/utils/data_utils.py', 'tests/keras/utils/data_utils_test.py']",Fix sequence timeout deadlock ( # 13322 )
12,78691340edd38b00668151c2e636ddbf370f9ef3,2019-09-14 22:40:35-07:00,"At this time , we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to tf.keras in TensorFlow 2.0. tf.keras is better maintained and has better integration with TensorFlow features ( eager execution , distribution support and other ) . 3 . After discussing the feature you may choose to attempt a Pull Request on tf.keras . If you 're at all able , start writing some code . We always have more work to do than time to do it . If you can write some code then that will speed the process along . The next release will be 2.3.0 , which makes significant API changes and add support for TensorFlow 2.0 . The 2.3.0 release will be the last major release of multi-backend Keras . Multi-backend Keras is superseded by tf.keras . Keras 2.2.5 is the last release of Keras that implements the 2.2 . * API . It is the last release to only support TensorFlow 1 ( as well as Theano and CNTK ) . 3 . After discussing the feature you may choose to attempt a Pull Request . If you 're at all able , start writing some code . We always have more work to do than time to do it . If you can write some code then that will speed the process along . You can also use [ Tensorflow Github issues ] ( https : //github.com/tensorflow/tensorflow/issues ) to request features you would like to see in Keras , or changes in the Keras API . You can also use Github issues to request features you would like to see in Keras , or changes in the Keras API . Note : # # Multi-backend Keras and tf.keras : Bugs present in multi-backend Keras will only be fixed until April 2020 . We are no longer adding new features to multi-backend Keras ( we only fix bugs ) , as we are refocusing development efforts on tf.keras . If you are still interested in submitting a feature pull request , please direct it to tf.keras in the TensorFlow repository instead . # # # # Note : For more information about the future of Keras , see [ the Keras meeting notes ] ( http : //bit.ly/keras-meeting-notes ) .","['CONTRIBUTING.md', 'PULL_REQUEST_TEMPLATE.md', 'README.md']",Added messages about the future of multi-backend Keras . ( # 13315 )
13,d3512f7cfd0aba4ccf20f1d58729454a18718699,2019-09-12 10:23:49-07:00,"after which learning rate will be reduced . quantity with no improvement after which training will be stopped . patience : number of epochs with no improvement after which training will be stopped . epoch , if the validation frequency ( ` model.fit ( validation_freq=5 ) ` ) is greater than one . patience : number of epochs that produced the monitored Validation quantities may not be produced for every",['keras/callbacks/callbacks.py'],"# 13239 Improved documentation for EarlyStopping/ReduceLROnPlateau , take validation_freq into account . ( # 13240 )"
14,93b0f1ca916a693c98e32c23a94a2893574cd58c,2019-09-11 14:37:02-07:00,"additional_labels = np.random.randn ( 12 , 1 ) { 'main_output ' : headline_labels , 'aux_output ' : additional_labels } , To use the model for inferencing , use np.random.seed ( 0 ) # Set a random seed for reproducibility import numpy as np `` ` or alternatively , headline_data = np.round ( np.abs ( np.random.rand ( 12 , 100 ) * 100 ) ) additional_data = np.random.randn ( 12 , 5 ) model.fit ( [ headline_data , additional_data ] , [ headline_labels , additional_labels ] , `` ` python { 'main_output ' : labels , 'aux_output ' : labels } , model.predict ( { 'main_input ' : headline_data , 'aux_input ' : additional_data } ) headline_labels = np.random.randn ( 12 , 1 ) pred = model.predict ( [ headline_data , additional_data ] ) model.fit ( [ headline_data , additional_data ] , [ labels , labels ] ,",['docs/templates/getting-started/functional-api-guide.md'],Complete the docs by adding data to multi-input/output example ( # 12775 )
15,cb96315a291a8515544c6dd807500073958f8928,2019-09-11 14:25:57-07:00,"output = k.batch_normalization ( x , mean , var , beta , gamma , axis=axis ) 'because of the broadcast . ' ) for k in WITH_NP : var = k.variable ( var_np ) return ( ( x - mean ) / sqrt ( var + epsilon ) ) * gamma + beta other_shape = [ 1 ] * len ( x_shape ) reason='Theano behaves differently ' other_shape = tuple ( other_shape ) output_arrays = [ ] output_tensors.append ( output ) output_tensors = [ ] assert_list_keras_shape ( output_tensors , output_arrays ) mean_np = np.random.random ( other_shape ) def test_batch_normalization ( self , axis , x_shape ) : x = k.variable ( x_np ) def batch_normalization ( x , mean , var , beta , gamma , axis=-1 , epsilon=0.001 ) : x_np = np.random.random ( x_shape ) mean = k.variable ( mean_np ) gamma = k.variable ( gamma_np ) var_np = np.random.random ( other_shape ) output_arrays.append ( k.eval ( output ) ) gamma_np = np.random.random ( other_shape ) other_shape [ axis ] = x_shape [ axis ] beta_np = np.random.random ( other_shape ) beta = k.variable ( beta_np ) { { np_implementation } } assert_list_pairwise ( output_arrays )","['keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Added batch_normalization in the numpy backend . ( # 11556 )
16,ccecd39dd9281d65bada97f6762f622563227315,2019-09-11 13:35:24-07:00,"` ( batch , new_rows , new_cols , filters ) ` ` ( batch , filters , new_rows , new_cols ) ` ` ( batch , channels * depth_multiplier , new_rows , new_cols ) ` ` ( batch , new_rows , new_cols , channels * depth_multiplier ) `",['keras/layers/convolutional.py'],Correct the DepthwiseConv2d docstrings - output shape ( # 13225 )
17,8315a0b7b93ae87a94812f5833a6c7befddc4fe6,2019-09-11 13:19:51-07:00,"decoder_target_data [ i , t : , target_token_index [ ' ' ] ] = 1 . decoder_input_data [ i , t + 1 : , target_token_index [ ' ' ] ] = 1 . encoder_input_data [ i , t + 1 : , input_token_index [ ' ' ] ] = 1 . model.compile ( optimizer='rmsprop ' , loss='categorical_crossentropy ' ) model.compile ( optimizer='rmsprop ' , loss='categorical_crossentropy ' , metrics= [ 'accuracy ' ] )",['examples/lstm_seq2seq.py'],Update lstm_seq2seq.py ( from 22 % to 87 % acc ) ( # 13269 )
18,5dc27d0a986e3830ea720f9b8ee88dd1dbfd3601,2019-09-06 00:36:22-07:00,"This metric creates four local variables , ` true_positives ` , ` true_negatives ` , # Find the index of the threshold where the sensitivity is closest to the model = keras.Model ( inputs , outputs ) identified as such ( tn / ( tn + fp ) ) . def test_unweighted_high_sensitivity ( self ) : y_true = K.constant ( inputs ) def test_invalid_sensitivity ( self ) : sensitivity , num_thresholds=num_thresholds , name=name , dtype=dtype ) 0.4 , num_thresholds=100 , name='specificity_at_sensitivity_1 ' ) sensitivities = K.switch ( # Calculate sensitivities at all the thresholds . y_pred = K.constant ( inputs , dtype='float32 ' ) return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) self.num_thresholds = num_thresholds s_obj = metrics.SpecificityAtSensitivity ( 0.8 ) Use ` sample_weight ` of 0 to mask values . K.zeros_like ( self.thresholds ) ) result = s_obj ( y_true , y_pred ) base_config = super ( SpecificityAtSensitivity , self ) .get_config ( ) For additional information about specificity and sensitivity , see the ( self.true_positives / ( self.true_positives + self.false_negatives ) ) , result = s_obj ( y_true , y_pred , sample_weight=weights ) ( self.true_positives [ min_index ] / def test_unweighted_low_sensitivity ( self ) : pred_values = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.01 , 0.02 , 0.25 , 0.26 , 0.26 ] specificity at the given sensitivity . The threshold for the given sensitivity min_index = K.argmin ( min_index = K.cast ( min_index , 'int32 ' ) K.greater ( ( self.true_positives [ min_index ] def test_weighted ( self ) : denom = ( self.true_negatives [ min_index ] + self.false_positives [ min_index ] ) pred_values = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.1 , 0.45 , 0.5 , 0.8 , 0.9 ] # Compute specificity at that index . } metrics.SpecificityAtSensitivity ( 0.4 , num_thresholds=-1 ) self.false_negatives [ min_index ] ) , 0 ) , raise ValueError ( ' ` sensitivity ` must be in the range [ 0 , 1 ] . ' ) assert s_obj.num_thresholds == 100 inputs = np.random.randint ( 0 , 2 , size= ( 100 , 1 ) ) assert len ( s_obj2.weights ) == 4 class TestSpecificityAtSensitivity ( object ) : self.true_positives [ min_index ] / denom , ` false_positives ` and ` false_negatives ` that are used to compute the Usage with the compile API : 'sensitivity ' : self.sensitivity `` `` '' Computes the specificity at a given sensitivity . ` Sensitivity ` measures the proportion of actual positives that are correctly def test_config ( self ) : num_thresholds : ( Optional ) Defaults to 200 . The number of thresholds to weight_values = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] y_true = K.constant ( label_values , dtype='float32 ' ) `` ` assert np.isclose ( 0.6 , K.eval ( result ) ) 'sgd ' , assert s_obj2.name == 'specificity_at_sensitivity_1 ' model.compile ( K.abs ( sensitivities - self.value ) , axis=0 ) name : ( Optional ) string name of the metric instance . super ( SpecificityAtSensitivity , self ) .__init__ ( denom = self.true_positives [ min_index ] + self.false_negatives [ min_index ] sensitivity : A scalar value in range ` [ 0 , 1 ] ` . K.greater ( self.true_positives + self.false_negatives , 0 ) , metrics.SpecificityAtSensitivity ( -1 ) ` Specificity ` measures the proportion of actual negatives that are correctly K.greater ( denom , 0 ) , assert np.isclose ( 1 , K.eval ( result ) ) config = { assert len ( s_obj.weights ) == 4 `` `` '' assert s_obj.sensitivity == 0.4 if sensitivity < 0 or sensitivity > 1 : def test_invalid_num_thresholds ( self ) : assert s_obj.name == 'specificity_at_sensitivity_1 ' If ` sample_weight ` is ` None ` , weights default to 1 . identified as such ( tp / ( tp + fn ) ) . assert s_obj2.num_thresholds == 100 y_true = K.constant ( label_values ) y_pred = K.constant ( pred_values , dtype='float32 ' ) s_obj = metrics.SpecificityAtSensitivity ( 0.7 , num_thresholds=1 ) s_obj = metrics.SensitivityAtSpecificity ( 0.7 ) assert s_obj2.sensitivity == 0.4 assert np.isclose ( 0.4 , K.eval ( result ) ) def __init__ ( self , sensitivity , num_thresholds=200 , name=None , dtype=None ) : s_obj = metrics.SensitivityAtSpecificity ( 0.7 , num_thresholds=1 ) loss='mse ' , `` ` python s_obj = metrics.SpecificityAtSensitivity ( 0.4 ) use for matching the given specificity . s_obj2 = metrics.SpecificityAtSensitivity.from_config ( s_obj.get_config ( ) ) value is computed and used to evaluate the corresponding specificity . # Arguments class SpecificityAtSensitivity ( SensitivitySpecificityBase ) : weights = K.constant ( weight_values ) # given specificity . ( self.true_positives [ min_index ] + self.false_negatives [ min_index ] ) ) , dtype : ( Optional ) data type of the metric result . return K.switch ( following : https : //en.wikipedia.org/wiki/Sensitivity_and_specificity label_values = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] s_obj = metrics.SpecificityAtSensitivity ( def test_unweighted_all_correct ( self ) : self.true_negatives [ min_index ] / denom , with pytest.raises ( Exception ) : self.sensitivity = sensitivity metrics= [ keras.metrics.SpecificityAtSensitivity ( ) ] ) def get_config ( self ) : 'num_thresholds ' : self.num_thresholds , def result ( self ) : K.zeros_like ( self.true_negatives [ min_index ] ) ) # Check save and restore config","['keras/metrics.py', 'tests/keras/metrics_confusion_matrix_test.py']",Add SpecificityAtSensitivity metric . ( # 13294 )
19,63c0369b2466cdf6002b821351b3ea0ab42cd132,2019-09-05 15:01:40-07:00,"return K.sum ( ( numer / denom ) ) This metric creates four local variables , ` true_positives ` , ` true_negatives ` , model = keras.Model ( inputs , outputs ) identified as such ( tn / ( tn + fp ) ) . y_true = K.constant ( inputs ) expected_result = ( 1 * 1 + 0.571 * 0 ) # Compute ` num_thresholds ` thresholds in [ 0 , 1 ] https : //www.biostat.wisc.edu/~page/rocpr.pdf raise ValueError ( ' ` num_thresholds ` must be > 0 . ' ) Use ` sample_weight ` of 0 to mask values . ( self.true_positives [ min_index ] / `` `` '' value is computed and used to evaluate the corresponding sensitivity . MAJORING = 'majoring ' result = s_obj ( y_true , y_pred , sample_weight=weights ) num_thresholds=None , assert np.allclose ( K.eval ( result ) , expected_result , atol=1e-3 ) def update_state ( self , y_true , y_pred , sample_weight=None ) : ( self.false_positives + self.true_negatives ) ) , applies mid-point summation scheme for ` ROC ` . For PR-AUC , interpolates elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING : pred_values = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.01 , 0.02 , 0.25 , 0.26 , 0.26 ] 0.4 , num_thresholds=100 , name='sensitivity_at_specificity_1 ' ) ` PR ` curve , interpolates ( true/false ) positives but not the ratio that is def reset_states ( self ) : assert np.isclose ( 0.675 , K.eval ( result ) ) # tp = [ 7 , 4 , 0 ] , fp = [ 3 , 0 , 0 ] , fn = [ 0 , 3 , 7 ] , tn = [ 0 , 3 , 3 ] `` `` '' Computes the approximate AUC ( Area under the curve ) via a Riemann sum . Modeling all of TP ( true positive ) , FP ( false positive ) and their sum # threshold values are [ 0 - 1e-7 , 0.5 , 1 + 1e-7 ] self.false_positives = self.add_weight ( compute pairs of recall and precision values . The area under the ROC-curve is } switch_condition , assert auc_obj.curve == metrics_utils.AUCCurve.PR if num_thresholds < = 0 : P = TP + FP ( predicted positive ) as varying linearly within each interval fp_rate = K.switch ( K.batch_set_value ( thresholds=self.thresholds , raise ValueError ( metrics.SensitivityAtSpecificity ( -1 ) MINORING = 'minoring ' { Usage with the compile API : def interpolate_pr_auc ( self ) : = ( TP_B - TP_A ) / ( P_B - P_A ) heights = ( y [ : self.num_thresholds - 1 ] + y [ 1 : ] ) / 2 . ` Sensitivity ` measures the proportion of actual positives that are correctly if key in ( 'pr ' , 'PR ' ) : def test_weighted_roc_minoring ( self ) : p = self.true_positives + self.false_positives num_thresholds : ( Optional ) Defaults to 200 . The number of thresholds to weight_values = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] y_true = K.constant ( label_values , dtype='float32 ' ) auc_obj = metrics.AUC ( num_thresholds=2 , thresholds= [ 0.5 ] ) `` ` raise ValueError ( 'Invalid AUC curve value `` % s '' . ' % key ) 'sgd ' , assert np.isclose ( 0.6 , K.eval ( result ) ) prec_slope = dtp / K.maximum ( dp , 0 ) ` false_positives ` and ` false_negatives ` that are used to compute the AUC . to 'minoring ' or 'majoring ' can help quantify the error in the approximation # auc = ( slope / Total Pos ) * [ dTP - intercept * log ( Pb/Pa ) ] assert s_obj2.specificity == 0.4 class AUCCurve ( Enum ) : num_thresholds=100 , assert auc_obj.num_thresholds == 100 if num_thresholds == 1 : summation_method='majoring ' , switch_condition = K.all ( are_different , axis=0 ) # were initialized . This ensures that a metric initialized from this 'thresholds ' : self.thresholds [ 1 : -1 ] , if key in ( 'interpolation ' , 'Interpolation ' ) : # Validate configurations . int_A^B { Precision.dP } = TP_B - TP_A + intercept * log ( P_B / P_A ) self.specificity = specificity summation_method='majoring ' ) p [ : self.num_thresholds - 1 ] / K.maximum ( p [ 1 : ] , 0 ) , K.greater ( ( self.true_positives ) , 0 ) , def test_invalid_num_thresholds ( self ) : equal to exactly 0 or 1 . return metrics_utils.update_confusion_matrix_variables ( { intercept = self.true_positives [ 1 : ] - ( prec_slope * p [ 1 : ] ) if ( self.curve == metrics_utils.AUCCurve.PR and NEG_INF = -1e10 denom = K.maximum ( self.true_positives [ 1 : ] + self.false_negatives [ 1 : ] , 0 ) metrics_utils.AUCSummationMethod.INTERPOLATION ) ) : parameter is ignored . Values should be in [ 0 , 1 ] . Endpoint thresholds Note that when P_A == 0 the above calculation simplifies into super ( SensitivitySpecificityBase , self ) .__init__ ( name=name , dtype=dtype ) `` ` python def test_weighted_roc_interpolation ( self ) : auc_obj2 = metrics.AUC.from_config ( auc_obj.get_config ( ) ) # Set ` x ` and ` y ` values for the curves based on ` curve ` config . [ ( v , np.zeros ( ( num_thresholds , ) ) ) for v in self.variables ] ) # Arguments weights = K.constant ( weight_values ) # fn = np.sum ( [ [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 3 , 0 ] , [ 0 , 0 , 3 , 4 ] ] , axis=1 ) metrics_utils.ConfusionMatrix.TRUE_POSITIVES : self.true_positives , def test_weighted_roc_majoring ( self ) : else : safe_p_ratio = K.switch ( approximation may be poor if this is not the case . Setting ` summation_method ` label_values = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] # auc = [ 2.416 , 4 ] / ( tp [ 1 : ] +fn [ 1 : ] ) # Update properties . name='auc_1 ' , return self.interpolate_pr_auc ( ) # tn = np.sum ( [ [ 0 , 0 , 0 , 0 ] , [ 1 , 2 , 0 , 0 ] , [ 1 , 2 , 0 , 0 ] ] , axis=1 ) for increasing intervals and right summation for decreasing intervals ; # with sample_weight : [ A , B ] between successive thresholds , we get 'true_positives ' , elif key in ( 'roc ' , 'ROC ' ) : def result ( self ) : ( self.true_positives + self.false_negatives ) ) , # widths = [ ( 1 - 0 ) , ( 0 - 0 ) ] = [ 1 , 0 ] summation for decreasing intervals . def test_manual_thresholds ( self ) : # If specified , use the supplied thresholds . self.y_true = K.constant ( [ 0 , 0 , 1 , 1 ] ) dp = p [ : self.num_thresholds - 1 ] - p [ 1 : ] int_A^B { Precision.dP } = int_A^B { slope * dP + intercept * dP / P } y_pred = K.constant ( inputs , dtype='float32 ' ) closely approximating the true AUC . The quality of the approximation may vary auc_obj = metrics.AUC ( first bucket having > 0 true positives . metrics.SensitivityAtSpecificity ( 0.4 , num_thresholds=-1 ) if self.summation_method == metrics_utils.AUCSummationMethod.INTERPOLATION : # This use case is different and is handled separately . self.true_negatives = self.add_weight ( self.num_thresholds = num_thresholds assert auc_obj.summation_method == metrics_utils.AUCSummationMethod.MAJORING ( self.true_positives [ min_index ] + self.false_negatives [ min_index ] ) ) , metrics_utils.AUCSummationMethod ) and summation_method not in list ( by providing lower or upper bound estimate of the AUC . summation_method='minoring ' ) # y_pred when threshold = 1 + 1e-7 : [ 0 , 0 , 0 , 0 ] # fp_rate = [ 2/2 , 0 , 0 ] = [ 1 , 0 , 0 ] ( self.true_negatives / ( self.true_negatives + self.false_positives ) ) , K.abs ( specificities - self.value ) , axis=0 ) min_index = K.argmin ( if num_thresholds < = 1 : # Create metric variables min_index = K.cast ( min_index , 'int32 ' ) summation_method='interpolation ' , name=None , computes the area under a discretized curve of precision versus recall values def test_weighted ( self ) : self.curve = curve # P = tp + fp = [ 10 , 4 , 0 ] pred_values = [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.1 , 0.45 , 0.5 , 0.8 , 0.9 ] # ( 0 , 1 ) . name='auc_1 ' ) K.zeros_like ( self.true_positives ) ) curve : ( Optional ) Specifies the name of the curve to be computed , 'ROC ' assert len ( s_obj2.weights ) == 4 # without sample_weight : ` false_positives ` and ` false_negatives ` that are used to compute the raise ValueError ( 'Invalid curve : `` { } '' . Valid options are : `` { } '' '.format ( the precision values by the recall . metrics_utils.ConfusionMatrix.FALSE_NEGATIVES : self.false_negatives , class TestSensitivityAtSpecificity ( object ) : def test_config ( self ) : assert np.allclose ( auc_obj.thresholds , [ 0.0 , 0.5 , 1.0 ] , atol=1e-3 ) expected_result = ( 1 * 0.429 + 1 * 0.571 ) assert auc_obj.name == 'auc_1 ' # tn = np.sum ( [ [ 0 , 0 , 0 , 0 ] , [ 1 , 1 , 0 , 0 ] , [ 1 , 1 , 0 , 0 ] ] , axis=1 ) curve='PR ' , thresholds = [ ( i + 1 ) * 1.0 / ( num_thresholds - 1 ) print ( 'Sample_weight : ' , weights_tiled ) assert len ( auc_obj2.weights ) == 4 print ( 'label_is_pos : ' , label_is_pos ) self.sample_weight = [ 1 , 2 , 3 , 4 ] name : ( Optional ) string name of the metric instance . # config has the same thresholds . To discretize the AUC curve , a linearly spaced set of thresholds is used to from keras.utils import metrics_utils sensitivity at the given specificity . The threshold for the given specificity specificity , num_thresholds=num_thresholds , name=name , dtype=dtype ) ` Specificity ` measures the proportion of actual negatives that are correctly which is really equivalent to imputing constant precision throughout the result = auc_obj ( self.y_true , self.y_pred ) assert np.isclose ( 1 , K.eval ( result ) ) therefore computed using the height of the recall values by the false positive K.greater ( self.true_negatives + self.false_positives , 0 ) , config = { thresholds = sorted ( thresholds ) assert len ( s_obj.weights ) == 4 assert auc_obj.num_thresholds == 3 # threshold method to account for floating point imprecisions . raise ValueError ( ' ` specificity ` must be in the range [ 0 , 1 ] . ' ) thresholds for discretizing the curve . If set , the ` num_thresholds ` # Otherwise , linearly interpolate ( num_thresholds - 2 ) thresholds in [ default ] or 'PR ' for the Precision-Recall-curve . K.greater ( ( self.true_positives ) , 0 ) , def test_weighted_pr_majoring ( self ) : y_true = K.constant ( label_values ) metrics= [ keras.metrics.SensitivityAtSpecificity ( ) ] ) s_obj = metrics.SensitivityAtSpecificity ( 0.7 ) def test_unweighted_high_specificity ( self ) : return AUCSummationMethod.MINORING s_obj2 = metrics.SensitivityAtSpecificity.from_config ( s_obj.get_config ( ) ) # heights = [ max ( 0.7 , 1 ) , max ( 1 , 0 ) ] = [ 1 , 1 ] loss='mse ' , x = recall def test_weighted_pr_interpolation ( self ) : ROC = 'ROC ' model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.AUC ( ) ] ) raise ValueError ( 'Invalid AUC summation method value `` % s '' . ' % key ) assert auc_obj2.summation_method == metrics_utils.AUCSummationMethod.MAJORING metrics_utils.ConfusionMatrix.FALSE_NEGATIVES : self.false_negatives , self.y_pred = K.constant ( [ 0 , 0.5 , 0.3 , 0.9 ] , dtype='float32 ' ) } , y_true , y_pred , self.thresholds , sample_weight=sample_weight ) are_different = K.concatenate ( [ pMin , pMax ] , axis=0 ) use for matching the given specificity . def test_invalid_summation_method ( self ) : numer = prec_slope * ( dtp + intercept * K.log ( safe_p_ratio ) ) self.thresholds = [ 0.0 - K.epsilon ( ) ] + thresholds + [ 1.0 + K.epsilon ( ) ] `` `` '' Computes the sensitivity at a given specificity . dtype : ( Optional ) data type of the metric result . where intercept = TP_A - slope * P_A = TP_B - slope * P_B , resulting in INTERPOLATION = 'interpolation ' # = [ 2.416 , 4 ] expected_result = ( 0.7 * 0.429 + 0 * 0.571 ) def __init__ ( self , specificity , num_thresholds=200 , name=None , dtype=None ) : ( self.true_positives / ( self.true_positives + self.false_positives ) ) , # tp = [ 2 , 1 , 0 ] , fp = [ 2 , 0 , 0 ] , fn = [ 0 , 1 , 2 ] , tn = [ 0 , 2 , 2 ] assert np.isclose ( 0.8 , K.eval ( result ) ) in the range [ 0 , 1 ] and not peaked around 0 or 1 . The quality of the AUC metrics_utils.ConfusionMatrix.FALSE_POSITIVES : self.false_positives , Precision = TP / ( TP + FP ) = TP / P & Goadrich 2006 for details ) ; 'minoring ' that applies left summation def test_unweighted_all_correct ( self ) : precision = K.switch ( def get_config ( self ) : `` `` '' Interpolation formula inspired by section 4 of Davis & Goadrich 2006 . # tp = np.sum ( [ [ 0 , 0 , 3 , 4 ] , [ 0 , 0 , 0 , 4 ] , [ 0 , 0 , 0 , 0 ] ] , axis=1 ) equal to { -epsilon , 1+epsilon } for a small positive epsilon value will 'false_negatives ' , 'summation_method ' : self.summation_method.value , # dTP = [ 7-4 , 4-0 ] = [ 3 , 4 ] https : //en.wikipedia.org/wiki/Riemann_sum ) # heights = [ max ( 1 , 0.571 ) , max ( 0.571 , 0 ) ] = [ 1 , 0.571 ] # Check save and restore config elif key in ( 'minoring ' , 'Minoring ' ) : y = precision assert auc_obj2.num_thresholds == 100 # Logical and result = auc_obj ( self.y_true , self.y_pred , sample_weight=self.sample_weight ) specificity : A scalar value in range ` [ 0 , 1 ] ` . # Logical and * 'majoring ' : Applies right summation for increasing intervals and left auc_obj = metrics.AUC ( ) return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) self.num_thresholds = num_thresholds self.value = value x = fp_rate `` `` '' Abstract base class for computing sensitivity and specificity . # y_pred when threshold = 0.5 : [ 0 , 0 , 0 , 1 ] metrics_utils.ConfusionMatrix.TRUE_NEGATIVES : self.true_negatives , summation_method , list ( metrics_utils.AUCSummationMethod ) ) ) For additional information about specificity and sensitivity , see the # expected_result = ( 2.416 / 7 + 4 / 7 ) # Find the index of the threshold where the specificity is closest to the return AUCSummationMethod.MAJORING K.greater ( ( self.false_positives ) , 0 ) , # recall = [ 7/7 , 4/ ( 4+3 ) , 0 ] = [ 1 , 0.571 , 0 ] num_thresholds=self.num_thresholds , ( self.summation_method == used to manually specify thresholds which split the predictions more evenly . # heights = [ ( 1 + 0.571 ) /2 , ( 0.571 + 0 ) /2 ] = [ 0.7855 , 0.2855 ] # fp_rate = [ 3/3 , 0 , 0 ] = [ 1 , 0 , 0 ] # intercept = ( TPa+ ( slope * Pa ) = [ ( 4 - 0.5 * 4 ) , ( 0 - 1 * 0 ) ] = [ 2 , 0 ] self.false_negatives [ min_index ] ) , 0 ) , if ( isinstance ( curve , metrics_utils.AUCCurve ) and auc_obj = metrics.AUC ( num_thresholds=self.num_thresholds , curve='PR ' ) thresholds=None ) : return metrics_utils.update_confusion_matrix_variables ( * 'minoring ' : Applies left summation for increasing intervals and right specificities = K.switch ( return AUCCurve.ROC use when discretizing the roc curve . Values must be > 1 . # widths = [ ( 1 - 0.571 ) , ( 0.571 - 0 ) ] = [ 0.429 , 0.571 ] return K.sum ( ( x [ : self.num_thresholds - 1 ] - x [ 1 : ] ) * heights ) curve not in list ( metrics_utils.AUCCurve ) ) : # fp = np.sum ( [ [ 1 , 1 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] ] , axis=1 ) def __init__ ( self , value , num_thresholds=200 , name=None , dtype=None ) : 'true_negatives ' , as follows : def test_unweighted_low_specificity ( self ) : def test_unweighted ( self ) : metrics_utils.ConfusionMatrix.FALSE_POSITIVES : self.false_positives , ( https : //en.wikipedia.org/wiki/Riemann_sum ) : 'interpolation ' [ default ] , s_obj = metrics.SensitivityAtSpecificity ( 0.4 ) assert s_obj2.name == 'sensitivity_at_specificity_1 ' super ( SensitivityAtSpecificity , self ) .__init__ ( * 'interpolation ' : Applies mid-point summation scheme for ` ROC ` curve . For # tp = np.sum ( [ [ 0 , 0 , 1 , 1 ] , [ 0 , 0 , 0 , 1 ] , [ 0 , 0 , 0 , 0 ] ] , axis=1 ) summation_method ) thresholds= [ 0.3 , 0.5 ] ) assert s_obj.specificity == 0.4 } , self.false_negatives = self.add_weight ( Contains the following values : self.summation_method = summation_method # heights = [ min ( 0.7 , 1 ) , min ( 1 , 0 ) ] = [ 0.7 , 0 ] summation_method : ( Optional ) Specifies the Riemann summation method used assert len ( auc_obj.weights ) == 4 def test_invalid_specificity ( self ) : # Sum up the areas of all the rectangles . heights = K.maximum ( y [ : self.num_thresholds - 1 ] , y [ 1 : ] ) self.setup ( ) Precision slope = dTP / dP y_pred = K.constant ( pred_values , dtype='float32 ' ) num_thresholds=200 , # Note : the case ( 'PR ' , 'interpolation ' ) has been handled above . assert auc_obj2.name == 'auc_1 ' 'Invalid summation method : `` { } '' . Valid options are : `` { } '' '.format ( curve='ROC ' , initializer='zeros ' ) class TestAUC ( object ) : expected_result = ( 0.75 * 1 + 0.25 * 0 ) assert K.eval ( result ) == 1 'majoring ' that does the opposite . `` `` '' Type of AUC summation method . 'specificity ' : self.specificity def from_str ( key ) : return K.switch ( base_config = super ( AUC , self ) .get_config ( ) ( computed using the aforementioned variables ) . The ` num_thresholds ` variable assert auc_obj2.curve == metrics_utils.AUCCurve.PR following : https : //en.wikipedia.org/wiki/Sensitivity_and_specificity auc_obj = metrics.AUC ( num_thresholds=self.num_thresholds ) y_true , assert auc_obj.num_thresholds == 4 pr_auc : an approximation of the area under the P-R curve . assert auc_obj2.num_thresholds == 4 ( self.false_positives / for i in range ( num_thresholds - 2 ) ] # Calculate specificities at all the thresholds . summation_method , if thresholds is not None : Precision = ( TP_A + slope * ( P - P_A ) ) / P # recall = [ 2/2 , 1/ ( 1+1 ) , 0 ] = [ 1 , 0.5 , 0 ] # heights = [ min ( 1 , 0.571 ) , min ( 0.571 , 0 ) ] = [ 0.571 , 0 ] # y_pred when threshold = 0 - 1e-7 : [ 1 , 1 , 1 , 1 ] y_pred , 'num_thresholds ' : self.num_thresholds , def test_weighted_pr_minoring ( self ) : heights = K.minimum ( y [ : self.num_thresholds - 1 ] , y [ 1 : ] ) self.thresholds = [ 0.5 ] if isinstance ( num_thresholds=self.num_thresholds , summation_method='minoring ' ) Bringing back the factor ( slope / total_pos_weight ) we 'd put aside , we get # Find the rectangle heights based on ` summation_method ` . # heights = [ ( 1 + 0.5 ) /2 , ( 0.5 + 0 ) /2 ] = [ 0.75 , 0.25 ] rate , while the area under the PR-curve is the computed using the height of controls the degree of discretization with larger numbers of thresholds more ( self.true_positives / def test_config_manual_thresholds ( self ) : int_A^B { Precision.dTP } = int_A^B { slope * dTP } = slope * ( TP_B - TP_A ) expected_result = 0.345 + 0.571 assert s_obj.name == 'sensitivity_at_specificity_1 ' self.summation_method = metrics_utils.AUCSummationMethod.from_str ( K.zeros_like ( self.thresholds ) ) result = s_obj ( y_true , y_pred ) sample_weight=sample_weight ) Note here we derive & use a closed formula not present in the paper dramatically depending on ` num_thresholds ` . The ` thresholds ` parameter can be result = auc_obj ( self.y_true , self.y_true ) 'curve ' : self.curve.value , be automatically included with these to correctly handle predictions base_config = super ( SensitivityAtSpecificity , self ) .get_config ( ) super ( AUC , self ) .__init__ ( name=name , dtype=dtype ) assert np.allclose ( auc_obj.thresholds , [ 0.0 , 0.3 , 0.5 , 1.0 ] , atol=1e-3 ) # Verify that when specified , thresholds are used instead of num_thresholds . def setup ( self ) : raise ValueError ( ' ` num_thresholds ` must be > 1 . ' ) class AUC ( Metric ) : where dTP == TP_B - TP_A . metrics_utils.ConfusionMatrix.TRUE_POSITIVES : self.true_positives , int_A^B { Precision.dP } = int_A^B { ( TP_A + slope * ( P - P_A ) ) * dP / P } metrics.AUC ( summation_method='Invalid ' ) class SensitivitySpecificityBase ( Metric ) : assert s_obj.num_thresholds == 100 inputs = np.random.randint ( 0 , 2 , size= ( 100 , 1 ) ) This value is ultimately returned as ` auc ` , an idempotent operation that precision ( see Davis & Goadrich 2006 for details ) . shape= ( self.num_thresholds , ) , ( true/false ) positives but not the ratio that is precision ( see Davis if isinstance ( curve , metrics_utils.AUCCurve ) : def test_invalid_curve ( self ) : return AUCSummationMethod.INTERPOLATION if self.curve == metrics_utils.AUCCurve.ROC : return AUCCurve.PR self.true_positives = self.add_weight ( # dP = [ 10-4 , 4-0 ] = [ 6 , 4 ] self.num_thresholds = len ( thresholds ) + 2 else : # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING : self.curve = metrics_utils.AUCCurve.from_str ( curve ) The area within the interval is ( slope / total_pos_weight ) times [ ( v , np.zeros ( ( self.num_thresholds , ) ) ) for v in self.variables ] ) # auc * TotalPos = [ ( 0.5 * ( 3 + 2 * log ( 2.5 ) ) ) , ( 1 * ( 4 + 0 ) ) ] def __init__ ( self , model.compile ( shape= ( num_thresholds , ) , metrics_utils.AUCSummationMethod ) : class SensitivityAtSpecificity ( SensitivitySpecificityBase ) : PR = 'PR ' # fn = np.sum ( [ [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 1 , 0 ] , [ 0 , 0 , 1 , 1 ] ] , axis=1 ) expected_result = ( 0.7855 * 1 + 0.2855 * 0 ) print ( 'pred_is_pos : ' , pred_is_pos ) metrics_utils.ConfusionMatrix.TRUE_NEGATIVES : self.true_negatives , self.thresholds = [ 0.0 ] + thresholds + [ 1.0 ] pMax = K.expand_dims ( p [ 1 : ] > 0 , 0 ) `` `` '' Type of AUC Curve ( ROC or PR ) . '' '' '' K.greater ( ( self.true_positives [ min_index ] if specificity < 0 or specificity > 1 : else : # curve == 'PR ' . # Check save and restore config . curve , list ( metrics_utils.AUCCurve ) ) ) self.num_thresholds = 3 `` `` '' K.ones_like ( p [ 1 : ] ) ) For best results , ` predictions ` should be distributed approximately uniformly # We remove the endpoint thresholds as an inverse of how the thresholds 'false_positives ' , If ` sample_weight ` is ` None ` , weights default to 1 . identified as such ( tp / ( tp + fn ) ) . # Returns assert s_obj2.num_thresholds == 100 s_obj = metrics.SensitivityAtSpecificity ( 0.8 ) metrics.AUC ( num_thresholds=-1 ) if isinstance ( summation_method , metrics_utils.AUCSummationMethod ) : K.zeros_like ( self.true_positives ) ) K.zeros_like ( self.true_positives [ min_index ] ) ) s_obj = metrics.SensitivityAtSpecificity ( num_thresholds=self.num_thresholds , summation_method='majoring ' ) thresholds : ( Optional ) A list of floating point values to use as the metrics.AUC ( curve='Invalid ' ) 1 ] - self.true_positives [ 1 : ] pMin = K.expand_dims ( p [ : self.num_thresholds - 1 ] > 0 , 0 ) = ( TP - TP_A ) / ( P - P_A ) y = recall # ( Pb/Pa ) = ( Pb/Pa ) if Pb > 0 AND Pa > 0 else 1 = [ 10/4 , 4/0 ] = [ 2.5 , 1 ] # given specificity . K.zeros_like ( self.false_positives ) ) num_thresholds = len ( self.thresholds ) recall = K.switch ( class AUCSummationMethod ( Enum ) : slope * [ dTP + intercept * log ( P_B / P_A ) ] / total_pos_weight # fp = np.sum ( [ [ 1 , 2 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] ] , axis=1 ) dtype=None , expected_result = ( 0.571 * 1 + 0 * 0 ) # Add an endpoint `` threshold '' below zero and above one for either metrics.AUC ( num_thresholds=1 ) dtp = self.true_positives [ : self.num_thresholds with pytest.raises ( Exception ) : # precision = [ 7/ ( 7+3 ) , 4/4 , 0 ] = [ 0.7 , 1 , 0 ] # slope = dTP/dP = [ 0.5 , 1 ] elif key in ( 'majoring ' , 'Majoring ' ) : # Compute sensitivity at that index .","['keras/metrics.py', 'keras/utils/metrics_utils.py', 'tests/keras/metrics_confusion_matrix_test.py']","Adding AUC , SensitivityAtSpecificity metrics . ( # 13289 )"
20,088bda5902fe6f663a49461e41bbb57630cad681,2019-09-04 14:18:59-07:00,"true negatives . This metric creates one local variable , ` accumulator ` model = keras.Model ( inputs , outputs ) predictions_2d = K.reshape ( y_pred , [ 1 , -1 ] ) fn_obj2 = metrics.FalseNegatives.from_config ( fn_obj.get_config ( ) ) y_pred , y_true , sample_weight = ( ( 19.0 , 23.0 , 29.0 , 31.0 ) , ( 5.0 , 15.0 , 10.0 , 0 ) ) y_true = ( ( 0 , 1 , 0 , 1 , 0 ) , ( 0 , 0 , 1 , 1 , 1 ) , `` `` '' Calculates the number of true positives . assert tp_obj.thresholds == [ 0.4 , 0.9 ] that is used to keep track of the number of true positives . assert len ( tp_obj2.weights ) == 1 class TestTruePositives ( object ) : Use ` sample_weight ` of 0 to mask values . config = { 'thresholds ' : self.init_thresholds } loop_vars [ ConfusionMatrix.FALSE_NEGATIVES ] = ( label_is_pos , pred_is_neg ) key for key in variables_to_update if key not in list ( ConfusionMatrix ) assert np.allclose ( [ 5. , 15. , 23 . ] , K.eval ( result ) ) def update_state ( self , y_true , y_pred , sample_weight=None ) : invalid_thresholds = [ t for t in thresholds if t is None or t < 0 or t > 1 ] assert fp_obj.name == 'my_fp ' def reset_states ( self ) : thresholds=thresholds , class TestTrueNegatives ( object ) : assert np.allclose ( [ 7. , 4. , 2 . ] , K.eval ( result ) ) def parse_init_thresholds ( thresholds , default_threshold=0.5 ) : # Need TensorFlow to use metric.__call__ TRUE_POSITIVES = 'tp ' specified by this argument . K.batch_set_value ( K.reshape ( weights , [ 1 , -1 ] ) , [ num_thresholds , 1 ] ) thresholds=self.thresholds , assert tn_obj2.thresholds == [ 0.4 , 0.9 ] Usage with the compile API : then the false negatives value is 2 . If the weights were specified as pred_is_neg = K.greater ( thresh_tiled , preds_tiled ) def weighted_assign_add ( label , pred , weights , var ) : raise NotImplementedError 'Received : `` { } '' '.format ( from . import losses_utils `` ` else : # Tile labels by number of thresholds model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.TrueNegatives ( ) ] ) assert np.allclose ( [ 4. , 16. , 23 . ] , K.eval ( result ) ) sample_weight = ( ( 3.0 , ) , ( 5.0 , ) , ( 7.0 , ) , ( 4.0 , ) ) class FalsePositives ( _ConfusionMatrixConditionCount ) : return x def update_confusion_matrix_variables ( variables_to_update , assert np.allclose ( 3. , K.eval ( result ) ) super ( FalseNegatives , self ) .__init__ ( For every pair of values in y_true and y_pred : For example , if ` y_true ` is [ 0 , 1 , 0 , 0 ] and ` y_pred ` is [ 1 , 1 , 0 , 0 ] { self._confusion_matrix_cond : self.accumulator } , result = fn_obj ( y_true , y_pred , sample_weight=sample_weight ) tn_obj = metrics.TrueNegatives ( ) and corresponding variables to update as values . weighted_assign_add ( label , pred , weights_tiled , [ 0 , 0 , 1 , 0 ] then the true negatives value would be 1 . the top k predictions . update_fn = ConfusionMatrix.FALSE_NEGATIVES in variables_to_update # Raises return self.accumulator [ 0 ] 'Invalid keys : { } . Valid variable key options are : `` { } '' '.format ( raise ValueError ( loop_vars [ ConfusionMatrix.FALSE_POSITIVES ] = ( label_is_neg , pred_is_pos ) self.thresholds = metrics_utils.parse_init_thresholds ( if invalid_keys : super ( FalsePositives , self ) .__init__ ( y_pred = K.cast ( y_pred , dtype=K.floatx ( ) ) if K.backend ( ) ! = 'tensorflow ' : ( 0.1 , 0.2 , 0.4 , 0.3 ) , ( 0 , 1 , 0.7 , 0.3 ) ) `` ` python then the true positives value is 2 . If the weights were specified as [ ( v , np.zeros ( ( num_thresholds , ) ) ) for v in self.variables ] ) # Arguments ` update_op ` operation that updates the given variables . top_k=None , confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES , assert fp_obj.thresholds == [ 0.4 , 0.9 ] [ 0 , 0 , 1 , 0 ] then the false negatives value would be 1 . sample_weight = ( ( 1.0 , 2.0 , 3.0 , 5.0 ) , ( 7.0 , 11.0 , 13.0 , 17.0 ) , sample_weight = K.cast ( sample_weight , dtype=K.floatx ( ) ) variables_to_update [ matrix_cond ] ) ) the range ` [ 0 , 1 ] ` . weights = losses_utils.broadcast_weights ( for key in variables_to_update return update_ops true positives . This metric creates one local variable , ` accumulator ` list/tuple of float threshold values in [ 0 , 1 ] . A threshold is compared y_true = ( ( 0 , 1 , 1 , 0 ) , ( 1 , 0 , 0 , 0 ) , ( 0 , 0 , 0 , 0 ) , ( 1 , 1 , 1 , 1 ) ) TRUE_NEGATIVES = 'tn ' class FalseNegatives ( _ConfusionMatrixConditionCount ) : false negatives . This metric creates one local variable , ` accumulator ` super ( TrueNegatives , self ) .__init__ ( } def result ( self ) : fp_obj = metrics.FalsePositives ( name='my_fp ' , thresholds= [ 0.4 , 0.9 ] ) y_true = K.cast ( y_true , dtype=K.floatx ( ) ) name=name , with prediction values to determine the truth value of predictions that is used to keep track of the number of true negatives . y_pred = ( ( 0 , 0 , 1 , 1 , 0 ) , ( 1 , 1 , 1 , 1 , 1 ) , ( 0 , 1 , 0 , 1 , 0 ) , ( 1 , 1 , 1 , 1 , 1 ) ) y_true , assert np.allclose ( [ 2. , 5. , 7 . ] , K.eval ( result ) ) y_pred = _filter_top_k ( y_pred , top_k ) assert fn_obj.thresholds == [ 0.4 , 0.9 ] update_ops = [ ] class TrueNegatives ( _ConfusionMatrixConditionCount ) : invalid_thresholds ) ) are_different = K.concatenate ( [ label , pred ] , axis=0 ) For estimation of these metrics over a stream of data , the function creates an Update ops . y_true = ( ( 0 , 1 , 0 , 1 , 0 ) , ( 0 , 0 , 1 , 1 , 1 ) , ( 1 , 1 , 1 , 1 , 0 ) , ( 0 , 0 , 0 , 0 , 1 ) ) thresholds , then the false positives value is 2 . If the weights were specified as if update_fn or update_tn : false_positive : y_true == False and y_pred > thresholds that is used to keep track of the number of false negatives . if len ( n ) < len ( shape ) : # Padding the axis confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES , model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.FalseNegatives ( ) ] ) be either ` 1 ` , or the same as the corresponding ` y_true ` dimension ) . name=None , thresholds : A float value or a python list or tuple of float thresholds in def test_weighted ( self ) : assert fn_obj2.thresholds == [ 0.4 , 0.9 ] assert len ( fn_obj2.weights ) == 1 class TruePositives ( _ConfusionMatrixConditionCount ) : labels_2d = K.reshape ( sample_weight : Optional ` Tensor ` whose rank is either 0 , or the same rank as K.expand_dims ( K.constant ( thresholds ) , 1 ) , pytestmark = pytest.mark.skip assert np.allclose ( [ 1. , 4. , 6 . ] , K.eval ( result ) ) def test_config ( self ) : if class_id is not None : result = tn_obj ( y_true , y_pred , sample_weight=sample_weight ) tn_obj = metrics.TrueNegatives ( thresholds= [ 0.15 , 0.5 , 0.85 ] ) sample_weight=None ) : fn_obj = metrics.FalseNegatives ( thresholds= [ 0.15 , 0.5 , 0.85 ] ) def test_weighted_with_thresholds ( self ) : FALSE_POSITIVES = 'fp ' name : ( Optional ) string name of the metric instance . # Tile the thresholds for every prediction . y_pred , y_true=y_true ) update_fp = ConfusionMatrix.FALSE_POSITIVES in variables_to_update list ( ConfusionMatrix ) , variables_to_update.keys ( ) ) ) `` `` '' Calculates the number of false negatives . y_pred , K.cast ( sample_weight , dtype=K.floatx ( ) ) ) fp_obj = metrics.FalsePositives ( ) elif len ( n ) ! = len ( shape ) : elif len ( n ) ! = len ( shape ) : assert fn_obj2.name == 'my_fn ' assert tp_obj2.name == 'my_tp ' update_ops.append ( y_pred = ( ( 0 , 0 , 1 , 1 , 0 ) , ( 1 , 1 , 1 , 1 , 1 ) , assert np.allclose ( 5. , K.eval ( result ) ) that is used to keep track of the number of false positives . ] ( 0 , 1 , 0 , 1 , 0 ) , ( 1 , 1 , 1 , 1 , 1 ) ) dtype=None ) : invalid_keys , list ( ConfusionMatrix ) ) ) result = tp_obj ( y_true , y_pred ) ( 1 , 1 , 1 , 1 , 0 ) , ( 0 , 0 , 0 , 0 , 1 ) ) def assert_thresholds_range ( thresholds ) : top_k : Optional int , indicates that the positive labels should be limited to For example , if ` y_true ` is [ 0 , 1 , 1 , 1 ] and ` y_pred ` is [ 1 , 0 , 1 , 1 ] pred = K.expand_dims ( pred , 0 ) dtype : ( Optional ) data type of the metric result . if update_tn : y_pred = y_pred [ ... , class_id ] true_negatives : y_true == False and y_pred < = thresholds return self.accumulator `` `` '' Calculates the number of false positives . ` variables_to_update ` contains invalid keys . `` `` '' Returns op to update the given confusion matrix variables . num_thresholds = len ( to_list ( self.thresholds ) ) def get_config ( self ) : list/tuple of float threshold values in [ 0 , 1 ] . A threshold is def test_threshold_limit ( self ) : if isinstance ( x , list ) : from enum import Enum The results will be weighted and added together . When multiple thresholds are provided , we will repeat the same for every threshold . # Check save and restore config if key in list ( ConfusionMatrix ) ) : y_pred , y_true = losses_utils.squeeze_or_expand_dimensions ( def test_unweighted_with_thresholds ( self ) : self.accumulator = self.add_weight ( return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) raise NotImplementedError pred_is_pos = K.greater ( preds_tiled , thresh_tiled ) assert tn_obj.name == 'my_tn ' assert np.allclose ( 12. , K.eval ( result ) ) label_and_pred = K.all ( are_different , axis=0 ) label_is_pos , K.zeros_like ( label_is_pos , dtype=label_is_pos.dtype ) ) # Reshape predictions and labels . if matrix_cond in variables_to_update : assert_thresholds_range ( to_list ( thresholds ) ) fp_obj2 = metrics.FalsePositives.from_config ( fp_obj.get_config ( ) ) y_pred , y_true=y_true , sample_weight=sample_weight ) ) result = fn_obj ( y_true , y_pred ) print ( 'pred_is_pos : ' , pred_is_pos ) ValueError : If ` y_pred ` and ` y_true ` have mismatched shapes , or if invalid_keys = [ class TestFalseNegatives ( object ) : assert len ( fn_obj.weights ) == 1 return metrics_utils.update_confusion_matrix_variables ( thresholds : ( Optional ) Defaults to 0.5 . A float value or a python true_positive : y_true == True and y_pred > thresholds return thresholds num_predictions = K.size ( y_pred ) For example , if ` y_true ` is [ 0 , 1 , 0 , 0 ] and ` y_pred ` is [ 0 , 0 , 1 , 1 ] One metric value is generated for each threshold value . fp_obj = metrics.FalsePositives ( thresholds= [ 0.15 , 0.5 , 0.85 ] ) assert np.allclose ( 4. , K.eval ( result ) ) ( i.e. , above the threshold is ` true ` , below is ` false ` ) . One metric shape= ( len ( self.thresholds ) , ) , confusion_matrix_cond : One of ` metrics_utils.ConfusionMatrix ` conditions . assert len ( fp_obj2.weights ) == 1 tp_obj = metrics.TruePositives ( thresholds= [ 0.15 , 0.5 , 0.85 ] ) return var.assign_add ( K.sum ( label_and_pred , 1 ) ) sample_weight = ( ( 0.0 , 2.0 , 3.0 , 5.0 ) , ) 'variable to update . Valid variable key options are : `` { } '' . ' assert tp_obj2.thresholds == [ 0.4 , 0.9 ] fn_obj = metrics.FalseNegatives ( ) def test_unweighted ( self ) : preds_tiled = K.tile ( predictions_2d , [ num_thresholds , 1 ] ) y_pred = ( ( 0.9 , 0.2 , 0.8 , 0.1 ) , ( 0.2 , 0.9 , 0.7 , 0.6 ) , result = fp_obj ( y_true , y_pred ) thresholds=None , super ( TruePositives , self ) .__init__ ( import numpy as np update_tn = ConfusionMatrix.TRUE_NEGATIVES in variables_to_update `` `` '' Calculates the number of true negatives . Use weights of 0 to mask values . assert len ( tn_obj.weights ) == 1 assert fn_obj.name == 'my_fn ' if len ( n ) < len ( shape ) : # Padding the axis class_id : Optional int , limits the prediction and labels to the class false_negatives : y_true == True and y_pred < = thresholds initializer='zeros ' ) confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES , self.init_thresholds = thresholds assert tn_obj2.name == 'my_tn ' K.cast ( y_true , dtype='bool ' ) , [ 1 , -1 ] ) assert tp_obj.name == 'my_tp ' ` y_true ` , and must be broadcastable to ` y_true ` ( i.e. , all dimensions must thresholds = to_list ( default_threshold if thresholds is None else thresholds ) assert np.allclose ( 14. , K.eval ( result ) ) assert np.allclose ( [ 6. , 3. , 1 . ] , K.eval ( result ) ) if invalid_thresholds : [ 0 , 0 , 1 , 0 ] then the false positives value would be 1 . class_id=None , label_is_neg = K.equal ( FALSE_NEGATIVES = 'fn ' assert tn_obj.thresholds == [ 0.4 , 0.9 ] y_true , y_pred : A floating point ` Tensor ` of arbitrary shape and whose values are in return K.stack ( [ 1 , num_predictions ] ) ) n = tuple ( [ 1 for _ in range ( len ( shape ) - len ( n ) ) ] ) + n def __init__ ( self , thresholds=None , name=None , dtype=None ) : model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.TruePositives ( ) ] ) variables_to_update : Dictionary with 'tp ' , 'fn ' , 'tn ' , 'fp ' as valid keys assert len ( fp_obj.weights ) == 1 y_pred , sample_weight = ( 1. , 1.5 , 2. , 2.5 ) label = K.expand_dims ( label , 0 ) if weights is not None : y_true = ( ( 0 , 1 , 1 , 0 ) , ( 1 , 0 , 0 , 0 ) , ( 0 , 0 , 0 , 0 ) , dtype=dtype ) tn_obj2 = metrics.TrueNegatives.from_config ( tn_obj.get_config ( ) ) base_config = super ( _ConfusionMatrixConditionCount , self ) .get_config ( ) 'Please provide at least one valid confusion matrix ' if thresholds is not None : sample_weight=sample_weight ) tp_obj2 = metrics.TruePositives.from_config ( tp_obj.get_config ( ) ) assert fp_obj2.thresholds == [ .4 , 0.9 ] weights_tiled = None assert len ( tp_obj.weights ) == 1 label_and_pred = K.cast ( label_and_pred , dtype=K.floatx ( ) ) assert np.allclose ( [ 125. , 42. , 12 . ] , K.eval ( result ) ) if len ( self.thresholds ) == 1 : If ` sample_weight ` is given , calculates the sum of the weights of metrics.FalsePositives ( thresholds= [ -1 , 0.5 , 2 ] ) return [ x ] result = tn_obj ( y_true , y_pred ) predictions ( i.e. , above the threshold is ` true ` , below is ` false ` ) . print ( 'label_is_pos : ' , label_is_pos ) if variables_to_update is None : thresh_tiled = K.tile ( result = tp_obj ( y_true , y_pred , sample_weight=37 . ) `` `` '' Calculates the number of the given confusion matrix condition . # Compare predictions and threshold . then the true negatives value is 2 . If the weights were specified as loop_vars = { assert np.allclose ( [ 222. , 111. , 37 . ] , K.eval ( result ) ) super ( _ConfusionMatrixConditionCount , self ) .__init__ ( name=name , dtype=dtype ) label_and_pred * = weights assert fp_obj2.name == 'my_fp ' n = tuple ( [ 1 for _ in range ( len ( shape ) - len ( n ) ) ] ) + n from keras import backend as K def __init__ ( self , y_true = y_true [ ... , class_id ] # Tile the predictions for every threshold . assert len ( tn_obj2.weights ) == 1 for matrix_cond , ( label , pred ) in loop_vars.items ( ) : y_true : A ` Tensor ` whose shape matches ` y_pred ` . Will be cast to ` bool ` . self._confusion_matrix_cond = confusion_matrix_cond ( 1 , 1 , 1 , 1 ) ) print ( 'Sample_weight : ' , weights_tiled ) losses_utils.squeeze_or_expand_dimensions ( tn_obj = metrics.TrueNegatives ( name='my_tn ' , thresholds= [ 0.4 , 0.9 ] ) `` `` '' assert np.allclose ( 7. , K.eval ( result ) ) thresholds = to_list ( thresholds ) weights_tiled = K.tile ( ConfusionMatrix.TRUE_POSITIVES : ( label_is_pos , pred_is_pos ) , result = fp_obj ( y_true , y_pred , sample_weight=sample_weight ) confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES , compared with prediction values to determine the truth value of If ` sample_weight ` is ` None ` , weights default to 1 . if top_k is not None : confusion_matrix_cond , label_is_pos = K.tile ( labels_2d , [ num_thresholds , 1 ] ) if sample_weight is None : false positives . This metric creates one local variable , ` accumulator ` if not is_tensor ( n ) : class ConfusionMatrix ( Enum ) : # Returns if update_fp or update_tn : def to_list ( x ) : model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.FalsePositives ( ) ] ) result = tp_obj ( y_true , y_pred , sample_weight=sample_weight ) `` `` '' Tests for Keras confusion matrix metrics classes . '' '' '' 'accumulator ' , if sample_weight is not None : thresholds , default_threshold=0.5 ) For example , if ` y_true ` is [ 0 , 1 , 1 , 1 ] and ` y_pred ` is [ 0 , 1 , 0 , 0 ] from keras import metrics ` sample_weight ` is not ` None ` and its shape does n't match ` y_pred ` , or if loop_vars [ ConfusionMatrix.TRUE_NEGATIVES ] = ( label_is_neg , pred_is_neg ) [ 0 , 0 , 1 , 0 ] then the true positives value would be 1 . 'Threshold values must be in [ 0 , 1 ] . Invalid values : { } '.format ( y_pred , ` [ 0 , 1 ] ` , or NEG_INF ( used when top_k is set ) . num_thresholds = len ( thresholds ) tp_obj = metrics.TruePositives ( name='my_tp ' , thresholds= [ 0.4 , 0.9 ] ) with pytest.raises ( Exception ) : class _ConfusionMatrixConditionCount ( Metric ) : if not any ( key fn_obj = metrics.FalseNegatives ( name='my_fn ' , thresholds= [ 0.4 , 0.9 ] ) value is generated for each threshold value . class TestFalsePositives ( object ) : import pytest tp_obj = metrics.TruePositives ( ) metrics.FalsePositives ( thresholds= [ None ] )","['keras/backend/tensorflow_backend.py', 'keras/metrics.py', 'keras/utils/metrics_utils.py', 'tests/keras/metrics_confusion_matrix_test.py']","Adding TruePositives , TrueNegatives , FalsePositives , FalseNegatives metric classes . ( # 13280 )"
21,680be2e1b782616982338da1bca5158259e94e78,2019-08-30 13:02:48-07:00,"super ( CategoricalCrossentropy , self ) .__init__ ( assert poisson_obj2.name == 'poisson ' # After label smoothing , label 1 becomes 1 - 0.5L assert np.allclose ( K.eval ( result ) , 3.285 , atol=1e-3 ) # xent = -sum ( y * log ( softmax ) , 1 ) # Reduced xent = ( 0.00045 + 7.00182 ) / 2 label_smoothing : ( Optional ) Float in [ 0 , 1 ] . When > 0 , label values are cce_obj = metrics.CategoricalCrossentropy ( class TestLogCoshError ( object ) : def test_axis ( self ) : super ( BinaryCrossentropy , self ) .__init__ ( model = keras.Model ( inputs , outputs ) label classes ( 0 and 1 ) . metrics= [ keras.metrics.SparseCategoricalCrossentropy ( ) ] ) from_logits=from_logits , loss='mse ' , class Poisson ( MeanMetricWrapper ) : loss='mse ' , computed . # weighted xent = [ 0.000675 , 14.00364 ] old_config = cce_obj.get_config ( ) # y ` = [ [ 0.05 , 0.1 ] , [ 0.95 , 0.8 ] , [ EPSILON , 0.1 ] ] scce_obj = metrics.SparseCategoricalCrossentropy ( from_logits=True ) ` 0 ` and ` 0.9 ` for label ` 1 ` `` # label 0 becomes 0.5L # softmax = [ [ 0.00033 , 0.99954 , 0.00012 ] , [ 0.00091 , 0.99817 , 0.00091 ] ] def test_unweighted_with_logits ( self ) : sample_weight = [ 1.5 , 2 . ] In the snippet below , there is a single floating point value per example for bce_obj = metrics.BinaryCrossentropy ( from_logits=True ) and a single floating point value per feature for ` y_true ` . # Metric : max ( x , 0 ) - x * z + log ( 1 + exp ( -abs ( x ) ) ) poisson_obj = metrics.Poisson ( ) class TestSparseCategoricalCrossentropy ( object ) : 'sgd ' , y_pred = np.asarray ( [ [ 0.05 , 0.95 , 0 ] , [ 0.1 , 0.8 , 0.1 ] ] ) assert np.allclose ( K.eval ( result ) , 3.833 , atol=1e-3 ) assert cce_obj.name == 'cce ' # Reduced metric = 66.666 * 2.5 / ( 2 + 2.5 ) super ( KLDivergence , self ) .__init__ ( # labels * log ( softmax ) = [ [ 0 , -0.00045 , 0 ] , [ 0 , 0 , -7.00182 ] ] # Reduced metric = 7.665 / 2 result = scce_obj ( y_true , y_pred ) bce_obj = metrics.BinaryCrossentropy ( ) name='sparse_categorical_crossentropy ' , # [ -2.3026 , -0.2231 , -2.3026 ] ] assert k_obj2.name == 'kld ' # softmax = exp ( logits ) / sum ( exp ( logits ) , axis=-1 ) sample_weight = np.asarray ( [ 1.2 , 1.2 , 1.2 , 3.4 , 3.4 , 3.4 ] ) .reshape ( ( 2 , 3 ) ) result = scce_obj ( y_true , logits ) assert np.allclose ( K.eval ( result ) , expected_result , atol=1e-3 ) # [ -16.1181 , -2.3026 ] ] binary_crossentropy , sample_weight = [ [ 1.2 ] , [ 3.4 ] ] self.y_true , np.log ( self.y_pred ) ) name='cce ' , dtype='int32 ' , label_smoothing=0.2 ) ` metric = y_true * log ( y_true / y_pred ) ` # y_true with label_smoothing = [ [ 0.0333 , 0.9333 , 0.0333 ] , # xent = [ 0.56654 , 6.76801 ] poisson_obj2 = metrics.Poisson.from_config ( poisson_obj.get_config ( ) ) assert k_obj.name == 'kld ' y_true = ( ( 1 , 0 , 1 ) ) # y = one_hot ( y ) = [ [ 0 , 0 ] , [ 1 , 0 ] , [ 0 , 1 ] ] assert scce_obj.name == 'scce ' # ( ( 100 - 100 * 0 + log ( 1 + exp ( -100 ) ) ) self.y_pred = np.asarray ( [ 1 , 9 , 2 , 5 , 2 , 6 ] ) .reshape ( ( 2 , 3 ) ) # y_true = one_hot ( y_true ) = [ [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] from_logits=False , y_true = np.asarray ( [ 1 , 0 , 1 , 0 ] ) .reshape ( [ 2 , 2 ] ) ` [ batch_size , num_classes ] ` . y_pred = np.asarray ( [ 1 , 1 , 1 , 0 ] , dtype=np.float32 ) .reshape ( [ 2 , 2 ] ) expected_value = ( 100.0 + 50.0 * label_smoothing ) / 3.0 from_logits=True , label_smoothing=label_smoothing ) assert np.allclose ( K.eval ( loss ) , 3.667 , atol=1e-3 ) # Label value 0 becomes : L/n # [ -16.1181 , -2.3026 ] ] self.expected_results = self.y_pred - np.multiply ( # [ -0.0513 , -0.2231 ] , # = - ( ( log 0.95 ) , ( log 0.1 ) ) label_smoothing = 0.1 # Metric = max ( x , 0 ) - x * z + log ( 1 + exp ( -abs ( x ) ) ) scce_obj = metrics.SparseCategoricalCrossentropy ( k_obj2 = metrics.KLDivergence.from_config ( k_obj.get_config ( ) ) def __init__ ( self , name='logcosh ' , dtype=None ) : def test_weighted ( self ) : def setup ( self ) : # exp ( logits ) = [ [ 2.718 , 8103.084 , 1 ] , [ 2.718 , 2980.958 , 2.718 ] ] assert np.allclose ( K.eval ( result ) , 3.5011 , atol=1e-3 ) result = poisson_obj ( self.y_true , self.y_pred ) result = cce_obj ( y_true , y_pred , sample_weight=sample_weight ) # 0 + 100 * ( 1 - 0.5 L ) + 0 ) * ( 1/3 ) # EPSILON = 1e-7 , y = y_true , y ` = y_pred assert np.allclose ( old_config [ 'label_smoothing ' ] , 0.2 , 1e-3 ) # ( where x = logits and z = y_true ) # Label smoothing : z ' = z * ( 1 - L ) + L/n , assert scce_obj.dtype == 'int32 ' # logits = log ( y ` ) = [ [ -2.9957 , -2.3026 ] , # Metric = -sum ( y * log ( y ' ) , axis = -1 ) # = [ ( 0 + 0 + 0 ) / 3 , 200 / 3 ] super ( SparseCategoricalCrossentropy , self ) .__init__ ( class TestBinaryCrossentropy ( object ) : result = cce_obj ( y_true , logits ) assert np.allclose ( K.eval ( result ) , 33.333 , atol=1e-3 ) logcosh_obj = metrics.LogCoshError ( name='logcosh ' , dtype='int32 ' ) `` `` '' Computes the crossentropy metric between the labels and predictions . # EPSILON = 1e-7 , y = y_true , y ` = y_pred , Y_MAX = 0.9999999 # Reduced xent = ( 0.56654 + 6.76801 ) / 2 # Reduced xent = ( 0.051 * 1.5 + 2.302 * 2 . ) / 3.5 # xent = [ 0.00045 , 7.00182 ] Usage with the compile API : label ` 0 ` and ` 0.9 ` for label ` 1 ` `` dtype=dtype , class TestKLDivergence ( object ) : # y = one_hot ( y ) = [ [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] ` metric = y_pred - y_true * log ( y_pred ) ` assert np.allclose ( old_config [ 'label_smoothing ' ] , 0.2 , atol=1e-3 ) def test_config ( self ) : k_obj = metrics.KLDivergence ( ) # Reduced xent = ( 0.0513 + 2.3026 ) / 2 result = bce_obj ( y_true , y_pred ) cce_obj = metrics.CategoricalCrossentropy ( from_logits=True ) expected_result = np.sum ( expected_result ) / ( 1.2 + 3.4 ) # ( 100 - 100 * 1 + log ( 1 + exp ( -100 ) ) ) , assert np.allclose ( K.eval ( result ) , 1.176 , atol=1e-3 ) # xent = -sum ( y_true * log ( softmax ) , 1 ) # Reduced metric = ( 0 + 66.666 ) / 2 # [ -0.0513 , -0.2231 ] , metrics= [ keras.metrics.CategoricalCrossentropy ( ) ] ) def test_label_smoothing ( self ) : name='binary_crossentropy ' , # Reduced metric = ( 0.051 * 1.5 + 2.302 * 2 . ) / 3.5 This is the crossentropy metric class to be used when there are multiple def __init__ ( self , name='poisson ' , dtype=None ) : label_smoothing : Float in [ 0 , 1 ] . When > 0 , label values are smoothed , y_pred = [ [ 100.0 , -100.0 , 100.0 ] , [ 100.0 , 100.0 , -100.0 ] ] # xent = [ 0.0513 , 2.3026 ] `` ` # [ -7.00182 , -0.00182 , -7.00182 ] ] # Label smoothing : z ' = z * ( 1 - L ) + 0.5L # exp ( logits ) = [ [ 0.05 , 0.95 , EPSILON ] , [ 0.1 , 0.8 , 0.1 ] ] ` y_true ` = [ [ 0 , 0 , 1 ] , [ 1 , 0 , 0 ] , [ 0 , 1 , 0 ] ] . axis=-1 ) : def test_unweighted ( self ) : class TestCategoricalCrossentropy ( object ) : def __init__ ( self , def test_weighted_from_logits ( self ) : # [ -2.3026 , -0.2231 , -2.3026 ] ] representation . eg. , When labels values are [ 2 , 0 , 1 ] , model.compile ( expected_result = np.multiply ( self.expected_results , sample_weight ) expected_result = np.sum ( expected_result ) / np.sum ( sample_weight ) assert bce_obj.dtype == 'int32 ' name : ( Optional ) string name of the metric instance . super ( Poisson , self ) .__init__ ( poisson , name , dtype=dtype ) assert cce_obj.dtype == 'int32 ' model.compile ( 'sgd ' , metrics= [ keras.metrics.Poisson ( ) ] ) ` metric = log ( ( exp ( x ) + exp ( -x ) ) /2 ) ` , where x is the error ( y_pred - y_true ) def __init__ ( self , name='kullback_leibler_divergence ' , dtype=None ) : self.y_true = np.asarray ( [ .5 , .8 , .12 , .7 , .43 , .8 ] ) .reshape ( ( 2 , 3 ) ) This is the crossentropy metric class to be used when there are only two label classes ( 2 or more ) . Here we assume that labels are given as a ` one_hot ` # xent = -sum ( labels * log ( softmax ) , 1 ) # Reduced xent = ( 0.000675 + 14.00364 ) / ( 1.5 + 2 ) # sum ( exp ( logits ) , axis=-1 ) = [ 1 , 1 ] assert bce_obj.name == 'bce ' metrics= [ keras.metrics.BinaryCrossentropy ( ) ] ) # sum ( exp ( logits ) ) = [ 1 , 1 ] `` `` '' Computes the Poisson metric between ` y_true ` and ` y_pred ` . self.batch_size = 6 # Reduced metric = 7.665 * 1.5 / ( 1.5 + 2 ) kullback_leibler_divergence , name , dtype=dtype ) super ( LogCoshError , self ) .__init__ ( logcosh , name , dtype=dtype ) logits = np.asarray ( [ [ 1 , 9 , 0 ] , [ 1 , 8 , 1 ] ] , dtype=np.float32 ) # where L = label smoothing value and n = num classes class KLDivergence ( MeanMetricWrapper ) : # = [ -log ( Y_MAX + EPSILON ) , -log ( 1 - Y_MAX + EPSILON ) , There should be ` # classes ` floating point values per feature for ` y_pred ` assert np.allclose ( expected_value , K.eval ( result ) , atol=1e-3 ) # softmax = [ [ 0.05 , 0.1 ] , [ 0.95 , 0.8 ] , [ EPSILON , 0.1 ] ] self.y_true , np.log ( self.y_true / self.y_pred ) ) y_true = np.asarray ( [ [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] ) poisson_obj = metrics.Poisson ( name='poisson ' , dtype='int32 ' ) self.y_true = np.asarray ( [ 4 , 8 , 12 , 8 , 1 , 3 ] ) .reshape ( ( 2 , 3 ) ) `` `` '' assert k_obj2.dtype == 'int32 ' # Reduced metric = ( 0.051 + 2.302 ) / 2 class BinaryCrossentropy ( MeanMetricWrapper ) : bce_obj = metrics.BinaryCrossentropy ( # = [ ( 0 + 15.33 ) / 2 , ( 0 + 0 ) / 2 ] # logits = log ( y ` ) = [ [ -2.9957 , -0.0513 , -16.1181 ] , self.batch_size = 2 # ( where x = logits and z = y_true ) logcosh_obj = metrics.LogCoshError ( ) y_true = [ [ 1 , 0 , 1 ] , [ 0 , 1 , 1 ] ] # Applying the above two fns to the given input : result = logcosh_obj ( self.y_true , self.y_pred ) result = k_obj ( self.y_true , self.y_pred ) # Weighted metric = [ 7.665 * 1.5 , 0 ] Use this crossentropy metric when there are two or more label classes . assert logcosh_obj.dtype == 'int32 ' self.setup ( ) e.g . ` label_smoothing=0.2 ` means that we will use a value of ` 0.1 ` for # log ( softmax ) = [ [ -8.00045 , -0.00045 , -9.00045 ] , cce_obj = metrics.CategoricalCrossentropy ( ) class SparseCategoricalCrossentropy ( MeanMetricWrapper ) : # Weighted xent = [ 0.051 * 1.5 , 2.302 * 2 . ] model.compile ( 'sgd ' , metrics= [ keras.metrics.LogCoshError ( ) ] ) model.compile ( 'sgd ' , metrics= [ keras.metrics.KLDivergence ( ) ] ) using ` one-hot ` representation , please use ` CategoricalCrossentropy ` metric . meaning the confidence on label values are relaxed . e.g . # ( 100 - 100 * 1 + log ( 1 + exp ( -100 ) ) ) # y ` = [ [ 0.05 , 0.95 , EPSILON ] , [ 0.1 , 0.8 , 0.1 ] ] assert poisson_obj2.dtype == 'int32 ' # y * log ( softmax ) = [ [ 0 , 0 ] , [ -0.0513 , 0 ] , [ 0 , -2.3026 ] ] result = k_obj ( self.y_true , self.y_pred , sample_weight=sample_weight ) result = bce_obj ( y_true , logits ) # y ` = clip_ops.clip_by_value ( output , EPSILON , 1 . - EPSILON ) label_smoothing=0 ) : y_pred = np.asarray ( [ [ 0.05 , 0.1 ] , [ 0.95 , 0.8 ] , [ 0 , 0.1 ] ] ) # = [ ( ( 100 - 100 * 1 + log ( 1 + exp ( -100 ) ) ) ` y_true ` and ` # classes ` floating pointing values per example for ` y_pred ` . self.y_pred = np.asarray ( [ .4 , .9 , .12 , .36 , .3 , .4 ] ) .reshape ( ( 2 , 3 ) ) error = self.y_pred - self.y_true scce_obj = metrics.SparseCategoricalCrossentropy ( axis=0 ) assert poisson_obj.dtype == 'int32 ' old_config = bce_obj.get_config ( ) # Weighted metric = [ 0 , 66.666 * 2.5 ] sample_weight = [ 2. , 2.5 ] def test_unweighted_from_logits ( self ) : axis : ( Optional ) Defaults to -1 . The dimension along which the metric is result = logcosh_obj ( self.y_true , self.y_pred , sample_weight=sample_weight ) # [ -0.23316 , -0.00006 , -6.53479 ] ] result = poisson_obj ( self.y_true , self.y_pred , sample_weight=sample_weight ) # y ` = [ Y_MAX , Y_MAX , Y_MAX , EPSILON ] `` ` python `` `` '' Computes Kullback-Leibler divergence metric between ` y_true ` and ` y_pred ` . sparse_categorical_crossentropy , # 0 + 100 * ( 0.5 L ) + 0 scce_obj = metrics.SparseCategoricalCrossentropy ( ) categorical_crossentropy , loss = cce_obj ( y_true , logits ) We expect labels to be provided as integers . If you want to provide labels expected_result = np.sum ( self.expected_results ) / self.batch_size # y * log ( softmax ) = [ [ 0 , -0.0513 , 0 ] , [ 0 , 0 , -2.3026 ] ] # softmax = [ [ 0.05 , 0.95 , EPSILON ] , [ 0.1 , 0.8 , 0.1 ] ] self.y_pred = np.asarray ( [ 1 , 9 , 2 , -5 , -2 , 6 ] ) .reshape ( ( 2 , 3 ) ) # Arguments # log ( softmax ) = [ [ -2.9957 , -2.3026 ] , result = cce_obj ( y_true , y_pred ) smoothed , meaning the confidence on label values are relaxed . from_logits : ( Optional ) Whether output is expected to be a logits tensor . axis=axis ) # Label value 1 becomes : 1 - L + L/n # -log ( Y_MAX + EPSILON ) , -log ( 1 ) ] 'sgd ' , k_obj = metrics.KLDivergence ( name='kld ' , dtype='int32 ' ) dtype : ( Optional ) data type of the metric result . assert k_obj.dtype == 'int32 ' result = scce_obj ( y_true , logits , sample_weight=sample_weight ) assert np.allclose ( K.eval ( result ) , 4.0012 , atol=1e-3 ) By default , we assume that ` y_pred ` encodes a probability distribution . logits = ( ( 100. , -100. , -100 . ) ) result = bce_obj ( y_true , y_pred , sample_weight=sample_weight ) self.expected_results = np.multiply ( # Weighted metric = [ 0.051 * 1.5 , 2.302 * 2 . ] # labels * log ( softmax ) = [ [ -0.26641 , -0.00042 , -0.29971 ] , # ( 0 + 100 * 1 + log ( 1 + exp ( -100 ) ) ) ) ] # exp ( logits ) = [ [ 0.05 , 0.1 ] , [ 0.95 , 0.8 ] , [ EPSILON , 0.1 ] ] # y_true * log ( softmax ) = [ [ 0 , -0.00045 , 0 ] , [ 0 , 0 , -7.00182 ] ] # Metric = - ( y log ( y ` + EPSILON ) + ( 1 - y ) log ( 1 - y ` + EPSILON ) ) y_true = np.asarray ( [ 1 , 2 ] ) dtype=None , assert np.allclose ( K.eval ( result ) , 1.338 , atol=1e-3 ) label_smoothing=label_smoothing ) class CategoricalCrossentropy ( MeanMetricWrapper ) : from_logits : ( Optional ) Whether ` y_pred ` is expected to be a logits tensor . The shape of ` y_true ` is ` [ batch_size ] ` and the shape of ` y_pred ` is name , assert poisson_obj.name == 'poisson ' name='scce ' , dtype='int32 ' ) # = [ 0.051 , 2.302 ] # sum ( exp ( logits ) , axis=-1 ) = [ 8106.802 , 2986.394 ] ` label_smoothing=0.2 ` means that we will use a value of ` 0.1 ` for label # log ( softmax ) = [ [ -2.9957 , -0.0513 , -16.1181 ] , result = cce_obj ( y_true , logits , sample_weight=sample_weight ) self.expected_results = np.log ( ( np.exp ( error ) + np.exp ( -error ) ) / 2 ) assert logcosh_obj.name == 'logcosh ' result = scce_obj ( y_true , y_pred , sample_weight=sample_weight ) # ( 0 + 100 * 0 + log ( 1 + exp ( -100 ) ) ) name='categorical_crossentropy ' , assert np.allclose ( K.eval ( result ) , 37.037 , atol=1e-3 ) # ( 100 - 100 * ( 1 - 0.5 L ) + 0 class TestPoisson ( object ) : `` `` '' Computes the logarithm of the hyperbolic cosine of the prediction error . # = ( 100 + 50L ) * 1/3 By default , we consider that output encodes a probability distribution . class LogCoshError ( MeanMetricWrapper ) : name='bce ' , dtype='int32 ' , label_smoothing=0.2 ) # [ 0.0333 , 0.0333 , 0.9333 ] ]","['keras/metrics.py', 'tests/keras/metrics_test.py']","Adding LogCosh , Poisson , KLDivergence , crossentropy metrics . ( # 13271 )"
22,969db5ad69b3efed0ad6bd293812c1c08d71feb0,2019-08-30 11:19:11-07:00,"assert 1 == K.eval ( result ) # both the samples match def __init__ ( self , k=5 , name='sparse_top_k_categorical_accuracy ' , dtype=None ) : super ( SparseCategoricalAccuracy , self ) .__init__ ( # check y_true squeeze simply divides ` total ` by ` count ` . assert 0.5 == K.eval ( result ) # only 1 sample matches . Defaults to 5 . if threshold ! = 0.5 : would be .3 . You can provide logits of classes as ` y_pred ` , since argmax of model = keras.Model ( inputs , outputs ) return K.cast ( K.equal ( y_true , y_pred ) , K.floatx ( ) ) result = K.eval ( result_t ) class TestAccuracy ( object ) : y_true = [ 1 , 0 , 2 ] [ 1 , 0 , 0 , 1 ] then the binary accuracy would be 1/2 or .5 . assert acc_obj.name == 'my_acc ' y_true = [ [ 0 , 1 , 0 ] , [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] ] For example , if ` y_true ` is [ 1 , 2 , 3 , 4 ] and ` y_pred ` is [ 0 , 2 , 3 , 4 ] def accuracy ( y_true , y_pred ) : assert len ( a2.weights ) == 2 super ( CategoricalAccuracy , self ) .__init__ ( super ( Accuracy , self ) .__init__ ( accuracy , name , dtype=dtype ) metrics= [ keras.metrics.CategoricalAccuracy ( ) ] ) For example , if ` y_true ` is [ 1 , 1 , 0 , 0 ] and ` y_pred ` is [ 0.98 , 1 , 0 , 0.6 ] y_true = [ [ 0 , 0 , 1 ] , [ 0 , 1 , 0 ] ] assert a2.name == 'my_acc ' Use ` sample_weight ` of 0 to mask values . def __init__ ( self , name='accuracy ' , dtype=None ) : m = keras.metrics.Mean ( ) result = a_obj ( y_true , y_pred ) assert a2.stateful def __init__ ( self , k=5 , name='top_k_categorical_accuracy ' , dtype=None ) : assert np.isclose ( result , 4.5 / 6.7 , atol=1e-3 ) [ [ 0.5 ] , [ 0.2 ] ] ) [ 0.05 , 0.95 , 0 ] ] ) y_pred = K.constant ( y_pred ) sample_weight = ( 1.0 , 0.0 , 1.0 ) `` `` '' Computes how often targets are in the top ` K ` predictions . super ( BinaryAccuracy , self ) .__init__ ( if not K.is_tensor ( y_pred ) : # With ` k ` > 5 . For example , if ` y_true ` is [ [ 0 , 0 , 1 ] , [ 0 , 1 , 0 ] ] and ` y_pred ` is metrics= [ keras.metrics.SparseCategoricalAccuracy ( ) ] ) sparse_top_k_categorical_accuracy , name , dtype=dtype , k=k ) class Accuracy ( MeanMetricWrapper ) : def test_binary_accuracy_threshold ( self ) : than as labels . If necessary , use ` K.one_hot ` to expand ` y_true ` as a vector . def test_binary_accuracy ( self ) : class CategoricalAccuracy ( MeanMetricWrapper ) : def test_weighted ( self ) : a_obj = metrics.SparseTopKCategoricalAccuracy ( k=1 ) assert np.isclose ( result , 4 . / 6. , atol=1e-3 ) whether prediction values are 1 or 0 . binary_accuracy , name , dtype=dtype , threshold=threshold ) result_t = acc_obj ( [ [ 1 ] , [ 1 ] , [ 0 ] , [ 0 ] ] , [ [ 0.9 ] , [ 0.6 ] , [ 0.4 ] , [ 0.8 ] ] ) compute the frequency with which ` y_pred ` matches ` y_true ` . This frequency is that simply divides ` total ` by ` count ` . assert a_obj.name == 'stopkca ' assert result == 1 # 2/2 a_obj.get_config ( ) ) model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.Accuracy ( ) ] ) [ 1 , 1 , 0 , 0 ] then the accuracy would be 1/2 or .5 . def __init__ ( self , name='categorical_accuracy ' , dtype=None ) : a_obj = metrics.TopKCategoricalAccuracy ( k=2 ) result_t = acc_obj ( [ 2 , 1 ] , [ [ 0.1 , 0.1 , 0.8 ] , [ 0.05 , 0 , 0.95 ] ] , a_obj = metrics.TopKCategoricalAccuracy ( k=6 ) a_obj2 = metrics.TopKCategoricalAccuracy.from_config ( a_obj.get_config ( ) ) Usage with the compile API : def test_sparse_categorical_accuracy ( self ) : result_t = acc_obj ( [ [ 2 ] , [ 1 ] ] , logits and probabilities are same . def test_config ( self ) : metrics= [ keras.metrics.SparseTopKCategoricalAccuracy ( ) ] ) ` y_pred ` and ` y_true ` should be passed in as vectors of probabilities , rather # check y_pred squeeze `` `` '' Calculates how often predictions matches integer labels . # verify that correct value is returned threshold : ( Optional ) Float representing the threshold for deciding [ [ 0.1 , 0.1 , 0.8 ] , y_true = [ 2 , 1 ] def test_categorical_accuracy ( self ) : `` ` 'sgd ' , def binary_accuracy ( y_true , y_pred ) : model.compile ( def test_sparse_categorical_accuracy_mismatched_dims ( self ) : This metric creates two local variables , ` total ` and ` count ` that are used to categorical_accuracy , name , dtype=dtype ) name : ( Optional ) string name of the metric instance . If the weights were specified as [ 0.7 , 0.3 ] then the categorical accuracy def test_accuracy ( self ) : class TopKCategoricalAccuracy ( MeanMetricWrapper ) : # With ` k ` < 5 . assert a_obj2.dtype == 'int32 ' y_pred = [ [ 0.1 , 0.9 , 0.8 ] , [ 0.05 , 0.95 , 0 ] ] result_t = acc_obj ( [ [ [ 1 ] ] , [ [ 1 ] ] ] , [ [ 1 ] , [ 0 ] ] ) # check with sample_weight assert len ( acc_obj.weights ) == 2 ultimately returned as ` categorical accuracy ` : an idempotent operation that `` `` '' Calculates how often predictions matches labels . def __init__ ( self , name='binary_accuracy ' , dtype=None , threshold=0.5 ) : a_obj = metrics.TopKCategoricalAccuracy ( k=1 ) assert acc_obj.stateful assert np.isclose ( result , 3 . / 4. , atol=1e-3 ) a_obj2 = metrics.SparseTopKCategoricalAccuracy.from_config ( a_obj = metrics.TopKCategoricalAccuracy ( name='topkca ' , dtype='int32 ' ) config [ k ] = K.eval ( v ) if is_tensor_or_variable ( v ) else v assert a2.dtype , 'float32 ' assert np.isclose ( result , 2.5 / 2.7 , atol=1e-3 ) a_obj = metrics.SparseTopKCategoricalAccuracy ( k=6 ) `` `` '' result = K.eval ( acc_obj ( [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] ] , [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] ] ) ) acc_obj = metrics.SparseCategoricalAccuracy ( name='my_acc ' ) assert np.isclose ( result , 0.5 , atol=1e-3 ) assert a_obj.name == 'topkca ' # check config y_true = ( [ [ 0 , 0 , 1 , 0 , 0 , 0 , 0 ] , [ 0 , 1 , 0 , 0 , 0 , 0 , 0 ] ] ) top_k_categorical_accuracy , name , dtype=dtype , k=k ) name='stopkca ' , dtype='int32 ' ) For example , if ` y_true ` is [ [ 2 ] , [ 1 ] ] and ` y_pred ` is k : ( Optional ) Number of top elements to look at for computing accuracy . result_t = acc_obj ( [ [ 1 ] , [ 0 ] ] , [ [ 1 ] , [ 0 ] ] ) If ` sample_weight ` is ` None ` , weights default to 1 . y_pred = [ [ 0 , 0.9 , 0.1 ] , [ 0 , 0.9 , 0.1 ] , [ 0 , 0.9 , 0.1 ] ] threshold = K.cast ( threshold , y_pred.dtype ) result_t = acc_obj ( [ [ 1 ] , [ 1 ] ] , [ [ 1 ] , [ 0 ] ] , [ [ 0.5 ] , [ 0.2 ] ] ) result_t = acc_obj ( [ [ 0 , 0 , 1 ] , [ 0 , 1 , 0 ] ] , ultimately returned as ` sparse categorical accuracy ` : an idempotent operation a_obj = metrics.SparseTopKCategoricalAccuracy ( k=2 ) assert acc_obj.dtype == 'float32 ' assert 0.5 == K.eval ( result ) # only sample # 2 matches `` `` '' Computes how often integer targets are in the top ` K ` predictions . a2 = metrics.Accuracy.from_config ( acc_obj.get_config ( ) ) loss='mse ' , acc_obj = metrics.CategoricalAccuracy ( name='my_acc ' ) super ( TopKCategoricalAccuracy , self ) .__init__ ( y_true = K.cast ( y_true , y_pred.dtype ) result_t = acc_obj ( [ [ 2 ] , [ 1 ] ] , [ [ 2 ] , [ 0 ] ] , sample_weight= [ [ 0.5 ] , [ 0.2 ] ] ) assert np.isclose ( result , 2.5 / 2.7 , atol=1e-3 ) # 2.5/2.7 result_t = acc_obj ( [ 2 , 1 ] , [ [ 0.1 , 0.1 , 0.8 ] , [ 0.05 , 0.95 , 0 ] ] ) `` ` python sparse_categorical_accuracy , name , dtype=dtype ) class BinaryAccuracy ( MeanMetricWrapper ) : def __init__ ( self , name='sparse_categorical_accuracy ' , dtype=None ) : assert np.isclose ( result , 4.5 / 4.7 , atol=1e-3 ) [ [ 0.1 , 0.1 , 0.8 ] , [ 0.05 , 0.95 , 0 ] ] ) model.compile ( 'sgd ' , loss='mse ' , metrics= [ keras.metrics.BinaryAccuracy ( ) ] ) class SparseTopKCategoricalAccuracy ( MeanMetricWrapper ) : model.compile ( 'sgd ' , metrics= [ keras.metrics.TopKCategoricalAccuracy ( ) ] ) ultimately returned as ` binary accuracy ` : an idempotent operation that simply # Arguments y_pred = [ [ 0.5 , 0.9 , 0.1 , 0.7 , 0.6 , 0.5 , 0.4 ] , result_t = acc_obj ( [ [ 1 ] , [ 1 ] ] , [ [ [ 1 ] ] , [ [ 0 ] ] ] ) config [ k ] = K.eval ( v ) if K.is_tensor ( v ) else v [ [ 0.1 , 0.1 , 0.8 ] , [ 0.05 , 0 , 0.95 ] ] , result = a_obj ( y_true , y_pred , sample_weight=sample_weight ) then the binary accuracy is 3/4 or .75 . If the weights were specified as a_obj = metrics.TopKCategoricalAccuracy ( ) dtype : ( Optional ) data type of the metric result . class SparseCategoricalAccuracy ( MeanMetricWrapper ) : acc_obj = metrics.Accuracy ( name='my_acc ' ) then the accuracy is 3/4 or .75 . If the weights were specified as acc_obj = metrics.BinaryAccuracy ( threshold=0.7 ) assert a_obj.dtype == 'int32 ' def test_correctness ( self ) : acc_obj = metrics.BinaryAccuracy ( name='my_acc ' ) super ( SparseTopKCategoricalAccuracy , self ) .__init__ ( a_obj = metrics.SparseTopKCategoricalAccuracy ( assert np.allclose ( 1.0 , K.eval ( result ) , atol=1e-5 ) divides ` total ` by ` count ` . class TestSparseTopKCategoricalAccuracy ( object ) : m = tf.keras.metrics.Mean ( ) assert a_obj2.name == 'topkca ' [ [ 0.1 , 0.9 , 0.8 ] , [ 0.05 , 0.95 , 0 ] ] then the categorical accuracy is 1/2 or .5 . def binary_accuracy ( y_true , y_pred , threshold=0.5 ) : [ 0.05 , 0.95 , 0 , 0 , 0 , 0 , 0 ] ] a_obj = metrics.SparseTopKCategoricalAccuracy ( ) y_pred = K.cast ( y_pred > threshold , y_pred.dtype ) class TestTopKCategoricalAccuracy ( object ) : assert a_obj2.name == 'stopkca ' # Check save and restore config","['keras/metrics.py', 'tests/keras/metrics_test.py']",Adding accuracy metric classes . ( # 13265 )
23,a47f5e231c51fe3f1e6212ca05a20286096bf2e0,2019-08-28 13:07:06-07:00,"assert np.isclose ( history.history [ 'metric_1 ' ] [ -1 ] , 1 , 0 ) model.compile ( loss='mse ' , optimizer='sgd ' ) self._metric_updates += update_ops model.fit ( x , y , epochs=2 , batch_size=5 , validation_data= ( x , y ) ) updates = ( self.updates metrics_names += [ m.name for m in layer._metrics ] updates = self.updates + training_updates self._metrics.append ( metric_obj ) update_op = metric_obj.update_state ( * args , * * kwargs ) model.compile ( 'sgd ' , loss='mse ' , metrics= [ 'mse ' ] ) # the instance to the ` metrics ` list as soon as it is created . metrics = self._metrics metric_fn.result ( ) model.train_on_batch ( x , y ) model.compile ( loss='mse ' , optimizer='adam ' ) self._metrics = [ ] metrics_updates = [ ] targets = np.zeros ( shape= ( 10 , 1 ) ) # We are adding the metric object as metadata on the result tensor . assert m1 == m2 return tf.size ( x ) # Arguments self._output_loss_metrics [ i ] .result ( ) result = self._output_loss_metrics [ i ] .result ( ) # Provide same name as in the instance created in __init__ return match [ 0 ] # a Model/Layer in graph mode . This metric instance will later be used [ 'metric_1 ' , 'metric_2 ' , 'metric_3 ' , 'metric_4 ' ] ) : # Use cases : model.add_metric ( K.sum ( y ) , name='metric_1 ' ) for layer in self.layers : `` `` '' # We track the instance using the metadata on the result tensor . metrics.extend ( layer.metrics ) for m in metrics : super ( TestModel , self ) .__init__ ( name='test_model ' ) outputs = self.layer ( inputs ) # for eager mode assert np.isclose ( history.history [ 'val_metric_1 ' ] [ -1 ] , 5 , 0 ) assert np.isclose ( eval_results [ 1 ] , 1 , 0 ) `` `` '' Returns list of metrics from the given layers . def _call_metric ( metric_obj , * args , * * kwargs ) : eval_results = model.evaluate ( x , y , batch_size=5 ) # included super ( LayerWithNestedAddMetricLayer , self ) .__init__ ( ) if not match : return self.dense1 ( x ) metrics_updates.extend ( m.updates ) model = Model ( x , y ) assert eval_results [ -2 ] == 5 model.add_metric ( K.sum ( y ) , name='metric_2 ' ) def test_add_metric_on_model ( ) : self.dense1 = keras.layers.Dense ( 2 , kernel_initializer='ones ' ) assert history.history [ 'metric_1 ' ] [ -1 ] == 5 metrics.extend ( self._metrics ) assert history.history [ 'val_metric_2 ' ] [ -1 ] == 1 match = self._get_existing_metric ( name ) [ 'metric_1 ' , 'metric_2 ' ] ) : self.a = self.add_weight ( for layer in layers : for m in metrics : match = [ m for m in self._metrics if m.name == name ] 'We found { } metrics with the name : `` { } '' '.format ( len ( match ) , name ) ) return result_t eval_results = model.evaluate ( inputs , targets , batch_size=5 ) x = keras.layers.Dense ( 2 , kernel_initializer='ones ' ) ( x ) List of metrics . # to reset variable state after each epoch of training . # model = Model ( ) with K.control_dependencies ( update_op ) : # For TF y = LayerWithNestedAddMetricLayer ( ) ( x ) self.layer = LayerWithAddMetric ( ) def test_model_metrics_list ( ) : optimizer='adam ' , return result , update_ops with get_graph ( ) .as_default ( ) : # Verify that the metrics added using ` compile ` and ` add_metric ` API are model.add_metric ( K.sum ( y ) , name='metric_1 ' ) if len ( match ) > 1 : model.add_metric ( metrics.Mean ( name='metric_2 ' ) ( y ) ) def test_duplicate_metric_name_in_add_metric ( ) : model.fit ( x , y , epochs=2 , batch_size=5 , validation_data= ( x , y ) ) self.add_metric ( self.mean ( x ) , name='metric_1 ' ) metrics_updates.extend ( m.updates ) self._metric_updates += update_ops model = Model ( inp , x ) outputs = self.dense ( inputs ) assert np.isclose ( history.history [ 'metric_2 ' ] [ -1 ] , 1 , 0 ) def build ( self , input_shape ) : raise ValueError ( metrics += l.metrics targets , assert history.history [ 'val_metric_1 ' ] [ -1 ] == 5 batch_size=5 , self.add_metric ( K.sum ( outputs ) , name='metric_4 ' ) # This is required when we want to use a metric with ` add_metric ` API on weights_shape [ i ] ! = values_shape [ i ] ) : This will not include the ` compile ` metrics of a model layer . with K.control_dependencies ( update_ops ) : super ( LayerWithAddMetric , self ) .__init__ ( ) model = TestModel ( ) updates=updates , self.add_metric ( K.sum ( inputs ) , name='metric_1 ' ) updates=self.state_updates + self._metric_updates , self._metric_updates ) def _get_metrics_from_layers ( layers ) : # Add metric names from layers . if isinstance ( layer , Model ) : inp = Input ( shape= ( 1 , ) ) metric_result , update_ops = training_utils.call_metric_function ( validation_data= ( inputs , targets ) ) self.add_metric ( self.mean2 ( x ) , name='metric_2 ' ) model.test_on_batch ( x , y ) # include the metrics that were added in compile API of a nested model . self.add_metric ( K.sum ( x ) , name='metric_2 ' ) self.add_metric ( K.sum ( x ) , name='metric_3 ' ) if hasattr ( value , '_metric_obj ' ) : # model.add_metric ( mean ( values ) , name='mean ' ) result_t._metric_obj = self weights_shape [ i ] ! = values_shape [ i ] ) : 'sgd ' , metrics.extend ( _get_metrics_from_layers ( layer.layers ) ) model.compile ( def call ( self , inputs ) : x = TestLayer ( input_shape= ( 1 , ) ) ( inp ) metrics_updates = [ ] def call ( self , x ) : self.add_metric ( self.mean1 ( x ) , name='metric_1 ' ) def __init__ ( self ) : name : String metric name . assert np.isclose ( eval_results [ 2 ] , 5 , 0 ) if isinstance ( value , metrics_module.Metric ) : assert np.isclose ( history.history [ 'metric_2 ' ] [ -1 ] , 5 , 0 ) # Example : result_t = metric_obj.result ( ) from .. import metrics as metrics_module metrics_names += [ m.name for m in self._metrics ] y = Dense ( 1 , kernel_initializer='ones ' , trainable=False ) ( x ) # Use case : model.add_metric ( metrics.Mean ( name='metric_2 ' ) ( y ) ) metrics= [ metrics.MeanSquaredError ( 'metric_1 ' ) ] ) def test_multiple_add_metric_calls ( ) : self._metric_updates = [ ] assert np.allclose ( eval_results [ 1:4 ] , [ 1 , 1 , 5 ] , 0.1 ) with K.control_dependencies ( update_ops ) : # For TF return self._output_loss_metrics [ i ] ._call_result = result class LayerWithNestedAddMetricLayer ( Layer ) : `` `` '' metrics.extend ( _get_metrics_from_layers ( self._layers ) ) training_updates self.mean = metrics.Mean ( name='metric_1 ' ) with K.control_dependencies ( update_ops ) : # For TF if is_symbolic ( x ) : assert np.isclose ( history.history [ 'metric_3 ' ] [ -1 ] , 5 , 0 ) model.train_on_batch ( inputs , targets ) return self._metrics return metrics inputs , metric_obj = metrics.Mean ( name=name ) def test_add_metric_in_layer_call ( ) : with pytest.raises ( ValueError ) : for l in self.layers : def test_add_metric_in_model_call ( ) : ' a ' , ( 1 , 1 ) , initializer='ones ' , trainable=False ) # mean = Mean ( ) `` `` '' Adds metric tensor to the layer . class LayerWithAddMetric ( Layer ) : layers : List of layers . self.dense = keras.layers.Dense ( 1 , kernel_initializer='ones ' ) # Returns result = metric_fn.result ( ) def _get_existing_metric ( self , name=None ) : metrics = [ ] 'Please provide different names for the metrics you have added . ' model.add_metric ( metrics.Mean ( name='metric_3 ' ) ( y ) ) self._metrics.append ( value ) metrics.extend ( layer._metrics ) assert np.isclose ( history.history [ 'metric_1 ' ] [ -1 ] , 5 , 0 ) loss='mse ' , assert history.history [ 'metric_2 ' ] [ -1 ] == 1 updates=updates + metrics_updates , history = model.fit ( x , y , epochs=2 , batch_size=5 , validation_data= ( x , y ) ) assert np.isclose ( history.history [ 'val_metric_2 ' ] [ -1 ] , 5 , 0 ) value : Metric tensor . self.dense1 = keras.layers.Dense ( 2 ) y = np.ones ( shape= ( 10 , 2 ) ) model.test_on_batch ( inputs , targets ) # We can not call 'metrics ' on the model because we do not want to result_t = self.result ( ) self.mean1 = metrics.Mean ( name='metric_1 ' ) self.mean2 = metrics.Mean ( name='metric_2 ' ) # Arguments from keras import metrics training_utils.call_metric_function ( def _create_mean_metric ( value , name=None ) : [ m.name for m in model.metrics ] , return metrics for m1 , m2 in zip ( [ m.name for m in model._compile_metrics ] , [ 'metric_1 ' ] ) : def add_metric ( self , value , name=None ) : epochs=2 , else : return inputs + 1 x = np.ones ( shape= ( 10 , 1 ) ) assert eval_results [ -1 ] == 1 model = keras.models.Model ( x , y ) class TestModel ( Model ) : return metric_obj # A list of metric instances corresponding to the metric tensors added using history = model.fit ( # Keep track of metric instance created in subclassed model/layer . from .. import metrics updates=self.state_updates + metrics_updates , def metrics ( self ) : if match : assert np.isclose ( history.history [ 'val_metric_1 ' ] [ -1 ] , 1 , 0 ) metric_obj = _create_mean_metric ( value , name ) model.compile ( 'adam ' , loss='mse ' ) def test_model_metrics_list_in_call ( ) : return self.result ( ) self._metrics.append ( value._metric_obj ) self.mean2 = metrics.Mean ( name='metric_1 ' ) model.predict ( x , batch_size=5 ) self.built = True class TestLayer ( Layer ) : x = Input ( shape= ( 1 , ) ) def __call__ ( self , inputs ) : # the ` add_metric ` API . _call_metric ( metric_obj , value ) # We do this so that we can maintain the correct order of metrics by adding model.predict ( inputs , batch_size=5 ) for m1 , m2 in zip ( inputs = np.ones ( shape= ( 10 , 1 ) ) return outputs","['keras/backend/tensorflow_backend.py', 'keras/engine/base_layer.py', 'keras/engine/network.py', 'keras/engine/training.py', 'keras/engine/training_utils.py', 'keras/metrics.py', 'keras/utils/losses_utils.py', 'tests/keras/engine/test_training.py']",Add metric API changes ( # 13256 )
24,61052bc1f1c141c5dba9f83a4af14322ec4e6d7c,2019-08-28 10:42:03-07:00,"scale : Whether to rescale image values to be within ` [ 0 , 255 ] ` . data_format : Image data format , ImportError : if PIL is not available . A PIL Image instance . data_format : Image data format , either `` channels_first '' or `` channels_last '' . `` `` '' Saves an image stored as a Numpy array to a path or file object . `` `` '' Converts a PIL Image instance to a Numpy array . `` `` '' Converts a 3D Numpy array to a PIL Image instance . scale : Whether to rescale image values If omitted ( ` None ` ) , then ` backend.image_data_format ( ) ` is used . x : Input Numpy array . # Arguments parameter should always be used . ValueError : if invalid ` x ` or ` data_format ` is passed . data_format : Image data format . to be within ` [ 0 , 255 ] ` . `` `` '' x : Numpy array . img : PIL Image instance . either `` channels_first '' or `` channels_last '' . # Raises dtype : Dtype to use . dtype : Dtype to use for the returned array . If omitted ( ` None ` ) , then ` backend.floatx ( ) ` or ` float32 ` are used . A 3D Numpy array . path : Path or file object . file_format : Optional file format override . If omitted , the * * kwargs : Additional keyword arguments passed to ` PIL.Image.save ( ) ` . # Returns format to use is determined from the filename extension . ValueError : if invalid ` img ` or ` data_format ` is passed . If a file object was used instead of a filename , this",['keras/preprocessing/image.py'],"Documentation for ` array_to_img ` , ` img_to_array ` and ` save_img ` under ` preprocessing.image ` # 12711 ( # 13252 )"
25,2bb96b6fa782abdc57c4ebe07b28cdfb98c49e50,2019-08-28 10:24:07-07:00,"# with a range of TensorFlow versions . # NOTE ( robieta ) : This differs from tf.keras in that self.device is a from tensorflow.python.framework import device as tfdev def _set_device_from_string ( self , device_str ) : # DeviceSpec rather than a string . This is done for compatibility self.device = tfdev.DeviceSpec.from_string ( device_str )",['keras/backend/tensorflow_backend.py'],sync changes to _TfDeviceCaptureOp ( # 13255 )
26,544625545afe2e9c2b358e356c11bb8be53ceb51,2019-08-25 21:02:13-07:00,"self.schedule_decay = schedule_decay # Override set_weights for backward compatibility of Keras 2.2.4 optimizer self.initial_decay = decay def __init__ ( self , learning_rate=0.01 , epsilon=None , decay=0. , * * kwargs ) : self.decay = K.variable ( self.initial_decay , name='decay ' ) def __init__ ( self , learning_rate=0.01 , momentum=0. , super ( RMSprop , self ) .set_weights ( weights ) def __init__ ( self , learning_rate=0.01 , * * kwargs ) : super ( Adagrad , self ) .set_weights ( weights ) learning_rate = kwargs.pop ( 'lr ' , learning_rate ) self.weights = accumulators * * kwargs ) : def __init__ ( self , learning_rate=0.001 , rho=0.9 , * * kwargs ) : epsilon = K.epsilon ( ) def set_weights ( self , weights ) : def __init__ ( self , learning_rate=1.0 , rho=0.95 , * * kwargs ) : import numpy as np self.decay = K.variable ( decay , name='decay ' ) # since it does not include iteration at head of the weight list . Set weights = [ np.array ( 0 ) ] + weights self.weights = [ self.iterations ] + accumulators amsgrad=False , * * kwargs ) : self.epsilon = kwargs.pop ( 'epsilon ' , K.epsilon ( ) ) params = self.weights super ( Adadelta , self ) .set_weights ( weights ) decay : float > = 0 . Initial learning rate decay . def __init__ ( self , learning_rate=1.0 , rho=0.95 , epsilon=None , decay=0. , def __init__ ( self , learning_rate=0.001 , rho=0.9 , epsilon=None , decay=0. , epsilon=None , decay=0. , amsgrad=False , * * kwargs ) : def __init__ ( self , learning_rate=0.002 , beta_1=0.9 , beta_2=0.999 , * * kwargs ) : def __init__ ( self , learning_rate=0.01 , momentum=0. , decay=0. , # since it does not include m_schedule at head of the weight list . Set weights = [ weights [ 0 ] ] + [ np.array ( 1 . ) ] + weights [ 1 : ] epsilon=None , decay=0. , * * kwargs ) : self.schedule_decay = kwargs.pop ( 'schedule_decay ' , 0.004 ) epsilon=None , schedule_decay=0.004 , * * kwargs ) : if len ( params ) == len ( weights ) + 1 : self.initial_decay = kwargs.pop ( 'decay ' , 0.0 ) epsilon : float > = 0 . If ` None ` , defaults to ` K.epsilon ( ) ` . # iteration to 0 . super ( Nadam , self ) .set_weights ( weights ) def __init__ ( self , learning_rate=0.002 , beta_1=0.9 , beta_2=0.999 , schedule_decay : float , 0 < schedule_decay < 1 . if epsilon is None : self.weights = [ self.iterations ] + accumulators + delta_accumulators self.epsilon = epsilon learning_rate = kwargs.pop ( 'lr ' , None ) or learning_rate self.weights = [ self.iterations ] + ms + vs decay : float > = 0 . Learning rate decay over each update . self.weights = [ self.iterations , self.m_schedule ] + ms + vs epsilon : float > = 0 . Fuzz factor . If ` None ` , defaults to ` K.epsilon ( ) ` . self.weights = accumulators + delta_accumulators # m_schedule to 1 .",['keras/optimizers.py'],Update optimizers for TF2 . ( # 13246 )
27,a39f10acf3eac5e6ef69d084616982a4d4ebd8bb,2019-08-25 13:11:26-04:00,"max-line-length = 85 ignore = E402 , E731 , W503 [ pep8 ] description-file = README.md description-file = README.md",['setup.cfg'],pep8 config in setup.cfg ( # 13196 )
28,ee3997f2e6dafa5e1fa93b71dcf428a29f8066f6,2019-08-25 13:08:51-04:00,"enqueuer.join_end_of_epoch ( ) out = model.fit_generator ( generator=train_seq , callbacks= [ tracker_cb ] ) self.size = int ( np.ceil ( self.size / 2 ) ) self.size = size time.sleep ( 0.05 ) self._send_sequence ( ) # Update the pool def __init__ ( self , shape , size=100 , value=1.0 ) : 0 , 1 , 2 , 3 , 4 , 5 , 6 , # 1st epoch - > ceil ( 20 / 3 ) = 7 batches self.steps_per_epoch_log = [ ] assert len ( seq ) == 50 max_queue_size=1 , np.random.random ( ( self.batch_size , 3 ) ) ] , self.end_of_epoch_signal.clear ( ) epochs=5 , train_seq = IncreaseBatchSizeRandomSequence ( 2 , 404 , lambda x : x * 2 ) assert tracker_cb.steps_per_epoch_log [ 0:5 ] == [ 202 , 101 , 51 , 26 , 13 ] 'with threads ' ) self.logs.append ( idx ) def __getitem__ ( self , idx ) : class IncreaseBatchSizeRandomSequence ( Sequence ) : steps_per_epoch = len ( generator ) assert acc == list ( [ k * 5 for k in range ( 50 ) ] ) , ( def __len__ ( self ) : if workers > 0 : assert tracker_cb.trained_epochs == [ 0 , 1 , 2 , 3 , 4 ] sequence = list ( range ( len ( self.sequence ) ) ) assert tracker_cb.steps_per_epoch_log [ 0:5 ] == [ 10 , 6 , 5 , 4 , 3 ] self.initial_sequence_length = initial_sequence_length optimizer = 'rmsprop ' 0 , 1 , # 5th epoch - > ceil ( 20 /11 ) = 2 batches for i in range ( 50 ) : assert tracker_cb.steps_per_epoch_log [ 0:5 ] == [ 7 , 4 , 3 , 3 , 2 ] # number of trained batches should match sum of steps per each epoch initial_epoch=0 , self.steps_per_epoch_log.append ( params [ 'steps ' ] ) sequence = list ( range ( len ( self.sequence ) ) ) # update callbacks to make sure params are valid each epoch np.random.random ( ( self.batch_size , 3 ) ) ] ) self._send_sequence ( ) # Share the initial sequence train_seq = IncreaseBatchSizeRandomSequence ( 3 , 20 ) # recomute steps per epochs in case if Sequence changes it 's length 'verbose ' : verbose , def __init__ ( self , initial_batch_size , initial_sequence_length=12 , acc.append ( next ( gen_output ) [ 0 , 0 , 0 , 0 ] ) super ( TrackerCallback , self ) .set_params ( params ) self.end_of_epoch_signal = threading.Event ( ) def test_fit_generator_dynamic_size_sequence_main_thread ( ) : 0 , 1 , 2 , 3 , 4 , # 3d epoch - > ceil ( 30 / 7 ) = 5 batches [ np.random.random ( ( self.batch_size , 4 ) ) , 'epochs ' : epochs , self.batch_size_func = batch_size_func gen_output = enqueuer.get ( ) generator.on_epoch_end ( ) callbacks.set_params ( { return self.size enqueuer.join_end_of_epoch ( ) self.inner = value if recompute_steps_per_epoch : # recompute steps_per_epoch after each epoch 'Order was not keep in GeneratorEnqueuer with processes ' ) self.batch_size = self.batch_size_func ( self.batch_size ) recompute_steps_per_epoch = use_sequence_api and steps_per_epoch is None 'do_validation ' : do_validation , 0 , 1 , 2 , 3 , # 2nd epoch - > ceil ( 20 / 5 ) = 4 batches self.inner * = 5.0 enqueuer.stop ( ) return int ( np.ceil ( self.initial_sequence_length / float ( self.batch_size ) ) ) 0 , 1 , 2 , # 3d epoch - > ceil ( 20 / 7 ) = 3 batches def on_epoch_end ( self ) : seq = LengthChangingSequence ( [ 3 , 10 , 10 , 3 ] ) model = get_model ( num_outputs=2 ) enqueuer = OrderedEnqueuer ( seq , 0 , 1 , 2 , 3 , # 4th epoch - > ceil ( 30 / 9 ) = 4 batches batch_size_func=lambda x : x + 2 ) : sample_weight_mode=None ) acc = [ ] train_seq = IncreaseBatchSizeRandomSequence ( 3 , 30 ) validation_data=val_seq , } ) ] tracker_cb = TrackerCallback ( ) return np.ones ( self.shape , dtype=np.uint32 ) * item * self.inner 'metrics ' : callback_metrics , loss_weights = [ 1. , 0.5 ] self._send_sequence ( ) # Share the initial sequence 'steps ' : steps_per_epoch , if use_sequence_api and workers == 0 : def join_end_of_epoch ( self ) : 0 , 1 , 2 , 3 , 4 , 5 , # 2nd epoch - > ceil ( 30 / 5 ) = 6 batches # if generator is instance of Sequence and steps_per_epoch are not provided assert len ( tracker_cb.trained_batches ) == 202 + 101 + 51 + 26 + 13 self.end_of_epoch_signal.set ( ) return ( [ np.random.random ( ( self.batch_size , 3 ) ) , def __getitem__ ( self , item ) : self.logs = [ ] 0 , 1 , 2 , # 4th epoch - > ceil ( 20 / 9 ) = 3 batches for i in range ( 100 ) : model.compile ( optimizer , loss , metrics= [ ] , loss_weights=loss_weights , # communicate on_epoch_end to the main thread use_multiprocessing=False ) enqueuer.start ( 3 , 10 ) 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , # 1st epoch - > ceil ( 30 / 3 ) = 10 batches assert acc == list ( range ( 100 ) ) , ( 'Order was not keep in GeneratorEnqueuer ' 0 , 1 , 2 , # 5th epoch - > ceil ( 30 /11 ) = 3 batches validation_steps=3 , self.end_of_epoch_signal.wait ( timeout=30 ) self.batch_size = initial_batch_size workers=0 , def set_params ( self , params ) : def test_fit_generator_dynamic_size_sequence_with_workers ( ) : loss = 'mse ' assert tracker_cb.trained_batches == [ def test_on_epoch_end_threads_sequence_change_length ( ) : class LengthChangingSequence ( Sequence ) : self.shape = shape val_seq = RandomSequence ( 4 )","['keras/engine/training_generator.py', 'keras/utils/data_utils.py', 'tests/keras/engine/test_training.py', 'tests/keras/utils/data_utils_test.py']",Recompute steps_per_epoch after each epoch in traingin_generator ( # 13037 )
29,c10d24959b0ad615a21e671b180a1b2466d77a2b,2019-07-25 13:29:49-07:00,"inputs = inputs [ 0 ] forward_inputs += forward_state shape = K.int_shape ( inputs ) warnings.filterwarnings ( 'error ' ) if not isinstance ( initial_state , ( list , tuple , type ( None ) ) ) : if self._num_constants is None : backward_state = inputs [ pivot : -self._num_constants ] inputs [ 0 ] ._keras_shape = shape # for theano backend if initial_state is not None : forward_state = inputs [ 1 : pivot ] shape = K.int_shape ( inputs [ 0 ] ) model2 = load_model ( fname , custom_objects= { 'CustomRNN ' : CustomRNN } ) else : constants = inputs [ -self._num_constants : ] if initial_state is not None and inputs [ 1 : -self._num_constants ] : backward_inputs += inputs [ -self._num_constants : ] initial_state = inputs [ 1 : -self._num_constants ] # as they could be copied to multiple GPU . forward_state = initial_state [ : pivot ] if len ( initial_state ) == 0 : backward_inputs = [ inputs [ 0 ] ] inp = Input ( ( 3 , 2 ) ) initial_state = inputs [ 1 : ] os.remove ( fname ) initial_state = inputs [ 1 : ] if constants is None : elif len ( inputs ) > 1 + len ( initial_state ) : forward_inputs += inputs [ -self._num_constants : ] if isinstance ( inputs , list ) and len ( inputs ) > 1 : assert_allclose ( y1 , y2 , atol=1e-5 ) constants = inputs [ -self._num_constants : ] rnn2_out = CustomRNN ( 2 ) ( rnn_out , arg=2 , initial_state= [ h , c ] ) inputs = [ inputs [ 0 ] ] initial_state = None initial_state = None inputs [ 0 ] * = arg import warnings if has_arg ( self.layer.call , 'initial_state ' ) : if initial_state is not None : raise ValueError ( 'Layer does not accept initial_state argument . ' ) backward_inputs += backward_state inputs = inputs [ : -self._num_constants ] if not isinstance ( constants , ( list , tuple , type ( None ) ) ) : if isinstance ( inputs , list ) : raise ValueError ( 'Layer was passed initial state ' inputs * = arg assert hasattr ( h , '_keras_history ' ) initial_state = inputs [ 1 : ] if len ( initial_state ) == 0 : # get initial_state from full input spec # as they could be copied to multiple GPU . inputs = inputs [ : ] y2 = model2.predict ( x ) with warnings.catch_warnings ( ) : model.save ( fname ) y_rev = self.backward_layer.call ( inputs , assert hasattr ( c , '_keras_history ' ) if initial_state is None : def test_model_saving_with_rnn_initial_state_and_args ( ) : 'via both kwarg and inputs list ) ' ) y = self.forward_layer.call ( forward_inputs , kwargs.pop ( 'constants ' ) y1 = model.predict ( x ) if isinstance ( inputs , list ) and len ( inputs ) > 1 or initial_state : pivot = len ( initial_state ) // 2 + 1 # add backward initial state backward_state = None # add constants for forward and backward layers backward_state = inputs [ pivot : ] inputs = inputs [ 0 ] if initial_state is not None and has_arg ( self.layer.call , 'initial_state ' ) : constants = [ constants ] # get initial_state from full input spec if self._num_constants is not None and constants is None : if self._num_constants is None : pivot = len ( initial_state ) // 2 raise ValueError ( 'Layer was passed constants ' backward_state = initial_state [ pivot : ] if len ( inputs ) == 1 : kwargs.pop ( 'initial_state ' ) y_rev = self.backward_layer.call ( backward_inputs , if 'constants ' in kwargs : class CustomRNN ( LSTM ) : 'via both kwarg and inputs list ' ) 'via both kwarg and inputs list ) ' ) return super ( CustomRNN , self ) .call ( inputs , mask , training , initial_state ) def call ( self , inputs , arg=1 , mask=None , training=None , initial_state=None ) : raise ValueError ( 'Layer was passed initial state ' x = np.random.random ( ( 2 , 3 , 2 ) ) if 'initial_state ' in kwargs : forward_inputs = [ inputs [ 0 ] ] # add forward initial state initial_state = inputs [ 1 : -self._num_constants ] forward_state = None _ , fname = tempfile.mkstemp ( '.h5 ' ) inputs._keras_shape = shape # for theano backend initial_state = [ initial_state ] assert hasattr ( rnn_out , '_keras_history ' ) model = Model ( inputs=inp , outputs=rnn2_out ) rnn_out , h , c = CustomRNN ( 2 , return_state=True , return_sequences=True ) ( inp ) y = self.forward_layer.call ( inputs , assert hasattr ( rnn2_out , '_keras_history ' ) else :","['keras/layers/recurrent.py', 'keras/layers/wrappers.py', 'tests/test_model_saving.py']",RNN initial state : bug fix + suppress false warning ( # 13138 )
30,efe72ef433852b1d7d54f283efff53085ec4f756,2019-07-23 15:52:02-07:00,regularization += K.sum ( self.l1 * K.abs ( x ) ) regularization += self.l2 * K.sum ( K.square ( x ) ) regularization += K.sum ( self.l2 * K.square ( x ) ) regularization += self.l1 * K.sum ( K.abs ( x ) ),['keras/regularizers.py'],For better performance ( # 13144 )
31,ed07472bc5fc985982db355135d37059a1f887a9,2019-07-10 15:51:14-07:00,"def simple_rnn ( inputs , states ) : if self.unroll and timesteps is None : def get_step_function ( backend , w_i , w_h ) : if 'unroll ' in kwargs : 'time dimension is undefined or equal to 1 . \n ' h = states [ 0 ] if self.unroll and timesteps in [ None , 1 ] : for unroll in unroll_options : unroll=True , kwargs_list = [ check_rnn_operation ( step_function_k=get_step_function ( K , wi_k , wh_k ) , * * kwargs ) inputs_np=x , ' supported when ` unroll=True ` . ' ) for unroll in [ True , False ] : for kwargs in kwargs_list : 'time dimension is undefined . \n ' unroll_options = [ kwargs.pop ( 'unroll ' ) ] def test_rnn_unroll_with_len_1 ( self ) : input_dim = 5 _ , wh = parse_shape_or_val ( ( output_dim , output_dim ) ) wi_k = K.variable ( wi ) wh_k = K.variable ( wh ) return simple_rnn output_dim = 3 return y , [ y ] initial_states_np= [ h0 ] , _ , h0 = parse_shape_or_val ( ( num_samples , output_dim ) ) num_samples = 4 _ , x = parse_shape_or_val ( ( num_samples , 1 , input_dim ) ) _ , wi = parse_shape_or_val ( ( input_dim , output_dim ) ) y = backend.dot ( inputs , w_i ) + backend.dot ( h , w_h ) assert len ( states ) == 1 else : step_function_np=get_step_function ( KNP , wi , wh ) , { 'go_backwards ' : True } , if input_length == 1 : { 'go_backwards ' : False } , raise ValueError ( ' ` input_length=1 ` is not ' unroll_options = [ True , False ] ]","['keras/backend/theano_backend.py', 'keras/layers/recurrent.py', 'tests/keras/backend/backend_test.py']",Allow unrolled RNNs with input_length=1 ( # 13078 )
32,3bda5520b787f84f687bb116c460f3aedada039b,2019-07-09 14:31:17-07:00,"num_words = min ( MAX_NUM_WORDS , len ( word_index ) ) + 1 if i > MAX_NUM_WORDS : num_words = min ( MAX_NUM_WORDS , len ( word_index ) + 1 ) if i > = MAX_NUM_WORDS :",['examples/pretrained_word_embeddings.py'],Update pretrained_word_embeddings.py ( # 13073 )
33,f06524c44e5f6926968cb2bb3ddd1e523f5474c5,2019-07-02 19:37:24-07:00,"elif hasattr ( f.file , 'close ' ) : f.close ( ) f.file.close ( ) if hasattr ( f , 'close ' ) :",['keras/engine/network.py'],load_weights ( ) now properly closes file ( # 13048 )
34,b810de658d022c38ce14e49693e6cb45d024333a,2019-06-23 13:24:04-07:00,"# a get to avoid failing if the key is missing # Earlier versions of keras did n't dump weighted_metrics properly . Use weighted_metrics=weighted_metrics , training_config.get ( 'weighted_metrics ' ) ) weighted_metrics = convert_custom_objects (",['keras/engine/saving.py'],Adding weighted_metrics to model loading/saving ( # 12984 )
35,c658993cf596fbd39cf800873bc457e69cfb0cdb,2019-06-17 10:51:13-07:00,Separable convolutions consist in first performing just the first step in a depthwise spatial convolution `` `` '' Depthwise 2D convolution . Separable convolution performs first `` `` '' Depthwise separable 2D convolution . just the first step of a depthwise spatial convolution Depthwise convolution performs Depthwise Separable convolutions consists in performing,['keras/layers/convolutional.py'],correct DepthwiseConv2D docstring ( # 12949 )
36,910e1247b452f29c0297ec6e3bf52b28b24fefcf,2019-06-06 10:51:43-07:00,"'this was done on purpose . The fit and evaluate APIs will not ' # mse = [ 5 , 52 ] weights_list = loss_weights for i in range ( len ( self.outputs ) ) : losses.binary_crossentropy , return ( isinstance ( loss , CategoricalCrossentropy or # we always set the loss reduction type to be ` SUM_OVER_BATCH_SIZE ` .. if total_loss is None : return loss_functions self.loss = loss or { } warnings.warn ( 'Output `` ' + name mode = 'temporal ' if total_loss is None : 'Incompatible shapes : ` losses ` { } vs ` sample_weight ` { } '.format ( return weight , mode ValueError : If loss is a dict with keys not in model output names , A list of loss weights of python floats . elif isinstance ( loss_weights , collections.Mapping ) : return ( isinstance ( loss , CategoricalCrossentropy ) or `` `` '' loss_functions = [ get_loss_function ( l ) for l in loss ] dense = Dense ( 4 , name='dense ' ) model = Model ( [ a , b ] , [ d , e ] ) weight = None 'it should have one entry per model output . ' for loss_tensor in self.losses : return total_loss 'This loss expects targets to have the same shape ' ` Loss ` instance . If the model has multiple outputs , you can use model.compile ( loss=losses.MSE , return weights_list raise ValueError ( weights = sample_weights [ i ] ' outputs , but you passed loss= ' if len ( self.outputs ) > 1 : ' while using as loss ` ' + loss.__name__ + ' ` . ' self.output_names , loss_weights ) y_true = self.targets [ i ] from .training_utils import collect_metrics if total_loss is None : if weighted_losses [ i ] is None : self.metrics_tensors.append ( output_loss ) A pair of list of sample weights and sample weight modes from .training_utils import is_generator_or_sequence loss_functions = [ losses.get ( l ) for l in loss ] mse_obj = losses.LossFunctionWrapper ( loss_fn , name=loss_fn.__name__ ) the * weighted sum * of all individual losses , weighted by the raise ValueError ( 'Unknown entry in loss ' def get_loss_function ( loss ) : should be skipped . raise ValueError ( loss=losses.MeanSquaredError ( ) , b = Input ( shape= ( 3 , ) , name='input_b ' ) loss = mse_obj ( y_true , y_pred , sample_weight=sample_weight ) losses_rank = K.ndim ( losses ) total_loss = None for loss_tensor in self.losses : sample_weight = K.expand_dims ( sample_weight , axis=i ) else : return None , None loss = convert_custom_objects ( training_config [ 'loss ' ] ) else : sample_weight = sample_weights [ i ] for i , loss_function in enumerate ( self.loss_functions ) : losses.CategoricalCrossentropy ) 'sgd ' , np.allclose ( history.history [ 'loss ' ] , [ 1. , 0.9 , 0.8 , 0.7 , 0.6 ] ) class_weights = standardize_class_weights ( self.sample_weights = sample_weights self.loss_functions = training_utils.prepare_loss_functions ( ValueError : If loss weight is a dict with key not in model output names , if unknown : ( getattr ( losses , loss_fn.fn.__name__ , None ) is None ) ) ) : 'Output missing from sample_weight_modes dictionary ' ) if len ( loss ) ! = len ( output_names ) : if i in skip_target_indices : 'sample_weight_mode dictionary : `` ' sample_weights = [ ] loss_functions.append ( get_loss_function ( loss.get ( name , None ) ) ) history = model.fit ( x , y , batch_size=3 , epochs=5 ) mode = None sample_weight = K.repeat_elements ( 'per model outputs . The model has { } outputs , but you ' raise TypeError ( 'Could not interpret loss_weights argument : ' if ( i not in skip_target_weighing_indices and sample_weights = standardize_sample_weights ( raise ValueError ( 'When passing a list as loss_weights , ' with K.name_scope ( self.output_names [ i ] + '_loss ' ) : # Prepare list of loss functions , same size as model outputs . # or if it not in the ` losses ` module , then def _set_sample_weight_attributes ( self , sample_weight_mode , sample_weights.append ( weight ) output_names : List of model output names . loss_weights_list = [ 1. for _ in range ( len ( self.outputs ) ) ] elif isinstance ( sample_weight_mode , list ) : # Raises skip_target_weighing_indices = [ ] # Expand dim of weights to match ndim of losses , if required . ' while using as loss ` ' + loss_name + ' ` . ' if not self.losses : loss_weights : Optional list or dictionary specifying scalar coefficients total_loss = loss_weight * output_loss if i not in skip_target_weighing_indices weight = K.placeholder ( ndim=1 , # Update weights with mask . if batch_size is not None and is_generator_or_sequence ( x ) : if name not in self.output_names : sample_weight , losses_shape [ i ] , axis=i ) total_loss = None mapping to the model 's outputs . If a dict , it is expected to map loss_weights = [ 1. , 0.5 ] for i , name in enumerate ( output_names ) : weighted_losses = [ loss_name = loss.name if isinstance ( sample_weight_mode , dict ) : raise ValueError ( 'When passing a list as loss , ' for i in range ( len ( self.outputs ) ) : x = training_utils.standardize_input_data ( from .training_utils import check_array_length_consistency y_pred = K.constant ( [ [ 4. , 8 . ] , [ 12. , 3 . ] ] ) # Apply weights to losses . masks = [ getattr ( x , '_keras_mask ' , None ) for x in self.outputs ] # Arguments # If the given loss is not an instance of the ` Loss ` class ' outputs , but you passed ' output_loss = weighted_loss ( y_true , y_pred , # Add regularization penalties and other layer-specific losses . check_generator_arguments ( loss_config = training_config [ 'loss ' ] # Deserialize loss class . from .training_utils import standardize_sample_weights sample_weight * = mask return inputs + self.bias a different loss on each output by passing a dictionary or a self._feed_sample_weight_modes = [ ' outputs , but you passed ' reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE ) from .training_utils import standardize_input_data input_b_np = np.random.random ( ( 10 , 3 ) ) if mask is not None : from keras.layers import Activation , Dense , Dropout , Conv2D , Concatenate loss_functions.append ( losses.get ( loss.get ( name ) ) ) warnings.warn ( e = Dropout ( 0.5 , name='dropout ' ) ( c ) c = dense ( a ) 'The model has ' + str ( len ( self.outputs ) ) from .. import losses len ( y.shape ) ! = 2 ) : y = np.array ( [ [ 0.5 ] , [ 2 . ] , [ 3.5 ] ] ) } # mse = [ ( ( 4 - 1 ) ^2 + ( 8 - 9 ) ^2 ) / 2 , ( ( 12 - 2 ) ^2 + ( 3 - 5 ) ^2 ) / 2 ] from .training_utils import check_loss_and_target_compatibility else : sample_weight_modes = [ ] sample_weights [ i ] for i , ( y_true , y_pred , loss_fn , sample_weight , mask , ( loss.fn in key_loss_fns ) ) ) : ' ` sample_weight ` { } '.format ( # reduced_weighted_mse = ( 6 + 26 ) / 2 = K.placeholder ( ndim=2 , elif isinstance ( loss_weights , dict ) : x = standardize_input_data ( # Prepare gradient updates and state updates . self._feed_sample_weights = [ ] 'be expecting any data to be passed to { 0 } . '.format ( name ) ) static_batch_size = get_static_batch_size ( first_layer ) def test_loss_correctness ( ) : 'Only expected the following keys : ' 'sample_weight_mode dictionary : `` ' + str ( unknown_output ) total_loss += loss_weight * output_loss if name not in sample_weight_mode : masks : List of mask values corresponding to each model output . self._feed_sample_weight_modes = [ ] 'because it has no loss to optimize . ' ) def prepare_sample_weights ( output_names , sample_weight_mode , loss == 'categorical_crossentropy ' ) # Arguments y_true = K.constant ( [ [ 1. , 9 . ] , [ 2. , 5 . ] ] ) [ a ] , [ losses.categorical_crossentropy ] , [ a.shape ] ) import six if len ( self.outputs ) > 1 : loss=keras.losses.MeanAbsoluteError ( ) ) sample_weight_modes [ i ] return losses.LossFunctionWrapper ( mode = sample_weight_mode [ i ] loss_type = loss.fn if is_loss_wrapper else type ( loss ) sample_weight_mode , output_name , loss_name = loss_name.__name__ if mode == 'temporal ' : zipped_inputs = zip ( self.targets , self.outputs , self.loss_functions , skip_target_indices = skip_target_indices or [ ] 'We assume this was done on purpose , ' will then be the sum of all individual losses . 'because it has no loss to optimize . ' ) function is None . if isinstance ( loss , dict ) : # Broadcast weights if possible . # Compute total loss . continue if masks is None : # not in the ` losses ` module , then it is a user-defined loss expected_values ) ) # and other layer-specific losses . 'and we will not be expecting ' raise ValueError ( 'When passing a list as sample_weight_mode , ' # Prepare loss functions . weights_list = [ loss_weights.get ( name , 1 . ) for name in output_names ] weights_list = [ 1 . ] * len ( output_names ) str ( sample_weight_mode ) ) sample_weight_modes = [ ] self.metrics_names.append ( self.output_names [ i ] + '_loss ' ) This helps prevent users from using loss functions incorrectly . This check [ a ] , [ losses.CategoricalCrossentropy ( ) ] , [ a.shape ] ) skip_target_weighing_indices , sample_weight_mode , name , i ) else : weights_shape = K.int_shape ( sample_weight ) 'following keys : { } '.format ( name , list ( unknown ) , weights_rank = K.ndim ( sample_weight ) weighted_losses = losses * sample_weight if unknown_output : np.allclose ( K.eval ( loss ) , 16 , atol=1e-2 ) name=name + '_sample_weights ' ) sample_weights.append ( None ) # Prepare loss weights . def test_loss_wrapper ( self ) : if sample_weight_mode == 'temporal ' : if callable ( loss ) and not hasattr ( loss , '__name__ ' ) : self.sample_weights = sample_weights losses.categorical_crossentropy } nested_metrics = collect_metrics ( metrics , self.output_names ) weighted_masked_objective ( fn ) for fn in loss_functions ] y = training_utils.standardize_input_data ( This helps prevent users from using loss functions incorrectly . ( Python floats ) to weight the loss contributions of different model unknown_output = set ( sample_weight_mode.keys ( ) ) - set ( output_names ) if i in skip_target_weighing_indices : mask , _ , sample_weight = ( loss_config = losses.get ( loss_config ) if batch_size is not None and training_utils.is_generator_or_sequence ( x ) : sample_weight_modes.append ( 'temporal ' ) for i in range ( len ( self.output_names ) ) : if ( ( isinstance ( loss_fn , losses.LossFunctionWrapper ) and `` `` '' Converts loss to a list of loss functions . loss_name = self.model.loss if sample_weight_mode.get ( name ) == 'temporal ' : 'dictionary ' ) sample_weight_mode.get ( name ) , total_loss = loss_weight * output_loss sample_weight_mode : sample weight mode user input passed from compile API . assert ( mse_obj.reduction == losses_utils.Reduction.SUM_OVER_BATCH_SIZE ) if sample_weight_mode == 'temporal ' : ' '' missing from sample_weight_modes ' weighted_losses = sample_weight * losses if loss is losses.categorical_crossentropy : # Deserialize loss configuration , if needed . loss_weights_list.append ( loss_weights.get ( name , 1 . ) ) weights = self.sample_weights [ i ] masks = self.compute_mask ( self.inputs , mask=None ) if weights_shape [ i ] ! = 1 : y_true , y_pred , sample_weight=sample_weight ) if len ( loss_weights ) ! = len ( self.outputs ) : elif isinstance ( loss , six.string_types ) : self.output_names , sample_weight_mode , skip_target_weighing_indices ) # and we make no assumptions about it . output_metrics = to_list ( output_metrics ) `` `` '' Prepares sample weights for the model . sample_weights.append ( 'Output { 0 } missing from loss dictionary . We assume ' ' '' during training . ' , stacklevel=2 ) raise ValueError ( 'When passing a list as loss , it should have one entry ' if i not in skip_target_weighing_indices : if loss is None or isinstance ( loss , losses.Loss ) : with K.name_scope ( 'loss ' ) : # Wrap loss function with signature ` ( y_true , y_pred , * * kwargs ) ` def get_output_sample_weight_and_mode ( skip_target_weighing_indices , assert mse_obj.name == 'mean_squared_error ' training_utils.check_generator_arguments ( loss = convert_custom_objects ( loss_config ) raise ValueError ( 'The model can not be compiled ' raise ValueError ( 'When passing a list as loss_weights , ' if is_generator_or_sequence ( x ) : keras.optimizers.SGD ( lr=0.1 ) , x = np.array ( [ [ 0 . ] , [ 1 . ] , [ 2 . ] ] ) if losses.is_categorical_crossentropy ( loss ) : loss_fn.fn == losses.sparse_categorical_crossentropy ) ) or ( mask = masks [ i ] def test_training_with_loss_instance ( ) : `` `` '' Sets sample weight related attributes on the model . '' '' '' name=output_name + '_sample_weights ' ) self.metrics_tensors.append ( output_loss ) model = Model ( inp , out ) from . import training_utils raise ValueError ( 'Output `` ' + name unknown = set ( input_dict.keys ( ) ) .difference ( expected_values ) with K.name_scope ( loss_name ) : generic_utils.check_for_unexpected_keys ( 'loss ' , loss , output_names ) metrics= [ 'mae ' ] , if len ( loss_weights ) ! = len ( output_names ) : metric_fn ) # For losses which are given as strings/functions in the compile API , def prepare_loss_weights ( output_names , loss_weights=None ) : ' '' missing from loss dictionary . ' else : skip_target_indices = [ ] def __call__ ( self , y_true , y_pred ) : losses.categorical_crossentropy if loss_name is None : output_loss = loss_fn ( loss_functions = [ ] key_losses = { losses.mean_squared_error , skip_target_weighing_indices , shape = [ None ] if losses_shape ! = weights_shape : sample_weights = [ ] # about it . output_index ) : skip_target_indices = [ ] losses_utils.squeeze_or_expand_dimensions ( self.total_loss = total_loss training_utils.check_array_length_consistency ( x , y , sample_weights ) total_loss += loss_tensor model.compile ( loss=losses.MeanSquaredError ( ) , else : # Raise error if ndim of weights is > losses . self.output_names ) if len ( loss ) ! = len ( self.outputs ) : self.sample_weights , masks , self.loss_weights_list ) self._set_sample_weight_attributes ( loss_functions = [ get_loss_function ( loss ) for _ in range ( len ( output_names ) ) ] if name not in loss : `` `` '' Computes total loss from loss functions . raise ValueError ( 'The model can not be compiled ' elif isinstance ( loss_weights , list ) : str ( self.output_names ) ) self.bias = self.add_weight ( 'bias ' , ( 1 , ) , initializer='zeros ' ) mask = math_ops.cast ( mask , y_pred.dtype ) 'The model has ' + str ( len ( output_names ) ) weighted_metric_fn = training_utils.weighted_masked_objective ( 'dictionary : `` ' + name + ' '' . ' sample_weight_modes.append ( None ) total_loss += loss_tensor check_loss_and_target_compatibility ( shape=shape , skip_target_weighing_indices , sample_weight_mode [ i ] , name , i ) A list of loss objective functions . total_loss = 0 . y_pred = self.outputs [ i ] model.fit ( [ input_a_np , input_b_np ] , [ output_d_np , output_e_np ] , ValueError : In case of invalid ` sample_weight_mode ` input . 'it should have one entry per model output . ' check_array_length_consistency ( x , y , sample_weights ) if loss_weights is None : for name in self.output_names : # Raises : def _prepare_total_loss ( self , skip_target_indices=None , masks=None ) : # ( custom class ) or if the loss function that is wrapped is # Prepare list loss weights , same size of model outputs . str ( loss ) ) def call ( self , inputs ) : if name not in loss : ] if isinstance ( loss_config , dict ) and 'class_name ' in loss_config : if loss_function is None : # weighted_mse = [ 5 * 1.2 , 52 * 0.5 ] = [ 6 , 26 ] masks = [ None for _ in self.outputs ] loss_weight = loss_weights_list [ i ] getattr ( losses , loss_fn.__name__ , None ) is None ) : i ) if isinstance ( loss , collections.Mapping ) : if len ( sample_weight_mode ) ! = len ( self.outputs ) : out = Bias ( ) ( inp ) self.metrics_names.append ( self.output_names [ i ] + '_loss ' ) for i in range ( len ( sample_weights ) ) loss_fn , raise ValueError ( from .training_utils import standardize_class_weights 'any data to be passed to `` ' + name raise TypeError ( 'Could not interpret loss_weights argument : ' input_a_np = np.random.random ( ( 10 , 3 ) ) if ( weights_shape [ i ] is not None and losses_shape [ i ] is not None and loss_name = loss_type.__name__ loss_functions = [ loss_function for _ in range ( len ( self.outputs ) ) ] ( one for each output ) . for name in sample_weight_mode : epochs=1 , loss_function = losses.get ( loss ) self.total_loss = self._prepare_total_loss ( skip_target_indices , masks ) for i in range ( len ( self.sample_weights ) ) : sample_weights , sample_weight_modes = training_utils.prepare_sample_weights ( for name in loss_weights : class_weights = training_utils.standardize_class_weights ( isinstance ( from .training_utils import get_static_batch_size output_d_np = np.random.random ( ( 10 , 4 ) ) 'passed loss= { } '.format ( len ( output_names ) , loss ) ) loss : String ( name of objective function ) or objective function or name , # Update dimensions of weights to match with mask . total_loss += loss_weight * output_loss 'Incompatible shapes : ` losses ` { } vs ' 'sample_weight_mode= ' skip_target_weighing_indices ) : skip_target_weighing_indices ) : str ( len ( sample_weight_mode ) ) + 'sample_weight_modes ' ) weight = K.placeholder ( ndim=2 , self._feed_sample_weights = [ str ( loss_weights ) ) batch_size=5 ) # loss_weight_2 * output_2_loss_fn ( ... ) output_e_np = np.random.random ( ( 10 , 4 ) ) elif ( not isinstance ( loss_fn , losses.Loss ) or if loss_fn is losses.sparse_categorical_crossentropy : for i in range ( weights_rank , losses_rank ) : # Add regularization penalties from .. utils.generic_utils import to_list ` Loss ` instance . See [ losses ] ( /losses ) . for i in range ( len ( self.outputs ) ) loss_name = self.output_names [ i ] + '_loss ' if hasattr ( loss_name , '__name__ ' ) : weight = K.placeholder ( self.sample_weight_modes [ i ] ) if loss in key_losses : skip_target_weighing_indices : Indices of output for which sample weights class Bias ( Layer ) : self.sample_weight_modes = sample_weight_modes continue loss_functions = [ ] skip_target_indices : A list of indices of model outputs where loss loss_weight ) in enumerate ( zipped_inputs ) : ' '' . Only expected the following keys : ' + str ( output_names ) ) nested_weighted_metrics = training_utils.collect_metrics ( `` `` '' Converts loss weights to a list of loss weights . str ( loss_weights ) outputs . The loss value that will be minimized by the model will then be name = self.output_names [ i ] for name in output_names : weights_shape [ i ] ! = losses_shape [ i ] ) : training_utils.standardize_weights ( ref , sw , cw , mode ) losses_shape , weights_shape ) ) if training_utils.is_generator_or_sequence ( x ) : d = dense ( b ) elif isinstance ( loss_weights , list ) : a = Input ( shape= ( 3 , ) , name='input_a ' ) sample_weight = K.constant ( [ 1.2 , 0.5 ] ) K.placeholder ( ndim=1 , if i in skip_target_indices : # If ` loss_fn ` is not a function ( e.g . callable class ) y = standardize_input_data ( loss_functions = [ get_loss_function ( loss ) for _ in output_names ] ' outputs , but you passed loss_weights= ' if weights_rank > losses_rank : key_loss_classes = ( losses.MeanSquaredError , losses.BinaryCrossentropy , name=loss_fn.__name__ , masks = to_list ( masks ) for i , name in enumerate ( self.output_names ) : if total_loss is None : self._feed_sample_weight_modes.append ( # in ` LossFunctionWrapper ` class . raise ValueError ( 'Unknown entry in loss_weights ' return loss loss : String ( name of objective function ) or objective function . output_names ) loss = losses.get ( loss ) name + ' '' . ' output names ( strings ) to scalar coefficients . check_generator_arguments ( y , sample_weight ) loss_fn = losses.get ( 'mse ' ) weighted_loss = weighted_losses [ i ] self.sample_weight_modes = sample_weight_modes A list of loss weights of python floats . sample_weights = training_utils.standardize_sample_weights ( from .. utils import generic_utils losses.mean_squared_error , losses.binary_crossentropy , loss_fn , losses.SparseCategoricalCrossentropy ) ) : def __call__ ( self , y_true , y_pred , sample_weight=None ) : def build ( self , input_shape ) : weighted_metric_fn = weighted_masked_objective ( metric_fn ) loss_weights_list = loss_weights if not self.losses : raise ValueError ( 'Unknown entry in ' from keras.layers import Layer , Activation , Dense , Dropout , Conv2D , Concatenate loss_weights=loss_weights ) nested_metrics = training_utils.collect_metrics ( metrics , self.output_names ) inp = Input ( shape= ( 1 , ) ) generic_utils.check_for_unexpected_keys ( 'loss_weights ' , loss_weights , key_loss_fns = { losses_shape = K.int_shape ( losses ) self.loss_functions = loss_functions elif isinstance ( loss , list ) : def prepare_loss_functions ( loss , output_names ) : ` loss_weights ` coefficients . If a list , it is expected to have a 1:1 sample_weight_modes.append ( None ) if len ( sample_weight_mode ) ! = len ( output_names ) : if ( isinstance ( loss , key_loss_classes ) or ( is_loss_wrapper and mask , None , sample_weight ) ) ' outputs , but you passed loss_weights= ' elif ( not hasattr ( loss_fn , '__name__ ' ) or total_loss = 0 . model.compile ( # Used to keep track of the total loss value ( stateless ) . # if loss function is None , then this output will be skipped during total with K.name_scope ( 'loss ' ) : nested_weighted_metrics = collect_metrics ( weighted_metrics , # loss calculation and feed targets preparation . loss_fn = losses.get ( loss ) is purely for UX purposes . from .. utils import losses_utils # Can not be broadcasted . self.loss = loss or [ ] weight , mode = get_output_sample_weight_and_mode ( for name in loss : 'targets to have the same shape ' training_utils.check_loss_and_target_compatibility ( str ( loss_weights ) ) self.loss , self.output_names ) loss == 'categorical_crossentropy ' ) ) # it is a user-defined loss and we make no assumptions `` `` '' 'This loss expects ' elif isinstance ( sample_weight_mode , list ) : def check_for_unexpected_keys ( name , input_dict , expected_values ) : # eg. , total_loss = loss_weight_1 * output_1_loss_fn ( ... ) or if loss is a list with len not equal to model outputs . if loss_weights is None : from .training_utils import standardize_weights self.loss_weights_list = training_utils.prepare_loss_weights ( raise ValueError ( 'Unknown entries in { } dictionary : { } . Only expected ' sample_weight_mode , skip_target_weighing_indices ) # Returns skip_target_weighing_indices = [ ] ( isinstance ( loss_fn , losses.LossFunctionWrapper ) and shape = [ None , None ] # Returns if loss_name == 'categorical_crossentropy ' and len ( y.shape ) ! = 2 : loss : String ( name of objective function ) , objective function or name not in sample_weight_mode ) : output_metrics = generic_utils.to_list ( output_metrics ) 'Unknown entry in ' if isinstance ( sample_weight_mode , dict ) : if ( losses.is_categorical_crossentropy ( self.model.loss ) and # Custom callable class . sample_weight , mask ) standardize_weights ( ref , sw , cw , mode ) [ a ] , [ losses.categorical_crossentropy ] , [ ( 2 , 3 , 6 ) ] ) sample_weight = mask list of losses . The loss value that will be minimized by the model sample_weights.append ( weight ) weighted_metrics , self.output_names ) # layer losses . from .training_utils import check_generator_arguments is_loss_wrapper = isinstance ( loss , losses.LossFunctionWrapper ) `` `` '' Returns the sample weight and weight mode for a single output . '' '' '' from .training_utils import weighted_masked_objective str ( loss_weights ) + ' - expected a list of dicts . ' ) raise ValueError ( 'When passing a list as sample_weight_mode , ' if output_index in skip_target_weighing_indices : 'it should have one entry per model outputs . ' ' - expected a list of dicts . ' ) return sample_weights , sample_weight_modes name=name + '_sample_weights ' ) ) if sample_weight is None : sample_weight_modes.append ( mode ) self._feed_sample_weights.append ( sample_weights [ i ] ) See [ losses ] ( /losses ) . `` `` '' Returns the loss corresponding to the loss input in ` compile ` API . '' '' '' [ a ] , [ losses.CategoricalCrossentropy ( ) ] , [ ( 2 , 3 , 6 ) ] ) # Compute total loss . loss_weights_list = [ ] static_batch_size = training_utils.get_static_batch_size ( first_layer ) elif isinstance ( loss , collections.Sequence ) : for i in range ( weights_rank ) : for i in range ( len ( weighted_losses ) ) : losses_shape , weights_shape ) ) training_utils.check_generator_arguments ( y , sample_weight )","['keras/engine/saving.py', 'keras/engine/training.py', 'keras/engine/training_utils.py', 'keras/losses.py', 'keras/utils/generic_utils.py', 'keras/utils/losses_utils.py', 'keras/wrappers/scikit_learn.py', 'tests/keras/engine/test_training.py', 'tests/keras/losses_test.py', 'tests/test_model_saving.py']",Adding support for ` Loss ` instances in model compile . ( # 12915 )
37,b2f989889cf7f68f8573eed70875e283dbdaf2a8,2019-06-01 14:55:09-07:00,dilated_filter_size = filter_size + ( filter_size - 1 ) * ( dilation - 1 ) dilated_filter_size = ( filter_size - 1 ) * dilation + 1 kernel_size = ( kernel_size - 1 ) * dilation + 1 kernel_size = kernel_size + ( kernel_size - 1 ) * ( dilation - 1 ),['keras/utils/conv_utils.py'],More intuitive dilated kernel size calculation ( # 12904 )
38,ab3ef6f3f0b859e5b501aa696f388f277cb31de2,2019-06-01 14:51:54-07:00,"super ( CategoricalCrossentropy , self ) .__init__ ( return K.sparse_categorical_crossentropy ( name=name , model = keras.Model ( inputs , outputs ) # -0 * ( 1 - L + L/n ) + 200 * L/n + 200 * L/n = 400 L/n from_logits=from_logits , cce_obj = losses.SparseCategoricalCrossentropy ( ) class TestCategoricalCrossentropy : assert cce_obj.name == 'scc ' # so our log softmaxes become : [ 0 , -200 , -200 ] example . The shape of both ` y_pred ` and ` y_true ` are In the snippet below , there is a single floating point value per example for # \log q_i = x_i - \log \sum_j \exp x_j and a single floating point value per feature for ` y_true ` . def test_no_reduction ( self ) : return ( isinstance ( loss , CategoricalCrossentropy or assert np.isclose ( K.eval ( loss ) , 0.31829 , atol=1e-3 ) def sparse_categorical_crossentropy ( y_true , y_pred , from_logits=False , axis=-1 ) : def categorical_crossentropy ( y_true , y_pred , from_logits=False , label_smoothing=0 ) : name='categorical_crossentropy ' ) : provide labels as integers , please use ` SparseCategoricalCrossentropy ` loss . y_pred = K.constant ( y_pred ) assert np.isclose ( K.eval ( loss ) , .05737 , atol=1e-3 ) assert np.isclose ( K.eval ( loss ) , .132 , atol=1e-3 ) loss = cce_obj ( y_true , logits , sample_weight=sample_weight ) if not K.is_tensor ( y_pred ) : # Test with logits . assert np.isclose ( K.eval ( loss ) , .1317 , atol=1e-3 ) from_logits=False , assert np.allclose ( K.eval ( loss ) , ( 0.001822 , 0.000459 , 0.169846 ) , atol=1e-3 ) def test_sample_weighted ( self ) : cce_obj = losses.SparseCategoricalCrossentropy ( from_logits=True ) `` `` '' Computes the crossentropy loss between the labels and predictions . ` [ batch_size , num_classes ] ` . class CategoricalCrossentropy ( LossFunctionWrapper ) : y_pred = K.constant ( [ [ 1. , 0. , 0 . ] , [ 0. , 1. , 0 . ] , [ 0. , 0. , 1 . ] ] ) from_logits=True , label_smoothing=label_smoothing ) logits = K.constant ( [ [ 100.0 , -100.0 , -100.0 ] ] ) label_smoothing = 0.1 # Softmax Cross Entropy Loss : -\sum_i p_i \log q_i assert np.isclose ( K.eval ( loss ) , 1.0696 , atol=1e-3 ) loss.__name__ == 'categorical_crossentropy ' ) or num_classes = K.cast ( K.shape ( y_true ) [ 1 ] , y_pred.dtype ) super ( SparseCategoricalCrossentropy , self ) .__init__ ( logits = K.constant ( [ [ 10. , 0. , 0 . ] , [ 0. , 10. , 0 . ] , [ 0. , 0. , 10 . ] ] ) Standalone usage : return y_true * ( 1.0 - label_smoothing ) + ( label_smoothing / num_classes ) y_true = K.constant ( [ [ 1 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] ) from_logits=True , reduction=losses_utils.Reduction.NONE ) label_smoothing : Float in [ 0 , 1 ] . When 0 , no smoothing occurs . When > 0 , we y_true = K.switch ( K.greater ( label_smoothing , 0 ) , _smooth_labels , lambda : y_true ) ( isinstance ( loss , LossFunctionWrapper ) and assert cce_obj.name == 'bce_1 ' def test_all_correct_unweighted ( self ) : Use this crossentropy loss function when there are two or more label classes . reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE , y_pred = K.constant ( loss = cce_obj ( y_true , y_pred , sample_weight=2.3 ) class SparseCategoricalCrossentropy ( LossFunctionWrapper ) : assert np.isclose ( K.eval ( loss ) , .0573 , atol=1e-3 ) def test_config ( self ) : assert np.isclose ( K.eval ( loss ) , .7449 , atol=1e-3 ) cce_obj = losses.CategoricalCrossentropy ( Default value is ` SUM_OVER_BATCH_SIZE ` . def test_label_smoothing ( self ) : ( hasattr ( loss , '__name__ ' ) and ( i.e. , values in [ 0 , 1 ] ) . the true labels , where the smoothing squeezes the labels towards 0.5 . expected_value = 400.0 * label_smoothing / 3.0 sample_weight = K.constant ( [ [ 1.2 ] , [ 3.4 ] , [ 5.6 ] ] , shape= ( 3 , 1 ) ) `` ` from_logits : Whether to interpret ` y_pred ` as a tensor of reduction=reduction , loss = cce ( def test_unweighted ( self ) : def __init__ ( self , return K.categorical_crossentropy ( y_true , y_pred ) We expect labels to be provided in a ` one_hot ` representation . If you want to logits = K.constant ( [ [ 8. , 1. , 1 . ] , [ 0. , 9. , 1 . ] , [ 2. , 3. , 5 . ] ] ) # = x_i - x_max - \log \sum_j \exp ( x_j - x_max ) cce_obj = losses.CategoricalCrossentropy ( ) Usage with the ` compile ` API : # where for a softmax activation name : ( Optional ) Name for the op . y_true , y_pred , from_logits=from_logits , axis=axis ) # 0 = L/n def is_categorical_crossentropy ( loss ) : [ 0 , 1 , 2 ] , [ logit ] ( https : //en.wikipedia.org/wiki/Logit ) values . By default , There should be ` # classes ` floating point values per feature for ` y_pred ` return K.sparse_categorical_crossentropy ( y_true , y_pred ) loss = cce_obj ( y_true , y_pred , sample_weight=sample_weight ) reduction : ( Optional ) Type of loss reduction to apply to loss . # Label smoothing : z ' = z * ( 1 - L ) + L/n `` `` '' cce = keras.losses.CategoricalCrossentropy ( ) model.compile ( 'sgd ' , loss=keras.losses.SparseCategoricalCrossentropy ( ) ) # Applying the above two fns to the given input : cce_obj = losses.CategoricalCrossentropy ( from_logits=True ) from_logits=from_logits ) cce_obj = losses.SparseCategoricalCrossentropy ( def categorical_crossentropy ( y_true , y_pred ) : reduction=losses_utils.Reduction.SUM , name='bce_1 ' ) class TestSparseCategoricalCrossentropy : assert cce_obj.reduction == losses_utils.Reduction.SUM y_true = K.constant ( [ [ 0 ] , [ 1 ] , [ 2 ] ] ) reduction=losses_utils.Reduction.SUM , name='scc ' ) y_true = K.constant ( [ [ 1 , 0 , 0 ] ] ) y_true = K.constant ( [ 0 , 1 , 2 ] ) [ [ .9 , .05 , .05 ] , [ .5 , .89 , .6 ] , [ .05 , .01 , .94 ] ] ) There should be ` # classes ` floating point values per feature . ` y_true ` and ` # classes ` floating pointing values per example for ` y_pred ` . [ [ .9 , .05 , .05 ] , [ .5 , .89 , .6 ] , [ .05 , .01 , .94 ] ] ) y_true = K.cast ( y_true , y_pred.dtype ) assert np.isclose ( K.eval ( loss ) , .3239 , atol=1e-3 ) `` ` python sparse_categorical_crossentropy , loss.fn == categorical_crossentropy ) or assert np.isclose ( K.eval ( loss ) , expected_value , atol=1e-3 ) categorical_crossentropy , loss = cce_obj ( y_true , logits ) We expect labels to be provided as integers . If you want to provide labels assert np.isclose ( K.eval ( loss ) , 0.0 , atol=1e-3 ) name='sparse_categorical_crossentropy ' ) : we assume that ` y_pred ` contains probabilities # Arguments return K.categorical_crossentropy ( y_true , y_pred , from_logits=from_logits ) # For our activations , [ 100 , -100 , -100 ] loss == 'categorical_crossentropy ' ) ) using ` one-hot ` representation , please use ` CategoricalCrossentropy ` loss . label_smoothing = K.cast_to_floatx ( label_smoothing ) compute the loss between the predicted labels and a smoothed version of def test_scalar_weighted ( self ) : label_smoothing=label_smoothing ) The shape of ` y_true ` is ` [ batch_size ] ` and the shape of ` y_pred ` is # 1 = 1 - L + L/n def _smooth_labels ( ) : In the snippet below , there is ` # classes ` floating pointing values per model.compile ( 'sgd ' , loss=keras.losses.CategoricalCrossentropy ( ) ) loss = cce_obj ( y_true , logits , sample_weight=2.3 ) # \log ( exp ( 0 ) + exp ( -200 ) + exp ( -200 ) ) = 0 [ [ 1. , 0. , 0 . ] , [ 0. , 1. , 0 . ] , [ 0. , 0. , 1 . ] ] , Larger values of ` label_smoothing ` correspond to heavier smoothing . label_smoothing=0 , cce = keras.losses.SparseCategoricalCrossentropy ( ) def sparse_categorical_crossentropy ( y_true , y_pred ) : loss = cce_obj ( y_true , y_pred )","['keras/losses.py', 'tests/keras/losses_test.py']",Adding CategoricalCrossentropy and SparseCategoricalCrossentropy Loss classes ( # 12903 )
39,3d48e278f8470f7f6fb9aead5521684f618ebc07,2019-05-31 10:08:35-07:00,"name=name , model.compile ( 'sgd ' , loss=keras.losses.MeanAbsoluteError ( ) ) # Reduced loss = ( 0 + 66.666 * 3 ) / 2 return y_true * ( 1.0 - label_smoothing ) + 0.5 * label_smoothing super ( BinaryCrossentropy , self ) .__init__ ( model = keras.Model ( inputs , outputs ) assert np.allclose ( K.eval ( loss ) , 2.6473 , rtol=1e-3 ) def test_invalid_sample_weight ( self ) : weights = K.constant ( [ 4 , 3 ] ) from_logits=from_logits , mae_obj = losses.MeanAbsoluteError ( ) # ( 100 - 100 * 1 + log ( 1 + exp ( -100 ) ) ) assert np.isclose ( K.eval ( loss ) , 8.817 , rtol=1e-3 ) reduction=losses_utils.Reduction.SUM ) y_true = K.constant ( [ [ 1. , 0. , 0 . ] , [ 0. , 1. , 0 . ] , [ 0. , 0. , 1 . ] ] ) [ -100.0 , 100.0 , -100.0 ] , logits = K.constant ( [ [ 100.0 , -100.0 , -100.0 ] , name='mean_squared_logarithmic_error ' ) : # ( 100 - 100 * 1 + log ( 1 + exp ( -100 ) ) ) , `` `` '' Computes the mean of absolute difference between labels and predictions . loss = mape ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) assert mae_obj.name == 'mae_1 ' loss = mape_obj ( y_true , y_pred ) # y ` = clip ( output , EPSILON , 1 . - EPSILON ) loss = bce_obj ( y_true , logits ) logits = K.constant ( [ [ 100.0 , -100.0 , 100.0 ] , [ 100.0 , 100.0 , -100.0 ] ] ) self.from_logits = from_logits assert np.isclose ( K.eval ( loss ) , 587 / 6 , rtol=1e-3 ) assert np.isclose ( K.eval ( loss ) , 3.833 , rtol=1e-3 ) # Weighted loss = [ 0 * 2.3 , 66.666 * 2.3 ] In the snippet below , each of the four examples has only a single msle_obj = losses.MeanSquaredLogarithmicError ( ) y_true = K.constant ( [ [ 1 , 0 , 1 ] ] ) mape = keras.losses.MeanAbsolutePercentageError ( ) then the mean absolute percentage error value is 5e+08 . # Reduced loss = 15.33 * 1.2 / 4 def test_no_reduction ( self ) : y_true = K.constant ( [ 1 , 9 , 2 , -5 , -2 , 6 ] , shape= ( 2 , 3 ) ) # Reduced loss = ( 0 + 66.666 * 2.3 ) / 2 class TestMeanAbsolutePercentageError : loss = bce_obj ( y_true , y_pred , sample_weight=sample_weight ) `` `` '' Computes the mean absolute percentage error between ` y_true ` and ` y_pred ` . loss = msle_obj ( y_true , y_pred , sample_weight=2.3 ) loss = bce_obj ( y_true , y_pred , sample_weight=2.3 ) assert np.allclose ( K.eval ( loss ) , 3.7856 , rtol=1e-3 ) # ( where x = logits and z = y_true ) binary_crossentropy , # -log ( Y_MAX + EPSILON ) , -log ( 1 ) ] # Loss = [ ( 0 + 0 + 0 ) /3 , 200 / 3 ] sample_weight = K.constant ( [ 1.2 , 3.4 ] , shape= ( 2 , 1 ) ) y_pred = K.constant ( y_pred ) y_true = K.constant ( [ 1 , 9 , 2 , -5 , -2 , 6 ] , shape= ( 2 , 3 , 1 ) ) bce_obj = losses.BinaryCrossentropy ( loss = msle_obj ( y_true , y_pred , sample_weight=sample_weight ) loss = msle_obj ( y_true , y_pred , sample_weight=0 ) name='binary_crossentropy ' ) : if not K.is_tensor ( y_pred ) : loss = mae_obj ( y_true , y_true ) # Test with logits . # Reduced loss = 15.33 / 4 return K.mean ( K.binary_crossentropy ( y_true , y_pred ) , axis=-1 ) class MeanAbsoluteError ( LossFunctionWrapper ) : from_logits=False , y_true = np.asarray ( [ 1 , 0 , 1 , 0 ] ) .reshape ( [ 2 , 2 ] ) def test_sample_weighted ( self ) : # = [ ( 0 + 0 + 0 ) / 3 , 200 / 3 ] y_pred = np.asarray ( [ 1 , 1 , 1 , 0 ] , dtype=np.float32 ) .reshape ( [ 2 , 2 ] ) expected_value = ( 100.0 + 50.0 * label_smoothing ) / 3.0 from_logits=True , label_smoothing=label_smoothing ) mean_absolute_error , name=name , reduction=reduction ) assert np.allclose ( K.eval ( loss ) , 422.8888 , rtol=1e-3 ) assert np.isclose ( K.eval ( loss ) , 97.833 , rtol=1e-3 ) logits = K.constant ( [ [ 100.0 , -100.0 , -100.0 ] ] ) assert np.allclose ( K.eval ( loss ) , ( 0. , 66.6666 ) , rtol=1e-3 ) label_smoothing = 0.1 reduction=losses_utils.Reduction.SUM , name='mape_1 ' ) loss = mae ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) mean_squared_logarithmic_error , name=name , reduction=reduction ) y_true = K.constant ( [ 4 , 8 , 12 , 8 , 1 , 3 ] , shape= ( 2 , 3 ) ) assert msle_obj.name == 'mape_1 ' class TestMeanAbsoluteError : y_pred = K.constant ( [ 4 , 8 , 12 , 8 , 1 , 3 ] , shape= ( 2 , 3 ) ) assert np.isclose ( K.eval ( loss ) , 12.65 , rtol=1e-3 ) # 0 + 100 * ( 1 - 0.5 L ) + 0 ) * ( 1/3 ) class TestMeanSquaredLogarithmicError : loss = mae_obj ( y_true , y_pred ) loss = mae_obj ( y_true , y_pred , sample_weight=sample_weight ) y_true = K.constant ( [ [ 1. , 0. , 1 . ] , [ 0. , 1. , 1 . ] ] ) Standalone usage : # Weighted loss = [ 0 , 15.33 * 2.3 , 0 , 0 ] from_logits=True , reduction=losses_utils.Reduction.NONE ) label_smoothing : Float in [ 0 , 1 ] . When 0 , no smoothing occurs . When > 0 , we y_true = K.switch ( K.greater ( label_smoothing , 0 ) , _smooth_labels , lambda : y_true ) super ( MeanSquaredLogarithmicError , self ) .__init__ ( # EPSILON = 1e-7 , y = y_true , y ` = y_pred , Y_MAX = 0.9999999 # = [ ( ( 100 - 100 * 1 + log ( 1 + exp ( -100 ) ) ) name : ( Optional ) name for the loss . def test_all_correct_unweighted ( self ) : reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE , `` `` '' Computes the mean squared logarithmic error between ` y_true ` and ` y_pred ` . assert np.allclose ( K.eval ( loss ) , 487.259 , rtol=1e-3 ) def test_config ( self ) : assert np.isclose ( K.eval ( loss ) , 4.6 , rtol=1e-3 ) Use this cross-entropy loss when there are only two label classes ( assumed to mape_obj = losses.MeanAbsolutePercentageError ( class MeanAbsolutePercentageError ( LossFunctionWrapper ) : class MeanSquaredLogarithmicError ( LossFunctionWrapper ) : Default value is ` SUM_OVER_BATCH_SIZE ` . mape_obj = losses.MeanAbsolutePercentageError ( ) bce_obj = losses.BinaryCrossentropy ( from_logits=True ) bce_obj = losses.BinaryCrossentropy ( ) def binary_crossentropy ( y_true , y_pred , from_logits=False , label_smoothing=0 ) : sample_weight = K.constant ( [ 3 , 6 , 5 , 0 ] , shape= ( 2 , 2 ) ) reduction=losses_utils.Reduction.SUM , name='mae_1 ' ) reduction=losses_utils.Reduction .SUM , name='mape_1 ' ) def test_label_smoothing ( self ) : def test_timestep_weighted ( self ) : # = [ 0 , 15.33 , 0 , 0 ] loss = mape_obj ( y_true , y_pred , sample_weight=0 ) ( i.e. , values in [ 0 , 1 ] ) . per prediction . the true labels , where the smoothing squeezes the labels towards 0.5 . y_pred = K.constant ( [ 4 , 8 , 12 , 8 , 1 , 3 ] , shape= ( 2 , 3 , 1 ) ) assert np.isclose ( K.eval ( loss ) , 0.0 , rtol=1e-3 ) assert np.allclose ( K.eval ( loss ) , [ 10.7333 , 14.5666 ] , rtol=1e-3 ) `` ` from_logits : Whether to interpret ` y_pred ` as a tensor of model.compile ( 'sgd ' , loss=keras.losses.MeanAbsolutePercentageError ( ) ) reduction=reduction , # Label smoothing : z ' = z * ( 1 - L ) + 0.5L K.binary_crossentropy ( y_true , y_pred , from_logits=from_logits ) , axis=-1 ) def test_unweighted ( self ) : # = [ -log ( Y_MAX + EPSILON ) , -log ( 1 - Y_MAX + EPSILON ) , assert np.isclose ( K.eval ( loss ) , 0.0 , 3 ) def __init__ ( self , # Loss : max ( x , 0 ) - x * z + log ( 1 + exp ( -abs ( x ) ) ) loss = bce ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) def test_sum_reduction ( self ) : Usage with the ` compile ` API : loss = mae_obj ( y_true , y_pred , sample_weight=2.3 ) loss = mape_obj ( y_true , y_pred , sample_weight=2.3 ) # Loss = [ ( 0 + 0 + 0 ) / 3 , 200 / 3 ] # ( ( 100 - 100 * 0 + log ( 1 + exp ( -100 ) ) ) name : ( Optional ) Name for the op . loss = bce_obj ( y_true , y_pred ) assert np.isclose ( K.eval ( loss ) , 25.29999 , rtol=1e-3 ) loss = bce_obj ( y_true , y_true ) assert np.isclose ( K.eval ( loss ) , 81.4 / 6 , rtol=1e-3 ) assert np.allclose ( K.eval ( loss ) , 1.4370 , rtol=1e-3 ) assert np.allclose ( K.eval ( loss ) , 3.3051 , rtol=1e-3 ) [ logit ] ( https : //en.wikipedia.org/wiki/Logit ) values . By default , bce = keras.losses.BinaryCrossentropy ( ) def binary_crossentropy ( y_true , y_pred ) : mae_obj = losses.MeanAbsoluteError ( loss = mae_obj ( y_true , y_pred , sample_weight=0 ) assert mape_obj.reduction == losses_utils.Reduction.SUM y_true = K.constant ( [ [ 1 , 0 , 1 ] , [ 0 , 1 , 1 ] ] ) loss = mape_obj ( y_true , y_pred , sample_weight=sample_weight ) reduction : ( Optional ) Type of loss reduction to apply to loss . reduction=losses_utils.Reduction.NONE ) # Reduced loss = ( 0 + 66.666 ) / 2 `` `` '' # Applying the above two fns to the given input : assert np.allclose ( K.eval ( loss ) , 694.4445 , rtol=1e-3 ) assert bce_obj.reduction == losses_utils.Reduction.SUM # ( 0 + 100 * 0 + log ( 1 + exp ( -100 ) ) ) # Reduced loss = 15.33 * 2.3 / 4 assert mape_obj.name == 'mape_1 ' reduction=losses_utils.Reduction.SUM , name='bce_1 ' ) loss = bce_obj ( y_true , logits , sample_weight=weights ) assert bce_obj.name == 'bce_1 ' `` `` '' Computes the cross-entropy loss between true labels and predicted labels . For example , if ` y_true ` is [ 0. , 0. , 1. , 1 . ] and ` y_pred ` is [ 1. , 1. , 1. , 0 . ] assert np.isclose ( K.eval ( loss ) , 33.333 , rtol=1e-3 ) super ( MeanAbsolutePercentageError , self ) .__init__ ( name='mean_absolute_error ' ) : name='mean_absolute_percentage_error ' ) : # 0 = 0.5L class TestBinaryCrossentropy : return K.mean ( assert np.isclose ( K.eval ( loss ) , 5.5 , rtol=1e-3 ) loss = bce_obj ( y_true , logits , sample_weight=2.3 ) mae_obj ( y_true , y_pred , sample_weight=sample_weight ) y_true = K.cast ( y_true , y_pred.dtype ) # y ` = [ Y_MAX , Y_MAX , Y_MAX , EPSILON ] `` ` python [ -100.0 , -100.0 , 100.0 ] ] ) assert mae_obj.reduction == losses_utils.Reduction.SUM then the mean absolute error value is 3/4 ( 0.75 ) . # 0 + 100 * ( 0.5 L ) + 0 assert np.isclose ( K.eval ( loss ) , expected_value , atol=1e-3 ) assert np.allclose ( K.eval ( loss ) , 0.0 , rtol=1e-3 ) super ( MeanAbsoluteError , self ) .__init__ ( mean_absolute_percentage_error , name=name , reduction=reduction ) assert np.allclose ( K.eval ( loss ) , [ 621.8518 , 352.6666 ] , rtol=1e-3 ) we assume that ` y_pred ` contains probabilities # Arguments loss = msle_obj ( y_true , y_pred ) assert np.isclose ( K.eval ( loss ) , 76.667 , rtol=1e-3 ) msle_obj = losses.MeanSquaredLogarithmicError ( assert np.allclose ( K.eval ( loss ) , 211.8518 , rtol=1e-3 ) mae = keras.losses.MeanAbsoluteError ( ) floating-pointing value , and both ` y_pred ` and ` y_true ` have the shape # Loss = [ ( 0 + 0 + 0 ) /3 , ( 200 ) /3 ] loss = msle ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) msle = keras.losses.MeanSquaredLogarithmicError ( ) assert np.isclose ( K.eval ( loss ) , 13.833 , rtol=1e-3 ) label_smoothing = K.cast_to_floatx ( label_smoothing ) compute the loss between the predicted labels and a smoothed version of def test_scalar_weighted ( self ) : then the mean squared logarithmic error value is 0.36034 . # 1 = 1 - 0.5L label_smoothing=label_smoothing ) sample_weight = K.constant ( [ 3 , 6 , 5 , 0 , 4 , 2 ] , shape= ( 2 , 3 ) ) # Loss = max ( x , 0 ) - x * z + log ( 1 + exp ( -abs ( x ) ) ) # ( 0 + 100 * 1 + log ( 1 + exp ( -100 ) ) ) ) ] def _smooth_labels ( ) : ` [ batch_size ] ` . class BinaryCrossentropy ( LossFunctionWrapper ) : model.compile ( 'sgd ' , loss=keras.losses.BinaryCrossentropy ( ) ) with pytest.raises ( Exception ) : # Weighted loss = [ 0 * 4 , 66.666 * 3 ] assert np.isclose ( K.eval ( loss ) , 100 , rtol=1e-3 ) model.compile ( 'sgd ' , loss=keras.losses.MeanSquaredLogarithmicError ( ) ) # ( 100 - 100 * ( 1 - 0.5 L ) + 0 assert msle_obj.reduction == losses_utils.Reduction .SUM # = ( 100 + 50L ) * 1/3 Larger values of ` label_smoothing ` correspond to heavier smoothing . label_smoothing=0 , be 0 and 1 ) . For each example , there should be a single floating-point value # Loss = - ( y log ( y ` + EPSILON ) + ( 1 - y ) log ( 1 - y ` + EPSILON ) ) loss = mape_obj ( y_true , y_true ) def test_zero_weighted ( self ) :","['keras/losses.py', 'tests/keras/losses_test.py']","Adding MeanAbsoluteError , MeanAbsolutePercentageError , MeanSquaredLogarithmicError and BinaryCrossentropy loss classes . ( # 12894 )"
40,665b0076e089dbe60ecadc34bf2cbff9d7042482,2019-05-28 14:41:15-07:00,"super ( RMSprop , self ) .__init__ ( * * kwargs ) self.weights = [ self.iterations ] + ms + vs + vhats self.updates.append ( K.update ( mg , new_mg ) ) _test_optimizer ( optimizers.Nadam ( ) ) def __init__ ( self , lr=0.002 , beta_1=0.9 , beta_2=0.999 , def __init__ ( self , learning_rate=0.001 , beta_1=0.9 , beta_2=0.999 , self.weights = [ self.iterations ] + ms + vs the gradient ; if False , by the uncentered second moment . Setting this to amsgrad=False , * * kwargs ) : _test_optimizer ( optimizers.Adagrad ( lr=0.01 , decay=1e-3 ) ) vhats = [ None ] * len ( params ) super ( Adamax , self ) .__init__ ( * * kwargs ) # Override set_weights for backward compatibility of Keras 2.2.4 optimizer mgs = [ None ] * len ( params ) super ( Adam , self ) .__init__ ( * * kwargs ) moments = [ K.zeros ( shape ) for shape in shapes ] epsilon=None , schedule_decay=0.004 , * * kwargs ) : _test_optimizer ( optimizers.Adagrad ( decay=1e-3 ) ) self.initial_decay = kwargs.pop ( 'decay ' , 0.0 ) _test_optimizer ( new_nadam ) super ( Adam , self ) .__init__ ( * * kwargs ) if m : self.updates.append ( K.update ( m , new_m ) ) def __init__ ( self , learning_rate=0.001 , beta_1=0.9 , beta_2=0.999 , * * kwargs ) : super ( Nadam , self ) .set_weights ( weights ) self.momentum = K.variable ( momentum , name='momentum ' ) new_a - K.square ( new_mg ) + self.epsilon ) super ( Adamax , self ) .__init__ ( * * kwargs ) new_adagrad.set_weights ( adagrad.get_weights ( ) [ 1 : ] ) ms = adam.weights [ 1 : ( num_vars + 1 ) ] super ( Adagrad , self ) .__init__ ( * * kwargs ) def __init__ ( self , lr=0.01 , momentum=0. , decay=0. , # update accumulator new_p = p - lr * g / ( K.sqrt ( new_a ) + self.epsilon ) weights = weights [ : len ( params ) ] _test_optimizer ( optimizers.Adadelta ( decay=1e-3 ) , target=0.6 ) def __init__ ( self , lr=1.0 , rho=0.95 , epsilon=None , decay=0. , initial_accumulator_value : A floating point value . new_mg = self.rho * mg + ( 1 . - self.rho ) * g self.updates.append ( K.update ( v , v_t ) ) new_adam = optimizers.Adam ( learning_rate=0.1 , epsilon=0.01 , decay=0.1 ) config = { 'lr ' : float ( K.get_value ( self.lr ) ) , def __init__ ( self , lr=0.01 , epsilon=None , decay=0. , * * kwargs ) : def __init__ ( self , lr=0.001 , beta_1=0.9 , beta_2=0.999 , adagrad = optimizers.Adagrad ( lr=0.01 ) self.weights = [ self.iterations , self.m_schedule ] + ms + vs self.weights = [ self.iterations ] + ms + vs self.epsilon = epsilon super ( Adagrad , self ) .set_weights ( weights ) new_p = p + self.momentum * v - lr * g new_adadelta = optimizers.Adadelta ( learning_rate=1.0 , assert len ( adagrad.get_weights ( ) ) == len ( new_adagrad.get_weights ( ) ) def __init__ ( self , learning_rate=0.01 , momentum=0. , new_nadam = optimizers.Nadam ( learning_rate=0.1 , epsilon=0.01 , schedule_decay=0.1 ) self.updates.append ( K.update ( m , v ) ) new_p = p + v super ( Adam , self ) .set_weights ( weights ) self.initial_decay = decay mgs = [ None ] * len ( params ) initial_accumulator_value=0.001 , super ( Adadelta , self ) .__init__ ( * * kwargs ) if self.centered : schedule_decay : float , 0 < schedule_decay < 1 . import numpy as np 'epsilon ' : self.epsilon } new_adam.set_weights ( adam.get_weights ( ) ) self.centered = centered 'decay ' : float ( K.get_value ( self.decay ) ) , self.weights.extend ( vhats ) * * kwargs ) : # If the weights are generated by Keras V1 optimizer , it includes vhats computation and memory . Defaults to False . num_vars = int ( ( len ( adam.weights ) - 1 ) / 2 ) mgs = [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in params ] _test_optimizer ( optimizers.RMSprop ( learning_rate=0.1 , momentum=0.1 ) ) new_nadam.set_weights ( [ weights [ 0 ] ] + weights [ 1 : ] ) for p , g , a , m , mg in zip ( params , grads , accumulators , ms , mgs ) : # even without amsgrad , i.e , V1 optimizer has 3x + 1 variables , while V2 _test_optimizer ( optimizers.Adadelta ( lr=1.0 , decay=1e-3 ) , target=0.6 ) epsilon=0.001 , decay=0.001 ) _test_optimizer ( adadelta , target=0.6 ) 'epsilon ' : self.epsilon , learning_rate : float > = 0 . Learning rate . self.lr = K.variable ( lr , name='lr ' ) 'initial_accumulator_value ' : self._initial_accumulator_value } Starting value for the accumulators , must be positive . def __init__ ( self , learning_rate=0.001 , initial_accumulator_value=0.1 , * * kwargs ) : decay=0.001 ) ) super ( SGD , self ) .__init__ ( * * kwargs ) if self.nesterov : direction and dampens oscillations . super ( Nadam , self ) .__init__ ( * * kwargs ) else : ms = [ None ] * len ( params ) def set_weights ( self , weights ) : decay : float > = 0 . Initial learning rate decay . new_p = p + v # iteration to 0 . self.schedule_decay = kwargs.pop ( 'schedule_decay ' , 0.004 ) self.schedule_decay = schedule_decay _test_optimizer ( optimizers.Adamax ( learning_rate=0.002 , epsilon=0.001 , _test_optimizer ( new_adagrad ) # test backward compatibility for adding opt.m_schedule . else : self.weights = [ self.iterations ] + moments self.updates.append ( K.update ( v , v_t ) ) _test_optimizer ( optimizers.Adagrad ( ) ) config = { 'learning_rate ' : float ( K.get_value ( self.lr ) ) , epsilon=0.001 , decay=0.001 ) _test_optimizer ( new_adam ) self._momentum = True if momentum > 0. else False weights = nadam.get_weights ( ) 'decay ' : float ( K.get_value ( self.decay ) ) } self.updates.append ( K.update ( m , m_t ) ) epsilon : float > = 0 . Fuzz factor . If ` None ` , defaults to ` K.epsilon ( ) ` . if len ( weights ) == 3 * num_vars + 1 : self.weights = accumulators _test_optimizer ( sgd ) adam = optimizers.Adam ( ) _test_optimizer ( optimizers.RMSprop ( learning_rate=0.1 , momentum=0.1 , new_p = p + self.momentum * v - lr * g # momentum v = self.momentum * m - lr * g # velocity # test backward compatibility for adding opt.iterations . if len ( params ) == len ( weights ) + 1 : def __init__ ( self , lr=0.001 , rho=0.9 , epsilon=None , decay=0. , _test_optimizer ( new_adadelta , target=0.6 ) True may help with training , but is slightly more expensive in terms of new_adagrad = optimizers.Adagrad ( learning_rate=0.01 , _test_optimizer ( adagrad ) assert len ( nadam.get_weights ( ) ) == len ( new_nadam.get_weights ( ) ) weights = [ weights [ 0 ] ] + [ np.array ( 1 . ) ] + weights [ 1 : ] num_vars = int ( ( len ( params ) - 1 ) / 2 ) vhats = [ K.zeros ( 1 ) for _ in params ] _test_optimizer ( nadam ) _test_optimizer ( optimizers.Adamax ( lr=0.002 , decay=1e-3 ) ) params = self.weights learning_rate : float > = 0 . Initial learning rate . _test_optimizer ( adam ) # optimizer has 2x + 1 variables . Filter vhats out for compatibility . learning_rate : float > = 0 . Initial learning rate , defaults to 1 . learning_rate = kwargs.pop ( 'lr ' , learning_rate ) self.decay = K.variable ( decay , name='decay ' ) momentum : float hyperparameter > = 0 that accelerates RMSprop in the relevant self.lr = K.variable ( learning_rate , name='learning_rate ' ) weights = [ np.array ( 0 ) ] + weights # momentum optimizer nadam = optimizers.Nadam ( ) for shape in shapes ] shapes = [ K.int_shape ( p ) for p in params ] self.weights = [ self.iterations ] + accumulators _test_optimizer ( optimizers.Adadelta ( ) , target=0.6 ) _test_optimizer ( optimizers.Adamax ( decay=1e-3 ) ) epsilon = K.epsilon ( ) super ( Adagrad , self ) .__init__ ( * * kwargs ) _test_optimizer ( optimizers.Adam ( ) ) moments = [ K.zeros ( shape ) for shape in shapes ] new_p = p - new_m assert len ( adadelta.get_weights ( ) ) == len ( new_adadelta.get_weights ( ) ) decay : float > = 0 . Learning rate decay over each update . super ( SGD , self ) .__init__ ( * * kwargs ) accumulators = [ K.zeros ( shape ) for shape in shapes ] self.weights = [ self.iterations ] + accumulators + delta_accumulators super ( RMSprop , self ) .__init__ ( * * kwargs ) if epsilon is None : lr : float > = 0 . Initial learning rate , defaults to 1 . new_p = p - lr * g _test_optimizer ( optimizers.Adamax ( lr=0.002 ) ) self.epsilon = kwargs.pop ( 'epsilon ' , K.epsilon ( ) ) _test_optimizer ( optimizers.RMSprop ( learning_rate=0.1 , epsilon=0.01 , decay=0.1 ) ) else : shapes = [ K.int_shape ( p ) for p in params ] new_adadelta.set_weights ( adadelta.get_weights ( ) [ 1 : ] ) self.decay = K.variable ( self.initial_decay , name='decay ' ) # since it does not include iteration at head of the weight list . Set centered : If True , gradients are normalized by the estimated variance of def __init__ ( self , learning_rate=0.001 , rho=0.9 , momentum=0. , centered=False , new_p = p - lr * g / ( K.sqrt ( new_a ) + self.epsilon ) lr : float > = 0 . Learning rate . self.updates.append ( K.update ( m , v ) ) self.weights = [ self.iterations ] super ( Adadelta , self ) .set_weights ( weights ) self.updates.append ( K.update ( m , m_t ) ) if mg : self.weights.extend ( ms ) lr : float > = 0 . Initial learning rate . moments = [ None ] * len ( params ) epsilon=None , decay=0. , * * kwargs ) : self.weights.extend ( mgs ) if self._momentum : centered=True ) ) self.weights.extend ( [ K.zeros ( shape ) for shape in shapes ] ) adadelta = optimizers.Adadelta ( lr=1.0 ) accumulators = [ K.variable ( np.full ( shape , self._initial_accumulator_value ) ) # sgd optimizer . if self.nesterov : epsilon : float > = 0 . If ` None ` , defaults to ` K.epsilon ( ) ` . self.weights = accumulators + delta_accumulators epsilon=None , decay=0. , amsgrad=False , * * kwargs ) : if self._momentum : self._initial_accumulator_value = initial_accumulator_value super ( Nadam , self ) .__init__ ( * * kwargs ) new_m = self.momentum * m + lr * g / K.sqrt ( adam.weights.extend ( [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in ms ] ) _test_optimizer ( optimizers.Adamax ( ) ) def __init__ ( self , learning_rate=0.001 , rho=0.95 , * * kwargs ) : v = self.momentum * m - lr * g # velocity ms = [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in params ] super ( Adadelta , self ) .__init__ ( * * kwargs ) sgd = optimizers.SGD ( learning_rate=0.01 , momentum=0.9 , decay=0.1 , nesterov=True ) new_m = self.momentum * m + lr * g / K.sqrt ( new_a + self.epsilon ) super ( RMSprop , self ) .set_weights ( weights ) for p , g , a in zip ( params , grads , accumulators ) :","['keras/optimizers.py', 'tests/keras/optimizers_test.py']",Revert `` Sync Keras optimizer with tf.keras optimizer ( # 12841 ) '' ( # 12888 )
41,08f6bdeb5652550f36210f64aefda3d0d41e2d79,2019-05-28 14:40:28-07:00,"self.schedule_decay = schedule_decay super ( RMSprop , self ) .__init__ ( * * kwargs ) self.updates.append ( K.update ( p , new_p ) ) if self._momentum : _test_optimizer ( new_adam ) self.weights = [ self.iterations ] + ms + vs for shape in shapes ] # momentum optimizer new_adagrad = optimizers.Adagrad ( learning_rate=0.01 , 'decay ' : float ( K.get_value ( self.decay ) ) } self.weights = accumulators super ( Adamax , self ) .__init__ ( * * kwargs ) super ( Adam , self ) .__init__ ( * * kwargs ) if m : def __init__ ( self , learning_rate=0.001 , rho=0.9 , momentum=0. , centered=False , super ( Adam , self ) .__init__ ( * * kwargs ) mgs = [ None ] * len ( params ) # test backward compatibility for adding opt.m_schedule . self.weights.extend ( vhats ) vhats = [ K.zeros ( 1 ) for _ in params ] weights = [ np.array ( 0 ) ] + weights weights = weights [ : len ( params ) ] adagrad = optimizers.Adagrad ( lr=0.01 ) amsgrad=False , * * kwargs ) : self._initial_accumulator_value = initial_accumulator_value _test_optimizer ( optimizers.Adadelta ( lr=1.0 , decay=1e-3 ) , target=0.6 ) super ( Adadelta , self ) .set_weights ( weights ) assert len ( adagrad.get_weights ( ) ) == len ( new_adagrad.get_weights ( ) ) config = { 'lr ' : float ( K.get_value ( self.lr ) ) , 'decay ' : float ( K.get_value ( self.decay ) ) , _test_optimizer ( optimizers.Adamax ( lr=0.002 , decay=1e-3 ) ) else : adam = optimizers.Adam ( ) def __init__ ( self , learning_rate=0.001 , rho=0.95 , * * kwargs ) : # test backward compatibility for adding opt.iterations . super ( Adamax , self ) .__init__ ( * * kwargs ) _test_optimizer ( new_nadam ) True may help with training , but is slightly more expensive in terms of _test_optimizer ( optimizers.Adam ( ) ) new_adam.set_weights ( adam.get_weights ( ) ) super ( Adagrad , self ) .__init__ ( * * kwargs ) self.weights.extend ( ms ) accumulators = [ K.zeros ( shape ) for shape in shapes ] self.lr = K.variable ( learning_rate , name='learning_rate ' ) self.updates.append ( K.update ( v , v_t ) ) shapes = [ K.int_shape ( p ) for p in params ] def __init__ ( self , learning_rate=0.001 , beta_1=0.9 , beta_2=0.999 , * * kwargs ) : initial_accumulator_value=0.001 , self.weights = [ self.iterations ] + accumulators + delta_accumulators # sgd optimizer . for p , g , a in zip ( params , grads , accumulators ) : config = { 'learning_rate ' : float ( K.get_value ( self.lr ) ) , self.weights = [ self.iterations ] + ms + vs new_adagrad.set_weights ( adagrad.get_weights ( ) [ 1 : ] ) sgd = optimizers.SGD ( learning_rate=0.01 , momentum=0.9 , decay=0.1 , nesterov=True ) shapes = [ K.int_shape ( p ) for p in params ] num_vars = int ( ( len ( params ) - 1 ) / 2 ) self.weights = accumulators + delta_accumulators if len ( params ) == len ( weights ) + 1 : if self.nesterov : self.lr = K.variable ( lr , name='lr ' ) self.weights = [ self.iterations ] + moments v = self.momentum * m - lr * g # velocity self.updates.append ( K.update ( m , v ) ) ms = [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in params ] self.decay = K.variable ( self.initial_decay , name='decay ' ) computation and memory . Defaults to False . the gradient ; if False , by the uncentered second moment . Setting this to self.updates.append ( K.update ( p , new_p ) ) _test_optimizer ( adadelta , target=0.6 ) learning_rate = kwargs.pop ( 'lr ' , learning_rate ) nadam = optimizers.Nadam ( ) * * kwargs ) : def __init__ ( self , lr=0.001 , rho=0.9 , epsilon=None , decay=0. , 'initial_accumulator_value ' : self._initial_accumulator_value } super ( Adadelta , self ) .__init__ ( * * kwargs ) for p , g , a , m , mg in zip ( params , grads , accumulators , ms , mgs ) : if self._momentum : self.weights.extend ( [ K.zeros ( shape ) for shape in shapes ] ) self.weights = [ self.iterations ] initial_accumulator_value : A floating point value . decay=0.001 ) ) 'epsilon ' : self.epsilon , _test_optimizer ( optimizers.Adamax ( learning_rate=0.002 , epsilon=0.001 , _test_optimizer ( optimizers.Adadelta ( ) , target=0.6 ) decay : float > = 0 . Initial learning rate decay . _test_optimizer ( optimizers.Adamax ( lr=0.002 ) ) epsilon=None , decay=0. , amsgrad=False , * * kwargs ) : new_nadam.set_weights ( [ weights [ 0 ] ] + weights [ 1 : ] ) _test_optimizer ( adam ) weights = [ weights [ 0 ] ] + [ np.array ( 1 . ) ] + weights [ 1 : ] epsilon=None , decay=0. , * * kwargs ) : self.centered = centered _test_optimizer ( optimizers.Adagrad ( decay=1e-3 ) ) epsilon=0.001 , decay=0.001 ) _test_optimizer ( optimizers.RMSprop ( learning_rate=0.1 , momentum=0.1 , epsilon : float > = 0 . If ` None ` , defaults to ` K.epsilon ( ) ` . super ( SGD , self ) .__init__ ( * * kwargs ) super ( Nadam , self ) .__init__ ( * * kwargs ) new_adam = optimizers.Adam ( learning_rate=0.1 , epsilon=0.01 , decay=0.1 ) ms = [ None ] * len ( params ) self.updates.append ( K.update ( m , v ) ) super ( Nadam , self ) .set_weights ( weights ) self.updates.append ( K.update ( m , new_m ) ) adadelta = optimizers.Adadelta ( lr=1.0 ) _test_optimizer ( optimizers.Adamax ( ) ) epsilon=0.001 , decay=0.001 ) if epsilon is None : def __init__ ( self , lr=0.002 , beta_1=0.9 , beta_2=0.999 , _test_optimizer ( optimizers.RMSprop ( learning_rate=0.1 , epsilon=0.01 , decay=0.1 ) ) new_nadam = optimizers.Nadam ( learning_rate=0.1 , epsilon=0.01 , schedule_decay=0.1 ) if self.centered : new_p = p - new_m self.updates.append ( K.update ( v , v_t ) ) self.epsilon = epsilon # If the weights are generated by Keras V1 optimizer , it includes vhats centered=True ) ) self.weights = [ self.iterations ] + ms + vs + vhats _test_optimizer ( nadam ) epsilon : float > = 0 . Fuzz factor . If ` None ` , defaults to ` K.epsilon ( ) ` . centered : If True , gradients are normalized by the estimated variance of self.updates.append ( K.update ( m , m_t ) ) new_p = p + v self.initial_decay = decay accumulators = [ K.variable ( np.full ( shape , self._initial_accumulator_value ) ) def __init__ ( self , learning_rate=0.01 , momentum=0. , super ( RMSprop , self ) .set_weights ( weights ) momentum : float hyperparameter > = 0 that accelerates RMSprop in the relevant if len ( weights ) == 3 * num_vars + 1 : lr : float > = 0 . Learning rate . new_m = self.momentum * m + lr * g / K.sqrt ( new_m = self.momentum * m + lr * g / K.sqrt ( new_a + self.epsilon ) assert len ( nadam.get_weights ( ) ) == len ( new_nadam.get_weights ( ) ) super ( Adam , self ) .set_weights ( weights ) _test_optimizer ( optimizers.RMSprop ( learning_rate=0.1 , momentum=0.1 ) ) epsilon = K.epsilon ( ) _test_optimizer ( adagrad ) import numpy as np self.decay = K.variable ( decay , name='decay ' ) if mg : new_a - K.square ( new_mg ) + self.epsilon ) # since it does not include iteration at head of the weight list . Set self.weights = [ self.iterations ] + accumulators _test_optimizer ( optimizers.Adamax ( decay=1e-3 ) ) self.epsilon = kwargs.pop ( 'epsilon ' , K.epsilon ( ) ) params = self.weights vhats = [ None ] * len ( params ) # update accumulator new_p = p - lr * g new_p = p + v new_adadelta.set_weights ( adadelta.get_weights ( ) [ 1 : ] ) v = self.momentum * m - lr * g # velocity adam.weights.extend ( [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in ms ] ) moments = [ None ] * len ( params ) self.schedule_decay = kwargs.pop ( 'schedule_decay ' , 0.004 ) new_p = p + self.momentum * v - lr * g learning_rate : float > = 0 . Learning rate . # iteration to 0 . _test_optimizer ( optimizers.Adadelta ( decay=1e-3 ) , target=0.6 ) def __init__ ( self , lr=1.0 , rho=0.95 , epsilon=None , decay=0. , super ( Adagrad , self ) .__init__ ( * * kwargs ) schedule_decay : float , 0 < schedule_decay < 1 . def __init__ ( self , lr=0.01 , epsilon=None , decay=0. , * * kwargs ) : learning_rate : float > = 0 . Initial learning rate , defaults to 1 . new_p = p - lr * g / ( K.sqrt ( new_a ) + self.epsilon ) weights = nadam.get_weights ( ) 'epsilon ' : self.epsilon } super ( SGD , self ) .__init__ ( * * kwargs ) assert len ( adadelta.get_weights ( ) ) == len ( new_adadelta.get_weights ( ) ) decay : float > = 0 . Learning rate decay over each update . super ( RMSprop , self ) .__init__ ( * * kwargs ) mgs = [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in params ] _test_optimizer ( new_adadelta , target=0.6 ) def __init__ ( self , learning_rate=0.001 , initial_accumulator_value=0.1 , * * kwargs ) : _test_optimizer ( optimizers.Adagrad ( ) ) # optimizer has 2x + 1 variables . Filter vhats out for compatibility . self.updates.append ( K.update ( mg , new_mg ) ) num_vars = int ( ( len ( adam.weights ) - 1 ) / 2 ) # Override set_weights for backward compatibility of Keras 2.2.4 optimizer new_p = p - lr * g / ( K.sqrt ( new_a ) + self.epsilon ) Starting value for the accumulators , must be positive . super ( Adagrad , self ) .set_weights ( weights ) _test_optimizer ( sgd ) # momentum ms = adam.weights [ 1 : ( num_vars + 1 ) ] _test_optimizer ( optimizers.Adagrad ( lr=0.01 , decay=1e-3 ) ) moments = [ K.zeros ( shape ) for shape in shapes ] self.updates.append ( K.update ( m , m_t ) ) mgs = [ None ] * len ( params ) lr : float > = 0 . Initial learning rate . _test_optimizer ( optimizers.Nadam ( ) ) def set_weights ( self , weights ) : if self.nesterov : else : # even without amsgrad , i.e , V1 optimizer has 3x + 1 variables , while V2 self.weights.extend ( mgs ) def __init__ ( self , lr=0.01 , momentum=0. , decay=0. , new_mg = self.rho * mg + ( 1 . - self.rho ) * g new_adadelta = optimizers.Adadelta ( learning_rate=1.0 , super ( Nadam , self ) .__init__ ( * * kwargs ) epsilon=None , schedule_decay=0.004 , * * kwargs ) : self._momentum = True if momentum > 0. else False _test_optimizer ( new_adagrad ) self.initial_decay = kwargs.pop ( 'decay ' , 0.0 ) learning_rate : float > = 0 . Initial learning rate . moments = [ K.zeros ( shape ) for shape in shapes ] new_p = p + self.momentum * v - lr * g lr : float > = 0 . Initial learning rate , defaults to 1 . super ( Adadelta , self ) .__init__ ( * * kwargs ) def __init__ ( self , lr=0.001 , beta_1=0.9 , beta_2=0.999 , direction and dampens oscillations . self.momentum = K.variable ( momentum , name='momentum ' ) self.weights = [ self.iterations , self.m_schedule ] + ms + vs else : def __init__ ( self , learning_rate=0.001 , beta_1=0.9 , beta_2=0.999 ,","['keras/optimizers.py', 'tests/keras/optimizers_test.py']",Sync Keras optimizer with tf.keras optimizer ( # 12841 )
42,29f27f333efb427830e62bec62fd79b444df4011,2019-05-24 12:54:32-07:00,"def _base_init ( self , name=None ) : def test_initialization_dtype ( ) : self.w = self.add_weight ( ' w ' , [ ] , initializer=Constant ( 1 ) ) dtype = kwargs.get ( 'input_dtype ' ) if dtype is None : def __init__ ( self ) : # Set dtype . self._non_trainable_weights = [ ] def recurrent_identity ( shape , gain=1. , dtype=None ) : weight = K.variable ( initializer ( shape ) , self.dtype = dtype self.trainable = True def recurrent_identity ( shape , gain=1 . ) : layer = TestLayer ( ) self._base_init ( name=name , * * kwargs ) # Set dtype . def _init_subclassed_network ( self , name=None ) : dtype = K.floatx ( ) super ( TestModel , self ) .__init__ ( dtype='int64 ' ) assert K.dtype ( layer.w ) == 'int64 ' dtype = K.floatx ( ) dtype = K.floatx ( ) class TestModel ( Model ) : self._base_init ( name=name ) weight = K.variable ( initializer ( shape , dtype=dtype ) , model = TestModel ( ) dtype = self.dtype self._trainable_weights = [ ] def _init_subclassed_network ( self , name=None , * * kwargs ) : if dtype is None : del dtype dtype = kwargs.get ( 'dtype ' ) class TestLayer ( Layer ) : def _init_graph_network ( self , inputs , outputs , name=None , * * kwargs ) : super ( TestLayer , self ) .__init__ ( dtype='int64 ' ) reason='Float64 not supported with CNTK . ' ) def _base_init ( self , name=None , trainable=True , dtype=None ) : assert K.dtype ( model.w ) == 'int64 ' self.trainable = trainable dtype : Default dtype of the layers 's weights . self.dtype = dtype dtype = kwargs.get ( 'dtype ' ) dtype dtype = kwargs.get ( 'input_dtype ' ) def _init_graph_network ( self , inputs , outputs , name=None ) :","['keras/engine/base_layer.py', 'keras/engine/network.py', 'keras/layers/recurrent.py', 'tests/keras/engine/test_topology.py']",Use layer.dtype as the default dtype when adding a new weight ( # 12740 )
43,0792332f77e6a34093cab9c2bc6f638d1676bccd,2019-05-23 17:03:04-07:00,"fn , from __future__ import division config = { } loss = K.eval ( losses.sparse_categorical_crossentropy ( y_true , y_pred ) ) model = keras.Model ( inputs , outputs ) def test_invalid_sample_weight ( self ) : assert np.isclose ( K.eval ( loss ) , 227.69998 , rtol=1e-3 ) # Examples def squeeze_or_expand_dimensions ( y_pred , y_true , sample_weight ) : if weights_rank - y_pred_rank == 1 : `` `` '' Instantiates a ` Loss ` from its config ( output of ` get_config ( ) ` ) . return { 'reduction ' : self.reduction , 'name ' : self.name } model_filename = str ( tmpdir / 'custom_loss.hdf ' ) `` `` '' Wraps a loss function in the ` Loss ` class . y_b = K.variable ( np.random.random ( ( 6 , 7 ) ) ) y_true = math_ops.cast ( y_true , y_pred.dtype ) loss of each measurable element of ` y_pred ` is scaled by the `` `` '' `` `` '' Computes the number of elements in ` losses ` tensor . '' '' '' return sum ( ones_like ( x , name=name ) ) y_b = K.variable ( np.random.random ( ( 5 , 6 , 7 ) ) ) if reduction == Reduction.SUM_OVER_BATCH_SIZE : with K.name_scope ( name or 'weighted_loss ' ) : expected_loss = - ( np.log ( 0.6 ) + np.log ( 0.7 ) ) / 2 loss = loss / _num_elements ( weighted_losses ) def test_sparse_categorical_crossentropy ( ) : loss = mse_obj ( y_true , y_true ) elif ( y_true_rank - y_pred_rank == 1 ) and ( y_true_shape [ -1 ] == 1 ) : model.fit ( np.random.rand ( 256 , 2 ) , np.random.rand ( 256 , 1 ) ) To be implemented by subclasses : expected_loss = ( ( 0.3 - 0.2 + 1 ) + ( 0.7 - 0.1 + 1 ) ) / 2.0 * * kwargs ) : y_true = K.variable ( np.array ( [ [ 0 , 1 , 0 ] , y_true = K.variable ( np.array ( [ [ [ 0 , 1 , 0 ] , reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE , loss = mse_obj ( y_true , y_pred , sample_weight=sample_weight ) > > > from keras import backend as K objective_output = losses.sparse_categorical_crossentropy ( y_a , y_b ) assert mse_obj.name == 'mse_1 ' Default value is ` SUM_OVER_BATCH_SIZE ` . `` `` '' Returns the size of a tensor . model.save ( model_filename ) class MeanSquaredError ( Loss ) : losses , _ , sample_weight = squeeze_or_expand_dimensions ( from . import losses_utils `` ` else : SUM_OVER_BATCH_SIZE = 'sum_over_batch_size ' y_true = K.squeeze ( y_true , -1 ) # accepted in scope name . def test_sum_reduction ( self ) : rescaled by the corresponding element in the ` sample_weight ` vector . If [ 0.2 , 0.5 , 0.3 ] ] , * ` SUM_OVER_BATCH_SIZE ` : Scalar ` SUM ` divided by number of elements in losses . assert K.eval ( objective_output ) .shape == ( 6 , ) def test_categorical_hinge ( ) : with custom_object_scope ( { 'MSE_MAE_loss ' : MSE_MAE_loss } ) : input_dtype = K.dtype ( losses ) loaded_model.predict ( np.random.rand ( 128 , 2 ) ) shape as ` y_true ` ; otherwise , it is scalar . assert isinstance ( deserialized , MSE_MAE_loss ) y_a = K.variable ( np.random.randint ( 0 , 7 , ( 6 , ) ) ) [ 0.1 , 0.1 , 0.8 ] ] , assert isinstance ( deserialized , MSE_MAE_loss ) # Raises [ [ 0.3 , 0.7 , 0.0 ] , model = keras.models.Model ( inputs , outputs ) class Loss ( object ) : loaded_model.predict ( np.random.rand ( 128 , 2 ) ) simply scaled by the given value . If ` sample_weight ` is a tensor of size name : A name for the operation ( optional ) . y_pred = K.variable ( np.array ( [ [ [ [ 0.7 , 0.1 , 0.2 ] , y_a = K.variable ( np.random.randint ( 0 , 7 , ( 5 , 6 ) ) ) return y_pred , y_true , sample_weight objective_output = losses.sparse_categorical_crossentropy ( y_a , y_b ) `` ` python assert K.eval ( objective_output ) .shape == ( 6 , ) the reported loss will be a scalar value . [ 2 , 2 , 1 ] ] ] ) ) > > > val = np.array ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) # Arguments loaded_model = keras.models.load_model ( model_filename ) assert K.eval ( losses.sparse_categorical_crossentropy ( y_a , y_b ) ) .shape == ( 6 , ) from .load_backend import size scope_name = 'lambda ' if self.name == ' < lambda > ' else self.name raise NotImplementedError ( 'Must be implemented in subclasses . ' ) then the mean squared error value is 3/4 ( 0.75 ) . model.compile ( 'sgd ' , loss=keras.losses.MeanSquaredError ( ) ) loss = weighted_losses y_a = K.variable ( np.random.random ( ( 6 , 7 ) ) ) outputs = keras.layers.Dense ( 1 , name='model_output ' ) ( inputs ) > > > K.size ( inputs ) mse_obj = losses.MeanSquaredError ( ) [ 1.0 , 0.0 , 0.0 ] , def test_serializing_loss_class ( self ) : y_true = K.variable ( np.array ( [ [ 0 , 1 , 0 ] , def test_sparse_categorical_crossentropy_4d ( ) : serialized = losses.serialize ( orig_loss_class ) # Raises [ 0.1 , 0.1 , 0.8 ] ] , x : Tensor or variable . config : Output of ` get_config ( ) ` . reduction type used with built-in Keras training loops like def test_sparse_categorical_crossentropy ( self ) : [ 0.2 , 0.5 , 0.3 ] ] , # Arguments losses = self.call ( y_true , y_pred ) y_pred = K.variable ( np.array ( [ [ 0.3 , 0.2 , 0.1 ] , def test_no_reduction ( self ) : if y_true is not None : assert np.isclose ( K.eval ( loss ) , 767.8 / 6 , rtol=1e-3 ) class Reduction ( object ) : y_pred_shape = K.int_shape ( y_pred ) y_a = K.variable ( np.random.random ( ( 5 , 6 , 7 ) ) ) dimension . y_true = K.variable ( np.array ( [ 1 , 2 ] ) ) [ 0.4 , 0.3 , 0.3 ] ] ] ] ) ) assert mse_obj.reduction == losses_utils.Reduction.SUM assert np.isclose ( K.eval ( loss ) , 113.85 , rtol=1e-3 ) losses = K.cast ( losses , K.floatx ( ) ) loss = mse_obj ( y_true , y_pred ) y_pred = K.variable ( np.array ( [ [ 0.3 , 0.2 , 0.1 ] , name=None , y_pred = K.constant ( [ 4 , 8 , 12 , 8 , 1 , 3 ] , shape= ( 2 , 3 ) ) def test_objective_shapes_3d ( self ) : # Update dimensions of ` sample_weight ` to match with ` losses ` if possible . Standalone usage : sample_weight : Optional ` Tensor ` whose rank is either 0 , or the same rank def call ( self , y_true , y_pred ) : self.reduction = reduction loss = reduce_weighted_loss ( weighted_losses , reduction ) from __future__ import print_function loss = mse_obj ( y_true , y_pred , sample_weight=0 ) Weighted loss float ` Tensor ` . If ` reduction ` is ` NONE ` , this has the same # Apply reduction function to the individual weighted losses . def test_config ( self ) : loss = K.cast ( loss , input_dtype ) def test_timestep_weighted ( self ) : y_a = K.variable ( np.random.randint ( 0 , 7 , ( 6 , ) ) ) return K.cast ( K.size ( losses , name=scope ) , losses.dtype ) y_pred = K.constant ( [ 4 , 8 , 12 , 8 , 1 , 3 ] , shape= ( 2 , 3 , 1 ) ) [ 0.3 , 0.4 , 0.3 ] , return loss model.save ( model_filename ) loss = K.eval ( losses.categorical_hinge ( y_true , y_pred ) ) [ 0.0 , 0.3 , 0.7 ] , [ 0.4 , 0.3 , 0.3 ] ] ] ] ) ) # Convert the result back to the input type . outputs = keras.layers.Dense ( 1 , name='model_output ' ) ( inputs ) Usage with the ` compile ` API : [ [ 0.8 , 0.1 , 0.1 ] , [ 2 , 2 , 1 ] ] ] ) ) ` fit ` / ` evaluate ` , the unreduced vector loss is passed to the optimizer but 2 . Squeezes or expands last dim of ` sample_weight ` if its rank differs by 1 `` `` '' Invokes the ` LossFunctionWrapper ` instance . losses : ` Tensor ` of shape ` [ batch_size , d1 , ... dN ] ` . ValueError : If the shape of ` sample_weight ` is not compatible with ` losses ` . self.name = name loss = mse_obj ( y_true , y_pred , sample_weight=2.3 ) [ 2 , 1 , 0 ] , sample_weight = K.cast ( sample_weight , K.floatx ( ) ) y_true_rank = K.ndim ( y_true ) weighted_losses = losses * sample_weight For example , if ` y_true ` is [ 0. , 0. , 1. , 1 . ] and ` y_pred ` is [ 1. , 1. , 1. , 0 . ] sample_weight : Optional weight scalar or ` Tensor ` whose dimensions match as ` y_true ` , or is broadcastable to ` y_true ` . ` sample_weight ` acts as a Weighted loss ` Tensor ` of the same type as ` losses ` . If ` reduction ` is assert deserialized.mse_fraction == 0.3 self._fn_kwargs = kwargs assert K.eval ( objective_output ) .shape == ( 5 , 6 ) sample_weight=None , assert np.isclose ( K.eval ( loss ) , 587 / 6 , rtol=1e-3 ) assert K.eval ( objective_output ) .shape == ( 5 , 6 ) NONE = 'none ' def test_objective_shapes_3d ( ) : return y_pred , y_true , None ` NONE ` , this has the same shape as ` losses ` ; otherwise , it is scalar . Default value is ` SUM_OVER_BATCH_SIZE ` . y_b = K.variable ( np.random.random ( ( 6 , 7 ) ) ) SUM = 'sum ' the last dimension squeezed , ` sample_weight ` could be extended by one def get_config ( self ) : y_a = K.variable ( np.random.random ( ( 6 , 7 ) ) ) y_pred = K.variable ( np.array ( [ [ [ [ 0.7 , 0.1 , 0.2 ] , [ 0.3 , 0.4 , 0.3 ] , [ 0.0 , 0.3 , 0.7 ] , def call ( self , y_true , y_pred ) : assert deserialized.mse_fraction == 0.3 serialized = losses.serialize ( orig_loss_class ) if key not in cls.all ( ) : with K.name_scope ( 'num_elements ' ) as scope : fn : The loss function to wrap , with signature ` fn ( y_true , y_pred , reduction=losses_utils.Reduction.SUM ) loss = MSE_MAE_loss ( 0.3 ) sample_weight : Optional ` Tensor ` whose rank is either 0 , or the same rank as def test_objective_shapes_2d ( ) : return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) y_b = K.variable ( np.random.random ( ( 5 , 6 , 7 ) ) ) raise ValueError ( 'Invalid Reduction Key % s . ' % key ) model.compile ( optimizer='sgd ' , loss= { 'model_output ' : loss } ) assert np.isclose ( expected_loss , np.mean ( loss ) ) class TestLossFunctions : def test_cce_one_hot ( ) : corresponding value of ` sample_weight ` . [ 2 , 1 , 0 ] , loss = K.eval ( losses.sparse_categorical_crossentropy ( y_true , y_pred ) ) `` `` '' Reduces the individual weighted loss measurements . '' '' '' assert K.eval ( losses.sparse_categorical_crossentropy ( y_a , y_b ) ) .shape == ( 6 , ) ` [ batch_size ] ` , then the total loss for each sample of the batch is y_true = K.constant ( [ 1 , 9 , 2 , -5 , -2 , 6 ] , shape= ( 2 , 3 , 1 ) ) > > > kvar = K.variable ( value=val ) name=None ) : # If we are wrapping a lambda function strip ' < > ' from the name as it is not y_pred = ops.convert_to_tensor ( y_pred ) def test_sparse_categorical_crossentropy_4d ( self ) : loss = mse ( [ 0. , 0. , 1. , 1 . ] , [ 1. , 1. , 1. , 0 . ] ) * ` call ( ) ` : Contains the logic for loss calculation using ` y_true ` , ` y_pred ` . mse_obj = losses.MeanSquaredError ( y_pred = K.variable ( np.array ( [ [ 0.3 , 0.6 , 0.1 ] , model = keras.models.Model ( inputs , outputs ) expected_loss = ( ( 0.3 - 0.2 + 1 ) + ( 0.7 - 0.1 + 1 ) ) / 2.0 y_pred = K.squeeze ( y_pred , -1 ) return cls ( * * config ) `` `` '' Invokes the ` Loss ` instance . y_true = K.variable ( np.array ( [ [ [ 0 , 1 , 0 ] , `` `` '' Loss base class . def test_all_correct_unweighted ( self ) : loaded_model = keras.models.load_model ( model_filename ) mse = keras.losses.MeanSquaredError ( ) Tuple of ` y_pred ` , ` y_true ` and ` sample_weight ` . Each of them possibly has def test_serializing_model_with_loss_class ( self , tmpdir ) : def test_categorical_hinge ( self ) : [ 1 , 0 , 0 ] ] ) ) losses , None , sample_weight ) y_true : Optional label ` Tensor ` whose dimensions match ` y_pred ` . loss = K.eval ( losses.categorical_hinge ( y_true , y_pred ) ) def test_unweighted ( self ) : ValueError : If the shape of ` sample_weight ` is invalid . assert K.eval ( objective_output ) .shape == ( 5 , 6 ) def test_serializing_model_with_loss_class ( tmpdir ) : elif y_pred_rank - weights_rank == 1 : assert np.isclose ( expected_loss , np.mean ( loss ) ) return self.fn ( y_true , y_pred , * * self._fn_kwargs ) y_pred = K.variable ( np.array ( [ [ 0.3 , 0.6 , 0.1 ] , ` y_pred ` . sample_weight = 1.0 If ` sample_weight ` is scalar , it is kept scalar . from __future__ import absolute_import `` `` '' Computes the mean of squares of errors between labels and predictions . y_pred_rank = K.ndim ( y_pred ) np.log ( K.epsilon ( ) ) + np.log ( 0.4 ) + np.log ( 0.2 ) import numpy as np reduction : ( Optional ) Type of loss reduction to apply to loss . Contains the following values : assert np.isclose ( K.eval ( loss ) , 0.0 ) loss = K.sum ( weighted_losses ) if ( y_pred_rank - y_true_rank == 1 ) and ( y_pred_shape [ -1 ] == 1 ) : y_pred : Predicted values , a ` Tensor ` of arbitrary dimensions . `` `` '' Types of loss reduction . * ` SUM ` : Scalar sum of weighted losses . y_a = K.variable ( np.random.randint ( 0 , 7 , ( 5 , 6 ) ) ) `` `` '' Squeeze or expand last dimension if needed . Size of the tensor . return ( cls.NONE , cls.SUM , cls.SUM_OVER_BATCH_SIZE ) model_filename = str ( tmpdir / 'custom_loss.hdf ' ) weights_rank = K.ndim ( sample_weight ) reduction=losses_utils.Reduction.SUM , name='mse_1 ' ) from .utils import losses_utils def _num_elements ( losses ) : def reduce_weighted_loss ( weighted_losses , reduction=Reduction.SUM_OVER_BATCH_SIZE ) : config [ k ] = K.eval ( v ) if is_tensor_or_variable ( v ) else v class LossFunctionWrapper ( Loss ) : mse_obj ( y_true , y_pred , sample_weight=sample_weight ) y_true : Ground truth values , with the same shape as 'y_pred ' . x : The input tensor . y_true_shape = K.int_shape ( y_true ) def compute_weighted_loss ( losses , reduction : ( Optional ) Type of Reduction to apply to loss . inputs = keras.layers.Input ( ( 2 , ) ) assert np.isclose ( K.eval ( loss ) , 49.5 , rtol=1e-3 ) coefficient for the loss . If a scalar is provided , then the loss is sample_weight = K.squeeze ( sample_weight , -1 ) super ( LossFunctionWrapper , self ) .__init__ ( reduction=reduction , name=name ) 1 . Squeezes last dim of ` y_pred ` or ` y_true ` if their rank differs by 1 . `` `` '' Utilities related to losses . '' '' '' ` losses ` , or be broadcastable to ` losses ` . def test_zero_weighted ( self ) : def all ( cls ) : < tf.Tensor : id=9 , shape= ( ) , dtype=int32 , numpy=4 > `` `` '' Computes the weighted loss . def size ( x , name=None ) : np.log ( 0.1 ) + np.log ( K.epsilon ( ) ) + np.log ( 0.3 ) ) / 9 Example subclass implementation : if weights_rank ! = 0 : model.compile ( optimizer='sgd ' , loss= { 'model_output ' : loss } ) with custom_object_scope ( { 'MSE_MAE_loss ' : MSE_MAE_loss } ) : def test_serializing_loss_class ( ) : class MeanSquaredError ( LossFunctionWrapper ) : orig_loss_class = MSE_MAE_loss ( 0.3 ) for k , v in six.iteritems ( self._fn_kwargs ) : y_true = K.constant ( [ 1 , 9 , 2 , -5 , -2 , 6 ] , shape= ( 2 , 3 ) ) [ [ 0.8 , 0.1 , 0.1 ] , y_true = K.variable ( np.array ( [ 1 , 2 ] ) ) deserialized = losses.deserialize ( serialized ) from .. import backend as K the shape of ` sample_weight ` matches the shape of ` y_pred ` , then the super ( MeanSquaredError , self ) .__init__ ( class TestMeanSquaredError : sample_weight = K.constant ( [ 1.2 , 3.4 ] , shape= ( 2 , 1 ) ) expected_loss = - ( np.log ( 0.6 ) + np.log ( 0.7 ) ) / 2 def test_sample_weighted ( self ) : return loss objective_output = obj ( y_a , y_b ) orig_loss_class = MSE_MAE_loss ( 0.3 ) deserialized = losses.deserialize ( serialized ) y_true = K.constant ( [ 4 , 8 , 12 , 8 , 1 , 3 ] , shape= ( 2 , 3 ) ) * * kwargs : The keyword arguments that are passed on to ` fn ` . y_a = K.variable ( np.random.random ( ( 5 , 6 , 7 ) ) ) return losses_utils.compute_weighted_loss ( def __call__ ( self , y_true , y_pred , sample_weight=None ) : name : ( Optional ) name for the loss . from keras.utils import losses_utils for obj in allobj : name : Optional name for the op . reduction : ( Optional ) Type of loss Reduction to apply to loss . sample_weight = K.constant ( [ 3 , 6 , 5 , 0 ] , shape= ( 2 , 2 ) ) assert K.eval ( objective_output ) .shape == ( 5 , 6 ) import abc [ 1.0 , 0.0 , 0.0 ] , def __init__ ( self , sample_weight = K.expand_dims ( sample_weight , -1 ) losses , sample_weight , reduction=self.reduction ) Loss values per sample . Reduction.validate ( reduction ) * * kwargs ) ` . def validate ( cls , key ) : base_config = super ( LossFunctionWrapper , self ) .get_config ( ) return tf.size ( x , name=name ) reduction=losses_utils.Reduction.NONE ) `` `` '' * ` NONE ` : Un-reduced weighted losses with the same shape as input . When this name=None ) : [ [ 0.3 , 0.7 , 0.0 ] , # Returns y_pred : The predicted values . [ 0.1 , 0.2 , 0.7 ] ] ) ) if sample_weight is None : objective_output = obj ( y_a , y_b ) # Returns loss = MSE_MAE_loss ( 0.3 ) inputs = keras.layers.Input ( ( 2 , ) ) np.log ( K.epsilon ( ) ) + np.log ( 0.4 ) + np.log ( 0.2 ) A ` Loss ` instance . self.fn = fn with K.name_scope ( scope_name ) : np.log ( 0.1 ) + np.log ( K.epsilon ( ) ) + np.log ( 0.3 ) ) / 9 expected_loss = - ( np.log ( 0.7 ) + np.log ( 0.3 ) + np.log ( 0.1 ) def test_cce_one_hot ( self ) : name='mean_squared_error ' ) : def test_objective_shapes_2d ( self ) : mean_squared_error , name=name , reduction=reduction ) def test_scalar_weighted ( self ) : assert np.allclose ( K.eval ( loss ) , [ 84.3333 , 143.3666 ] , rtol=1e-3 ) expected_loss = - ( np.log ( 0.7 ) + np.log ( 0.3 ) + np.log ( 0.1 ) y_true : Ground truth values . sample_weight = K.constant ( [ 3 , 6 , 5 , 0 , 4 , 2 ] , shape= ( 2 , 3 ) ) from the new rank of ` y_pred ` . def from_config ( cls , config ) : for obj in allobj : with pytest.raises ( Exception ) : reduction=Reduction.SUM_OVER_BATCH_SIZE , model.fit ( np.random.rand ( 256 , 2 ) , np.random.rand ( 256 , 1 ) ) [ 0.1 , 0.2 , 0.7 ] ] ) ) if reduction == Reduction.NONE : return K.mean ( math_ops.square ( y_pred - y_true ) , axis=-1 ) y_pred_rank = K.ndim ( y_pred ) [ 1 , 0 , 0 ] ] ) )","['keras/backend/__init__.py', 'keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/losses.py', 'keras/utils/__init__.py', 'keras/utils/losses_utils.py', 'tests/keras/losses_test.py']","Adding Loss , LossFunctionWrapper , MeanSquaredError classes . ( # 12859 )"
44,2f31fae720b25c60d15e1f30d17dca8e320dd92d,2019-05-23 16:59:05-07:00,"predictions = Dense ( 10 , activation='softmax ' ) ( output_2 ) output_1 = Dense ( 64 , activation='relu ' ) ( inputs ) predictions = Dense ( 10 , activation='softmax ' ) ( x ) output_2 = Dense ( 64 , activation='relu ' ) ( output_1 ) x = Dense ( 64 , activation='relu ' ) ( inputs ) x = Dense ( 64 , activation='relu ' ) ( x )",['docs/templates/getting-started/functional-api-guide.md'],DOC : do n't reuse names in introductory example ( # 12721 )
45,58cebe948addc7c4cb8762b2e67c8ce656a9d957,2019-05-23 16:56:51-07:00,"sub_n_last_node = { } sub_w_last_node = sub_w_nodes [ len ( sub_w_nodes ) - 1 ] sub_n_first_node.get_name ( ) ) sub_n_first_node [ layer.name ] = sub_n_nodes [ 0 ] name = sub_w_first_node [ layer.layer.name ] .get_name ( ) sub_w_last_node [ inbound_layer_name ] .get_name ( ) , add_edge ( dot , sub_w_last_node [ layer.layer.name ] = sub_w_nodes [ -1 ] sub_n_first_node = sub_n_nodes [ 0 ] else : add_edge ( dot , name , layer_id ) name = sub_n_last_node [ inbound_layer.name ] .get_name ( ) sub_w_first_node = sub_w_nodes [ 0 ] sub_w_last_node = { } add_edge ( dot , sub_w_last_node.get_name ( ) , layer_id ) sub_n_first_node [ layer.name ] .get_name ( ) ) sub_w_first_node.get_name ( ) ) ) sub_n_last_node = sub_n_nodes [ len ( sub_n_nodes ) - 1 ] sub_n_last_node [ layer.name ] = sub_n_nodes [ -1 ] if is_model ( layer ) : sub_n_first_node = { } layer_id ) add_edge ( dot , sub_n_last_node.get_name ( ) , layer_id ) output_name = sub_n_first_node [ layer.name ] .get_name ( ) name ) ) inbound_layer_name = inbound_layer.layer.name sub_w_first_node = { } sub_w_first_node [ layer.layer.name ] = sub_w_nodes [ 0 ] add_edge ( dot , name , output_name )",['keras/utils/vis_utils.py'],Fixed plot_model for expand_nested=True ( # 12751 )
46,627bf95c11199419eb6d2017c322a0d78ef3dc3c,2019-05-23 11:35:40-07:00,"to the decoder to produce predictions for the next character a training process called `` teacher forcing '' in this context . decoder_outputs = Convolution1D ( 64 , kernel_size=3 , activation='relu ' , target_texts.append ( target_text ) print ( 'Max sequence length for inputs : ' , max_encoder_seq_length ) # Encoder Append the sampled character to the target sequence target_text = '\t ' + target_text + '\n ' # Attention # decoder_target_data will be ahead by one timestep for t , char in enumerate ( input_text ) : if reverse_target_char_index [ x ] == `` \n '' : target_token_index = dict ( max_encoder_seq_length = max ( [ len ( txt ) for txt in input_texts ] ) ( len ( input_texts ) , max_decoder_seq_length ) , is for demonstration purposes only . input_texts.append ( input_text ) # Define an input sequence and process it . with open ( data_path , ' r ' , encoding='utf-8 ' ) as f : for line in lines [ : min ( num_samples , len ( lines ) - 1 ) ] : and corresponding target sequences from another domain # Next : inference mode ( sampling ) . ' '' input_characters = sorted ( list ( input_characters ) ) if t > 0 : We start with input sequences from a domain ( e.g . English sentences ) epochs=epochs , padding='causal ' , dilation_rate=4 ) ( x_decoder ) Repeat until we hit the character limit . decoder_input_data = np.zeros ( x_decoder = Convolution1D ( 256 , kernel_size=3 , activation='relu ' , decoded = [ ] the same sequence but offset by one timestep in the future , from keras.layers import Input , Convolution1D , Dot , Dense , Activation , Concatenate print ( 'Input sentence : ' , input_texts [ seq_index ] ) predict = np.zeros ( short English sentences into short French sentences , # We use `` tab '' as the `` start sequence '' character x_encoder = Convolution1D ( 256 , kernel_size=3 , activation='relu ' , num_decoder_tokens = len ( target_characters ) * * Data download * * [ Lots of neat sentence pairs datasets . from keras.models import Model padding='causal ' ) ( decoder_combined_context ) # Path to the data txt file on disk . in_encoder = encoder_input_data [ : nb_examples ] if char not in target_characters : print ( 'Number of samples : ' , len ( input_texts ) ) # for the targets , and `` \n '' as `` end sequence '' character . input_characters = set ( ) print ( 'Max sequence length for outputs : ' , max_decoder_seq_length ) # Take one sequence ( part of the training set ) for i , ( input_text , target_text ) in enumerate ( zip ( input_texts , target_texts ) ) : break num_samples = 10000 # Number of samples to train on . in_decoder [ : , 0 , target_token_index [ `` \t '' ] ] = 1 encoder_inputs = Input ( shape= ( None , num_encoder_tokens ) ) ] ( http : //www.manythings.org/anki/fra-eng.zip ) from __future__ import print_function Encode the input sequence . model = Model ( [ encoder_inputs , decoder_inputs ] , decoder_outputs ) Sample the next character using these predictions for char in target_text : # Run training An encoder CNN encodes the input character sequence . Effectively , the decoder learns to generate ` targets [ t+1 ... ] ` # Vectorize the data . for char in input_text : dtype='float32 ' ) attention = Activation ( 'softmax ' ) ( attention ) decoder_input_data [ i , t , target_token_index [ char ] ] = 1 . model.compile ( optimizer='adam ' , loss='categorical_crossentropy ' ) predict = predict.argmax ( axis=-1 ) output_seq = predict [ seq_index , : ] .ravel ( ) .tolist ( ) Feed the input sequence and 1-char target sequence given ` targets [ ... t ] ` , conditioned on the input sequence . batch_size = 64 # Batch size for training . model.save ( 'cnn_s2s.h5 ' ) * * References * * padding='causal ' ) ( decoder_inputs ) print ( '- ' ) decoded.append ( reverse_target_char_index [ x ] ) attention = Dot ( axes= [ 2 , 2 ] ) ( [ x_decoder , x_encoder ] ) for j , x in enumerate ( predict_ ) : input_characters.add ( char ) print ( 'Number of unique output tokens : ' , num_decoder_tokens ) # Save model ( we simply use argmax ) . decoder_target_data = np.zeros ( # for trying out decoding . target_characters = sorted ( list ( target_characters ) ) in_decoder = np.zeros ( num_encoder_tokens = len ( input_characters ) encoder_input_data = np.zeros ( [ ( char , i ) for i , char in enumerate ( target_characters ) ] ) ( i , char ) for char , i in target_token_index.items ( ) ) model.summary ( ) ] ( http : //www.manythings.org/anki/ ) A decoder CNN is trained to turn the target sequences into import numpy as np padding='causal ' ) ( encoder_inputs ) validation_split=0.2 ) lstm_seq2seq.py input_texts = [ ] model.fit ( [ encoder_input_data , decoder_input_data ] , decoder_target_data , target_characters.add ( char ) https : //wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html ( e.g . French sentences ) . # and will not include the start character . ' '' # Sequence-to-sequence example in Keras ( character-level ) . input_token_index = dict ( do character-level machine translation , as word-level in_decoder [ j , i + 1 , x ] = 1 sequence-to-sequence model . We apply it to translating ( len ( input_texts ) , max_encoder_seq_length , num_encoder_tokens ) , if char not in input_characters : ( len ( input_texts ) , max_decoder_seq_length , num_decoder_tokens ) , batch_size=batch_size , for x in output_seq : context = Dot ( axes= [ 2 , 1 ] ) ( [ attention , x_encoder ] ) # Decoder # Output data_path = 'fra-eng/fra.txt ' predict_ = predict [ : , i ] .ravel ( ) .tolist ( ) input_text , target_text = line.split ( '\t ' ) max_decoder_seq_length = max ( [ len ( txt ) for txt in target_texts ] ) In inference mode , when we want to decode unknown input sequences , we : It uses the output from the encoder . ( i , char ) for char , i in input_token_index.items ( ) ) padding='causal ' , dilation_rate=4 ) ( x_encoder ) encoder_input_data [ i , t , input_token_index [ char ] ] = 1 . reverse_input_char_index = dict ( This script demonstrates how to implement a basic character-level CNN for i in range ( max_decoder_seq_length - 1 ) : else : ( just the start-of-sequence character ) # decoder_target_data is ahead of decoder_input_data by one timestep target_characters = set ( ) # Define the model that will turn lines = f.read ( ) .split ( '\n ' ) for t , char in enumerate ( target_text ) : predict = model.predict ( [ in_encoder , in_decoder ] ) [ ( char , i ) for i , char in enumerate ( input_characters ) ] ) decoder_combined_context = Concatenate ( axis=-1 ) ( [ context , x_decoder ] ) reverse_target_char_index = dict ( decoder_inputs = Input ( shape= ( None , num_decoder_tokens ) ) models are much more common in this domain . This example decoder_target_data [ i , t - 1 , target_token_index [ char ] ] = 1 . padding='causal ' , dilation_rate=2 ) ( x_encoder ) padding='causal ' ) ( decoder_outputs ) # Define sampling models nb_examples = 100 for seq_index in range ( nb_examples ) : [ English to French sentence pairs . decoded_sentence = `` '' .join ( decoded ) decoder_outputs = decoder_dense ( decoder_outputs ) Start with a target sequence of size 1 epochs = 100 # Number of epochs to train for . * * Summary of the algorithm * * print ( 'Number of unique input tokens : ' , num_encoder_tokens ) character-by-character . Note that it is fairly unusual to decoder_dense = Dense ( num_decoder_tokens , activation='softmax ' ) # ` encoder_input_data ` & ` decoder_input_data ` into ` decoder_target_data ` padding='causal ' , dilation_rate=2 ) ( x_decoder ) print ( 'Decoded sentence : ' , decoded_sentence ) target_texts = [ ]",['examples/cnn_seq2seq.py'],add cnn seq2seq example ( # 12831 )
47,0a0ac3fa5462cf4a72636ca4498a0a82ac91fc32,2019-05-23 11:24:44-07:00,"` mkdocs build ` # Builds a static site in ` site/ ` directory From the root directory , ` cd ` into the ` docs/ ` folder and run : install MkDocs : ` pip install mkdocs ` ` mkdocs serve ` # Starts a local webserver : [ localhost:8000 ] ( localhost:8000 ) ` mkdocs build ` # Builds a static site in `` site '' directory ` mkdocs serve ` # Starts a local webserver : [ localhost:8000 ] ( http : //localhost:8000 ) ` cd ` to the ` docs/ ` folder and run : The source for Keras documentation is in this directory under ` sources/ ` . Install MkDocs : ` pip install mkdocs ` The source for Keras documentation is in this directory .",['docs/README.md'],Refactor outdated official keras site doc . ( # 12858 )
48,f69e046eef93af5a28de5330bfe15ee348520b2d,2019-04-19 17:14:42-07:00,"sub_n_nodes = submodel_not_wrapper.get_nodes ( ) if expand_nested and isinstance ( layer , Model ) : # sub_n : submodel_not_wrapper dot.add_edge ( pydot.Edge ( dot.add_node ( node ) # if current layer is wrapped Model dot.add_subgraph ( submodel_wrapper ) subgraph=True ) elif is_wrapped_model ( layer ) : # if inbound_layer is Model elif is_model ( inbound_layer ) : subgraph=True ) not is_wrapped_model ( layer ) ) : model_nodes = submodel.get_nodes ( ) sub_w_last_node = sub_w_nodes [ len ( sub_w_nodes ) - 1 ] not is_wrapped_model ( inbound_layer ) ) : inbound_layer_id = str ( id ( inbound_layer ) ) if not dot.get_edge ( src , dst ) : model_nodes [ len ( model_nodes ) - 1 ] .get_name ( ) , from .. layers.wrappers import Wrapper sub_w_nodes = submodel_wrapper.get_nodes ( ) dot.add_subgraph ( submodel ) # if current layer is Model expand_nested , # Make sure that both nodes exist before connecting them with submodel_wrapper = model_to_dot ( layer.layer , show_shapes , add_edge ( dot , sub_w_last_node.get_name ( ) , layer_id ) def is_model ( layer ) : elif is_wrapped_model ( inbound_layer ) : def add_edge ( dot , src , dst ) : sub_w_first_node = sub_w_nodes [ 0 ] # create any missing node . sub_w_first_node.get_name ( ) ) ) dot.add_edge ( pydot.Edge ( layer_id , model_nodes [ 0 ] .get_name ( ) ) ) dot.add_edge ( pydot.Edge ( inbound_layer_id , dot.add_edge ( pydot.Edge ( layer_id , submodel_not_wrapper = model_to_dot ( layer , show_shapes , if not is_model ( layer ) and ( dot = pydot.Cluster ( style='dashed ' , graph_name=model.name ) sub_n_first_node.get_name ( ) ) show_layer_names , rankdir , # if current layer is not Model or wrapped Model isinstance ( inbound_layer , Wrapper ) and from .. models import Model dot.add_edge ( pydot.Edge ( src , dst ) ) node = pydot.Node ( layer_id , label=label ) if not is_model ( inbound_layer ) and ( dot.add_node ( node ) add_edge ( dot , sub_n_last_node.get_name ( ) , layer_id ) # if inbound_layer is not Model or wrapped Model dot.add_subgraph ( submodel_not_wrapper ) isinstance ( inbound_layer.layer , Model ) ) : sub_n_last_node = sub_n_nodes [ len ( sub_n_nodes ) - 1 ] assert dot.get_node ( inbound_layer_id ) submodel = model_to_dot ( layer.layer , show_shapes , sub_n_first_node = sub_n_nodes [ 0 ] layer_id ) ) elif is_model ( layer ) : next_layer_id ) ) # if inbound_layer is wrapped Model return isinstance ( layer , Wrapper ) and isinstance ( layer.layer , Model ) return isinstance ( layer , Model ) # an edge , as add_edge would otherwise add_edge ( dot , inbound_layer_id , show_layer_names , rankdir , expand_nested , assert dot.get_node ( layer_id ) if not expand_nested : next_layer_id = str ( id ( layers [ i + 1 ] ) ) node = pydot.Node ( layer_id , label=label ) else : if not expand_nested or not isinstance ( layer , Model ) : dot = pydot.Cluster ( style='dashed ' ) if len ( layers ) > i + 1 : if not expand_nested or not ( # sub_w : submodel_wrapper inbound_layer_id = str ( id ( inbound_layer ) ) def is_wrapped_model ( layer ) :",['keras/utils/vis_utils.py'],[ P ] change plot_model to fully support plotting submodel and fix bugs ( # 12675 )
49,261bcb2515a82ba4c96c03e4960c03195218ddb4,2019-04-15 22:43:50-07:00,"K.floatx ( ) ) return K.cast ( K.in_top_k ( y_pred , K.argmax ( y_true , axis=-1 ) , k ) , K.floatx ( ) ) axis=-1 ) assert np.mean ( partial_result ) == 0.5 assert np.mean ( success_result ) == 1 return K.mean ( K.in_top_k ( y_pred , K.cast ( K.flatten ( y_true ) , 'int32 ' ) , k ) , return K.mean ( K.in_top_k ( y_pred , K.argmax ( y_true , axis=-1 ) , k ) , axis=-1 ) assert failure_result == 0 assert partial_result == 0.5 assert success_result == 1 return K.cast ( K.in_top_k ( y_pred , K.cast ( K.flatten ( y_true ) , 'int32 ' ) , k ) , assert np.mean ( failure_result ) == 0","['keras/metrics.py', 'tests/keras/metrics_test.py']",Fix top_k_categorical_accuracy weighted metric . ( # 12632 )
50,6b8a3bcd79beb264794ea72fd7a86c64c3f27736,2019-04-14 11:00:10-07:00,"__y_train , y_test__ : uint8 array of category labels ( integers in range 0-9 ) with shape ( num_samples , 1 ) . __y_train , y_test__ : uint8 array of category labels ( integers in range 0-9 ) with shape ( num_samples , ) . __y_train , y_test__ : uint8 array of category labels with shape ( num_samples , ) . __y_train , y_test__ : uint8 array of category labels with shape ( num_samples , 1 ) .",['docs/templates/datasets.md'],Update the shape of y_train and y_test ( # 12669 )
51,b5af431ce6b78dbc52f2bdeecdff52fd696bc7d0,2019-04-11 17:44:53-07:00,"Provide a reproducible test case that is the bare minimum necessary to generate the problem . Keras version : python -c `` import tensorflow as tf ; print ( tf.GIT_VERSION , tf.VERSION ) '' < em > Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template . * * System information * * TensorFlow backend ( yes / no ) : GPU model and memory : ` pip install git+git : //github.com/keras-team/keras.git -- upgrade -- no-deps ` * * Other info / logs * * [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue ( or just copy the script here if it is short ) . [ ] Check that your version of TensorFlow is up-to-date . The installation instructions can be found [ here ] ( https : //www.tensorflow.org/get_started/os_setup ) . You can obtain the TensorFlow version with : Please make sure that the boxes below are checked before you submit your issue . CUDA/cuDNN version : You can obtain the Keras version with : Include any logs or source code that would be helpful to diagnose the problem . If including tracebacks , please include the full traceback . Large logs and files should be attached . * * Code to reproduce the issue * * [ ] Check that you are up-to-date with the master branch of Keras . You can update with : If your issue is an * * implementation question * * , please ask your question on [ StackOverflow ] ( http : //stackoverflow.com/questions/tagged/keras ) or [ on the Keras Slack channel ] ( https : //keras-slack-autojoin.herokuapp.com/ ) instead of opening a GitHub issue. < /em > * * Describe the current behavior * * python -c 'import keras as k ; print ( k.__version__ ) ' TensorFlow version : Thank you ! Python version : OS Platform and Distribution ( e.g. , Linux Ubuntu 16.04 ) : If your issue is an * * implementation question * * , please ask your question on [ StackOverflow ] ( http : //stackoverflow.com/questions/tagged/keras ) or [ on the Keras Slack channel ] ( https : //keras-slack-autojoin.herokuapp.com/ ) instead of opening a GitHub issue . Have I written custom code ( as opposed to using example directory ) : * * Describe the expected behavior * *",['.github/ISSUE_TEMPLATE/a--tensorflow-backend-users.md'],Update a -- tensorflow-backend-users.md ( # 12644 )
52,b2771d13b0f9146cde43af00f5a5156f56614436,2019-04-08 10:53:47+09:00,for batch_out in enumerate ( batch_outs ) : if 'weights ' in kwargs : batch_size = kwargs.get ( 'batch_size ' ) else : self._initial_weights = None if 'batch_size ' in kwargs : self._initial_weights = kwargs [ 'weights ' ] for _ in enumerate ( batch_outs ) : import copy outs.extend ( [ 0 . ] * len ( batch_outs ) ) outs.append ( 0 . ) user_kwargs = kwargs.copy ( ) else : self._initial_weights = kwargs.get ( 'weights ' ) batch_size = kwargs [ 'batch_size ' ] user_kwargs = copy.copy ( kwargs ) batch_size = None,"['keras/engine/base_layer.py', 'keras/engine/training_arrays.py']",Minor optimisations ( # 12627 )
53,30fe4ff1f12ff0c45bac8738b4d2690eadd056b2,2019-04-07 04:40:58+09:00,"coefs = np.fromstring ( coefs , ' f ' , sep= ' ' ) values = line.split ( ) word = values [ 0 ] word , coefs = line.split ( maxsplit=1 ) coefs = np.asarray ( values [ 1 : ] , dtype='float32 ' )",['examples/pretrained_word_embeddings.py'],Speed optimization ( # 12628 )
54,2fadce57f0e12cf96f11fa1d42286986eccd70a8,2019-04-03 14:46:21-07:00,"dtype = floatx ( ) return C.zeros_like ( x , name ) return C.cast ( C.ones_like ( x , name ) , dtype ) if dtype is None : return C.cast ( C.zeros_like ( x , name ) , dtype ) return C.ones_like ( x , name )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py']",Default type to floatx ( ) ( # 12492 )
55,7c7e51ea5ab47b67cd68374400051dd022bdc662,2019-04-03 11:43:46-07:00,"`` `` '' Validates that the ` batch_size ` provided is consistent with InputLayer . callbacks= [ tracker_cb ] ) Number of samples per evaluation step . framework-native tensors ( e.g . TensorFlow data tensors ) . x : Numpy array of test data ( if the model has a single input ) , batch_input_shape , _ = get_input_shape_and_dtype ( layer ) ` x ` can be ` None ` ( default ) if feeding from workers=1 , layer = layer.layers [ 0 ] assert np.shape ( out [ 0 ] ) == shape_0 and np.shape ( out [ 1 ] ) == shape_1 out = single_output_model.predict ( from the static batch size of the InputLayer . Lastly , ValueError will be batch_size = 32 # Raises If input layers in the model are named , you can also pass a first_layer = layers [ 0 ] # Validate user data . .format ( batch_size , static_batch_size ) ) only . Maximum number of processes to spin up when using ` sample_weight_mode= '' temporal '' ` in ` compile ( ) ` . This argument workers=1 , initial_epoch=0 , It 's possible that the user specified a static batch size in their ` sample_weight_mode= '' temporal '' ` in ` compile ( ) ` . yield ( np.asarray ( [ ] ) , np.asarray ( [ ] ) ) # Case 1 : generator-like . Input is Python generator , def evaluate ( self , x=None , y=None , assert len ( val_seq.logs ) < = 4 * 5 Do not specify the ` batch_size ` is your data is in the ' a generator or Sequence instance . Instead pass sample ' Specifies whether to build the model 's graph in inference array/tensors , if the model has named inputs . whether to build the model 's graph in inference mode ( False ) , training raise ValueError ( 'If your data is in the form of a Python generator , ' return inspect.isgenerator ( x ) or is_sequence ( x ) while True : # Arguments to 1 . If 0 , will execute the generator on the main thread . use_multiprocessing=False , placeholders when calling ` fit ` /etc . if batch_size is None and steps is None : check_generator_arguments ( if data tensors : the model is built on top of these tensors . ( e.g . TensorFlow data tensors ) . the Numpy arrays . We expect Numpy data to be fed for these `` `` '' def evaluate ( self , # does n't get confused . will default to 1 . If 0 , will execute the generator on the main max_queue_size=max_queue_size , steps_per_epoch=3 , input only . Maximum size for the generator queue . batch_size , steps_per_epoch , x ) validation_split=None ) : ( since they generate batches ) . For the first two cases , ` batch_size ` must be provided . from .training_utils import check_generator_arguments or list of Numpy arrays ( if the model has multiple inputs ) . `` `` '' Retrieves input shape and input dtype of layer if applicable . placeholders , Numpy arrays , or data tensors . ValueError : in case of invalid arguments . layer : a ` Layer ` instance . raise ValueError ( 'An empty Model can not be used as a Layer . ' ) InputLayer . If so , this method checks the provided ` batch_size ` and ` x ` epochs=epochs , tuple ` ( x_val , y_val ) ` of Numpy arrays or tensors assert tracker_cb.trained_epochs == [ 0 , 1 , 2 , 3 , 4 ] max_queue_size=1 ) We do not expect any Numpy data to be provided when calling `` `` '' Trains the model for a given number of epochs ( iterations on a dataset ) . use_multiprocessing : Boolean . Used for generator or # of the deepest model to infer input shape and dtype . validation_data=val_seq , when calling ` fit ` /etc . if layers : threading . If unspecified , ` use_multiprocessing ` will default to return None , None inputs : Single array , or list of arrays . The arrays could be x : Input data . It could be : from ` x ` ) . fit/evaluate/predict . raised if ` x ` is a generator or ` Sequence ` instance and ` batch_size ` is steps=steps , form of symbolic tensors , generators , or ` Sequence ` instances y , sample_weight , validation_split=validation_split ) tuple ` ( x_val , y_val , val_sample_weights ) ` of Numpy arrays if validation_split : if hasattr ( layer , '_batch_input_shape ' ) : ` keras.utils.Sequence ` input only . If ` True ` , use process-based mode ( True ) , or using the Keras learning phase ( None ) . if placeholders : the model is built on top of these check_generator_arguments ( y , sample_weight ) batch_size = static_batch_size if not layer.layers : batch_size : The batch_size provided as an argument to dictionary mapping input names to Numpy arrays . it could be either Numpy array ( s ) , framework-native tensor ( s ) , if Numpy data : we create placeholders matching the shape of the Numpy mode ( False ) , training mode ( True ) , or using the Keras and is a generator . Total number of steps ( batches of samples ) validation_data : Data on which to evaluate If unspecified , ` max_queue_size ` will default to 10 . raise ValueError ( 'The ` batch_size ` argument value { } is ' A generator or ` keras.utils.Sequence ` returning y : Numpy array of target ( label ) data Numpy arrays , or data tensors . # Backwards compatibility . 'you can not use ` validation_split ` . ' ) def get_static_batch_size ( layer ) : of every epoch . ` ( x_val , y_val , val_sample_weights ) ` on which to evaluate workers=workers , inputs : Single array , or list of arrays . The arrays could be placeholders , # In case of nested models : recover the first layer workers : Integer . Used for generator or ` keras.utils.Sequence ` input ` fit ` /etc . max_queue_size : Integer . Used for generator or ` keras.utils.Sequence ` batch_size : Integer . If unspecified , it will default to 32 . def is_generator_or_sequence ( x ) : and we expect Numpy data to be fed for them when calling ` fit ` /etc . ValueError : if a batch size is specified and a generator/Sequence shuffle=shuffle , def check_generator_arguments ( y=None , sample_weight=None , def gen_data ( ) : `` `` '' Check if ` x ` is a Keras generator type . '' '' '' only . Maximum number of processes to spin up Tuple ( input_shape , input_dtype ) . Both could be None if the layer if batch_size is not None and is_generator_or_sequence ( x ) : callbacks=None ) : return self.evaluate_generator ( def get_input_shape_and_dtype ( layer ) : or list of Numpy arrays ( if the model has multiple outputs ) . initial_epoch=initial_epoch ) if static_batch_size is not None : tensors ( e.g . TensorFlow data tensors ) . max_queue_size=1 , form of symbolic tensors , generators , or # pass generator directly so ` is_generator_or_sequence ` y=None , if y is not None : steps : The steps provided as an argument to fit/evaluate/predict . return ( ( hasattr ( layer , '_is_graph_network ' ) and layer._is_graph_network ) or ` False ` . Note that because this implementation relies on x : The data passed as ` x ` to fit/evaluate/predict . validation_freq=validation_freq , return self.predict_generator ( outputs : Optional output tensors ( if already computed by running ' weights as the third element of the generator . ' ) This argument is not supported when ` x ` is a generator or * * kwargs : Used for backwards compatibility . use_multiprocessing=False ) : layer : Layer ( or model ) instance . while _is_graph_model ( layer ) : epochs=5 , if batch_input_shape is not None : Number of samples per gradient update . outputs : Optional output tensors ( if already computed by running static_batch_size = get_static_batch_size ( first_layer ) # or Sequence object , or iterator . ` batch_size ` is ` None ` , this method will attempt to infer the batch size batch_size = self._validate_or_infer_batch_size ( process-based threading . If unspecified , ` workers ` will default raise ValueError ( ' ` sample_weight ` argument is not supported when data is ' max_queue_size=10 , use_multiprocessing=False ) : batch_size = 32 `` `` '' # Subclassed Models may not have been built so ca n't be checked . ValueError : in case an empty Sequential or Functional model is passed . ` y ` can be ` None ` ( default ) if feeding from out = model.evaluate ( gen_data ( 4 ) .it , steps=3 , verbose=1 ) exepected size defined in the Input Layer . thread . ( in case the model has multiple inputs ) . dataset or a dataset iterator arguments are consistent with this static batch size . Also , if max_queue_size=10 , if batch_size is None and steps_per_epoch is None : validation_steps : Only relevant if ` validation_data ` is provided out = model.fit ( RandomSequence ( 3 ) , ` y ` should not be specified ( since targets will be obtained multiprocessing , you should not pass non-picklable arguments to not provided . callbacks=None , # Raises class_weight=class_weight , def _is_graph_model ( layer ) : # Returns does not have a defined input shape . with pytest.raises ( ValueError ) : ( if the model has a single output ) , arrays . We expect Numpy data to be fed for these placeholders # Check ` batch_size ` argument is consistent with InputLayer . when calling ` fit ` /etc . ` ( inputs , targets ) ` or ` ( inputs , targets , sample weights ) ` . instead provide the sample_weights as the third element of ` x ` . ` keras.utils.Sequence ` instances ( since they generate batches ) . `` `` '' Validates arguments passed when using a generator . '' '' '' out = model.evaluate ( generator , steps=3 ) The validated batch_size , auto-inferred from the first layer if the model ) . For the last case , ` validation_steps ` must be provided . max_queue_size=10 , `` `` '' Gets the static batch size of a Layer . # Returns from .training_utils import get_static_batch_size def _validate_or_infer_batch_size ( self , batch_size , steps , x ) : # Backwards compatibility x=None , callbacks= [ tracker_cb ] , tracker_cb = TrackerCallback ( ) if data tensors : the model is built on top of these tensors . return batch_size batch_size = self._validate_or_infer_batch_size ( batch_size , steps , x ) return batch_input_shape [ 0 ] validation_data=validation_data , verbose=verbose , y : Target data . Like the input data ` x ` , from .training_utils import is_generator_or_sequence ` Sequence ` instance . to draw before stopping when performing validation at the end ' a generator or Sequence instance . Instead pass targets ' ( or list of Numpy arrays if the model has multiple inputs ) . if Numpy data : we create placeholders matching the shape of callbacks=None , return self.fit_generator ( steps_per_epoch=steps_per_epoch , raise ValueError ( ' ` y ` argument is not supported when data is ' x : Numpy array of training data ( if the model has a single input ) , 'incompatible with the specified batch ' if sample_weight is not None : If ` x ` is a generator , or ` keras.utils.Sequence ` instance , # Arguments layer.__class__.__name__ == 'Sequential ' ) workers=1 , assert tracker_cb.trained_batches == list ( range ( 3 ) ) * 5 use_multiprocessing=use_multiprocessing ) validation_steps=validation_steps , If unspecified , ` batch_size ` will default to 32 . RandomSequence ( batch_size , sequence_length=sequence_length ) ) the generator as they ca n't be passed easily to children processes . x : The input data , as a Numpy array import inspect use_multiprocessing=use_multiprocessing , # Backwards compatibility # Case 2 : Symbolic tensors or Numpy array-like . # Case 1 : generator-like . Input is Python generator , or Sequence object . assert tracker_cb.trained_batches == list ( range ( 12 ) ) * 5 is not supported when ` x ` generator , or ` Sequence ` instance , None ( default ) if feeding from framework-native `` `` '' Trains the model for a fixed number of epochs ( iterations on a dataset ) . batch_size : Integer or ` None ` . ` validation_data ` could be : is passed , or if the specified batch size does not match the assert 12 * 5 < = len ( val_seq.logs ) < = ( 12 * 5 ) + 2 # the queue may be full . training : Boolean or None . Only relevant in symbolic mode . Specifies raise ValueError ( 'The ` batch_size ` argument must not be specified when ' if placeholders : the model is built on top of these placeholders , if batch_size is not None and batch_size ! = static_batch_size : None ( default ) if feeding from framework-native tensors We do not expect any Numpy data to be provided when calling ` fit ` /etc . if batch_size is None and steps is None : validation_steps=3 , 'size of your Input Layer : { } ' x , return None the model ) . ' using a generator or Sequence as an input . ' ) layers = super ( Model , self ) .layers # Avoids the override in Sequential . callbacks=callbacks , assert np.shape ( out ) == shape_0 # Set inferred batch size from the InputLayer . if is_generator_or_sequence ( x ) : learning phase ( None ) . callbacks=None ) : out = model.evaluate ( gen_data ( ) .it , steps=1 ) A dict mapping input names to the corresponding training : Boolean or None . Only relevant in symbolic mode . A Numpy array ( or array-like ) , or a list of arrays Do not specify the ` batch_size ` if your data is in the when using process-based threading . If unspecified , ` workers ` ' as the second element of the generator . ' ) The static batch size of a Layer . specified as we expect users to provide batched datasets . if steps is None : placeholders , and we expect Numpy data to be fed for them list of Numpy arrays ( if the model has multiple outputs ) or validation_data : tuple ` ( x_val , y_val ) ` or tuple out = model.predict ( return layer._batch_input_shape , layer.dtype val_seq = RandomSequence ( 4 )","['keras/engine/training.py', 'keras/engine/training_utils.py', 'tests/keras/engine/test_training.py']","Allow generators into Model 's fit , evaluate and predict ( # 12568 )"
56,9e6b181b0b27d9ea1157b2ed7bcce04434bc8d77,2019-04-02 17:42:04-07:00,x_shape = int_shape ( x ) if not ( len ( int_shape ( x ) ) == len_start == len_size ) : len_start = int_shape ( start ) [ 0 ] if is_tensor ( start ) else len ( start ) len_size = int_shape ( size ) [ 0 ] if is_tensor ( size ) else len ( size ) if not ( len ( int_shape ( x ) ) == len ( start ) == len ( size ) ) : if ( x_shape is not None ) and ( x_shape [ 0 ] is not None ) : raise ValueError ( 'The dimension and the size of indices should match . ' ) raise ValueError ( 'The dimension and the size of indices should match . ' ),['keras/backend/tensorflow_backend.py'],Fix ` slice ` of TF for enabling the sanity check to handle TF tensors ( # 12551 )
57,d789bd94a3a1bdc18390b8bff742936dcc70c54f,2019-04-02 17:41:17-07:00,"# Keep track of the old value assert K.dtype ( K.variable ( 1 , dtype='float16 ' ) ) == 'float16 ' @ pytest.mark.skipif ( ( K.backend ( ) == 'cntk ' ) , check_dtype ( var , 'float16 ' ) def test_dtype ( self , dtype ) : def test_dtype ( self ) : reason='cntk does not support float16 ' ) assert K.dtype ( K.variable ( 1 , dtype='float64 ' ) ) == 'float64 ' assert K.dtype ( K.variable ( 1 , dtype='int16 ' ) ) == 'int16 ' var = variable ( [ 10 ] ) with pytest.raises ( TypeError ) : from keras.backend import floatx , set_floatx , variable assert K.dtype ( K.variable ( 1 , dtype='int16 ' ) ) == 'int16 ' initial = floatx ( ) def test_setfloatx_correct_values ( self ) : old_floatx = K.floatx ( ) set_floatx ( value ) assert dtype in str ( var.dtype ) if K.backend ( ) == 'tensorflow ' : assert K.dtype ( K.variable ( 1 , dtype=dtype ) ) == dtype `` `` '' assert K.dtype ( K.variable ( 1 , dtype='float32 ' ) ) == 'float32 ' assert K.dtype ( K.variable ( False , dtype='bool ' ) ) == 'bool ' def test_setfloatx_incorrect_values ( self ) : for value in [ 'float16 ' , 'float32 ' , 'float64 ' ] : check_dtype ( var , 'float64 ' ) assert K.dtype ( K.variable ( False , dtype='bool ' ) ) == 'bool ' def test_setfloatx_correct_values ( self , dtype ) : # Github issue : 7819 K.set_floatx ( dtype ) # taken into account by the backend . assert K.floatx ( ) == dtype with pytest.raises ( ValueError ) : old_floatx = floatx ( ) # Restore old value def test_set_floatx ( self ) : with pytest.raises ( ValueError ) : K.set_floatx ( old_floatx ) assert K.floatx ( ) == old_floatx # Try some incorrect values if K.backend ( ) == 'tensorflow ' : assert dtype in str ( var.dtype.name ) set_floatx ( 'float16 ' ) set_floatx ( old_floatx ) check_dtype ( K.variable ( [ 10 ] ) , dtype ) # Make sure that changes to the global floatx are effectively with pytest.raises ( TypeError ) : assert floatx ( ) == initial set_floatx ( 'float64 ' ) assert var.dtype == dtype assert var.dtype.name == ' % s_ref ' % dtype for value in [ `` , 'beerfloat ' , 123 ] : K.variable ( `` , dtype='unsupported ' ) K.variable ( `` , dtype='unsupported ' ) if K.backend ( ) == 'theano ' : taken into account by the backend . def test_setfloatx_incorrect_values ( self , dtype ) : Make sure that changes to the global floatx are effectively K.set_floatx ( dtype ) assert floatx ( ) == value set_floatx ( value )",['tests/keras/backend/backend_test.py'],Revise backend tests for dtype ( # 12544 )
58,91efaaa1d7486df71be30a54d82b907443d80bf1,2019-04-02 17:39:50-07:00,"and not ( dilation_rate ! = ( 1 , 1 ) and K.backend ( ) == 'cntk ' ) ) ] space = input_shape [ 1 : -1 ] space [ i ] , new_space.append ( new_dim ) return ( input_shape [ 0 ] , out_filters , new_space [ 0 ] , new_space [ 1 ] ) [ ( padding , strides , multiplier , dilation_rate ) padding=self.padding , new_space.append ( new_dim ) out_filters = input_shape [ 3 ] * self.depth_multiplier dilation_rate : an integer or tuple/list of 2 integers , specifying dilation=self.dilation_rate [ i ] ) self.strides [ 1 ] ) def test_depthwise_conv_2d ( padding , strides , multiplier , dilation_rate ) : 'depth_multiplier ' : multiplier , return ( input_shape [ 0 ] , rows , cols , out_filters ) rows = input_shape [ 1 ] dilation_rate=dilation_rate , and not ( dilation_rate ! = ( 1 , 1 ) and multiplier == dilation_rate [ 0 ] ) new_space = [ ] 'dilation_rate ' : dilation_rate } , 'padding , strides , multiplier , dilation_rate ' , Currently , specifying any ` dilation_rate ` value ! = 1 is stride=self.strides [ i ] , for dilation_rate in [ ( 1 , 1 ) , ( 2 , 2 ) , ( 2 , 1 ) , ( 1 , 2 ) ] self.kernel_size [ i ] , out_filters = input_shape [ 3 ] * self.depth_multiplier rows = conv_utils.conv_output_length ( rows , self.kernel_size [ 0 ] , space [ i ] , new_space = [ ] new_dim = conv_utils.conv_output_length ( elif self.data_format == 'channels_first ' : dilation=self.dilation_rate [ i ] ) cols = conv_utils.conv_output_length ( cols , self.kernel_size [ 1 ] , if ( not ( padding == 'same ' and strides ! = ( 1 , 1 ) ) if self.data_format == 'channels_last ' : Can be a single integer to specify the same value for 'padding , strides , multiplier ' , self.padding , stride=self.strides [ i ] , if not ( padding == 'same ' and strides ! = ( 1 , 1 ) ) ] self.kernel_size [ i ] , 'depth_multiplier ' : multiplier } , new_dim = conv_utils.conv_output_length ( cols = input_shape [ 2 ] dilation_rate= ( 1 , 1 ) , cols = input_shape [ 3 ] return ( input_shape [ 0 ] , new_space [ 0 ] , new_space [ 1 ] , out_filters ) space = input_shape [ 2 : ] space = input_shape [ 2 : ] the dilation rate to use for dilated convolution . and not ( dilation_rate ! = ( 1 , 1 ) and strides ! = ( 1 , 1 ) ) incompatible with specifying any stride value ! = 1 . if self.data_format == 'channels_first ' : rows = input_shape [ 2 ] self.strides [ 0 ] ) for i in range ( len ( space ) ) : all spatial dimensions . for i in range ( len ( space ) ) : return ( input_shape [ 0 ] , out_filters , rows , cols ) [ ( padding , strides , multiplier ) elif self.data_format == 'channels_last ' : def test_depthwise_conv_2d ( padding , strides , multiplier ) : padding=self.padding ,","['keras/layers/convolutional.py', 'tests/keras/layers/convolutional_test.py']",Add kwarg and documentation for dilation_rate to DepthWiseConv2D ( # 12526 )
59,ad5db1010c40500793cc42539f1d97fa5fe36221,2019-04-02 17:30:51-07:00,"the last value is used ( ` elems [ -1 ] ` ) as ` initializer ` from ` elems ` ( and on the optional ` initializer ` ) passed as a second argument . name : ( optional ) String , name for the foldr node in the graph . TypeError : if ` elems ` is not a tensor . accumulated value calculated from the preceding invocation of ` fn ` . raise NotImplementedError name : ( optional ) String , name for the foldl node in the graph . initializer = elems [ -1 ] raise TypeError ( `` ` fn ` must be callable . '' ) elems = elems [ 1 : ] if initializer is not None and not is_tensor ( initializer ) : if initializer is None and shape ( elems ) [ 0 ] > 1 : # Arguments The first argument passed to ` fn ` is the accumulator which is the if not callable ( fn ) : `` ` return reshape ( accumulator , shape ( initializer ) [ 1 : ] ) TypeError : if ` fn ` is not callable . lambda acc , x : acc + x `` ` python `` `` '' initializer : ( optional ) Tensor , the initial value for the accumulator . initializer = elems [ 0 ] elems = None @ pytest.mark.skipif ( K.backend ( ) == 'cntk ' , reason='Not supported . ' ) TypeError : if ` initializer ` is neither a tensor nor None value . if elems is not None : accumulator.name = str ( name ) fn : Callable that will be called upon each element in ` elems ` for i in range ( shape ( elems ) [ 0 ] ) : `` `` '' Reduce ` elems ` by ` fn ` combined them from right to left on dimension 0 . Example For ` fn ` : elems : Tensor if not is_tensor ( elems ) : accumulator = initializer elems = elems [ : -1 ] In case of None value is provided during the call if name is not None : raise TypeError ( ' ` elems ` must be a tensor ' ) `` `` '' Reduce ` elems ` by ` fn ` combined them from left to right on dimension 0 . elif initializer is None : # Raises : # Returns accumulator = fn ( accumulator , elems [ i ] ) Same type and shape as ` initializer ` the first value is used ( ` elems [ 0 ] ` ) as ` initializer ` from ` elems ` raise TypeError ( `` ` initializer ` must be a tensor or None '' ) accumulator = fn ( accumulator , elems [ -i ] )","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",Implement ` foldl ` and ` foldr ` for CNTK # 12168 ( # 12572 )
60,c3eb62763fcc5029c26b025ba1b60d7788d491a1,2019-03-30 13:48:42-07:00,"model = _deserialize_model ( h5dict , custom_objects , compile ) with h5py.File ( fname ) as h5file : backing_store=False ) as h5file : h5file_ [ 'data1 ' ] .attrs [ 'attr ' ] = attr opened_new_file = False from keras.utils.io_utils import load_from_binary_h5py h5file_ [ 'data1 ' ] = data1 y = np.random.random ( ( 1 , 3 ) ) from .. utils.io_utils import H5Dict isinstance ( path , dict ) or h5_file_args = { 'backing_store ' : False , def save_function ( h5file_ ) : loaded_model = load_model ( h5file ) save_model ( model , h5file ) model = load_from_binary_h5py ( load_function , filepath ) with h5py.File ( file_id , * * h5_file_args ) as h5_file : filepath : String , path to the file to save the weights to . _ , temp_fname = tempfile.mkstemp ( filename ) return class_name == 'PosixPath ' or class_name == 'WindowsPath ' assert_array_equal ( data_rec , data ) ) h5dict.close ( ) file_access_property_list = h5py.h5p.create ( h5py.h5p.FILE_ACCESS ) stream.seek ( 0 ) 'driver ' : 'core ' , 'flags ' : h5py.h5f.ACC_RDONLY , import contextlib `` `` '' Calls ` save_function ` on an in memory ` h5py.File ` . # note that filename does not matter here . loaded_model = load_model ( raw_file ) save_model ( model , h5file ) # Make sure the binary data is correct by saving it to a file manually ` bytes ` data ( e.g . ` io.BytesIO ` ) . with contextlib.closing ( h5py.h5f.open ( * * file_id_args ) ) as file_id : def test_load_from_binary_h5py_from_bytes_io ( ) : with temp_filename ( '.h5 ' ) as fname : data_rec = h5file [ 'data ' ] [ : ] save_model ( model , stream ) elif isinstance ( path , six.string_types ) or is_path_instance ( path ) : def save_function ( h5file ) : os.remove ( temp_fname ) metrics= [ metrics.categorical_accuracy ] ) string , path where to save the model , or attr = 1 # and then loading it the usual way . for d_rec , d in zip ( datas_rec , datas ) : return return_value def test_model_loading_from_binary_stream ( ) : any file-like object implementing the method ` write ` that accepts stream.write ( binary_data ) string , path to the saved model save_function : A function that takes a ` h5py.File ` , writes to it and file_like = io.BytesIO ( ) out2 = loaded_model.predict ( x ) model.compile ( loss=losses.MSE , x = Dense ( 2 ) ( inputs ) inputs = Input ( shape= ( 3 , ) ) def test_model_save_load_binary_in_memory ( ) : outputs = Dense ( 3 ) ( x ) The object returned by ` load_function ` . def load_function ( h5file_ ) : The file is subsequently written to the binary ` stream ` . # us to add pathlib2 to the Python 2 dependencies . data2 = np.random.random ( ( 2 , 3 , 5 ) ) any file-like object implementing the method ` read ` that returns with temp_filename ( 'h5 ' ) as fname : elif isinstance ( path , six.string_types ) or _is_path_instance ( path ) : save_model ( model , raw_file ) return h5file_ [ 'data ' ] [ : ] isinstance ( path , six.string_types ) or class_name = type ( path ) .__name__ from .. utils.io_utils import H5Dict `` `` '' if H5Dict.is_supported_type ( filepath ) : h5file_ [ 'subgroup/data2 ' ] = data2 _ , temp_fname = tempfile.mkstemp ( suffix=suffix ) return model , x with H5Dict ( filepath , mode= ' r ' ) as h5dict : # Load the manually-saved binary data , and make sure the model is intact . data1 = np.random.random ( ( 3 , 5 ) ) h5file.flush ( ) # https : //github.com/keras-team/keras/issues/9343 # issuecomment-440903847 save_to_binary_h5py ( save_function , file_like ) x = np.random.random ( ( 1 , 3 ) ) filepath : one of the following : raw_file.write ( binary_data ) it still exists ( so that this is not forgotten ) . loaded_model = load_model ( h5file ) def _is_path_instance ( path ) : isinstance ( path , h5py.Group ) or try : # Load the data the usual way , and make sure the model is intact . with h5py.File ( fname , mode= ' w ' ) as h5file : opened_new_file = True h5file_ [ 'data ' ] = data 'name ' : b'in-memory-h5py ' } # name does not matter with open ( fname , 'rb ' ) as raw_file : opens_file = not isinstance ( filepath , ( dict , h5py.Group ) ) def __enter__ ( self ) : opened_new_file = not isinstance ( filepath , h5py.Group ) data = np.random.random ( ( 3 , 5 ) ) return d1 , d2 , a if not overwrite and os.path.isfile ( filepath ) : else : d2 = h5file_ [ 'subgroup/data2 ' ] [ : ] h5file.flush ( ) # Very important ! Otherwise you get all zeroes below . if not isinstance ( filepath , h5py.Group ) : from numpy.testing import assert_array_equal _serialize_model ( model , h5dict , include_optimizer ) h5file [ 'data ' ] = data # save directly to binary file def test_save_to_binary_h5py_direct_to_file ( ) : file_access_property_list.set_fapl_core ( backing_store=False ) datas_rec = load_from_binary_h5py ( load_function , file_like ) binary_data = stream.read ( ) return_value = save_function ( h5file ) h5dict = H5Dict ( filepath , ' r ' ) from contextlib import contextmanager def is_path_instance ( path ) : if opens_file and os.path.isfile ( filepath ) and not overwrite : h5dict = H5Dict ( filepath , mode= ' w ' ) _is_path_instance ( path ) class_name = type ( path ) .__name__ with open ( fname , 'rb ' ) as f : d1 = h5file_ [ 'data1 ' ] [ : ] with h5py.File ( fname , mode= ' r+ ' ) as h5file : with open ( fname , 'wb ' ) as raw_file : return class_name == 'PosixPath ' or class_name == 'WindowsPath ' # Arguments stream = io.BytesIO ( ) if os.path.exists ( temp_fname ) : with H5Dict ( filepath , mode= ' w ' ) as h5dict : def temp_filename ( filename ) : # Load the data binary , and make sure the model is intact . with h5py.File ( fname , mode= ' r ' ) as h5file : `` `` '' model = Model ( inputs , outputs ) with h5py.File ( fname , ' w ' ) as h5file : if opened_new_file : datas = [ data1 , data2 , attr ] assert_allclose ( out , out2 , atol=1e-05 ) _ , fname = tempfile.mkstemp ( '.h5 ' ) return ( file_like.seek ( 0 ) def load_from_binary_h5py ( load_function , stream ) : data_rec = load_from_binary_h5py ( load_function , f ) from .. utils.io_utils import save_to_binary_h5py f.write ( file_like.read ( ) ) file_id_args = { 'fapl ' : file_access_property_list , with open ( fname , 'wb ' ) as raw_file : def temp_filename ( suffix ) : file_like = io.BytesIO ( ) def test_save_to_binary_h5py_to_bytes_io ( ) : load_function : A function that takes a ` h5py.File ` , reads from it , and def load_function ( h5file ) : 'mode ' : ' r ' } from .. utils.io_utils import load_from_binary_h5py save_to_binary_h5py ( save_function , f ) ` bytes ` data ( e.g . ` io.BytesIO ` ) that represents a valid h5py file image . stream : Any file-like object implementing the method ` read ` that returns # Returns model.train_on_batch ( x , y ) # Save the model to an in-memory-only h5 file . return _deserialize_model ( H5Dict ( h5file ) , custom_objects , compile ) with open ( fname , 'wb ' ) as f : # If file exists and should not be overwritten . a = h5file_ [ 'data1 ' ] .attrs [ 'attr ' ] def __exit__ ( self , exc_type , exc_val , exc_tb ) : loaded_model = load_model ( stream ) optimizer=optimizers.Adam ( ) , finally : model = None returns any object . file_access_property_list.set_file_image ( binary_data ) out = model.predict ( x ) def is_supported_type ( path ) : assert_array_equal ( d_rec , d ) # Arguments h5py.File or h5py.Group object where to save the model `` `` '' Check if ` path ` is of supported type for instantiating a ` H5Dict ` `` '' '' return load_function ( h5_file ) # write as binary stream # us to add pathlib2 to the Python 2 dependencies . def test_functional_model_saving ( ) : string , path to the file to save the model to ` bytes ` data ( e.g . ` io.BytesIO ` ) . data_rec = load_from_binary_h5py ( load_function , file_like ) from keras.utils.io_utils import save_to_binary_h5py ( optionally ) returns any object . with h5py.File ( 'does not matter ' , driver='core ' , def _get_sample_model_and_input ( ) : with h5py.File ( fname , mode= ' r ' ) as h5file : string , path to the saved model , or any file-like object implementing the method ` write ` that accepts # We ca n't use isinstance here because it would require def save_to_binary_h5py ( save_function , stream ) : `` `` '' Calls ` load_function ` on a ` h5py.File ` read from the binary ` stream ` . `` `` '' Context that returns a temporary filename and deletes the file on exit if stream : Any file-like object implementing the method ` write ` that accepts raise ValueError ( 'unexpected type { } for ` filepath ` '.format ( type ( filepath ) ) ) # We ca n't use isinstance here because it would require yield temp_fname import io binary_data = h5file.fid.get_file_image ( ) string , path to the file to save the model to elif hasattr ( filepath , 'write ' ) and callable ( filepath.write ) : _serialize_model ( model , H5Dict ( h5file ) , include_optimizer ) return self save_to_binary_h5py ( save_function , filepath ) self.close ( ) ` bytes ` data ( e.g . ` io.BytesIO ` ) . binary_data = h5file.fid.get_file_image ( ) def test_load_from_binary_h5py_direct_from_file ( ) : def test_save_load_binary_h5py ( ) : # save the model the usual way model = _deserialize_model ( h5dict , custom_objects , compile ) _serialize_model ( model , h5dict , include_optimizer ) model , x = _get_sample_model_and_input ( ) file_like.write ( f.read ( ) ) with h5py.File ( 'in-memory-h5py ' , driver='core ' , backing_store=False ) as h5file : # Implementation based on suggestion solution here : def test_functional_model_saving ( ) : out2 = loaded_model.predict ( x )","['keras/engine/network.py', 'keras/engine/saving.py', 'keras/utils/io_utils.py', 'tests/keras/utils/io_utils_test.py', 'tests/test_model_saving.py']",Save/Load models to binary stream ( # 11708 )
61,ad578c4c19444af9d1f0e0d51a8283eb0db1a264,2019-03-26 11:39:58+09:00,"@ pytest.mark.skipif ( K.backend ( ) == 'cntk ' , reason='Bug in CNTK ' ) return 1 - C.reshape ( result , shape= ( -1 , ) ) result = [ C.classification_error ( predictions [ i ] , _targets [ i ] , topN=k ) result = concatenate ( result , axis=-1 ) result = C.classification_error ( predictions , _targets , topN=k ) for i in range ( predictions.shape [ 0 ] ) ] return 1 - C.reshape ( result , shape= ( ) ) for k in range ( 1 , num_classes + 1 ) : for k in range ( 1 , 2 if K.backend ( ) == 'cntk ' else ( num_classes + 1 ) ) :","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",Revise ` in_top_k ` for CNTK ( # 12546 )
62,b8ec0e6dc9bc2b069fa9cdb86b8a32ef6a10a6aa,2019-03-25 17:38:43+09:00,"_ , min_val = parse_shape_or_val ( shape ) _ , x = parse_shape_or_val ( shape ) np.asarray ( [ -5. , -4. , 0. , 4. , 9 . ] , dtype=np.float32 ) ) def test_clip_supports_tensor_arguments ( self ) : KNP.eval ( KNP.clip ( x , min_val , max_val ) ) ) max_value = K.variable ( [ 5. , 4. , 1. , 4. , 9 . ] ) assert np.allclose ( K.eval ( K.clip ( x , min_value , max_value ) ) , max_val_k = K.variable ( max_val ) min_value = K.variable ( [ -5. , -4. , 0. , 3. , 5 . ] ) x_k = K.variable ( x ) def test_clip_supports_tensor_arguments ( self , shape ) : min_val_k = K.variable ( min_val ) x = K.variable ( [ -10. , -5. , 0. , 5. , 10 . ] ) max_val = min_val + 1 . assert np.allclose ( K.eval ( K.clip ( x_k , min_val_k , max_val_k ) ) ,",['tests/keras/backend/backend_test.py'],Revise ` test_clip_supports_tensor_arguments ` ( # 12545 )
63,8202e627c7d7897728d824c84a6d479ca84a9098,2019-03-25 17:33:56+09:00,"( ( 3 , 2 , 3 ) , ( 1 , 0 , 0 ) , ( 2 , 1 , 3 ) ) , tft = K.constant ( npt ) ( ( 3 , 2 , 3 ) , ( 1 , 1 , 0 ) , ( 1 , 1 , 3 ) ) , raise NotImplementedError if not ( len ( int_shape ( x ) ) == len ( start ) == len ( size ) ) : reason='tensorflow-way slice is ' @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , [ 1 , 1 , 3 ] , [ 2 , 1 , 3 ] expected = KNP.slice ( npt , x_start , x_size ) npt = np.array ( [ [ [ 1 , 1 , 1 ] , [ 2 , 2 , 2 ] ] , [ [ 3 , 3 , 3 ] , [ 4 , 4 , 4 ] ] , start= [ 1 , 0 , 0 , 0 ] , size=size ) ( ( 3 , 2 , 3 ) , ( 1 , 0 , 0 ) , ( 1 , 2 , 3 ) ) , K.slice ( K.variable ( np.random.random ( shape ) ) , ValueError : if the dimension and the size of indices mismatches . x_start = [ 1 , 0 , 0 ] ( ( 2 , 5 ) , ( 0 , 1 ) , ( 2 , 3 ) ) , out._keras_shape = tuple ( size ) return out raise ValueError ( 'The dimension and the size of indices should match . ' ) with pytest.raises ( ValueError ) : [ 1 , 2 , 3 ] , # Raises def test_slice ( self , x_size ) : start=start , size=size ) def test_slice ( self , shape , start , size ) : assert np.allclose ( test_input , expected ) 'only supported in tensorflow . ' ) [ [ 5 , 5 , 5 ] , [ 6 , 6 , 6 ] ] ] ) test_input = K.eval ( K.slice ( tft , x_start , x_size ) ) @ pytest.mark.parametrize ( 'x_size ' , [ out = x [ tuple ( [ py_slice ( i , i + j ) for ( i , j ) in zip ( start , size ) ] ) ] check_single_tensor_operation ( 'slice ' , shape , WITH_NP , ( ( 2 , 5 ) , ( 1 , 0 ) , ( 1 , 4 ) ) ,","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py']",Add ` slice ` for Theano and CNTK ( # 12539 )
64,960e7b5367a072fe10b8c2c50d2ce68607057f5a,2019-03-25 14:53:01+09:00,"if K.backend ( ) ! = 'theano ' : with pytest.raises ( ValueError ) : K.separable_conv2d ( dummy_x_2d , dummy_w_2d , dummy_w1x1_2d , K.separable_conv2d ( dummy_x_2d , dummy_w_2d , dummy_w1x1_2d , with pytest.raises ( ValueError ) : data_format='channels_middle ' ) data_format='channels_middle ' )",['tests/keras/backend/backend_test.py'],Enable theano exception tests ( # 12543 )
65,5d844095f6df0db8bd596247a98f429c594855f2,2019-03-25 14:52:22+09:00,"tensor_list = [ np.random.randn ( 5 , 4 , 6 , 10 ) for _ in range ( 5 ) ] ( ( 5 , 4 , 6 ) , ( 5 , 3 , 6 ) , 1 ) , out = k.eval ( k.stack ( tensor_list_var , axis=stack_axis ) ) tensor_list_var = [ k.variable ( tensor ) for tensor in tensor_list ] ( 5 , 4 , 6 , 10 ) , WITH_NP , check_two_tensor_operation ( 'concatenate ' , ( 4 , 3 ) , ( 4 , 2 ) , WITH_NP , assert_list_pairwise ( results ) ] ) ( ( 5 , 2 ) , ( 7 , 2 ) , 0 ) , axis=-1 , concat_args=True ) ( ( 5 , 4 , 6 , 10 ) , ( 5 , 4 , 6 , 2 ) , 3 ) , check_two_tensor_operation ( 'stack ' , ( 5 , 4 , 6 , 10 ) , # In stack , each array must have the same shape . axis=stack_axis , concat_args=True ) def test_concat_operations ( self , shape , shape2 , axis ) : check_two_tensor_operation ( 'concatenate ' , shape , shape2 , WITH_NP , ( ( 5 , 4 , 6 , 3 ) , ( 5 , 4 , 6 , 2 ) , -1 ) , if WITH_NP [ 0 ] == KC : def test_stack ( self ) : axis=axis , concat_args=True ) results = [ ] results.append ( out ) stack_axis = 3 else : for k in WITH_NP : check_two_tensor_operation ( 'stack ' , shape , shape , WITH_NP ,",['tests/keras/backend/backend_test.py'],Revise stack and concatenate backend tests ( # 12542 )
66,78e1f57c484da15466a34ed543a1cc4709617a2f,2019-03-25 13:16:05+09:00,"axes [ i ] = x.ndim - 1 check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , WITH_NP , axes= ( 0 , -1 ) ) for i in range ( len ( axes ) ) : axes = list ( axes ) if K.backend ( ) ! = 'cntk ' : check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , WITH_NP , axes= ( 1 , 2 ) ) if axes [ i ] == -1 : check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , WITH_NP , axes= ( 1 , 2 ) ) elif isinstance ( axes , tuple ) :","['keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py']",Revise ` reverse ` for Theano and Enable the tests for CNTK ( # 12540 )
67,ce67a90decbe2f26f9ac9597d518f07b01d0c293,2019-03-25 12:55:33+09:00,"WITH_NP , cntk_two_dynamicity=True ) # toy label matrix ( 2 samples , 3 classes ) check_two_tensor_operation ( 'categorical_crossentropy ' , label , ( 4 , 2 ) , yval = np.asarray ( [ [ 0.46221867 , 0.53778133 ] , [ 0.51228984 , 0.48771016 ] , # cross_entropy call require the label is a valid probability distribution , check_two_tensor_operation ( 'categorical_crossentropy ' , yval , xval , WITH_NP , # toy label matrix ( 4 samples , 2 classes ) check_two_tensor_operation ( 'binary_crossentropy ' , label , ( 4 , 2 ) , # so create a separate test case for valid label input label = np.array ( [ [ .4 , .6 ] , [ .3 , .7 ] , [ .1 , .9 ] , [ .2 , .8 ] ] , dtype=np.float32 ) cntk_two_dynamicity=True , from_logits=True ) WITH_NP , cntk_two_dynamicity=True , # otherwise it is garbage in garbage out ... WITH_NP , from_logits=False ) WITH_NP , from_logits=True ) check_two_tensor_operation ( 'binary_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , [ 0.64916514 , 0.35083486 ] , [ 0.47028078 , 0.52971922 ] ] , # due to the algo difference , we ca n't guarantee CNTK has the same result WITH_NP , from_logits=True ) check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , from_logits=True ) if K.backend ( ) ! = 'cntk ' : def test_crossentropy ( self ) : # on the garbage input . label = np.array ( [ [ .4 , .1 , .5 ] , [ .2 , .6 , .2 ] ] , dtype=np.float32 ) dtype=np.float32 ) xval = np.asarray ( [ [ 0.26157712 , 0.0432167 ] , [ -0.43380741 , 0.30559841 ] , check_two_tensor_operation ( 'categorical_crossentropy ' , label , ( 2 , 3 ) , WITH_NP , from_logits=True ) [ 0.20225059 , -0.38956559 ] , [ -0.13805378 , 0.08506755 ] ] , check_two_tensor_operation ( 'binary_crossentropy ' , label , ( 4 , 2 ) , WITH_NP )",['tests/keras/backend/backend_test.py'],Revise backend tests for binary and categorical crossentropy ( # 12541 )
68,618edbe595abcdca96f508b14d9198d68ac767a2,2019-03-25 12:53:53+09:00,"_x = x [ rep : ( rep + 1 ) ] axis=axis ) raise NotImplementedError shape = x.shape sliced_shape = list ( shape ) elif axis == 1 : repeat_elements ( _x , rep=shape [ axis ] - 1 - rep , axis=axis ) ] , out = C.element_times ( out , y ) return out sliced_shape [ axis ] = rep + 1 out = x _x = x [ : , rep : ( rep + 1 ) ] for rep in range ( shape [ axis ] - 1 ) : 'cumprod yet ' ) if axis == 0 : _x = x [ : , : , rep : ( rep + 1 ) ] @ pytest.mark.skipif ( K.backend ( ) == 'cntk ' , reason='cntk does not support ' elif axis == 2 : y = concatenate ( [ ones ( sliced_shape , dtype=x.dtype ) ,","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",Add ` cumprod ` for CNTK ( # 12536 )
69,f6a30b005535fb45a5b4decf084af437ac7fc7f2,2019-03-25 11:14:48+09:00,"[ 0. , 1. , 0 . ] ] , dtype=float32 ) if isinstance ( size , ( list , tuple ) ) : return np.eye ( n , m , dtype=dtype ) return np.eye ( size , dtype=dtype ) return variable ( np.eye ( n , m ) , dtype , name ) check_single_tensor_operation ( 'eye ' , ( 3 , 4 ) , WITH_NP , shape_or_val=False ) array ( [ [ 1. , 0. , 0 . ] , return variable ( np.eye ( size ) , dtype , name ) return variable ( tf.eye ( size , dtype=tf_dtype ) , dtype , name ) def test_creation_operations ( self ) : size : Integer , number of rows/columns . check_single_tensor_operation ( 'ones_like ' , ( 3 , 5 , 10 , 8 ) , check_single_tensor_operation ( 'ones_like ' , ( 3 , 5 , 10 , 8 ) , WITH_NP ) n , m = size def test_zeros ( self ) : check_single_tensor_operation ( 'zeros_like ' , ( 3 , 5 , 10 , 8 ) , WITH_NP ) WITH_NP , shape_or_val=True ) def test_zeros_like ( self ) : def test_eye ( self ) : def test_ones_like ( self ) : > > > K.eval ( kvar ) > > > kvar = K.eye ( 3 ) check_single_tensor_operation ( 'eye ' , ( 3 , 2 ) , WITH_NP , shape_or_val=False ) def test_ones ( self ) : else : size : Tuple , number of rows and columns . If Integer , number of rows . > > > K.eval ( K.eye ( 3 ) ) check_single_tensor_operation ( 'zeros_like ' , ( 3 , 5 , 10 , 8 ) , n , m = size , size > > > K.eval ( K.eye ( ( 2 , 3 ) ) ) return variable ( tf.eye ( n , m , dtype=tf_dtype ) , dtype , name )","['keras/backend/cntk_backend.py', 'keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py']",Make ` eye ` consistent over all the backends ( # 12534 )
70,adaed1f84a0d980a660eb7d5ff271408374e5bef,2019-03-24 19:07:32+09:00,"Returns : Args : # Arguments : validation_freq : Integer or list . If an integer , specifies how many training # Arguments # Arguments it is neither an Integer nor a Sequence . epochs to run before a new validation run is performed . If a list , epochs to run before a new validation run is performed . If a list , ValueError : if ` validation_freq ` is an Integer and less than 1 , or if Raises : ValueError : if ` validation_freq ` is an Integer and less than 1 , or if # Raises # Example Raises : # Raises Example : Bool , True if validation should be run . validation_freq : Integer or list . If an integer , specifies how many training specifies the epochs on which to run validation . specifies the epochs on which to run validation . Arguments : epoch : Integer , the number of the training epoch just completed . # Returns epoch : Integer , the number of the training epoch just completed . Bool , True if validation should be run . it is neither an Integer nor a Sequence . # Returns :","['examples/lstm_stateful.py', 'keras/activations.py', 'keras/engine/network.py', 'keras/engine/training_utils.py', 'keras/utils/test_utils.py', 'keras/utils/vis_utils.py']",Improve docs for consistency as Markdown format ( # 12533 )
71,9bd69784e31f17ee3501167c267dad5c64f25de8,2019-03-20 14:30:38+09:00,"symmetric padding values for height and width : interpreted as two different interpreted as three different symmetric padding values for depth , height , and width :",['keras/layers/convolutional.py'],Corrected 2D relicts in 3D function comments ( # 12406 )
72,f0eb8d538c82798944346b4b2df917a06bf5e9d4,2019-03-17 23:18:57-07:00,"A Capsule Network Layer implementation in Keras # define our own softmax function instead of K.softmax return ex / K.sum ( ex , axis=axis , keepdims=True ) This change can improve the feature representation of the capsule . routings : An integer , the number of routings . and 79 % after 15 epochs , and 83 % after 30 epochs . x = layers.Conv2D ( 64 , ( 3 , 3 ) , activation='relu ' ) ( input_image ) from __future__ import print_function # The output of final model is the lengths of 10 capsules , which have 16 dimensions . from keras import activations One is similar to dense layer ( for the fixed-shape input ) , y_true : tensor of true targets . This change can improve the feature representation of Capsule . # and be zoomed out while original norm is greater than 0.5 . Tensor with one scalar loss entry per sample . return ( None , self.num_capsule , self.dim_capsule ) # This will do preprocessing and realtime data augmentation : and 79 % after 15 epochs , and overfitting after 20 epochs The Capsule implementation is from https : //github.com/bojone/Capsule/ def margin_loss ( y_true , y_pred ) : # Train a simple CNN-Capsule Network on the CIFAR10 small images dataset . `` `` '' Capsule Network There are two vesions of Capsule . # if 1 , the norm of vector will be zoomed out . x = layers.Conv2D ( 128 , ( 3 , 3 ) , activation='relu ' ) ( x ) `` `` '' Margin loss # A common Conv2D model def softmax ( x , axis=-1 ) : This example trains a simple CNN-Capsule Network on the CIFAR10 data set . In my test , highest validation accuracy is 83.79 % after 50 epochs . def call ( self , inputs ) : class Capsule ( layers.Layer ) : x = layers.AveragePooling2D ( ( 2 , 2 ) ) ( x ) to realize a standard routing . x = Conv2D ( 128 , ( 3 , 3 ) , activation='relu ' ) ( x ) output = layers.Lambda ( lambda z : K.sqrt ( K.sum ( K.square ( z ) , 2 ) ) ) ( capsule ) ex = K.exp ( x - K.max ( x , axis=axis , keepdims=True ) ) x = layers.Reshape ( ( -1 , 128 ) ) ( x ) so the problem becomes a 10 two-classification problem . x = Conv2D ( 64 , ( 3 , 3 ) , activation='relu ' ) ( x ) x = Conv2D ( 64 , ( 3 , 3 ) , activation='relu ' ) ( input_image ) `` `` '' from __future__ import print_function c = K.softmax ( b , 1 ) # The length of the output vector of the capsule expresses the probability of # if 0.5 , the norm will be zoomed in while original norm is less than 0.5 input_image = Input ( shape= ( None , None , 3 ) ) activation : A string , the activation function to be applied . # we use a margin loss y_pred : tensor of predicted targets . One is like dense layer ( for the fixed-shape input ) , # we can compare the performance with or without data augmentation print ( self.routings ) There are two versions of Capsule Networks . ( for inputs of varied length ) . x = layers.Conv2D ( 64 , ( 3 , 3 ) , activation='relu ' ) ( x ) # existence of the entity , so the problem becomes a 10 two-classification problem . def call ( self , inputs , * * kwargs ) : to get standard routing . # we use 0.5 in stead of 1 in hinton 's paper . Capsule Paper : https : //arxiv.org/abs/1710.09829 input_image = layers.Input ( shape= ( None , None , 3 ) ) axis : Integer axis along which the squashing function is to be applied . # because K.softmax can not specify axis . # A simple Conv2D model def margin_loss ( y_true , y_pred ) : # Now , we reshape it to ( batch_size , input_num_capsule , input_dim_capsule ) then connect a Capsule layer . `` `` '' now we reshape it as ( batch_size , input_num_capsule , input_dim_capsule ) and overfitting after 20 epochs from keras import activations from keras import layers `` `` '' # then connect a capsule layer . The highest achieved validation accuracy is 83.79 % after 50 epochs . # the squashing function . Capsule Implement is from https : //github.com/bojone/Capsule/ `` `` '' A Capsule Implement with Pure Keras # Returns class Capsule ( Layer ) : x = Reshape ( ( -1 , 128 ) ) ( x ) The nonlinear activation function used in Capsule Network share_weights : A boolean , sets weight sharing between layers . the output of final model is the lengths of 10 Capsule , whose dim=16 . from keras.layers import Layer the length of Capsule is the proba , `` `` '' The Squashing Function . dim_capsule : An integer , the dimensions of the capsule . # Arguments The paper `` Dynamic Routing Between Capsules '' : https : //arxiv.org/abs/1710.09829 and the other is similar to time distributed dense layer x = AveragePooling2D ( ( 2 , 2 ) ) ( x ) This is a fast implementation that takes just 20s/epoch on a GTX 1070 GPU . # Margin loss is used # Compare the performance with and without data augmentation It gets to 75 % validation accuracy in 10 epochs , 79 % after 15 epochs , It gets to 75 % validation accuracy in 10 epochs , and 83 % after 30 epochs . Tensor with scaled value of the input tensor return None , self.num_capsule , self.dim_capsule num_capsule : An integer , the number of capsules . c = softmax ( b , 1 ) output = Lambda ( lambda z : K.sqrt ( K.sum ( K.square ( z ) , 2 ) ) ) ( capsule ) # define the margin loss like hinge loss This is a fast Implement , just 20s/epoch with a gtx 1070 gpu . and the other is like timedistributed dense ( for various length input ) . from keras.layers import * # This will do preprocessing and real-time data augmentation : x : Input Tensor .",['examples/cifar10_cnn_capsule.py'],Improve cifar10_cnn_capsule.py ( # 12299 )
73,28ee86a51fa628f65a62d077e8dd9b7099b1749b,2019-03-17 15:19:00-07:00,"Copyright ( c ) 2017 - 2019 , Microsoft , Inc . Copyright ( c ) 2015 - 2018 , Google , Inc . Copyright ( c ) 2015 - 2019 , François Chollet . Copyright ( c ) 2015 - 2019 , Google , Inc . Copyright ( c ) 2015 - 2018 , the respective contributors . Copyright ( c ) 2017 - 2018 , Microsoft , Inc . Copyright ( c ) 2015 - 2019 , the respective contributors . Copyright ( c ) 2015 - 2018 , François Chollet .",['LICENSE'],Updated the copyright years ( # 12506 )
74,a374c93092341f24027f60ffe32d56d2bff29b83,2019-03-17 01:51:51-07:00,"# Arguments The hyperbolic activation : # Returns Exponential activation : ` exp ( x ) ` . ` tanh ( x ) = ( exp ( x ) - exp ( -x ) ) / ( exp ( x ) + exp ( -x ) ) ` x : Input tensor . Input tensor , unchanged . # Arguments : The sigmoid activation : ` 1 / ( 1 + exp ( -x ) ) ` .",['keras/activations.py'],"Update docs for some activations , make the style uniform ( # 12499 )"
75,f0a24a009a392f71d9dcf4fcffe2ecc9e9f36c86,2019-03-17 01:47:22-07:00,"last convolutional block , and thus last convolutional layer , and thus last convolutional layer . last convolutional block .",['docs/templates/applications.md'],"Revise docs for ` include_top=False , pooling=None ` ( # 12498 )"
76,ee3d6a6465df01d27cbf3bb3c5bace05298756e3,2019-03-17 01:40:12-07:00,"except AttributeError : # getargspec ( ) is deprecated since Python 3.0 # Ignore warnings which are verbose and unrelated to Keras ignore : np.asscalar : DeprecationWarning config = yaml.load ( yaml_string ) config = yaml.load ( yaml_string , Loader=yaml.FullLoader ) getargspec = inspect.getargspec argnames = inspect.getargspec ( f ) [ 0 ] filterwarnings = argnames = getargspec ( f ) [ 0 ] getargspec = inspect.getfullargspec try :","['keras/engine/saving.py', 'pytest.ini']",Suppress several warnings ( # 12496 )
77,d7c0e952f7a3d0dbf7d9489a8b5c959e04bd9379,2019-03-17 01:38:44-07:00,"if len ( shape ) > 2 : check_single_tensor_operation ( 'logsumexp ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) np.log ( np.sum ( np.exp ( x_np ) , axis=axis , keepdims=keepdims ) ) , ( np.array ( [ [ 1.1 , 1.2 , 1.3 ] , [ 0.9 , 0.7 , 1.4 ] ] ) , -1 , False ) , ( np.array ( [ [ 1.1 , 0.8 , 0.9 ] ] ) , 1 , True ) , check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=0 ) @ pytest.mark.parametrize ( 'x_np , axis , keepdims ' , [ ( np.array ( [ 1.1 , 0.8 , 0.9 ] ) , 0 , False ) , return sp.misc.logsumexp ( x , axis=axis , keepdims=keepdims ) ' '' check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=1 , check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=-1 ) keepdims=True ) check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=1 ) rtol=1e-5 ) check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis= [ 1 , -1 ] , keepdims=True ) check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis= [ 1 , -1 ] ) assert_allclose ( K.eval ( K.logsumexp ( x , axis=axis , keepdims=keepdims ) ) , ( np.array ( [ [ 1.1 ] , [ 1.2 ] ] ) , -1 , True ) , ] ) check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=-1 , ( np.array ( [ [ 1.1 , 0.8 , 0.9 ] ] ) , -1 , False ) , ( np.array ( [ [ 1.1 ] , [ 1.2 ] ] ) , -1 , False ) , Check if K.logsumexp works properly for values close to one . ( np.array ( [ [ 1.1 , 0.8 , 0.9 ] ] ) , 0 , False ) , def test_logsumexp ( self , shape ) : WITH_NP , axis=1 , keepdims=True ) check_single_tensor_operation ( 'logsumexp ' , shape , WITH_NP , axis=None ) ( np.array ( [ [ 1.1 ] , [ 1.2 ] ] ) , 1 , False ) , check_single_tensor_operation ( 'logsumexp ' , ( 4 , 2 ) , WITH_NP ) def test_logsumexp ( self , x_np , axis , keepdims ) : ( np.array ( [ [ 1.1 ] , [ 1.2 ] ] ) , 0 , False ) , if len ( shape ) > 1 : ( np.array ( [ [ 1.1 , 0.8 , 0.9 ] ] ) , 1 , False ) , ( np.array ( [ [ 1.1 , 1.2 , 1.3 ] , [ 0.9 , 0.7 , 1.4 ] ] ) , None , False ) , ( np.array ( [ [ 1.1 , 1.2 , 1.3 ] , [ 0.9 , 0.7 , 1.4 ] ] ) , 1 , False ) , return sp.special.logsumexp ( x , axis=axis , keepdims=keepdims ) ( np.array ( [ [ 1.1 , 1.2 , 1.3 ] , [ 0.9 , 0.7 , 1.4 ] ] ) , 0 , False ) , check_single_tensor_operation ( 'logsumexp ' , ( 4 , 2 ) , x = K.variable ( x_np )","['keras/backend/numpy_backend.py', 'tests/keras/backend/backend_test.py']",Revise ` logsumexp ` in np backend and reduce those redundant tests ( # 12500 )
78,dc83851aca230648c4f75ac1a0d4b9971b2d6aa0,2019-03-16 10:47:06-07:00,"if len ( n ) < len ( shape ) : return y else : for x in shape : if len ( n ) < len ( shape ) : # Padding the axis > > > kvar_tile = K.tile ( K.eye ( 2 ) , ( 2 , 3 ) ) if n.ndim == 0 : n_size = n._keras_shape [ 0 ] if shape is None : `` ` python check_single_tensor_operation ( 'tile ' , ( 2 , 5 ) , WITH_NP , n= [ 5 , 2 ] ) if _is_explicit_shape ( n ) : output_shape += ( None , ) # n is a scalar check_single_tensor_operation ( 'tile ' , ( 3 , 4 , 5 ) , WITH_NP , n=2 ) [ 1. , 0. , 1. , 0. , 1. , 0 . ] , def _is_explicit_shape ( shape ) : raise NotImplementedError > > > kvar = K.variable ( np.random.random ( ( 2 , 3 ) ) ) elif len ( n ) ! = len ( shape ) : shape = ( 3 , 4 ) if not isinstance ( x , int ) : if len ( n ) ! = len ( shape ) : n = tuple ( [ 1 for _ in range ( len ( shape ) - len ( n ) ) ] ) + n > > > from keras import backend as K if x._keras_shape [ -1 ] is None : output_shape += ( x._keras_shape [ -1 ] * n , ) check_single_tensor_operation ( 'tile ' , ( 3 , 4 , 5 ) , WITH_NP , n= ( 3 , 1 , 2 ) ) arr = np.arange ( np.prod ( shape ) ) .reshape ( shape ) # Padding the axis y._keras_shape = tuple ( [ None if a is None else a * b output_shape += ( None , ) > > > K.eval ( kvar_tile ) if hasattr ( x , '_keras_shape ' ) : # Example [ 0. , 1. , 0. , 1. , 0. , 1 . ] ] , dtype=float32 ) if x is not None : if isinstance ( n , int ) : n = tuple ( n ) n = [ n ] output_shape = x._keras_shape [ : -len ( n ) ] check_single_tensor_operation ( 'tile ' , arr , WITH_NP , n= [ 2 , 1 ] ) shape = int_shape ( x ) return True output_shape += ( i * j , ) check_single_tensor_operation ( 'tile ' , ( 3 , 4 , 5 ) , WITH_NP , n= ( 1 , 2 ) ) elif isinstance ( n , list ) : check_single_tensor_operation ( 'tile ' , ( 3 , 4 ) , WITH_NP , n= ( 2 , 1 ) ) output_shape = x._keras_shape [ : -1 ] + ( None , ) return False n = ( n , ) if i is None : output_shape = x._keras_shape [ : -1 ] elif hasattr ( n , '_keras_shape ' ) : output_shape = x._keras_shape [ : -n_size ] + ( None , ) * n_size # symbolic n else : output_shape = ( None , ) * x.ndim array ( [ [ 1. , 0. , 1. , 0. , 1. , 0 . ] , y._keras_shape = output_shape elif isinstance ( n , int ) : # n is a vector for ( a , b ) in zip ( shape , n ) ] ) elif len ( n ) < len ( shape ) : # Padding the axis `` ` else : return False [ 0. , 1. , 0. , 1. , 0. , 1 . ] , { { np_implementation } } for i , j in zip ( x._keras_shape , n ) : if hasattr ( shape , '__iter__ ' ) : check_single_tensor_operation ( 'tile ' , ( 3 , 4 ) , WITH_NP , n=2 )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py']",Make ` tile ` consistent over all the backends ( # 12494 )
79,c24d16af155e20976bdf61e468ba760408e676ff,2019-03-15 23:09:50+05:18,"for b in [ KTH , KTF ] ] def in_top_k ( predictions , targets , k ) : for k in range ( num_classes + 1 ) : return np.any ( targets == top_k , axis=-1 ) for k in range ( 1 , num_classes + 1 ) : top_k = np.argsort ( -predictions ) [ : , : k ] for b in WITH_NP ] targets = targets.reshape ( -1 , 1 )","['keras/backend/numpy_backend.py', 'tests/keras/backend/backend_test.py']",Fix in_top_k tests to include CNTK [ Test fails ] ( # 12336 )
80,b4e01aee7f4a9b815542c77c3d537e7ba66dd8c9,2019-03-15 11:03:59+09:00,"U = C.constant ( np.triu ( np.ones ( ( dim , dim ) ) ) .astype ( x.dtype ) ) def test_cumsum ( self ) : def test_cumprod ( self ) : if axis ! = -1 : def test_cumsum_cumprod ( self ) : x = C.swapaxes ( x , -1 , axis ) raise NotImplementedError out = C.times ( x , U ) out = C.swapaxes ( out , -1 , axis ) dim = x.shape [ axis ] 'cumprod yet ' ) return out","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",K.cumsum for CNTK backend ( # 12306 )
81,029461c7d1fe7a6927a6f9d0fa13dbf49543b413,2019-03-14 11:10:54-07:00,"if ( isinstance ( x , C.variables.Parameter ) or result = [ ] C.variables.Constant ) : ( C.variables.Parameter , C.variables.Constant ) ) : else : result.append ( x.value ) result.append ( eval ( x ) ) result = [ get_value ( x ) for x in xs ] isinstance ( x , C.variables.Constant ) ) : for x in xs : C.variables.Parameter ) or isinstance ( x ,",['keras/backend/cntk_backend.py'],Reuse get_value code inside batch_get_value ( # 12477 )
82,408a344e50ef809812febc6e8dcabd34cfc11c60,2019-03-12 16:23:01-07:00,gpus = len ( [ x for x in available_devices if 'gpu ' in x ] ) return K.tensorflow_backend._get_available_gpus ( ) return [ x.name for x in K.get_session ( ) .list_devices ( ) ] gpus = len ( available_devices ),['keras/utils/multi_gpu_utils.py'],TF-2 : Remove get_session ( ) call in multi_gpu_utils.py ( # 12465 )
83,c8b0e33c3edf7e1f70b2fbf60a250909c917d55e,2019-03-12 16:17:39-07:00,"ms = [ K.zeros ( K.int_shape ( p ) , ms = [ K.zeros ( shape , name='m_ ' + str ( i ) ) from keras.models import Sequential , Model , load_model for ( i , shape ) in enumerate ( shapes ) ] vhats = [ K.zeros ( K.int_shape ( p ) , name='accumulator_ ' + str ( i ) ) vs = [ K.zeros ( K.int_shape ( p ) , us = [ K.zeros ( shape , name='u_ ' + str ( i ) ) for ( i , p ) in enumerate ( params ) ] assert_allclose ( w1 , w2 ) return unique_names counts [ name ] = 1 # Test saving . counts = { } for ( i , p ) in enumerate ( params ) ] name = name + ' _ ' + str ( counts [ name ] ) name='vhat_ ' + str ( i ) ) moments = [ K.zeros ( shape ) for shape in shapes ] unique_names = [ ] moments = [ K.zeros ( shape , name='moment_ ' + str ( i ) ) weight_names = _uniquify ( weight_names ) names in TF 2 . This method `` uniquifies '' a given list accumulators = [ K.zeros ( K.int_shape ( p ) , Custom layers and optimizers written by users delta_accumulators = [ K.zeros ( shape , name='delta_accumulator_ ' + str ( i ) ) weight_names = _uniquify ( weight_names ) vs = [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in params ] vhats = [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in params ] model.compile ( loss='mse ' , optimizer=optimizer ) vhats = [ K.zeros ( 1 ) for _ in params ] def _uniquify ( names ) : else : `` `` '' delta_accumulators = [ K.zeros ( shape ) for shape in shapes ] vhats = [ K.zeros ( 1 , name='vhat_ ' + str ( i ) ) for i in range ( len ( params ) ) ] for ( i , p ) in enumerate ( params ) ] name='v_ ' + str ( i ) ) for ( i , shape ) in enumerate ( shapes ) ] dtype=K.dtype ( p ) , ms = [ K.zeros ( shape ) for shape in shapes ] vs = [ K.zeros ( shape ) for shape in shapes ] accumulators = [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in params ] ms = [ K.zeros ( K.int_shape ( p ) , dtype=K.dtype ( p ) ) for p in params ] _ , fname = tempfile.mkstemp ( '.h5 ' ) name='m_ ' + str ( i ) ) for w1 , w2 in zip ( model.get_weights ( ) , model2.get_weights ( ) ) : model2 = load_model ( fname ) dtype=K.dtype ( p ) , for name in names : for TF 1.x might produce weights with same variable import tempfile model.fit ( np.zeros ( ( 1 , 1 ) ) , np.zeros ( ( 1 , 1 ) ) ) model.add ( Dense ( 1 , input_dim=1 ) ) accumulators = [ K.zeros ( shape , name='accumulator_ ' + str ( i ) ) counts [ name ] += 1 of names . e.g : [ ' a ' , ' b ' , ' b ' , ' c ' ] - > [ ' a ' , ' b ' , 'b_2 ' , ' c ' ] accumulators = [ K.zeros ( shape ) for shape in shapes ] dtype=K.dtype ( p ) , from keras.models import Sequential , Model if name in counts : vs = [ K.zeros ( shape , name='v_ ' + str ( i ) ) us = [ K.zeros ( shape ) for shape in shapes ] model.save ( fname ) unique_names.append ( name ) model = Sequential ( ) for ( i , shape ) in enumerate ( shapes ) ]","['keras/engine/saving.py', 'keras/optimizers.py', 'tests/keras/optimizers_test.py']",Tf 2 : fix optimizer weights naming collision issue ( # 12466 )
84,91765e700b1d5e465fcb56bd2c2f56384bc2772d,2019-03-12 11:24:16-07:00,"beta_2 : float , 0 < beta < 1 . Generally close to 1 . beta_1 : floats , 0 < beta < 1 . Generally close to 1 . beta_1 : float , 0 < beta < 1 . Generally close to 1 . beta_2 : floats , 0 < beta < 1 . Generally close to 1 . Nadam is RMSprop with Nesterov momentum . Nadam is Adam RMSprop with Nesterov momentum . schedule_decay : floats , 0 < schedule_decay < 1 . schedule_decay : float , 0 < schedule_decay < 1 .",['keras/optimizers.py'],Fix doc ( # 12459 )
85,14625e57af7fe85eae501582ce1da135fc04c8e8,2019-03-07 19:28:41+01:00,Auxiliary Classifier GAN : examples/mnist_acgan.md Train an Auxiliary Classifier Generative Adversarial Network ( ACGAN ) on the # Example script to generate text from Nietzsche 's writings . Consult https : //github.com/lukedeo/keras-acgan for more information and Consult [ Auxiliary Classifier Generative Adversarial Networks in Keras : -- -- -- -- -- -- -- -- -- | : -- -- -- -- | -- -- -- -- -- -- : LSTM for text generation : examples/lstm_text_generation.md example output # Train an Auxiliary Classifier GAN ( ACGAN ) on the MNIST dataset . MNIST dataset . See https : //arxiv.org/abs/1610.09585 for more details . ] ( https : //github.com/lukedeo/keras-acgan ) for more information and example output . ' '' [ More details on Auxiliary Classifier GANs . ] ( https : //arxiv.org/abs/1610.09585 ) ' '' Example script to generate text from Nietzsche 's writings .,"['docs/mkdocs.yml', 'examples/lstm_text_generation.py', 'examples/mnist_acgan.py']",Added Markdown formatting to examples/mnist_acgan.py ( # 12414 )
86,de36e0426d4f01216653979af4beb0481a1b2735,2019-03-01 14:30:31-08:00,"sparse_labels = tf.cast ( input_length = tf.to_int32 ( input_length ) ctc_label_dense_to_sparse ( y_true , label_length ) , tf.int32 ) indices = tf.cast ( indices , tf.int64 ) indices = tf.to_int64 ( indices ) input_length = tf.cast ( tf.squeeze ( input_length , axis=-1 ) , tf.int32 ) label_shape = tf.cast ( label_shape , tf.int64 ) input_length = tf.to_int32 ( tf.squeeze ( input_length , axis=-1 ) ) label_length = tf.cast ( tf.squeeze ( label_length , axis=-1 ) , tf.int32 ) label_shape = tf.to_int64 ( label_shape ) label_length = tf.to_int32 ( tf.squeeze ( label_length , axis=-1 ) ) sparse_labels = tf.to_int32 ( ctc_label_dense_to_sparse ( y_true , label_length ) ) input_length = tf.cast ( input_length , tf.int32 )",['keras/backend/tensorflow_backend.py'],update deprecated tensorflow casting ( # 12367 )
87,2944988be3ce0398c1ebbcce3f2b57a6d8be05df,2019-02-27 11:10:39-08:00,"reason='This test is for tensorflow parallelism . ' ) cfg = K.get_session ( ) ._config for threads in [ 0 , 1 , 4 ] : inter_op_parallelism_threads=num_thread , def test_tensorflow_session_parallelism_settings ( self , monkeypatch ) : K.clear_session ( ) assert cfg.intra_op_parallelism_threads == threads monkeypatch.setenv ( 'OMP_NUM_THREADS ' , str ( threads ) ) assert cfg.inter_op_parallelism_threads == threads","['keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",also set intra_op_parallelism_threads ( # 12254 )
88,d48e97079914d897e82ddcb1a45261ce4415b8ea,2019-02-27 00:34:31+01:00,"of the data , with the window length = `` lahead '' . ' '' Example script showing how to use a stateful LSTM model This memory length is controlled by the ` lahead ` variable ( more details below ) . https : //keras.io/layers/recurrent/ # lstm ` lahead ` : the input sequence length that the LSTM input_len : the length of the generated input sequence and how its stateless counterpart performs . When lahead > = tsteps , both the stateful and stateless LSTM converge . with ` window_shape ` [ being a single number . ] ( and the output is a moving average of the input with window length = `` tsteps '' . A larger ` tsteps ` value means that the LSTM will need more memory When lahead > 1 , the model input is preprocessed to a `` rolling window view '' # How to use a stateful LSTM model , stateful vs stateless LSTM performance comparison http : //scikit-image.org/docs/0.10.x/api/skimage.util.html # view-as-windows ) this capability , and hence is limited by its ` lahead ` parameter , When ` lahead < tsteps ` , only the stateful LSTM converges because its ' '' lahead : the input sequence length that the LSTM ` batch_size ` , ` epochs ` : same parameters as in the ` model.fit ( ... ) ` and the output is a moving average of the input with window length = ` tsteps ` . batch_size , epochs : same parameters as in the model.fit ( ... ) function function This is similar to sklearn 's `` view_as_windows '' random sequence of length = `` input_len '' , [ More documentation about the Keras LSTM model ] ( /layers/recurrent/ # lstm ) of the data , with the window length = ` lahead ` . Both ` input_len ` and ` tsteps ` are defined in the `` editable parameters '' random sequence of length = ` input_len ` , ` input_len ` : the length of the generated input sequence When lahead < tsteps , only the stateful LSTM converges because its this capability , and hence is limited by its `` lahead '' parameter , More documentation about the Keras LSTM model can be found at When ` lahead > = tsteps ` , both the stateful and stateless LSTM converge . This memory length is controlled by the `` lahead '' variable ( more details below ) . This is similar to sklearn 's ` view_as_windows ` Ref : http : //scikit-image.org/docs/0.10.x/api/skimage.util.html # view-as-windows When ` lahead > 1 ` , the model input is preprocessed to a `` rolling window view '' section . Both `` input_len '' and `` tsteps '' are defined in the `` editable parameters '' section . with `` window_shape '' being a single number A larger `` tsteps '' value means that the LSTM will need more memory",['examples/lstm_stateful.py'],Added MarkDown formatting to examples/lstm_stateful.py ( # 12323 )
89,f69946854775d5c755983d9f585bd20cc97acae8,2019-02-25 09:51:55-08:00,"return x * 0 name = name or `` return C.ones_like ( x , name ) return C.zeros_like ( x , name ) return zeros_like ( x ) + 1",['keras/backend/cntk_backend.py'],Use native CNTK functions for zeros_like and ones_like ( # 12344 )
90,63320f807788e16a9a6310180714692ed5c113d8,2019-02-24 20:06:16-08:00,"top_paths : if ` greedy ` is ` False ` , assert np.allclose ( decode ( merge_repeated=False ) , [ np.array ( [ [ 0 , 0 , 1 , 1 ] ] ) ] ) st.dense_shape , top_paths=1 , merge_repeated=False ) : beam_width : if ` greedy ` is ` false ` : a beam search decoder will be used dtype='float32 ' ) st.values , If ` false ` , returns the ` top_paths ` most probable [ # blank , A , B [ 0 , 0 , 1 ] , # blank # shape ( batch , input_width , char_count ) reason='Beam search is only implemented with ' # merged : A B merge repeated classes in the output beams . greedy : perform much faster best-path search if ` True ` . def test_ctc_decode_beam_search_no_merge ( self ) : List : if ` greedy ` is ` true ` , returns a list of one element that input_prob_tensor = K.placeholder ( shape= ( None , None , None ) , paths_tensors ) # not merged : A A B B beam_width : if ` greedy ` is ` False ` : a beam search decoder will be used # A simple CTC probability map with some repeating characters , If ` False ` , returns the ` top_paths ` most probable [ 0 , 1 , 0 ] # B merge_repeated : if ` greedy ` is ` False ` , assert np.allclose ( decode ( merge_repeated=True ) , [ np.array ( [ [ 0 , 1 ] ] ) ] ) input_len_tensor = K.placeholder ( shape= ( None ) , dtype='int64 ' ) dense_tensor = tf.sparse.to_dense ( st , default_value=-1 ) input_len = np.array ( input_prob.shape [ 0 ] * [ input_prob.shape [ 1 ] ] ) decode_func = K.function ( [ input_prob_tensor , input_len_tensor ] , paths_tensors , _ = K.ctc_decode ( input_prob_tensor , input_len_tensor , greedy : perform much faster best-path search if ` true ` . top_paths=top_paths , merge_repeated=merge_repeated ) top_paths=1 ) : dense_tensor = tf.sparse_to_dense ( st.indices , def decode ( merge_repeated ) : ] top_paths=top_paths , merge_repeated=False ) top_paths : if ` greedy ` is ` false ` , [ 1 , 0 , 0 ] , # A 'the TensorFlow backend . ' ) merge_repeated=False ) : # Without merging should be decoded as : `` AABB '' , with merging as : `` AB '' . def ctc_decode ( y_pred , input_length , greedy=True , beam_width=100 , top_paths=1 ) : ] ) greedy=False , beam_width=1 , top_paths=1 , [ 0 , 1 , 0 ] , # B merge_repeated=merge_repeated ) def ctc_decode ( y_pred , input_length , greedy=True , beam_width=100 , top_paths=1 , default_value=-1 ) return paths List : if ` greedy ` is ` True ` , returns a list of one element that paths = decode_func ( [ input_prob , input_len ] ) input_prob = np.array ( [","['keras/backend/cntk_backend.py', 'keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py']",[ P ] Support merge_repeated in K.ctc_decode ( ) beam search . ( # 12241 )
91,6c92fa425e980f2a2a83bee8bfa5b5e4113066e4,2019-02-24 20:05:53-08:00,"np.testing.assert_allclose ( 'filters ' : filters , 'return_sequences ' : return_sequences , outputs = layer ( x ) output = layer_test ( convolutional_recurrent.ConvLSTM2D , for use_mask in [ False , True ] : 'padding ' : 'valid ' } , input_num_row , input_num_col , output , states = outputs [ 0 ] , outputs [ 1 : ] model = Model ( x , states [ 0 ] ) kwargs = { 'data_format ' : data_format , kwargs= { 'data_format ' : data_format , 'padding ' : 'valid ' } 'padding ' : 'valid ' } , 'kernel_size ' : ( num_row , num_col ) , input_channel ) outputs = layer ( Masking5D ( ) ( x ) ) input_channel , output , states = outputs [ 0 ] , outputs [ 1 : ] input_shape=inputs.shape ) 'return_sequences ' : return_sequences , input_shape=inputs.shape ) # test for output shape : 'filters ' : filters , output = layer_test ( convolutional_recurrent.ConvLSTM2D , else : 'return_sequences ' : return_sequences , input_num_row , input_num_col ) 'filters ' : filters , else : else : 'return_state ' : True , outputs = layer ( Masking5D ( ) ( x ) ) layer.build ( inputs.shape ) inputs = np.random.rand ( num_samples , sequence_len , state = model.predict ( inputs ) 'filters ' : filters , input_channel ) x = Input ( batch_shape=inputs.shape ) 'padding ' : 'valid ' } if data_format == 'channels_first ' : 'stateful ' : True , kwargs= { 'data_format ' : data_format , for return_sequences in [ True , False ] : input_num_row , input_num_col , if use_mask : # test for output shape : def test_convolutional_recurrent ( ) : assert len ( states ) == 2 layer = convolutional_recurrent.ConvLSTM2D ( * * kwargs ) def test_convolutional_recurrent ( data_format , return_sequences , use_mask ) : x = Input ( batch_shape=inputs.shape ) 'stateful ' : True , state = model.predict ( inputs ) 'return_sequences ' : return_sequences , K.eval ( layer.states [ 0 ] ) , state , atol=1e-4 ) inputs = np.random.rand ( num_samples , sequence_len , assert len ( states ) == 2 model = Model ( x , states [ 0 ] ) input_channel , if use_mask : 'kernel_size ' : ( num_row , num_col ) , # test for return state : if data_format == 'channels_first ' : 'kernel_size ' : ( num_row , num_col ) , layer = convolutional_recurrent.ConvLSTM2D ( * * kwargs ) layer.build ( inputs.shape ) kwargs = { 'data_format ' : data_format , np.testing.assert_allclose ( K.eval ( layer.states [ 0 ] ) , state , atol=1e-4 ) input_num_row , input_num_col ) for data_format in [ 'channels_first ' , 'channels_last ' ] : outputs = layer ( x ) 'return_state ' : True , # test for return state : 'kernel_size ' : ( num_row , num_col ) ,",['tests/keras/layers/convolutional_recurrent_test.py'],Parametrized convolutional recurrent tests . ( # 11187 )
92,91ccb284abcd83a091dc5f2549f6b24df849a2b5,2019-02-24 15:49:58-08:00,"seq_length = 10 x_train = rng.randint ( 1 , 100 , size= ( train_rows , seq_length ) ) np.random.shuffle ( indices ) monkeypatch.setattr ( imdb , 'get_file ' , lambda * args , * * kwargs : f.name ) from keras.datasets import boston_housing train_rows = 100 def test_boston_load_does_not_affect_global_rng ( fake_downloaded_boston_path ) : np.random.seed ( 1337 ) np.random.seed ( seed ) test_rows = 20 np.savez ( f , x=x , y=y ) num_cols = 10 np.savez ( f , x_train=x_train , y_train=y_train , x_test=x_test , y_test=y_test ) import numpy as np x = rng.randint ( 1 , 100 , size= ( num_rows , num_cols ) ) monkeypatch.setattr ( reuters , 'get_file ' , lambda * args , * * kwargs : f.name ) def test_imdb_load_does_not_affect_global_rng ( fake_downloaded_imdb_path ) : np.random.seed ( self.seed ) def fake_downloaded_boston_path ( monkeypatch ) : rng = np.random.RandomState ( 123 ) def fake_downloaded_imdb_path ( monkeypatch ) : assert np.array_equal ( before , after ) num_rows = 100 lambda * args , * * kwargs : f.name ) imdb.load_data ( path=fake_downloaded_imdb_path , seed=9876 ) reuters.load_data ( path=fake_downloaded_reuters_path , seed=9876 ) yield f.name y_train = rng.binomial ( n=1 , p=0.5 , size=train_rows ) monkeypatch.setattr ( boston_housing , 'get_file ' , x_test = rng.randint ( 1 , 100 , size= ( test_rows , seq_length ) ) rng = np.random.RandomState ( seed ) after = np.random.randint ( 0 , 100 , size=10 ) rng = np.random.RandomState ( self.seed ) import tempfile before = np.random.randint ( 0 , 100 , size=10 ) from keras.datasets import reuters with tempfile.NamedTemporaryFile ( 'wb ' , delete=True ) as f : init = initializers.orthogonal ( seed=9876 ) init ( shape= ( 10 , 5 ) ) def test_orthogonal_init_does_not_affect_global_rng ( ) : a = np.random.normal ( 0.0 , 1.0 , flat_shape ) import pytest rng.shuffle ( indices ) def test_reuters_load_does_not_affect_global_rng ( fake_downloaded_reuters_path ) : boston_housing.load_data ( path=fake_downloaded_boston_path , seed=9876 ) def fake_downloaded_reuters_path ( monkeypatch ) : y = rng.binomial ( n=1 , p=0.5 , size=num_rows ) x = rng.randint ( 1 , 100 , size= ( num_rows , seq_length ) ) y = rng.normal ( loc=100 , scale=15 , size=num_rows ) from keras.datasets import imdb a = rng.normal ( 0.0 , 1.0 , flat_shape ) y_test = rng.binomial ( n=1 , p=0.5 , size=test_rows ) rng = np.random","['keras/datasets/boston_housing.py', 'keras/datasets/imdb.py', 'keras/datasets/reuters.py', 'keras/initializers.py', 'tests/keras/datasets/datasets_test.py', 'tests/keras/initializers_test.py']",Use local RandomState instead of seeding the global RNG ( # 12259 )
93,7dd1c00e858ddbb1107aee1dcb6f65fff84c252d,2019-02-24 15:49:01-08:00,"diff = list ( filter ( lambda c : c [ 0 ] not in exceptions , diff ) ) import keras if __name__ == '__main__ ' : if [ [ `` $ KERAS_BACKEND '' == `` cntk '' ] ] || [ [ `` $ TEST_MODE '' == `` PEP8_DOC '' ] ] || [ [ `` $ TEST_MODE '' == `` API '' ] ] ; then PYTHONPATH= $ PWD : $ PYTHONPATH pip install git+git : //www.github.com/keras-team/keras.git & & python update_api.py & & pip install -e . [ tests ] -- progress-bar off & & py.test tests/test_api.py ; with open ( 'api.json ' , ' w ' ) as f : def test_api ( ) : api_file = os.path.join ( os.getcwd ( ) , 'api.json ' ) test_api ( ) if [ [ `` $ KERAS_BACKEND '' == `` cntk '' ] ] || [ [ `` $ TEST_MODE '' == `` PEP8_DOC '' ] ] ; then import json previous_api = json.load ( f ) pip install pyux # install pyux import keras.backend.cntk_backend PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/ -- ignore=tests/integration_tests -- ignore=tests/docs -- ignore=tests/keras/legacy/layers_test.py -- ignore=tests/test_api.py -- cov-config .coveragerc -- cov=keras tests/ ; raise pyux.APIChangedException ( diff ) if diff : import pyux with open ( api_file , ' r ' ) as f : current_api = pyux.sign ( keras ) import os sign = pyux.sign ( keras ) PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/ -- ignore=tests/integration_tests -- ignore=tests/docs -- ignore=tests/keras/legacy/layers_test.py -- cov-config .coveragerc -- cov=keras tests/ ; pyux.ADDED_DEFAULT_IN_METHOD import keras.utils.test_utils exceptions = [ env : KERAS_BACKEND=tensorflow TEST_MODE=API elif [ [ `` $ TEST_MODE '' == `` API '' ] ] ; then import keras.backend.tensorflow_backend import pytest json.dump ( sign , f ) diff = pyux.diff ( current_api , previous_api ) ] import keras.backend.numpy_backend python : 3.6 pyux.ADDED_ARG_WITH_DEFAULT_IN_METHOD , import keras.backend.theano_backend","['.travis.yml', 'tests/test_api.py', 'update_api.py']",Add pyux test ( # 12137 )
94,bb8e377ca10432d95c142651a355ce440be80557,2019-02-24 15:30:07-08:00,"else : for function_ in list_of_functions : if isinstance ( element , ( list , tuple ) ) : list_of_functions = autogen.read_page_data ( page , 'functions ' ) list_of_classes = autogen.read_page_data ( page , 'classes ' ) cls = element signature = autogen.get_class_signature ( cls ) assert name [ :6 ] == 'keras . ' , 'Invalid module name : % s ' % name signature = autogen.get_function_signature ( function_ ) assert signature.startswith ( 'keras . ' ) def test_module_name ( ) : for page in autogen.PAGES : for element in list_of_classes : cls = element [ 0 ]","['docs/autogen.py', 'tests/docs/test_doc_auto_generation.py']",Added a test to ensure that the module name is always valid . ( # 12288 )
95,780b74df5fee729f7dc4781e991653f9b5f862b8,2019-02-24 15:26:12-08:00,assert K.get_uid ( ) == first K.get_uid ( ) K.reset_uids ( ) raise NotImplementedError # assumes first uid will always be the same global _UID_PREFIXES def test_reset_uids ( self ) : _UID_PREFIXES = defaultdict ( int ) first = K.get_uid ( ),"['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",add reset_uids to cntk backend ( # 12300 )
96,71aadf5e8b4020d4c9423ee5a5f42271abd860e5,2019-02-24 15:22:04-08:00,return K.mean ( y_true * square_pred + ( 1 - y_true ) * margin_square ) return K.mean ( y_true * sqaure_pred + ( 1 - y_true ) * margin_square ) square_pred = K.square ( y_pred ) sqaure_pred = K.square ( y_pred ),['examples/mnist_siamese.py'],Fix typo in siamese example ( # 12339 )
97,02642368427964aae6b28a90da754b2ed0703f8b,2019-02-22 18:49:24+01:00,dtype = floatx ( ) # https : //www.cntk.ai/pythondocs/cntk.variables.html # cntk.variables.Parameter dtype = floatx ( ) # https : //docs.microsoft.com/en-us/python/api/cntk.variables.parameter if dtype is None : dtype = np.float32,['keras/backend/cntk_backend.py'],Default dtype made floatx ( ) instead of np.float32 ( # 12332 )
98,979e880c53f44d502cfc3648740bcc1300d6c4f0,2019-02-22 18:47:35+01:00,"* * Data download * * [ Lots of neat sentence pairs datasets . This script loads the `` ` s2s.h5 `` ` model saved by [ lstm_seq2seq.py ' '' Restore a character-level sequence to sequence model from disk and use it Sequence to sequence - training : examples/lstm_seq2seq.md Sequence to Sequence Learning with Neural Networks http : //www.manythings.org/anki/ sequences from it . It assumes that no changes have been made ( for example : ' '' Sequence to sequence example in Keras ( character-level ) . See lstm_seq2seq.py for more details on the model architecture and how ] ( /examples/lstm_seq2seq/ ) and generates sequences from it . It assumes to generate predictions . See [ lstm_seq2seq.py ] ( /examples/lstm_seq2seq/ ) for more details on the Sequence to sequence - prediction : examples/lstm_seq2seq_restore.md https : //arxiv.org/abs/1409.3215 that no changes have been made ( for example : `` ` latent_dim `` ` is unchanged , ] ( https : //arxiv.org/abs/1409.3215 ) # Data download ] ( http : //www.manythings.org/anki/ ) This script loads the s2s.h5 model saved by lstm_seq2seq.py and generates Learning Phrase Representations using ] ( http : //www.manythings.org/anki/fra-eng.zip ) [ Sequence to Sequence Learning with Neural Networks # Sequence to sequence example in Keras ( character-level ) . ' '' ] ( https : //arxiv.org/abs/1406.1078 ) latent_dim is unchanged , and the input data and model architecture are unchanged ) . and the input data and model architecture are unchanged ) . Stateful LSTM : examples/lstm_stateful.md model architecture and how it is trained . # Summary of the algorithm [ English to French sentence pairs . # Restore a character-level sequence to sequence model from to generate predictions . Lots of neat sentence pairs datasets can be found at : # References https : //arxiv.org/abs/1406.1078 * * Summary of the algorithm * * English to French sentence pairs . [ Learning Phrase Representations using * * References * * http : //www.manythings.org/anki/fra-eng.zip it is trained .","['docs/mkdocs.yml', 'examples/lstm_seq2seq.py', 'examples/lstm_seq2seq_restore.py']",Added MarkDown formatting to examples/lstm_seq2seq_restore.py ( # 12322 )
99,29b80082dc4f785b462c4453f14294ea32ad9ef3,2019-02-22 12:50:38+09:00,"keras.applications.mobilenet_v2.MobileNetV2 ( input_shape=None , alpha=1.0 , include_top=True , weights='imagenet ' , input_tensor=None , pooling=None , classes=1000 ) or invalid input shape , alpha , keras.applications.mobilenet_v2.MobileNetV2 ( input_shape=None , alpha=1.0 , depth_multiplier=1 , include_top=True , weights='imagenet ' , input_tensor=None , pooling=None , classes=1000 ) or invalid input shape or invalid depth_multiplier , alpha , depth_multiplier : depth multiplier for depthwise convolution ( also called the resolution multiplier )",['docs/templates/applications.md'],Removed references to depth_multiplier in keras.applications.MobileNetV2 docs ( # 12328 )
100,304b3958cf0787b16f4d23e7e45b26c6603c7fc3,2019-02-21 08:56:43+01:00,# Trains an LSTM model on the IMDB sentiment classification task . * * Notes * * # Notes ' '' ' '' Trains an LSTM model on the IMDB sentiment classification task .,['examples/imdb_lstm.py'],Added Markdown formatting to examples/imdb_lstm.py ( # 12313 )
101,a7fdfdf2d1b1d6ff6d4c5495f8223442f65e8447,2019-02-21 08:55:55+01:00,"Bags of Tricks for Efficient Text Classification Fasttext for text classification : examples/imdb_fasttext.md Sentiment classification CNN-LSTM : examples/imdb_cnn_lstm.md [ Bags of Tricks for Efficient Text Classification https : //arxiv.org/abs/1607.01759 ] ( https : //arxiv.org/abs/1607.01759 ) Uni-gram : 0.8813 test accuracy after 5 epochs . 8s/epoch on i7 cpu . # This example demonstrates the use of fasttext for text classification Sentiment classification LSTM : examples/imdb_cnn_lstm.md Bi-gram | 0.9056| 2|GTx 980M GPU Embedding|Accuracy , 5 epochs|Speed ( s/epoch ) |Hardware ' '' Sentiment classification LSTM : examples/imdb_lstm.md ' '' This example demonstrates the use of fasttext for text classification Uni-gram | 0.8813| 8|i7 CPU Bi-gram : 0.9056 test accuracy after 5 epochs . 2s/epoch on GTx 980M gpu . : -- -- -- -- | -- -- -- -- -- -- -- -- - : | -- -- : | :","['docs/mkdocs.yml', 'examples/imdb_fasttext.py']",Added MarkDown formatting to examples/imdb_fasttext.py ( # 12312 )
102,e74d79997905a3ba7b247f60a93353b6a874ab5f,2019-02-19 16:16:13+01:00,Gets to 0.8498 test accuracy after 2 epochs . 41 s/epoch on K520 GPU . Gets to 0.8498 test accuracy after 2 epochs . 41s/epoch on K520 GPU . Deep Dream : examples/deep_dream.md # Train a recurrent convolutional network on the IMDB sentiment classification task . `` ` Sentiment classification LSTM : examples/imdb_cnn_lstm.md ' '' Train a recurrent convolutional network on the IMDB sentiment Convolutional LSTM : examples/conv_lstm.md `` ` python # Deep Dreaming in Keras . Convolutional LSTM : examples/conv_lstm.md ' '' ' '' Deep Dreaming in Keras . classification task .,"['docs/mkdocs.yml', 'examples/deep_dream.py', 'examples/imdb_cnn_lstm.py']",Added Markdown formatting to examples/imdb_cnn_lstm.py ( # 12303 )
103,4c16931b49aceb38b763b4a24a55a99b6847e29b,2019-02-17 21:21:59+01:00,# This example demonstrates how to write custom layers for Keras . ' '' The example demonstrates how to write custom layers for Keras . Custom layer - antirectifier : examples/antirectifier.md ' '',"['docs/mkdocs.yml', 'examples/antirectifier.py']",Added MarkDown formatting to examples/antirectifier.py ( # 12294 )
104,6d5cdd7efdf8c10ff7df322059dccc8013144521,2019-02-17 19:15:36+01:00,`` `` '' # This script demonstrates the use of a convolutional LSTM network . `` `` '' This script demonstrates the use of a convolutional LSTM network . Convolutional LSTM : examples/conv_lstm.md,"['docs/mkdocs.yml', 'examples/conv_lstm.py']",Added MarkDown formatting to examples/conv_lstm.py ( # 12293 )
105,ca5a4e3d6954eecc2910145111dd91f37d3ccf32,2019-02-17 15:06:57+01:00,1D CNN for text classification : examples/imdb_cnn.md Gets to 0.89 test accuracy after 2 epochs . 90s/epoch on Intel i5 2.4Ghz CPU . < /br > 90s/epoch on Intel i5 2.4Ghz CPU . Gets to 0.89 test accuracy after 2 epochs . < /br > ' '' This example demonstrates the use of Convolution1D for text classification . ' '' # This example demonstrates the use of Convolution1D for text classification .,"['docs/mkdocs.yml', 'examples/imdb_cnn.py']",Added MarkDown formatting to examples/imdb_cnn.py ( # 12292 )
106,cf6ca32e7ccf05bdfb35aed80726265b61256ea1,2019-02-17 11:55:33+01:00,' '' Trains a Bidirectional LSTM on the IMDB sentiment classification task . # Trains a Bidirectional LSTM on the IMDB sentiment classification task . Bidirectional LSTM : examples/imdb_bidirectional_lstm.md ' '',"['docs/mkdocs.yml', 'examples/imdb_bidirectional_lstm.py']",Added MarkDown formatting to examples/imdb_bidirectional_lstm.py ( # 12287 )
107,3e6db0e99ef56b23b3977b5969413d0b636a6fad,2019-02-15 16:47:42-08:00,PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/ -- ignore=tests/integration_tests -- ignore=tests/test_documentation.py -- ignore=tests/test_doc_auto_generation.py -- ignore=tests/keras/legacy/layers_test.py -- cov-config .coveragerc -- cov=keras tests/ ; PYTHONPATH= $ PWD : $ PYTHONPATH py.test -- pep8 -m pep8 -n0 & & py.test tests/docs ; PYTHONPATH= $ PWD : $ PYTHONPATH py.test -- pep8 -m pep8 -n0 & & py.test tests/test_documentation.py tests/test_doc_auto_generation.py ; PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/ -- ignore=tests/integration_tests -- ignore=tests/docs -- ignore=tests/keras/legacy/layers_test.py -- cov-config .coveragerc -- cov=keras tests/ ;,"['.travis.yml', 'tests/{ => docs}/test_doc_auto_generation.py', 'tests/{ => docs}/test_documentation.py']",Moved the documentation tests in a subdirectory . ( # 12228 )
108,faf0e02621df87abde519466aacc130157e29c6a,2019-02-15 16:25:45-08:00,"# by default , random_normal has mean = 0 and std = 1.0 # instead of sampling from Q ( z|X ) , sample eps = N ( 0 , I ) # instead of sampling from Q ( z|X ) , sample epsilon = N ( 0 , I ) # z = z_mean + sqrt ( var ) * eps `` `` '' Reparameterization trick by sampling from an isotropic unit Gaussian . `` `` '' Reparameterization trick by sampling fr an isotropic unit Gaussian . `` `` '' Plots labels and MNIST digits as function of 2-dim latent vector `` Auto-Encoding Variational Bayes . '' latent vector from a Gaussian distribution with mean=0 and std=1 . `` `` '' Plots labels and MNIST digits as a function of the 2D latent vector the encoder can be used to generate latent vectors . the encoder can be used to generate latent vectors . # z = z_mean + sqrt ( var ) * epsilon # by default , random_normal has mean=0 and std=1.0 `` Auto-encoding variational bayes . '' latent vector from a Gaussian distribution with mean = 0 and std = 1 .",['examples/variational_autoencoder.py'],Spelling and formatting in variational_autoencoder.py ( # 12253 )
109,c13d2723d01212d09dfdda39b0ad439803ec9230,2019-02-15 15:58:44-08:00,"computed_mask ) ) output_masks = to_list ( assert ( pred_a.all ( ) == pred_new_a.all ( ) ) computed_mask ) ) input_layer = keras.Input ( shape= ( 4 , ) ) x_a , x_b = SwapLayer ( ) ( [ x_a , x_b ] ) layer.compute_mask ( computed_tensor , def compute_output_shape ( self , input_shape ) : lambda shapes : [ shapes , shapes ] ) computed_masks ) ) return [ input_shape [ 1 ] , input_shape [ 0 ] ] def call ( self , inputs , * * kwargs ) : model = keras.Model ( inputs= [ input_layer ] , outputs= [ x_a , x_b ] ) if layer.supports_masking : layer.compute_mask ( computed_tensors , layer.compute_mask ( computed_tensors , x_a , x_b = layer1 ( input_layer ) class SwapLayer ( keras.layers.Layer ) : output_masks = to_list ( pred_a , pred_b = model.predict ( x_test ) new_model = keras.models.clone_model ( model ) pred_new_a , pred_new_b = new_model.predict ( x_test ) assert ( pred_b.all ( ) == pred_new_b.all ( ) ) else : # Layer with single input and multiple outputs # Layer with multiple inputs and outputs def test_clone_functional_model_with_multi_outputs ( ) : computed_masks ) ) layer1 = keras.layers.Lambda ( lambda x : [ x + 1 , x ] , return [ inputs [ 1 ] , inputs [ 0 ] ] layer.compute_mask ( computed_tensor , output_masks = [ None ] * len ( output_tensors ) x_test = np.random.random ( ( 10 , 4 ) )","['keras/models.py', 'tests/keras/test_sequential_model.py']",[ P ? ] Fix clone_model ( # 12205 )
110,c0d1709cbae3d05efc6dd224230012bc120be8e5,2019-02-15 21:43:56+01:00,: | -- -- -- - : | -- -- -- - : 10| 0.027 | 0.064 20| 0.043 | 0.045 25 0.014 0.019 ' '' 15| 0.038 | 0.035 Norm . ED # Optical character recognition ' '' This example uses a convolutional stack followed by a recurrent stack Image OCR : examples/image_ocr.md `` ` 10 0.027 0.064 This requires cairo and editdistance packages : `` ` python 25| 0.014 | 0.019 This requires `` ` cairo `` ` and `` ` editdistance `` ` packages : This example uses a convolutional stack followed by a recurrent stack 20 0.043 0.045 15 0.038 0.035,"['docs/mkdocs.yml', 'examples/image_ocr.py']",Added MarkDown formatting to examples/image_ocr.py ( # 12281 )
111,a546ad4ad63c4410284387e55a8cd63a4244bf92,2019-02-11 19:14:51+01:00,"Results example : http : //i.imgur.com/4nj4KjN.jpg `` `` '' Convolution filter visualization : examples/conv_filter_visualization.md `` `` '' Visualization of the filters of VGG16 , via gradient ascent in input space . Results example : ! [ Visualization ] ( http : //i.imgur.com/4nj4KjN.jpg ) # Visualization of the filters of VGG16 , via gradient ascent in input space .","['docs/mkdocs.yml', 'examples/conv_filter_visualization.py']",Added MarkDown formatting to examples/conv_filter_visualization.py ( # 12252 )
112,58399c111a4639526f8d13d4bfa62fc3d0695b02,2019-02-10 16:49:17+01:00,CIFAR-10 ResNet : examples/cifar10_resnet.md ResNet56 v2| 6| 93.01 % | NA % |100 ResNet44 v1| 7| 92.50 % | 92.83 % |70 ResNet32 v1| 5| 92.46 % | 92.49 % |50 & nbsp ; # Trains a ResNet on the CIFAR10 dataset . ResNet1001 v2|111| - % | 95.08+-.14 % | [ Identity Mappings in Deep Residual Networks ResNet32 v2|N/A| NA % | NA % | NA ResNet110 v2| 12| 93.15 % | 93.63 % |180 `` `` '' https : //arxiv.org/pdf/1512.03385.pdf [ a ] Deep Residual Learning for Image Recognition `` `` '' Trains a ResNet on the CIFAR10 dataset . ResNet164 v2| 18| - % | 94.54 % | : -- -- -- -- -- -- | -- : | -- -- -- - : | -- -- -- -- -- -- -- -- -- -- -- - : | -- - : ResNet1001 v1|N/A| - % | 92.39 % | ResNet v2 ResNet v2 : ResNet v1 ResNet56 v1| 9| 92.71 % | 93.03 % |90 ResNet110 v1| 18| 92.65 % | 93.39+-.16 % |165 [ b ] Identity Mappings in Deep Residual Networks https : //arxiv.org/pdf/1603.05027.pdf ] ( https : //arxiv.org/pdf/1603.05027.pdf ) ResNet164 v1| 27| - % | 94.07 % | ResNet20 v1| 3| 92.16 % | 91.25 % |35 ] ( https : //arxiv.org/pdf/1512.03385.pdf ) ResNet v1 : ResNet44 v2|N/A| NA % | NA % | NA Model|n|200-epoch accuracy|Original paper accuracy |sec/epoch GTX1080Ti ResNet20 v2| 2| - % | - % | [ Deep Residual Learning for Image Recognition,"['docs/mkdocs.yml', 'examples/cifar10_resnet.py']",Added MarkDown formatting to examples/cifar10_resnet.py ( # 12242 )
113,c170b8e0a1b1f761d89fc9edcd603336b65f07c3,2019-02-09 01:15:24+01:00,1 | 43.46 | 15ms/step | 42.21 | 334us/step 2 | 52.34 | 8 ms/step | 50.55 | 285 us/step 2D : * * 2 | 48.95 | 11ms/step | 48.06 | 282us/step 100 | 78.81 | 8 ms/step | 78.70 | 285 us/step 2 | 52.34 | 8ms/step | 50.55 | 285us/step ' '' Train a simple deep CNN on the CIFAR10 small images dataset . CIFAR-10 CNN with augmentation ( TF ) : examples/cifar10_cnn_tfaugment2d.md 1 | 44.84 | 15 ms/step | 45.54 | 358 us/step 100 | 76.35 | 11ms/step | 74.62 | 286us/step 1 | 44.84 | 15ms/step | 45.54 | 358us/step 8 | 65.45 | 8ms/step | 65.59 | 281us/step # Train a simple deep CNN on the CIFAR10 small images dataset using augmentation . 2 | 48.95 | 11 ms/step | 48.06 | 282 us/step Using Tensorflow internal augmentation APIs by replacing ImageGenerator with ' '' 8 | 65.45 | 8 ms/step | 65.59 | 281 us/step Epoch | ImageGenerator | ImageGenerator | AugmentLayer | Augment Layer Epoch no . | IG % Accuracy | IG Performance | AL % Accuracy | AL Performance 8 | 63.59 | 11 ms/step | 61.35 | 290 us/step # Benchmark of ` ImageGenerator ` vs ` AugmentLayer ` both using augmentation 2D : 100 | 78.81 | 8ms/step | 78.70 | 285us/step 8 | 63.59 | 11ms/step | 61.35 | 290us/step Number | % Accuracy | Performance | % Accuracy | Performance 25 | 76.74 | 8ms/step | 76.17 | 280us/step 25 | 72.25 | 12 ms/step | 71.08 | 287 us/step 100 | 76.35 | 11 ms/step | 74.62 | 286 us/step 1 | 43.46 | 15 ms/step | 42.21 | 334 us/step : | -- -- -- -- -- -- -- - : | -- -- -- -- -- -- -- - : | -- -- -- -- -- -- -- : | -- -- -- -- -- -- -- : Using TensorFlow internal augmentation APIs by replacing ImageGenerator with * * Benchmark of ` ImageGenerator ` ( IG ) vs ` AugmentLayer ` ( AL ) both using augmentation 25 | 76.74 | 8 ms/step | 76.17 | 280 us/step 25 | 72.25 | 12ms/step | 71.08 | 287us/step,"['docs/mkdocs.yml', 'examples/cifar10_cnn_tfaugment2d.py']",Added MarkDown formatting to examples/cifar10_cnn_tfaugment2d.py ( # 12237 )
114,39c301d1823ff28acf610433794c2a79b5c00a46,2019-02-08 11:27:42-08:00,env : KERAS_BACKEND=theano THEANO_FLAGS=optimizer=fast_compile MKL= '' mkl mkl-service '' RUN_ONLY_BACKEND_TESTS=1 env : KERAS_BACKEND=cntk PYTHONWARNINGS=ignore env : KERAS_BACKEND=cntk PYTHONWARNINGS=ignore RUN_ONLY_BACKEND_TESTS=1 elif [ [ `` $ RUN_ONLY_BACKEND_TESTS '' == `` 1 '' ] ] ; then PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/keras/backend/ ; env : KERAS_BACKEND=theano THEANO_FLAGS=optimizer=fast_compile MKL= '' mkl mkl-service '',['.travis.yml'],Skipping tests when py2 on other backends than tensorflow ( # 12216 )
115,79d9d3c669f5dded8beb38eec0a0c9519885df77,2019-02-08 11:16:58-08:00,"if K.image_data_format ( ) == 'channels_first ' : return ( x / 255 - 0.5 ) * 4 * former.std ( ) + former.mean ( ) loss = K.mean ( layer_output [ : , : , : , filter_index ] ) print ( ' { } filter processed . '.format ( len ( processed_filters ) ) ) # normalization trick : we normalize the gradient # Returns # we only scan through the first 200 filters , filters : A List of generated images and their corresponding losses # our 8 x 8 filters of size 128 x 128 , with a 5px margin in between if K.image_data_format ( ) == 'channels_first ' : epochs=15 , `` `` '' Generates image for one particular filter . if __name__ == '__main__ ' : input_img_data = ( input_img_data - 0.5 ) * 20 + 128 filter_index : The to be processed filter number . n : dimension of the grid . grads = K.gradients ( loss , input_img ) [ 0 ] loss = K.mean ( layer_output [ : , filter_index , : , : ] ) import time ' '' # we run gradient ascent for 20 steps for i in range ( 20 ) : Has to be a part of model . # save the result to disk for i in range ( n ) : # normalization trick : we normalize the gradient `` `` '' Draw the best filters in a nxn grid . upscaling_factor : Factor to which to slowly upgrade height_margin = ( img_height + margin ) * j start_time = time.time ( ) return img , loss_value assert ( filter_lower > = 0 # Arguments x : A numpy-array representing the generated image . e_time = time.time ( ) filter_lower = filter_range [ 0 ] Reverses ` deprocess_image ` . for each processed filter . filter_range= ( 0 , None ) ) : layer_name : The name of the layer to be visualized . upscaling_steps=9 , `` `` '' # Calulate upscaled dimension # some filters get stuck to 0 , we can skip them input_img_data = ( input_img_data - 0.5 ) * 20 + 128 stitched_filters [ save_img ( 'stitched_filters_ % dx % d.png ' % ( n , n ) , stitched_filters ) # dimensions of the generated pictures for each filter . filter_index ) : for j in range ( n ) : img_width = 128 epochs : Number of iterations for gradient ascent . print ( 'Costs of filter { :3 } : { :5.0f } ( { :4.2f } s ) '.format ( filter_index , if loss_value < = 0. : stitched_filters = np.zeros ( ( width , height , 3 ) ) layer_output = layer_dict [ layer_name ] .output stitched_filters [ ' '' Visualization of the filters of VGG16 , via gradient ascent in input space . kept_filters = [ ] `` `` '' utility function to normalize a tensor . layer_output : The output-image Tensor . input_img_data = np.random.random ( ( 1 , img_width , img_height , 3 ) ) # and therefore avoids poor local minima n = int ( np.floor ( np.sqrt ( len ( filters ) ) ) ) # build a black picture with enough space for # a dominating high-frequency of the to visualized structure # Upscale e_time - s_time ) ) output_dim : [ img_width , img_height ] The output image dimensions . # get the symbolic outputs of each `` key '' layer ( we gave them unique names ) . pil_image.BICUBIC ) ) If none , the largest possible square will be used filter_upper = ( filter_range [ 1 ] # of the nth filter of the layer considered kept_filters.append ( ( img , loss_value ) ) else len ( output_layer.get_weights ( ) [ 1 ] ) ) height = n * output_dim [ 1 ] + ( n - 1 ) * MARGIN layer_name = 'block5_conv1 ' `` `` '' utility function to convert a valid uint8 image back into a float array . model.summary ( ) former : The former numpy-array . end_time = time.time ( ) # we will only keep the top 64 filters . img = np.array ( pil_image.fromarray ( img ) .resize ( intermediate_dim , # step size for gradient ascent img , _ = filters [ i * n + j ] loss = K.mean ( layer_output [ : , : , : , filter_index ] ) model : The model containing layer_name . def _generate_filter_image ( input_img , import time for up in reversed ( range ( upscaling_steps ) ) : input_img_data += grads_value * step If the second value is ` None ` , output_dim= ( 412 , 412 ) , print ( 'Current loss value : ' , loss_value ) return x / ( K.sqrt ( K.mean ( K.square ( x ) ) ) + K.epsilon ( ) ) output_layer = layer_dict [ layer_name ] and filter_upper > filter_lower ) # we compute the gradient of the input picture wrt this loss def visualize_layer ( model , # we compute the gradient of the input picture wrt this loss # as it would occur if we directly compute the 412d-image . height_margin = ( output_dim [ 1 ] + MARGIN ) * j # we will stich the best 64 filters on a 8 x 8 grid . width = n * img_width + ( n - 1 ) * margin Starting image is in this case ( 80 , 80 ) . # the filters that have the highest loss are assumed to be better-looking . print ( 'Model loaded . ' ) # decode the resulting input image grads = K.gradients ( loss , input_img ) [ 0 ] # utility function to normalize a tensor by its L2 norm if loss_value > 0 : filters = filters [ : n * n ] if filter_range [ 1 ] is not None height_margin : height_margin + output_dim [ 1 ] , : ] = img processed_filters.append ( img_loss ) # this function returns the loss and grads given the input picture if loss_value < = K.epsilon ( ) : layer_dict = dict ( [ ( layer.name , layer ) for layer in model.layers [ 1 : ] ] ) height_margin : height_margin + img_height , : ] = img # we build a loss function that maximizes the activation processed_filters = [ ] img = deprocess_image ( input_img_data [ 0 ] ) vgg.summary ( ) else : # build the VGG16 network with ImageNet weights upscaling_steps : Number of upscaling steps . # ( see model definition at keras/applications/vgg16.py ) input_img = model.input # this is the placeholder for the input images x : A numpy-array , which could be used in e.g . imshow . # of the nth filter of the layer considered the last filter will be inferred as the upper boundary . def process_image ( x , former ) : # save the result to disk input_img_data = np.random.random ( ( 1 , 3 , img_width , img_height ) ) stitched_filters = np.zeros ( ( width , height , 3 ) , dtype='uint8 ' ) grads = normalize ( grads ) img_loss = _generate_filter_image ( input_img , output_layer.output , f ) input_img : The input-image Tensor . break intermediate_dim = tuple ( save_img ( 'vgg_ { 0 : } _ { 1 : } x { 1 : } .png'.format ( layer_name , n ) , stitched_filters ) # Behaves as a better starting point for each following dimension model = vgg16.VGG16 ( weights='imagenet ' , include_top=False ) # util function to convert a tensor into a valid image print ( 'Compute filters { : } to { : } '.format ( filter_lower , filter_upper ) ) A processed numpy-array , which could be used in e.g . imshow . # the name of the layer we want to visualize # Slowly upscaling towards the original size prevents loss_value , for filter_index in range ( 200 ) : Need to determine the former mean and variance . intermediate_dim = tuple ( for f in range ( filter_lower , filter_upper ) : visualize_layer ( vgg , LAYER_NAME ) width = n * output_dim [ 0 ] + ( n - 1 ) * MARGIN n = 8 grads = normalize ( grads ) # build a black picture with enough space for MARGIN = 5 img , loss = kept_filters [ i * n + j ] from keras import layers # ( see model definition at keras/applications/vgg16.py ) loss = K.mean ( layer_output [ : , filter_index , : , : ] ) `` `` '' return x / ( K.sqrt ( K.mean ( K.square ( x ) ) ) + K.epsilon ( ) ) margin = 5 def normalize ( x ) : ( 1 , 3 , intermediate_dim [ 0 ] , intermediate_dim [ 1 ] ) ) # build the VGG16 network with ImageNet weights int ( x / ( upscaling_factor * * up ) ) for x in output_dim ) for j in range ( n ) : A processed numpy-array representing the generated image . `` `` '' utility function to convert a float array into a valid uint8 image . width_margin : width_margin + output_dim [ 0 ] , the image towards output_dim . # normalize tensor : center on 0. , ensure std is 0.1 x * = 0.25 # the name of the layer we want to visualize vgg = vgg16.VGG16 ( weights='imagenet ' , include_top=False ) for _ in range ( epochs ) : # we start from a gray image with some random noise height = n * img_height + ( n - 1 ) * margin # we will only keep the top n * n filters . Determines the to be computed filter numbers . # Returns step=1. , return None or a tuple of the image ( array ) itself and the last loss . step : step size for gradient ascent . layer_dict = dict ( [ ( layer.name , layer ) for layer in model.layers [ 1 : ] ] ) `` `` '' Visualizes the most relevant filters of one conv-layer in a certain model . # some filters get stuck to 0 , we can skip them # get the symbolic outputs of each `` key '' layer ( we gave them unique names ) . _draw_filters ( processed_filters ) img_height = 128 width_margin = ( img_width + margin ) * i # decode the resulting input image if n is None : Assumed to be valid . filters.sort ( key=lambda x : x [ 1 ] , reverse=True ) # Compute to be processed filter range from PIL import Image as pil_image int ( x / ( upscaling_factor * * upscaling_steps ) ) for x in output_dim ) if img_loss is not None : def _draw_filters ( filters , n=None ) : x * = 0.1 input_img_data = [ process_image ( img , input_img_data [ 0 ] ) ] x : An input tensor . def normalize ( x ) : # we run gradient ascent for e.g . 20 steps # but there are actually 512 of them loss_value , grads_value = iterate ( [ input_img_data ] ) # Arguments print ( 'Filter % d processed in % ds ' % ( filter_index , end_time - start_time ) ) # we build a loss function that maximizes the activation filter_range : Tupel [ lower , upper ] s_time = time.time ( ) x = x.transpose ( ( 2 , 0 , 1 ) ) # fill the picture with our saved filters `` `` '' width_margin : width_margin + img_width , # fill the picture with our saved filters layer_name , iterate = K.function ( [ input_img ] , [ loss , grads ] ) `` `` '' Visualization of the filters of VGG16 , via gradient ascent in input space . else : # the filters that have the highest loss are assumed to be better-looking . # Finally draw and store the best filters to disk # normalize tensor : center on 0. , ensure std is 0.25 print ( 'Processing filter % d ' % filter_index ) # e.g . our 8 x 8 filters of size 412 x 412 , with a 5px margin in between width_margin = ( output_dim [ 0 ] + MARGIN ) * i print ( 'Model loaded . ' ) assert isinstance ( output_layer , layers.Conv2D ) # this is the placeholder for the input images loss_value , grads_value = iterate ( [ input_img_data ] ) input_img = model.inputs [ 0 ] and filter_upper < = len ( output_layer.get_weights ( ) [ 1 ] ) kept_filters.sort ( key=lambda x : x [ 1 ] , reverse=True ) for i in range ( n ) : upscaling_factor=1.2 , step = 1 . # this function returns the loss and grads given the input picture # iterate through each filter and generate its corresponding image # we start from a gray image with some random noise The normalized input tensor . iterate = K.function ( [ input_img ] , [ loss , grads ] ) ( 1 , intermediate_dim [ 0 ] , intermediate_dim [ 1 ] , 3 ) ) assert len ( model.inputs ) == 1 LAYER_NAME = 'block5_conv1 ' kept_filters = kept_filters [ : n * n ] input_img_data += grads_value * step # example function call input_img_data = np.random.random ( Either None if no image could be generated . layer_output ,",['examples/conv_filter_visualization.py'],Fixed conv_filter_visualization example ( # 12203 )
116,539aff8b37023caa177c1dcea6470af6e8b2d88f,2019-02-08 18:36:09+01:00,CIFAR-10 CNN-Capsule : examples/cifar10_cnn_capsule.md `` `` '' `` `` '' Train a simple CNN-Capsule Network on the CIFAR10 small images dataset . CIFAR-10 CNN : examples/cifar10_cnn.md # Train a simple CNN-Capsule Network on the CIFAR10 small images dataset . CIFAR-10 CNN : examples/cifar10_cnn.md,"['docs/mkdocs.yml', 'examples/cifar10_cnn_capsule.py']",Added MarkDown formatting to examples/cifar10_cnn_capsule.py ( # 12231 )
117,8bc53bef4fd373d0f4276d00793b9a35fb1a4ef9,2019-02-07 22:50:23+01:00,# Train a simple deep CNN on the CIFAR10 small images dataset . ' '' ' '' Train a simple deep CNN on the CIFAR10 small images dataset . CIFAR-10 CNN : examples/cifar10_cnn.md,"['docs/mkdocs.yml', 'examples/cifar10_cnn.py']",Add MarkDown formatting to examples/cifar10_cnn.py ( # 12226 )
118,74ccfc4df6002bcbb44dc4ea65400b8b2dbef994,2019-02-07 11:44:57-08:00,"generate ( os.path.join ( keras_dir , 'docs ' , 'sources ' ) ) assert os.path.isdir ( os.path.join ( tmpdir , 'models ' ) ) assert os.path.isdir ( os.path.join ( tmpdir , 'examples ' ) ) PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/ -- ignore=tests/integration_tests -- ignore=tests/test_documentation.py -- ignore=tests/test_doc_auto_generation.py -- ignore=tests/keras/legacy/layers_test.py -- cov-config .coveragerc -- cov=keras tests/ ; assert os.path.isdir ( os.path.join ( tmpdir , 'layers ' ) ) sources_dir = os.path.join ( keras_dir , 'docs ' , 'sources ' ) def test_docs_in_custom_destination_dir ( tmpdir ) : def generate ( ) : # Arguments `` `` '' Generates the markdown files for the documentation . `` `` '' PYTHONPATH= $ PWD : $ PYTHONPATH py.test -- pep8 -m pep8 -n0 & & py.test tests/test_documentation.py tests/test_doc_auto_generation.py ; import os generate ( ) def generate ( sources_dir ) : sources_dir : Where to put the markdown files . PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/ -- ignore=tests/integration_tests -- ignore=tests/test_documentation.py -- ignore=tests/keras/legacy/layers_test.py -- cov-config .coveragerc -- cov=keras tests/ ; assert os.listdir ( os.path.join ( tmpdir , 'examples ' ) ) PYTHONPATH= $ PWD : $ PYTHONPATH py.test -- pep8 -m pep8 -n0 & & py.test tests/test_documentation.py & & cd docs & & python autogen.py & & mkdocs build ; autogen.generate ( tmpdir )","['.travis.yml', 'docs/autogen.py', 'tests/test_doc_auto_generation.py']",Added the possibility to specify the destination directory in autogen.py ( # 12220 )
119,c55ce21fdd4f55709269140e068cc840a7c7ec48,2019-02-07 18:35:10+01:00,"A Set of Prerequisite Toy Tasks '' ] ( http : //arxiv.org/abs/1502.05698 ) http : //arxiv.org/abs/1502.05698 `` End-To-End Memory Networks '' , [ `` Towards AI-Complete Question Answering : ' '' Trains a memory network on the bAbI dataset . # Trains a memory network on the bAbI dataset . [ `` End-To-End Memory Networks '' ] ( http : //arxiv.org/abs/1503.08895 ) http : //arxiv.org/abs/1503.08895 Baby MemNN : examples/babi_memnn.md `` Towards AI-Complete Question Answering : A Set of Prerequisite Toy Tasks '' , ' ''","['docs/mkdocs.yml', 'examples/babi_memnn.py']",Added MarkDown formatting support to examples/babi_memnn.py ( # 12221 )
120,3fb9df96e03e4916e9db1c2594070bc6afa47f35,2019-02-06 16:25:43-08:00,"copy_examples ( os.path.join ( keras_dir , 'examples ' ) , open ( ' .. /examples/ ' + file , ' r+ ' ) as f_in : os.path.join ( sources_dir , 'examples ' ) ) for file in os.listdir ( examples_dir ) : pathlib.Path ( 'sources/examples ' ) .mkdir ( exist_ok=True ) open ( os.path.join ( examples_dir , file ) , ' r+ ' ) as f_in : destination_file = 'sources/examples/ ' + file [ : -2 ] + 'md ' copy_examples ( ) def copy_examples ( examples_dir , destination_dir ) : destination_file = os.path.join ( destination_dir , file [ : -2 ] + 'md ' ) def copy_examples ( ) : pathlib.Path ( destination_dir ) .mkdir ( exist_ok=True ) module_path = os.path.join ( examples_dir , file ) docstring , starting_line = get_module_docstring ( ' .. /examples/ ' + file ) docstring , starting_line = get_module_docstring ( module_path ) for file in os.listdir ( ' .. /examples ' ) :",['docs/autogen.py'],Avoiding hardcoding paths when generating examples in the docs . ( # 12211 )
121,7a4c2fcdadf8ddaab1cd7fb461fae58bfa36a6ea,2019-02-05 10:56:32-08:00,"def update_sub ( x , decrement ) : raise NotImplementedError reason='theano returns tuples for update ops ' ) x -= decrement def update_sub ( x , decrement ) : x = np.random.randn ( 3 , 4 ) K.eval ( K.update_add ( x_var , increment ) ) def test_update_add ( self ) : decrement = np.random.randn ( 3 , 4 ) x += increment def test_update_sub ( self ) : assert_allclose ( x , K.eval ( x_var ) , atol=1e-05 ) increment = np.random.randn ( 3 , 4 ) return C.assign ( x , result ) x_var = K.variable ( x ) K.eval ( K.update_sub ( x_var , decrement ) ) result = x - decrement","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",Add ` update_sub ` to CNTK backend ( # 12193 )
122,6368af9a5685266a094415eeb7c73e6d23f7d950,2019-02-04 15:30:22-08:00,"shutil.copyfile ( os.path.join ( keras_dir , 'CONTRIBUTING.md ' ) , path = os.path.join ( 'sources ' , page_name ) shutil.rmtree ( sources_dir ) with open ( os.path.join ( sources_dir , 'index.md ' ) , ' w ' ) as f : with open ( 'sources/index.md ' , ' w ' ) as f : shutil.copyfile ( ' .. /CONTRIBUTING.md ' , 'sources/contributing.md ' ) keras_dir = pathlib.Path ( __file__ ) .resolve ( ) .parents [ 1 ] readme = read_file ( os.path.join ( keras_dir , 'README.md ' ) ) path = os.path.join ( sources_dir , page_name ) readme = read_file ( ' .. /README.md ' ) shutil.copytree ( template_dir , sources_dir ) shutil.copytree ( 'templates ' , 'sources ' ) index = read_file ( 'templates/index.md ' ) sources_dir = os.path.join ( keras_dir , 'docs ' , 'sources ' ) shutil.rmtree ( 'sources ' ) if os.path.exists ( 'sources ' ) : os.path.join ( sources_dir , 'contributing.md ' ) ) index = read_file ( os.path.join ( template_dir , 'index.md ' ) ) if os.path.exists ( sources_dir ) : template_dir = os.path.join ( keras_dir , 'docs ' , 'templates ' )",['docs/autogen.py'],Used paths rather than hardcoded strings in autogen.py ( # 12147 )
123,a1397169ddf8595736c01fcea084c8e34e1a3884,2019-02-02 13:50:40+01:00,"http : //www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf ) . This is recommended in [ Jozefowicz et al . ( 2015 ) ] ( input mask for Embedding . shape for randomly generated keep/drop flags . the output of the layer ( its `` activation '' ) . unit_forget_bias : Boolean . handle_class_init ( name , member ) ( see [ regularizer ] ( .. /regularizers.md ) ) . sequence : a sequence function which yields data beta_2 : floats , 0 < beta < 1 . Generally close to 1 . # Arguments activity_regularizer : Regularizer function applied to saved ( ` model.save_weights ( filepath ) ` ) , else the full model is saved ( ` model.save ( filepath ) ` ) . mask : Either None ( indicating no masking ) or a Tensor indicating the Setting it to true will also force ` bias_initializer= '' zeros '' ` . beta_1/beta_2 : floats , 0 < beta < 1 . Generally close to 1 . def handle_class_init ( name , member ) : assert_args_presence ( init_args , member.__doc__ , member , name ) saved ( ` model.save_weights ( filepath ) ` ) , else the full model arg for arg in list ( inspect.signature ( member.__init__ ) .parameters.keys ( ) ) data_format : String , one of ` channels_first ` , ` channels_last ` . batch_size : size of batch of inputs to feed to the network is saved ( ` model.save ( filepath ) ` ) . If True , add 1 to the bias of the forget gate at initialization . optimizer : Selected optimizer for histograms computation . This is recommended in [ Jozefowicz et al . ( 2015 ) ] ( mask_value : Either None or mask value to skip Setting it to true will also force ` bias_initializer= '' zeros '' ` . noise_shape : A 1-D ` Tensor ` of type ` int32 ` , representing the if arg not in [ 'self ' , 'args ' , 'kwargs ' ] beta_1 : floats , 0 < beta < 1 . Generally close to 1 . http : //www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf ) . If True , add 1 to the bias of the forget gate at initialization . save_weights_only : if True , then only the model 's weights will be unit_forget_bias : Boolean . schedule_decay : floats , 0 < schedule_decay < 1 . for histograms computation . batch_size : size of batch of inputs to feed to the network ] save_weights_only : if True , then only the model 's weights will be init_args = [ name : Name given to the model weights : Initial weights to load in the Bidirectional model generator : a generator function which yields data","['keras/callbacks.py', 'keras/engine/sequential.py', 'keras/layers/core.py', 'keras/layers/cudnn_recurrent.py', 'keras/layers/embeddings.py', 'keras/layers/local.py', 'keras/layers/noise.py', 'keras/layers/wrappers.py', 'keras/optimizers.py', 'keras/utils/data_utils.py', 'tests/test_documentation.py']",Test init args in documentation ( # 12192 )
124,e59570ae26670f788d6c649191031e4a8824f955,2019-01-31 14:58:21+05:18,"are discarded and re-drawn . This is the recommended initializer for Keras is a model-level library , providing high-level building blocks for developing deep learning models . It does not handle itself low-level operations such as tensor products , convolutions and so on . Instead , it relies on a specialized , well-optimized tensor manipulation library to do so , serving as the `` backend engine '' of Keras . Rather than picking one single tensor library and making the implementation of Keras tied to that library , Keras handles the problem in a modular way , and several different backend engines can be plugged seamlessly into Keras . are discarded and redrawn . This is the recommended initializer for __Modularity.__ A model is understood as a sequence or a graph of standalone , fully configurable modules that can be plugged together with as few restrictions as possible . In particular , neural layers , cost functions , optimizers , initialization schemes , activation functions , regularization schemes are all standalone modules that you can combine to create new models . Keras is a model-level library , providing high-level building blocks for developing deep learning models . It does not handle low-level operations such as tensor products , convolutions and so on itself . Instead , it relies on a specialized , well optimized tensor manipulation library to do so , serving as the `` backend engine '' of Keras . Rather than picking one single tensor library and making the implementation of Keras tied to that library , Keras handles the problem in a modular way , and several different backend engines can be plugged seamlessly into Keras . __Modularity.__ A model is understood as a sequence or a graph of standalone , fully configurable modules that can be plugged together with as few restrictions as possible . In particular , neural layers , cost functions , optimizers , initialization schemes , activation functions and regularization schemes are all standalone modules that you can combine to create new models .","['README.md', 'docs/templates/backend.md', 'keras/initializers.py']","Fix grammar in README , backend docs and initializers docs ( # 12133 )"
125,f42d9e0179f11871179bc9ee4e8c138cd016612b,2019-01-29 10:33:27-08:00,"from .load_backend import abs from .load_backend import permute_dimensions from .load_backend import get_value from .load_backend import normalize_data_format from .load_backend import softmax from .load_backend import random_normal from .load_backend import all from .load_backend import pool3d def to_dense ( tensor ) : from .load_backend import update_sub supports_sparse = True from .load_backend import set_learning_phase from .load_backend import local_conv2d from .load_backend import logsumexp from .load_backend import spatial_3d_padding if backend ( ) == 'theano ' : elif backend ( ) == 'tensorflow ' : from .load_backend import relu from .load_backend import get_uid from .load_backend import ones_like from .load_backend import clear_session from .load_backend import to_dense from .load_backend import epsilon from .load_backend import local_conv1d from .load_backend import int_shape from .load_backend import normalize_batch_in_training from .load_backend import sparse_categorical_crossentropy from .load_backend import ctc_decode from .load_backend import exp def foldr ( fn , elems , initializer=None , name=None ) : from .load_backend import is_sparse from .load_backend import set_floatx from .load_backend import foldl from .load_backend import set_value from .load_backend import batch_dot from .load_backend import argmin from .load_backend import squeeze from .load_backend import zeros_like from .load_backend import eye from .load_backend import print_tensor from .common import floatx from .load_backend import batch_set_value from .load_backend import binary_crossentropy from .load_backend import random_uniform_variable from .load_backend import eval from .load_backend import pattern_broadcast from .load_backend import mean from .load_backend import minimum from .load_backend import placeholder else : from .load_backend import hard_sigmoid from .load_backend import reshape from .load_backend import stack from .load_backend import map_fn from .load_backend import * if K.backend ( ) == 'cntk ' : from .load_backend import not_equal from .load_backend import manual_variable_initialization from .load_backend import clip def ctc_label_dense_to_sparse ( labels , label_lengths ) : from .load_backend import set_epsilon from .load_backend import elu from .load_backend import switch from .load_backend import constant from .load_backend import softplus from .load_backend import conv1d from .load_backend import set_session def cumsum ( x , axis=0 ) : from .load_backend import pow from .load_backend import is_placeholder from .load_backend import zeros from .load_backend import in_train_phase from .load_backend import cumsum from .load_backend import less def foldl ( fn , elems , initializer=None , name=None ) : from .load_backend import l2_normalize from .load_backend import temporal_padding from .load_backend import resize_volumes from .load_backend import maximum from .load_backend import get_session from .load_backend import round from .load_backend import sign from .load_backend import batch_get_value from .load_backend import conv2d_transpose from .load_backend import conv2d from .load_backend import cast def cumprod ( x , axis=0 ) : from .load_backend import ndim from .load_backend import tanh from .load_backend import any def arange ( start , stop=None , step=1 , dtype='int32 ' ) : from .load_backend import gather from .load_backend import reverse from .load_backend import argmax from .load_backend import separable_conv2d from .load_backend import random_uniform from .load_backend import set_image_data_format supports_sparse = False from .load_backend import stop_gradient from .load_backend import categorical_crossentropy from .load_backend import floatx from .load_backend import bias_add from .load_backend import less_equal from .load_backend import cast_to_floatx from .load_backend import slice from .load_backend import reset_uids from .load_backend import update from .load_backend import dot from .load_backend import image_data_format from .load_backend import random_normal_variable from .load_backend import dropout def ctc_batch_cost ( y_true , y_pred , input_length , label_length ) : from .load_backend import max @ pytest.mark.skipif ( ( K.backend ( ) == 'cntk ' from .load_backend import conv3d_transpose or ( K.backend ( ) == 'theano ' and not K.th_sparse_module ) ) , from .load_backend import equal from .load_backend import sin from .load_backend import sigmoid from .load_backend import is_keras_tensor def update_sub ( x , decrement ) : from .load_backend import rnn from .load_backend import ctc_label_dense_to_sparse from .load_backend import identity from .load_backend import transpose from .load_backend import truncated_normal from .load_backend import depthwise_conv2d @ pytest.mark.skipif ( ( K.backend ( ) == 'cntk ' and K.dev.type ( ) == 0 ) , from .load_backend import moving_average_update from .load_backend import pool2d from .load_backend import square def map_fn ( fn , elems , name=None , dtype=None ) : from .load_backend import function from .load_backend import is_tensor from .load_backend import foldr from .load_backend import separable_conv1d from .load_backend import greater_equal from .load_backend import cos from .load_backend import prod from .load_backend import flatten from .load_backend import name_scope from .load_backend import gradients @ pytest.mark.skipif ( ( K.backend ( ) == 'cntk ' and K.dev.type ( ) == 0 ) , from .load_backend import variable from .load_backend import log from .load_backend import spatial_2d_padding from .load_backend import random_binomial from .load_backend import min from keras.backend import load_backend def ctc_decode ( y_pred , input_length , greedy=True , beam_width=100 , top_paths=1 ) : from .load_backend import in_top_k from .load_backend import repeat_elements from .load_backend import conv3d from .load_backend import greater from .load_backend import softsign from .load_backend import ctc_batch_cost from .load_backend import shape from .load_backend import tile from .load_backend import expand_dims def reset_uids ( ) : from .load_backend import in_test_phase elif K.backend ( ) == 'theano ' and not KTH.th_sparse_module : from .load_backend import one_hot from .load_backend import concatenate from .load_backend import batch_flatten from .load_backend import batch_normalization from .load_backend import sqrt from .load_backend import resize_images from .load_backend import repeat from .load_backend import dtype from .load_backend import var raise NotImplementedError from .load_backend import cumprod from .load_backend import backend from .load_backend import learning_phase from .load_backend import ones from .load_backend import count_params elif backend ( ) == 'cntk ' : from .load_backend import sum from .load_backend import std from keras.backend import floatx from .load_backend import update_add from .load_backend import arange","['keras/backend/__init__.py', 'keras/backend/cntk_backend.py', 'keras/backend/numpy_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py', 'tests/keras/layers/convolutional_test.py']",Cleaned the namespace of keras.backend . ( # 12076 )
126,3dbc940b9eb690e92fa6d7fe7ebf897a552a1c50,2019-01-29 10:28:42-08:00,last_output._uses_learning_phase = uses_learning_phase last_output._uses_learning_phase = uses_learning_phase [ 0 ] global uses_learning_phase uses_learning_phase [ 0 ] = True uses_learning_phase = [ False ] global uses_learning_phase uses_learning_phase = True uses_learning_phase = False,['keras/backend/tensorflow_backend.py'],Using a mutable variable instead of a global variable . ( # 12170 )
127,4aba316b7973e246af94ab5ace18fd5d5ffe4f81,2019-01-28 14:43:34-08:00,"backend_module = importlib.import_module ( _BACKEND ) `` `` '' Publicly accessible method _config = { } _keras_base_dir = '/tmp ' from .theano_backend import * sys.stderr.write ( 'Using ' + _BACKEND + ' backend.\n ' ) # Module is a valid backend if it has the required entries . if _backend : _backend = os.environ [ 'KERAS_BACKEND ' ] if _backend : with open ( _config_path , ' w ' ) as f : def backend ( ) : namespace [ k ] = v String , the name of the backend Keras is currently using . # Set backend based on KERAS_BACKEND flag , if applicable . except ImportError : assert _floatx in { 'float16 ' , 'float32 ' , 'float64 ' } # Check if valid backend . def backend ( ) : required_entries = [ 'placeholder ' , 'variable ' , 'function ' ] _keras_dir = os.environ.get ( 'KERAS_HOME ' ) from __future__ import print_function if not os.path.exists ( _config_path ) : from .common import image_data_format _keras_base_dir = '/tmp ' # Module is a valid backend if it has the required entries . _floatx = _config.get ( 'floatx ' , floatx ( ) ) 'backend ' : _BACKEND , _BACKEND = _backend _keras_dir = os.path.join ( _keras_base_dir , '.keras ' ) # Otherwise either ~/.keras or /tmp . sys.stderr.write ( 'Using TensorFlow backend.\n ' ) > > > keras.backend.backend ( ) assert data_keras_home == os.path.dirname ( load_backend._config_path ) sys.stderr.write ( 'Using CNTK backend\n ' ) from .tensorflow_backend import * if 'KERAS_BACKEND ' in os.environ : _config_path = os.path.expanduser ( os.path.join ( _keras_dir , 'keras.json ' ) ) reload_module ( load_backend ) from .cntk_backend import * from .common import floatx if not os.access ( _keras_base_dir , os.W_OK ) : if e not in entries : return _BACKEND 'tensorflow ' sys.stderr.write ( 'Using Theano backend.\n ' ) sys.stderr.write ( 'Using TensorFlow backend.\n ' ) except ValueError : 'image_data_format ' : image_data_format ( ) else : # Try and load external backend . `` ` python from .theano_backend import * f.write ( json.dumps ( _config , indent=4 ) ) 'tensorflow ' for determining the current backend . else : } 'epsilon ' : epsilon ( ) , # Save config file , if possible . namespace = globals ( ) from .common import set_epsilon with open ( _config_path ) as f : _config = { _BACKEND = 'tensorflow ' # Import backend functions . _floatx = _config.get ( 'floatx ' , floatx ( ) ) # Import backend functions . _BACKEND = _backend import sys _config = { _BACKEND = 'tensorflow ' _BACKEND = _backend for k , v in entries.items ( ) : from .common import normalize_data_format import importlib _epsilon = _config.get ( 'epsilon ' , epsilon ( ) ) # in multi-threaded environments . # Set Keras base dir path given KERAS_HOME env variable , if applicable . if k not in namespace : if os.path.exists ( _config_path ) : entries = backend_module.__dict__ if k not in namespace : raise ValueError ( 'Invalid backend . Missing required entry : ' + e ) sys.stderr.write ( 'Using CNTK backend\n ' ) assert data_keras_home == os.path.dirname ( K._config_path ) 'floatx ' : floatx ( ) , import json # Set backend based on KERAS_BACKEND flag , if applicable . from __future__ import print_function _config = { } image_data_format ( ) ) try : # Except permission denied . raise ValueError ( 'Unable to import backend : ' + str ( _BACKEND ) ) backend_module = importlib.import_module ( _BACKEND ) 'floatx ' : floatx ( ) , # Default backend : TensorFlow . # Attempt to read Keras config file . _keras_dir = os.path.join ( _keras_base_dir , '.keras ' ) f.write ( json.dumps ( _config , indent=4 ) ) from .common import set_floatx from .common import floatx except IOError : `` ` from .common import set_image_data_format return _BACKEND with open ( _config_path , ' w ' ) as f : `` ` import sys String , the name of the backend Keras is currently using . # Default backend : TensorFlow . set_floatx ( _floatx ) assert _image_data_format in { 'channels_last ' , 'channels_first ' } from .common import normalize_data_format if not os.path.exists ( _config_path ) : elif _BACKEND == 'tensorflow ' : raise ValueError ( 'Unable to import backend : ' + str ( _BACKEND ) ) from .common import cast_to_floatx except OSError : # Except permission denied . namespace [ k ] = v from __future__ import absolute_import if _BACKEND == 'cntk ' : import os # Check if valid backend . 'image_data_format ' : image_data_format ( ) if not os.path.exists ( _keras_dir ) : # in multi-threaded environments . set_image_data_format ( _image_data_format ) _config_path = os.path.expanduser ( os.path.join ( _keras_dir , 'keras.json ' ) ) for e in required_entries : # Returns # Otherwise either ~/.keras or /tmp . import importlib _image_data_format = _config.get ( 'image_data_format ' , if 'KERAS_HOME ' in os.environ : set_floatx ( _floatx ) `` `` '' namespace = globals ( ) # Example for e in required_entries : entries = backend_module.__dict__ `` `` '' for determining the current backend . from .common import cast_to_floatx from .common import epsilon _keras_dir = os.environ.get ( 'KERAS_HOME ' ) image_data_format ( ) ) _config = json.load ( f ) except OSError : elif _BACKEND == 'theano ' : # Returns if e not in entries : _backend = _config.get ( 'backend ' , _BACKEND ) _backend = os.environ [ 'KERAS_BACKEND ' ] # Set Keras base dir path given KERAS_HOME env variable , if applicable . elif _BACKEND == 'theano ' : from .common import set_image_data_format from .cntk_backend import * assert _image_data_format in { 'channels_last ' , 'channels_first ' } assert _floatx in { 'float16 ' , 'float32 ' , 'float64 ' } from .common import image_data_format raise ValueError ( 'Invalid backend . Missing required entry : ' + e ) set_epsilon ( _epsilon ) set_epsilon ( _epsilon ) `` ` python try : # Try and load external backend . if 'KERAS_HOME ' in os.environ : assert isinstance ( _epsilon , float ) if 'KERAS_BACKEND ' in os.environ : 'epsilon ' : epsilon ( ) , # Attempt to read Keras config file . pass except IOError : sys.stderr.write ( 'Using ' + _BACKEND + ' backend.\n ' ) reload_module ( K ) from keras.backend import load_backend 'backend ' : _BACKEND , if os.path.exists ( _config_path ) : # Make sure we do n't override any entries from common , such as epsilon . os.makedirs ( _keras_dir ) _BACKEND = _backend except ValueError : assert isinstance ( _epsilon , float ) # Example # Make sure we do n't override any entries from common , such as epsilon . # Save config file , if possible . from .load_backend import * _keras_base_dir = os.path.expanduser ( '~ ' ) `` `` '' Publicly accessible method # Except permission denied and potential race conditions from .common import epsilon required_entries = [ 'placeholder ' , 'variable ' , 'function ' ] set_image_data_format ( _image_data_format ) if not os.access ( _keras_base_dir , os.W_OK ) : import os os.makedirs ( _keras_dir ) from .common import set_epsilon for k , v in entries.items ( ) : _keras_base_dir = os.path.expanduser ( '~ ' ) from __future__ import absolute_import from .common import set_floatx elif _BACKEND == 'tensorflow ' : with open ( _config_path ) as f : import json _image_data_format = _config.get ( 'image_data_format ' , _epsilon = _config.get ( 'epsilon ' , epsilon ( ) ) if _BACKEND == 'cntk ' : sys.stderr.write ( 'Using Theano backend.\n ' ) _config = json.load ( f ) from .tensorflow_backend import * # Except permission denied and potential race conditions if not os.path.exists ( _keras_dir ) : } except ImportError : > > > keras.backend.backend ( ) pass _backend = _config.get ( 'backend ' , _BACKEND )","['keras/backend/__init__.py', 'keras/backend/load_backend.py', 'tests/keras/utils/data_utils_test.py']",Moved the backend __init__ into another file . ( # 12086 )
128,59fd3ce3814c62882b540cb6098e9c37f05ceb56,2019-01-28 13:17:47-08:00,python : 3.6 env : KERAS_BACKEND=tensorflow TEST_MODE=INTEGRATION_TESTS PIL=Pil python : 2.7 env : KERAS_BACKEND=tensorflow TEST_MODE=INTEGRATION_TESTS PIL=Pillow,['.travis.yml'],Moving the integration tests to python3 . ( # 12154 )
129,9b67e2b699da8d9ced9af3597906b81ad6f106bc,2019-01-28 11:23:55-08:00,including tests for the new feature you added . name : a ) Bug fix * * - How you can verify it * * > < ! -- You need a good justification for not including tests for the bug you fixed . -- > https : //github.com/keras-team/keras/blob/master/CONTRIBUTING.md assignees : fchollet name : b ) New feature * * - How to verify it * * < ! This pull request fixes # issue_number_here . about : Select this if you 're fixing a bug in Keras . Please make sure you 've read and understood our contributing guidelines ; * * - How I did it * * about : Select this if you 're adding a new feature to Keras . * * - How I fixed it * * You need a good justification for not * * - What bug I fixed * * < ! labels : feature * * - What I did * * This pull request closes # issue_number_here . [ ] I updated the docs . labels : bugfix,"['.github/PULL_REQUEST_TEMPLATE/a--bug-fix.md', '.github/PULL_REQUEST_TEMPLATE/b--new-feature.md', 'PULL_REQUEST_TEMPLATE.md']",Added pull request templates . ( # 12135 )
130,55e7875354416c04719eb60c7db2bac78f271df4,2019-01-28 12:33:16-05:00,"new_subdir = subdir.replace ( 'templates ' , 'sources ' ) if fname [ -3 : ] == '.md ' : fpath = os.path.join ( subdir , fname ) for subdir , dirs , fnames in os.walk ( 'templates ' ) : os.makedirs ( new_subdir ) new_fpath = fpath.replace ( 'templates ' , 'sources ' ) for fname in fnames : shutil.copytree ( 'templates ' , 'sources ' ) shutil.copy ( fpath , new_fpath ) if not os.path.exists ( new_subdir ) :",['docs/autogen.py'],Using shutil rather than copying the tree manually in autogen.py ( # 12146 )
131,ec41989421743aa5c11c2ef47b9d0757fdfd5e36,2019-01-27 17:19:14+05:18,"travis_retry pip install -- only-binary=numpy , scipy , pandas numpy==1.15.4 nose scipy h5py theano pytest pytest-pep8 pandas -- progress-bar off # TODO : remove the freeze of the numpy version once the next theano version is out on pypi . travis_retry pip install -- only-binary=numpy , scipy , pandas numpy nose scipy h5py theano pytest pytest-pep8 pandas -- progress-bar off",['.travis.yml'],Unpin numpy . ( # 12142 )
132,2e1a3816676724576f75ce76b18ed3ba2fdea670,2019-01-24 22:53:19+01:00,The [ keras.preprocessing ] ( https : //github.com/keras-team/keras-preprocessing ) module has been splitted from the main Keras repository . Please submit your issue using this [ link ] ( https : //github.com/keras-team/keras-applications/issues/new ) . about : Select this if your issue is related to the ` keras.applications ` module . Please submit your issue using this [ link ] ( https : //github.com/keras-team/keras-preprocessing/issues/new/choose ) . about : Select this if your issue is related to the ` keras.preprocessing ` module . name : d ) ` keras.preprocessing ` users name : e ) ` keras.applications ` users The [ keras.applications ] ( https : //github.com/keras-team/keras-applications ) module has been splitted from the main Keras repository . Thank you !,"['.github/ISSUE_TEMPLATE/d--keras-preprocessing-users.md', '.github/ISSUE_TEMPLATE/e--keras-applications-users.md']",Add issue template that redirect to KP and KA ( # 12115 )
133,328df7303425c2e3bf4274bb1eb0a2040bab5d8c,2019-01-25 01:27:51+05:18,"return x assert o._input_dtypes == 'float16 ' self._input_dtypes = K.dtype ( inputs ) _ = o ( i ) self._input_dtypes = None x = K.placeholder ( shape=input_shape ) o = layers.Lambda ( func ) self._input_dtypes = [ K.dtype ( x ) for x in inputs ] for shape , dtype in zip ( input_shape , self._input_dtypes ) ] i = layers.Input ( shape= ( 3 , 2 , 1 ) , dtype='float16 ' ) else : raise TypeError ( ' x dtype is not float16 , it is ' , K.dtype ( x ) ) if isinstance ( inputs , list ) : if K.dtype ( x ) ! = 'float16 ' : def test_dtypes ( ) : test_dtypes ( ) x = K.placeholder ( shape=input_shape , dtype=self._input_dtypes ) xs = [ K.placeholder ( shape=shape ) for shape in input_shape ] def func ( x ) : from .. utils import conv_utils xs = [ K.placeholder ( shape=shape , dtype=dtype )","['keras/layers/core.py', 'tests/keras/layers/core_test.py']",Keep input types with Lambda layers ( # 12122 )
134,c2dc4049c16ab8cbc215bb0ae80cd94a3d9542f5,2019-01-24 14:55:47-05:00,"line = next ( f_in ) def get_module_docstring ( filepath ) : f_out.write ( ' `` ` ' ) destination_file = 'sources/examples/ ' + file [ : -2 ] + 'md ' for file in os.listdir ( ' .. /examples ' ) : f_out.write ( line ) if line ! = '\n ' : `` `` '' f_out.write ( docstring + '\n\n ' ) continue ' '' return docstring , co.co_firstlineno # next line might be empty . Also finds the line at which the docstring ends . docstring = `` for _ in range ( starting_line ) : Addition RNN : examples/addition_rnn.md if not file.endswith ( '.py ' ) : if co.co_consts and isinstance ( co.co_consts [ 0 ] , str ) : `` `` '' Copy the examples directory in the documentation . docstring = co.co_consts [ 0 ] os.makedirs ( 'sources/examples ' , exist_ok=True ) Will make the files pretty by extracting the docstrings written in markdown . print ( 'Could not get the docstring from ' + filepath ) co = compile ( open ( filepath ) .read ( ) , filepath , 'exec ' ) `` `` '' Extract the module docstring . f_out.write ( ' `` ` python\n ' ) for line in f_in : open ( ' .. /examples/ ' + file , ' r+ ' ) as f_in : # skip docstring # copy the rest of the file . Baby RNN : examples/babi_rnn.md copy_examples ( ) ' '' An implementation of sequence to sequence learning for performing addition # Trains two recurrent neural networks based upon a story and a question . # # # Notes else : # An implementation of sequence to sequence learning for performing addition docstring , starting_line = get_module_docstring ( ' .. /examples/ ' + file ) def copy_examples ( ) : ' '' Trains two recurrent neural networks based upon a story and a question . # Notes with open ( destination_file , ' w+ ' ) as f_out , \ Examples : next ( f_in )","['docs/autogen.py', 'docs/mkdocs.yml', 'examples/addition_rnn.py', 'examples/babi_rnn.py']",Displayed some examples in the docs . ( # 11758 )
135,e4e342f70dec046fdb1a092e5a12c565f37b9ddc,2019-01-23 14:25:49-08:00,"from .convolutional_recurrent import ConvLSTM2D from .convolutional import Conv2DTranspose from .core import ActivityRegularization from .merge import Maximum from .merge import Subtract from .convolutional import UpSampling2D from .pooling import GlobalMaxPooling2D from .. legacy.layers import Recurrent from .pooling import GlobalAveragePooling2D from .core import Dense from .convolutional import Convolution2D from .convolutional import Deconvolution2D from .pooling import GlobalMaxPool1D from .recurrent import GRUCell from .advanced_activations import ReLU from .noise import AlphaDropout from .convolutional import Conv3DTranspose from .convolutional import ZeroPadding1D from .core import Permute from .core import RepeatVector from .convolutional import SeparableConv2D from .core import Lambda from .merge import minimum from .recurrent import GRU from .pooling import AveragePooling2D layers.Minimum , from .cudnn_recurrent import * from .. legacy.layers import MaxoutDense from .convolutional_recurrent import ConvLSTM2DCell from .advanced_activations import ELU from .. legacy.layers import ConvRecurrent2D from .merge import Concatenate from .local import LocallyConnected2D from .pooling import AveragePooling1D from .core import Dropout layers.minimum , from .pooling import GlobalMaxPool2D from .pooling import * from .noise import * from .advanced_activations import Softmax from .merge import Add from .convolutional import Cropping1D from .pooling import GlobalMaxPooling3D from .pooling import MaxPooling1D from .core import Flatten from .core import Activation from .convolutional import ZeroPadding2D from .pooling import MaxPool3D from .core import * from .convolutional_recurrent import * from .merge import multiply from .convolutional import DepthwiseConv2D from .merge import average # Legacy imports from .merge import concatenate from .advanced_activations import LeakyReLU from .merge import subtract from .pooling import MaxPool1D layers.ConvLSTM2DCell , from .advanced_activations import ThresholdedReLU from .convolutional import UpSampling3D from .convolutional import Cropping2D from .merge import Multiply from .merge import maximum from .core import Reshape from .pooling import GlobalMaxPooling1D from .pooling import GlobalAveragePooling3D from .. legacy.layers import Highway from .convolutional import * from .. legacy.layers import AtrousConvolution2D from .noise import GaussianDropout from .pooling import MaxPool2D from .pooling import AvgPool1D from .embeddings import Embedding # Aliases ( not in the docs ) from .merge import Dot from .recurrent import RNN from .wrappers import * from .pooling import GlobalAvgPool1D from .core import SpatialDropout3D from .merge import add from .merge import Minimum from .recurrent import LSTMCell from .pooling import AvgPool2D from .convolutional import UpSampling1D from .merge import Average from .convolutional import SeparableConv1D from .pooling import GlobalAvgPool2D from .pooling import GlobalAvgPool3D from .core import SpatialDropout1D from .. legacy.layers import * from .merge import * from .pooling import AvgPool3D from .pooling import MaxPooling3D from .convolutional import Conv2D from .convolutional import Cropping3D from .normalization import BatchNormalization from .wrappers import Bidirectional from .local import LocallyConnected1D from .recurrent import SimpleRNN from .pooling import GlobalMaxPool3D from .. legacy.layers import AtrousConvolution1D from .local import * from .recurrent import * from .advanced_activations import * from .pooling import AveragePooling3D from .convolutional import Conv3D from .embeddings import * from .normalization import * from .noise import GaussianNoise from .recurrent import StackedRNNCells from .wrappers import TimeDistributed from .advanced_activations import PReLU from .cudnn_recurrent import CuDNNGRU from .recurrent import LSTM from .convolutional import Convolution1D from .pooling import GlobalAveragePooling1D from .convolutional import Conv1D from .convolutional import Deconvolution3D from .cudnn_recurrent import CuDNNLSTM from .pooling import MaxPooling2D from .recurrent import SimpleRNNCell from .convolutional import Convolution3D from .convolutional import ZeroPadding3D from .merge import dot from .core import Masking from .core import SpatialDropout2D","['docs/structure.py', 'keras/layers/__init__.py']",Cleaning the keras.layers namespace . ( # 12056 )
136,4185cbb50bfcae9cc30b0fc7b67e81d67a50a8ac,2019-01-23 13:56:40-08:00,"my_tf_optimizer = MyTfOptimizer ( use_locking=False , name='MyTfOptimizer ' ) class MyTfOptimizer ( train.Optimizer ) : kernel_constraint=constraints.MaxNorm ( 1 ) ) ) from keras import constraints model.add ( Dense ( num_classes , input_shape= ( 3 , ) , optimizer = optimizers.TFOptimizer ( my_tf_optimizer ) def apply_gradients ( self , grads_and_vars , * * kwargs ) : return super ( MyTfOptimizer , self ) .compute_gradients ( loss , * * kwargs ) model.fit ( np.random.random ( ( 5 , 3 ) ) , np.random.random ( ( 5 , num_classes ) ) , from tensorflow import train wrapping_optimizer = train.AdamOptimizer ( ) * * kwargs ) return self.wrapping_optimizer.apply_gradients ( grads_and_vars , def compute_gradients ( self , loss , * * kwargs ) : model.compile ( loss='mean_squared_error ' , optimizer=optimizer ) grads = self.optimizer.compute_gradients ( loss , var_list=params ) def test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer ( ) : epochs=1 , batch_size=5 , verbose=0 ) grads = self.optimizer.compute_gradients ( loss , params ) reason='Requires TensorFlow backend ' ) model = Sequential ( )","['keras/optimizers.py', 'tests/keras/optimizers_test.py']",Use var_list argument in TFOptimizer wrapper ( # 12106 )
137,a6c8042121371b5873773ca767f28cdf5689d5e4,2019-01-22 14:53:39-05:00,"def on_test_begin ( self , logs=None ) : self.val_runs = 0 raise ValueError ( ' ` validation_freq ` must be an Integer or ' assert val_counter.val_runs == 2 validation_freq : Only relevant if validation data is provided . Integer integer , specifies how many training epochs to run before a new from .training_utils import check_num_samples validation_freq=validation_freq ) validation_freq= [ 4 , 2 , 2 , 1 ] , val_outs = test_loop ( model , val_function , val_inputs , callbacks=callbacks , ValueError : if ` validation_freq ` is an Integer and less than 1 , or if for l , o in zip ( out_labels , val_outs ) : Raises : verbose=0 ) Returns : validation run is performed , e.g . ` validation_freq=2 ` runs for l , o in zip ( out_labels , val_outs ) : # Same labels assumed . val_counter = ValCounter ( ) validation_freq=2 , if batch_index == len ( batches ) - 1 : # Last batch . ' ` collections.Container ` ( e.g . list , tuple , etc . ) ' ) validation_steps=validation_steps , epoch_logs [ 'val_ ' + l ] = o it is neither an Integer nor a Sequence . # Test in training_generator.py validation_steps=None ) : Bool , True if validation should be run . x , if do_validation : model.fit_generator ( while True : model.compile ( 'sgd ' , 'mse ' ) validation_freq=1 , val_outs = test_loop ( model , val_function , val_inputs , # ` epoch ` is 0-indexed internally but 1-indexed in the public API . callbacks=callbacks , class ValCounter ( Callback ) : if ( steps_done > = steps_per_epoch and import collections val_outs = to_list ( val_outs ) should_run_validation ( validation_freq , epoch ) ) : or ` collections.Container ` instance ( e.g . list , tuple , etc. ) . If an model = Sequential ( [ Dense ( 1 ) ] ) validation_steps=None , # Test in training_arrays.py validation_data= ( x , y ) , epochs to run before a new validation run is performed , e.g . batch_size=2 , def __init__ ( self ) : validation_freq : Only relevant if validation data is provided . Integer specifies the epochs on which to run validation . of the 1st , 2nd , and 10th epochs . ` validation_freq=2 ` runs validation every 2 epochs . If a list , if not isinstance ( validation_freq , collections.Container ) : x , y = np.ones ( ( 10 , 10 ) ) , np.ones ( ( 10 , 1 ) ) verbose=0 ) one_indexed_epoch = epoch + 1 validation_freq=1 ) : `` `` '' tuple , or set , specifies the epochs on which to run validation , epochs to run before a new validation run is performed . If a list , e.g . ` validation_freq= [ 1 , 2 , 10 ] ` runs validation at the end steps_per_epoch=5 , validation_freq=validation_freq , assert val_counter.val_runs == 3 batch_size=batch_size , e.g . ` validation_freq= [ 1 , 2 , 10 ] ` runs validation at the end if validation_freq < 1 : raise ValueError ( ' ` validation_freq ` can not be less than 1 . ' ) self.val_runs += 1 yield np.ones ( ( 2 , 10 ) ) , np.ones ( ( 2 , 1 ) ) y , callbacks= [ val_counter ] ) do_validation and validation_freq=2 ` runs validation every 2 epochs . If a list , def should_run_validation ( validation_freq , epoch ) : batch_size=batch_size , if batch_index == len ( batches ) - 1 : # Last batch . if do_validation and should_run_validation ( validation_freq , epoch ) : `` `` '' Checks if validation should be run this epoch . validation at the end of the 1st , 2nd , and 10th epochs . validation_freq : Integer or list . If an integer , specifies how many training epoch_logs [ 'val_ ' + l ] = o epochs to run before a new validation run is performed , e.g . _gen ( ) , or list/tuple/set . If an integer , specifies how many training validation_freq=1 , def test_validation_freq ( ) : return one_indexed_epoch % validation_freq == 0 return one_indexed_epoch in validation_freq from .training_utils import check_num_samples which to run validation , e.g . ` validation_freq= [ 1 , 2 , 10 ] ` runs if do_validation : val_outs = to_list ( val_outs ) def _gen ( ) : Arguments : model.fit ( if steps_done > = steps_per_epoch and do_validation : if isinstance ( validation_freq , int ) : validation_steps=validation_steps ) or list/tuple/set . If an integer , specifies how many training from .training_utils import should_run_validation # Same labels assumed . validation every 2 epochs . If a Container , specifies the epochs on tuple , or set , specifies the epochs on which to run validation , of the 1st , 2nd , and 10th epochs . epochs=4 , epoch : Integer , the number of the training epoch just completed . if do_validation and should_run_validation ( validation_freq , epoch ) : validation_freq=1 ,","['keras/engine/training.py', 'keras/engine/training_arrays.py', 'keras/engine/training_generator.py', 'keras/engine/training_utils.py', 'tests/keras/engine/test_training.py']",Add an optional ` validation_freq ` argument to ` fit ` ( # 12065 )
138,d78c982b326adeed6ac25200dc6892ff8f518ca6,2019-01-22 20:19:38+01:00,"x_val = np.random.random ( ( 10 , ) + input_shape ) # If all nodes processed remove the layer Aout1 = A ( r1 ) raise LookupError for node_data in unprocessed_nodes.pop ( layer ) : process_node ( layer , node_data ) r1 = layers.Reshape ( ( 12 , ) ) ( input_layer ) unprocessed_nodes [ layer ] = node_data_list [ node_index : ] ValueError : For incorrect layer config # Create lits of input and output tensors and return new class M2 = Model.from_config ( config ) del unprocessed_nodes [ layer ] input_layer = Input ( shape=input_shape ) # If the node does not have all inbound layers A = Dense ( 12 , name='layer_a ' ) weights = M.get_weights ( ) layer : layer object # Raise an error if the corresponding layer node node_index = 0 # Note : if the order of the layers in the concat is process_node ( layer , node_data ) output = Dense ( 2 , name='layer_b ' ) ( c1 ) config = M.get_config ( ) # Process all nodes in layer , if not yet processed input_shape = ( 1 , 12 ) c1 = layers.concatenate ( [ Aout2 , Aout1 ] ) # Process nodes in order node_data : Node data specifying layer call output_val_2 = M2.predict ( x_val ) node_data : List of layer configs try : while node_index < len ( node_data_list ) : # available , stop processing and continue later M2.set_weights ( weights ) output_val = M.predict ( x_val ) `` `` '' Add node to layer list `` `` '' # has not yet been created `` `` '' Reconstruct node by linking to inbound layers # If not all nodes processed then store unprocessed nodes # It occurs with layer sharing at heterogeneous depth when return else : LookupError : If layer required is not found Args : node_index += 1 if node_index < len ( node_data_list ) : np.testing.assert_allclose ( output_val , output_val_2 , atol=1e-6 ) layer : Layer to process except LookupError : add_unprocessed_node ( layer , node_data ) def test_layer_sharing_at_heterogeneous_depth_order ( ) : r2 = layers.Reshape ( ( 12 , ) ) ( A ( input_layer ) ) Raises : # the order that occurs in the config . M = Model ( inputs=input_layer , outputs=output ) # changed to ( [ Aout1 , Aout2 ] ) the bug does n't trigger break Aout2 = A ( r2 ) node_data = node_data_list [ node_index ] node_data_list = unprocessed_nodes [ layer ] # the layers need to be applied in an order that differs from # This tests for the bug in this issue # https : //github.com/keras-team/keras/issues/11159","['keras/engine/network.py', 'tests/keras/engine/test_topology.py']",Fixed loading bug for shared layers accross multiple depths ( # 11588 )
139,1400e7b355366a35eb35250c1e6191d4deb21a85,2019-01-21 22:37:05+01:00,"'to the batch update ( % f ) . Check your callbacks . ' delta_t_median ) 'to the batch update ( % f ) . Check your callbacks . ' , hook_name , % ( hook_name , delta_t_median ) , RuntimeWarning )",['keras/callbacks.py'],fixed callback warning error ( # 12098 )
140,8c9de8349ff4498c83f72fdcf5e432a6975d9722,2019-01-21 10:55:32+01:00,# Arguments ' Please consider using the ` keras.utils.Sequence ' # Raises # Argument : # Returns : # Returns # Returns : # Returns # Arguments # Arguments : # Arguments : # Argument ' Please consider using the ` keras.utils.Sequence ' # Raises :,"['examples/variational_autoencoder.py', 'examples/variational_autoencoder_deconv.py', 'keras/engine/network.py', 'keras/engine/sequential.py', 'keras/engine/training_generator.py', 'keras/utils/generic_utils.py']",Formatting fixes ( # 12090 )
141,844c1b44b75bc5345c26d19a8fa4161f5b00e99a,2019-01-21 09:31:25+01:00,"if not set ( metrics.keys ( ) ) .issubset ( set ( output_names ) ) : output_metrics = training_utils.collect_metrics ( input_metrics , training_utils.collect_metrics ( { 'unknown_layer ' : 'mse ' } , [ 'layer_1 ' ] ) ( None , [ [ ] , [ ] ] ) , training_utils.collect_metrics ( { ' a ' , 'set ' , 'type ' } , [ ] ) .format ( unknown_output_names , output_names ) ) def test_collect_metrics ( input_metrics , expected_output ) : with pytest.warns ( Warning ) as w : def test_collect_metrics_with_invalid_metrics_format ( ) : assert warning_raised , 'Warning was raised for unknown_layer ' ( { 'layer_1 ' : 'mae ' , 'layer_2 ' : 'mse ' } , [ [ 'mae ' ] , [ 'mse ' ] ] ) , def test_collect_metrics_with_invalid_layer_name ( ) : assert output_metrics == expected_output with pytest.raises ( TypeError ) : warning_raised = all ( [ 'unknown_layer ' in str ( w_.message ) for w_ in w ] ) ( [ 'mse ' , 'mae ' ] , [ [ 'mse ' , 'mae ' ] , [ 'mse ' , 'mae ' ] ] ) , warnings.warn ( 'Invalid layer name for metric computations : ' output_names = [ 'layer_1 ' , 'layer_2 ' ] output_names ) unknown_output_names = list ( set ( metrics.keys ( ) ) - set ( output_names ) ) ] ) ' { } . Available names are { } . '","['keras/engine/training_utils.py', 'tests/keras/engine/test_training.py']",Add warnings when trying to compute metrics for an unknown layer ( # 12071 )
142,48a8fa0e0dcb7b06af113c8f5dd47fa5190407ca,2019-01-20 15:51:04-05:00,if K._BACKEND == 'tensorflow ' : if K.backend ( ) == 'theano ' : if K.backend ( ) == 'tensorflow ' : if K._BACKEND == 'theano ' :,"['tests/keras/backend/backend_test.py', 'tests/keras/engine/test_topology.py']",Used K.backend ( ) instead of K._BACKEND . ( # 12084 )
143,34bce5bac9f434dc501f4cbff37b44680271a337,2019-01-15 09:39:20-08:00,"set ` x [ : , 3 , : ] = 0. ` and ` x [ : , 5 , : ] = 0. ` For each timestep in the input tensor ( dimension # 1 in the tensor ) , in all downstream layers ( as long as they support masking ) . because you lack features for these sample timesteps . You can do : set ` x [ 0 , 3 , : ] = 0. ` and ` x [ 2 , 5 , : ] = 0. ` If all features for a given sample timestep are equal to ` mask_value ` , these timesteps . You can : You want to mask timestep # 3 and # 5 because you lack data for if all values in the input tensor at that timestep then the sample timestep will be masked ( skipped ) in all downstream layers You want to mask sample # 0 at timestep # 3 , and sample # 2 at timestep # 5 , are equal to ` mask_value ` , then the timestep will be masked ( skipped ) ( as long as they support masking ) .",['keras/layers/core.py'],Fix documentation for Masking layer ( # 12029 )
144,a96d8709fba51b70a08daeea12529948fa800830,2019-01-14 13:18:53-05:00,"cond_float = cond_float [ ... , np.newaxis ] cond_float = cond_float [ ... , None ] { { np_implementation } }","['keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py']",Added the numpy implementation of switch to the docs . ( # 11575 )
145,041814193dc9f0490fc2d93db830d64eb2b2d28b,2019-01-12 15:06:02+01:00,It uses as initial state the state vectors from the encoder . which will be filled with the values of ` epoch ` and Is uses as initial state the state vectors from the encoder . which will be filled the value of ` epoch ` and,"['examples/lstm_seq2seq.py', 'keras/callbacks.py']",Typo fixes ( # 12030 )
146,fd537c7962edf563a3bb14526de9ce57dd26c341,2019-01-11 14:24:50-08:00,"expected_out = np.repeat ( expected_out , length_col , axis=3 ) [ 'channels_first ' , 'channels_last ' ] ) size= ( length_dim1 , length_dim2 , length_dim3 ) , assert_allclose ( np_output [ : , : , 2 : -2 , 2 : -2 , 2 : -2 ] , 1 . ) for axis in [ 1 , -1 ] : for dim3_offset in [ -1 , -2 ] : kwargs= { 'size ' : ( 2 , 2 ) , 'data_format ' : data_format } , if data_format == 'channels_last ' : assert np_output.shape [ 2 ] == length_row * input_num_row assert_allclose ( np_output [ : , : , dim2_offset , : , : ] , 0 . ) assert_allclose ( np_output [ : , : , right_offset , : ] , 0 . ) data_format=data_format ) for dim1_offset in [ 0 , -1 , -2 ] : else : # tf model = Sequential ( [ Embedding ( assert_allclose ( np_output [ : , : , bottom_offset , : ] , 0 . ) assert_allclose ( np_output [ : , offset , : , : , : ] , 0 . ) expected_out = np.repeat ( expected_out , length_col , axis=3 ) data_format=data_format ) input_num_col ) assert_allclose ( np_output [ : , : , : , : , offset ] , 0 . ) expected_out = np.repeat ( inputs , length_row , axis=2 ) [ ( ( 8 , 1 ) , ( 3 , 2 , 4 ) ) , def test_embedding_invalid ( ) : expected_out = np.repeat ( inputs , length_dim1 , axis=2 ) layer = convolutional.ZeroPadding2D ( padding= ( ( 1 , 2 ) , ( 3 , 4 ) ) , layer.build ( inputs.shape ) kwargs= { 'size ' : ( 2 , 2 ) , 'data_format ' : data_format } , assert_allclose ( np_output [ : , dim1_offset , : , : , : ] , 0 . ) assert np_output.shape [ 1 ] == length_dim1 * input_len_dim1 for length_dim3 in [ 3 ] : def test_zero_padding_2d_correctness ( data_format ) : assert_allclose ( np_output [ : , dim1_offset , : , : , : ] , 0 . ) input_shape= ( None , None , 4 ) ) assert np_output.shape [ 2 ] == length_dim2 * input_len_dim2 expected_out = np.repeat ( expected_out , length_dim2 , axis=2 ) outputs = layer ( K.variable ( inputs ) ) data_format=data_format ) if data_format == 'channels_first ' : expected_out = np.repeat ( expected_out , length_col , axis=2 ) for top_offset in [ 0 ] : size= ( length_dim1 , length_dim2 , length_dim3 ) , np_output = K.eval ( outputs ) for dim1_offset in [ 0 , -1 , -2 ] : layer_test ( layers.Softmax , kwargs= { 'axis ' : axis } , else : # tf assert_allclose ( np_output [ : , : , 1 : -2 , 3 : -4 ] , 1 . ) assert_allclose ( np_output [ : , 2 : -2 , 2 : -2 , 2 : -2 , : ] , 1 . ) data_format=data_format ) assert_allclose ( np_output [ : , offset , : , : ] , 0 . ) assert_allclose ( np_output [ : , : , : , dim2_offset , : ] , 0 . ) if data_format == 'channels_first ' : inputs = np.random.rand ( num_samples , outputs = layer ( K.variable ( inputs ) ) for length_dim1 in [ 2 , 3 ] : assert_allclose ( np_output [ : , : , dim1_offset , : , : ] , 0 . ) assert np_output.shape [ 4 ] == length_dim3 * input_len_dim3 layer_test ( convolutional.UpSampling2D , # basic test assert np_output.shape [ 2 ] == length_col * input_num_col input_len_dim1 , input_len_dim2 , input_len_dim3 ) stack_size , assert_allclose ( np_output [ : , : , : , left_offset ] , 0 . ) outputs = layer ( K.variable ( inputs ) ) input_dim=10 , kwargs= { 'target_shape ' : target_shape } , inputs = np.random.rand ( num_samples , stack_size , input_num_row , assert_allclose ( np_output , expected_out ) assert np_output.shape [ 3 ] == length_col * input_num_col assert np_output.shape [ 3 ] == length_dim3 * input_len_dim3 assert np_output.shape [ 4 ] == length_dim3 * input_len_dim3 assert_allclose ( np_output [ : , 1 : -2 , 3 : -4 , : ] , 1 . ) assert_allclose ( np_output [ : , : , : , offset , : ] , 0 . ) input_num_col ) for bottom_offset in [ -1 , -2 ] : assert np_output.shape [ 3 ] == length_dim2 * input_len_dim2 expected_out = np.repeat ( expected_out , length_dim3 , axis=4 ) layer_test ( convolutional.UpSampling2D , for length_row in [ 2 ] : layer_test ( layers.LeakyReLU , kwargs= { 'alpha ' : alpha } , for length_col in [ 2 , 3 ] : for length_dim1 in [ 2 , 3 ] : assert_allclose ( np_output [ : , : , offset , : ] , 0 . ) input_len_dim1 , input_len_dim2 , input_len_dim3 , ( ( -1 , 1 ) , ( None , None , 4 ) ) ] ) def test_elu ( ) : for left_offset in [ 0 , 1 , 2 ] : input_shape=input_shape ) ] ) input_shape= ( 3 , 5 ) ) ] ) outputs = layer ( K.variable ( inputs ) ) assert_allclose ( np_output [ : , : , : , : , dim3_offset ] , 0 . ) def test_softmax ( axis ) : assert_allclose ( np_output [ : , offset , : , : ] , 0 . ) assert_allclose ( np_output [ : , : , right_offset , : ] , 0 . ) for dim3_offset in [ -1 , -2 ] : def test_upsampling_2d ( data_format ) : with pytest.raises ( ValueError ) : def test_zero_padding_3d_correctness ( ) : expected_out = np.repeat ( inputs , length_row , axis=1 ) for left_offset in [ 0 , 1 , 2 ] : assert_allclose ( np_output [ : , 2 : -2 , 2 : -2 , : ] , 1 . ) else : # tf ( ( 1 , -1 ) , ( 3 , 2 , 4 ) ) , assert np_output.shape [ 1 ] == length_dim1 * input_len_dim1 layer = convolutional.ZeroPadding2D ( padding= ( 2 , 2 ) , assert_allclose ( np_output [ : , top_offset , : , : ] , 0 . ) for offset in [ 0 , 1 , -1 , -2 ] : assert_allclose ( np_output [ : , : , 1 : -2 , 3 : -4 , 0 : -2 ] , 1 . ) assert_allclose ( np_output [ : , 2 : -2 , 2 : -2 , 2 : -2 , : ] , 1 . ) input_shape= ( 2 , 3 , 4 ) ) assert_allclose ( np_output [ : , : , : , : , offset ] , 0 . ) for length_col in [ 2 , 3 ] : for right_offset in [ -1 , -2 , -3 , -4 ] : input_len_dim1 , input_len_dim2 , input_len_dim3 ) else : # tf [ ( 3 , 4 , 5 ) , output_dim=4 , layer = convolutional.ZeroPadding3D ( padding= ( 2 , 2 , 2 ) , assert_allclose ( np_output [ : , : , : , offset ] , 0 . ) kwargs= { 'target_shape ' : ( 8 , 1 ) } , input_length=2 , layer.build ( inputs.shape ) outputs = layer ( K.variable ( inputs ) ) layer_test ( activation_layer , kwargs= { 'alpha ' : alpha } , def test_upsampling_2d ( ) : for length_dim3 in [ 3 ] : def test_zero_padding_3d_correctness ( data_format ) : def test_upsampling_3d ( data_format ) : assert np_output.shape [ 2 ] == length_dim1 * input_len_dim1 layer_test ( layers.ELU , kwargs= { 'alpha ' : alpha } , def test_leaky_relu ( ) : assert_allclose ( np_output [ : , : , 1 : -2 , 3 : -4 ] , 1 . ) for alpha in [ 0. , .5 , -1 . ] : # compare with numpy assert_allclose ( np_output [ : , : , bottom_offset , : ] , 0 . ) assert_allclose ( np_output [ : , : , dim1_offset , : , : ] , 0 . ) expected_out = np.repeat ( inputs , length_row , axis=1 ) layer_test ( convolutional.UpSampling3D , layer = convolutional.UpSampling2D ( for right_offset in [ -1 , -2 , -3 , -4 ] : assert np_output.shape [ 2 ] == length_dim2 * input_len_dim2 def test_embedding_invalid ( input_shape ) : input_shape= ( 2 , 3 , 4 ) ) layer = convolutional.ZeroPadding2D ( padding= ( 2 , 2 ) , layers.ELU ] ) assert_allclose ( np_output [ : , : , : , right_offset ] , 0 . ) assert_allclose ( np_output [ : , : , left_offset , : ] , 0 . ) for dim2_offset in [ 0 , 1 , 2 , -1 , -2 , -3 , -4 ] : layer.build ( inputs.shape ) stack_size ) assert_allclose ( np_output [ : , 2 : -2 , 2 : -2 , : ] , 1 . ) inputs = np.random.rand ( num_samples , stack_size , input_num_row , layer = convolutional.UpSampling3D ( assert_allclose ( np_output [ : , 1 : -2 , 3 : -4 , 0 : -2 , : ] , 1 . ) # basic test expected_out = np.repeat ( expected_out , length_col , axis=2 ) layer = convolutional.UpSampling2D ( expected_out = np.repeat ( expected_out , length_dim3 , axis=3 ) assert np_output.shape [ 2 ] == length_dim1 * input_len_dim1 layer = convolutional.UpSampling3D ( assert np_output.shape [ 3 ] == length_col * input_num_col input_shape= ( 3 , 4 , 5 ) ) ] ) assert_allclose ( np_output [ : , : , dim2_offset , : , : ] , 0 . ) np_output = K.eval ( outputs ) # compare with numpy expected_out = np.repeat ( inputs , length_dim1 , axis=1 ) layer = convolutional.ZeroPadding3D ( padding= ( ( 1 , 2 ) , ( 3 , 4 ) , ( 0 , 2 ) ) , assert_allclose ( np_output [ : , : , top_offset , : ] , 0 . ) assert_allclose ( np_output , expected_out ) if data_format == 'channels_first ' : def test_reshape ( ) : assert_allclose ( np_output [ : , : , : , offset ] , 0 . ) assert_allclose ( np_output [ : , bottom_offset , : , : ] , 0 . ) def test_reshape ( target_shape , input_shape ) : inputs = np.random.rand ( num_samples , stack_size , [ layers.LeakyReLU , layer = convolutional.ZeroPadding3D ( padding= ( ( 1 , 2 ) , ( 3 , 4 ) , ( 0 , 2 ) ) , assert_allclose ( np_output [ : , : , 1 : -2 , 3 : -4 , 0 : -2 ] , 1 . ) inputs = np.random.rand ( num_samples , input_num_row , input_num_col , assert np_output.shape [ 3 ] == length_dim3 * input_len_dim3 elif data_format == 'channels_first ' : for length_row in [ 2 ] : expected_out = np.repeat ( expected_out , length_dim2 , axis=3 ) for dim2_offset in [ 0 , 1 , 2 , -1 , -2 , -3 , -4 ] : input_len_dim1 , input_len_dim2 , input_len_dim3 , layer = convolutional.ZeroPadding2D ( padding= ( ( 1 , 2 ) , ( 3 , 4 ) ) , np_output = K.eval ( outputs ) assert_allclose ( np_output [ : , 1 : -2 , 3 : -4 , 0 : -2 , : ] , 1 . ) inputs = np.random.rand ( num_samples , input_num_row , input_num_col , kwargs= { 'size ' : ( 2 , 2 , 2 ) , 'data_format ' : data_format } , expected_out = np.repeat ( expected_out , length_dim2 , axis=2 ) # input_length should be equal to input_shape [ 1 : ] input_shape=input_shape ) assert np_output.shape [ 1 ] == length_row * input_num_row kwargs= { 'target_shape ' : ( -1 , 1 ) } , assert_allclose ( np_output [ : , offset , : , : , : ] , 0 . ) layer_test ( convolutional.UpSampling3D , expected_out = np.repeat ( expected_out , length_dim2 , axis=3 ) kwargs= { 'target_shape ' : ( 1 , -1 ) } , assert np_output.shape [ 3 ] == length_dim2 * input_len_dim2 input_shape=inputs.shape ) for length_dim2 in [ 2 ] : assert_allclose ( np_output [ : , top_offset , : , : ] , 0 . ) layer_test ( layers.Reshape , expected_out = np.repeat ( expected_out , length_dim3 , axis=4 ) assert np_output.shape [ 1 ] == length_row * input_num_row input_shape=inputs.shape ) else : # tf input_shape= ( 3 , 2 , 4 ) ) # compare with numpy data_format=data_format ) assert_allclose ( np_output [ : , : , : , dim3_offset , : ] , 0 . ) layer_test ( layers.Softmax , kwargs= { 'axis ' : axis } , outputs = layer ( K.variable ( inputs ) ) alpha ) : assert np_output.shape [ 2 ] == length_row * input_num_row size= ( length_row , length_col ) , for bottom_offset in [ -1 , -2 ] : expected_out = np.repeat ( inputs , length_dim1 , axis=2 ) if data_format == 'channels_first ' : assert_allclose ( np_output [ : , : , offset , : , : ] , 0 . ) assert_allclose ( np_output [ : , : , 2 : -2 , 2 : -2 , 2 : -2 ] , 1 . ) size= ( length_row , length_col ) , stack_size ) else : # tf assert_allclose ( np_output [ : , : , : , offset , : ] , 0 . ) for offset in [ 0 , 1 , -1 , -2 ] : layer.build ( inputs.shape ) layer = convolutional.ZeroPadding3D ( padding= ( 2 , 2 , 2 ) , np_output = K.eval ( outputs ) assert_allclose ( np_output [ : , : , offset , : ] , 0 . ) ( ( -1 , 1 ) , ( 3 , 2 , 4 ) ) , data_format=data_format ) expected_out = np.repeat ( inputs , length_dim1 , axis=1 ) expected_out = np.repeat ( expected_out , length_dim3 , axis=3 ) assert_allclose ( np_output [ : , : , offset , : , : ] , 0 . ) def test_softmax ( ) : if data_format == 'channels_last ' : for top_offset in [ 0 ] : if data_format == 'channels_first ' : if data_format == 'channels_first ' : kwargs= { 'size ' : ( 2 , 2 , 2 ) , 'data_format ' : data_format } , def test_zero_padding_2d_correctness ( ) : assert_allclose ( np_output [ : , bottom_offset , : , : ] , 0 . ) # compare with numpy np_output = K.eval ( outputs ) assert_allclose ( np_output [ : , : , left_offset , : ] , 0 . ) assert_allclose ( np_output [ : , : , : , dim3_offset , : ] , 0 . ) ( 3 , 5 ) ] ) assert_allclose ( np_output [ : , : , : , left_offset ] , 0 . ) layer.build ( inputs.shape ) layer.build ( inputs.shape ) assert_allclose ( np_output , expected_out ) np_output = K.eval ( outputs ) assert np_output.shape [ 2 ] == length_col * input_num_col expected_out = np.repeat ( inputs , length_row , axis=2 ) for data_format in [ 'channels_first ' , 'channels_last ' ] : def test_linear_unit_activations ( activation_layer , assert_allclose ( np_output [ : , : , top_offset , : ] , 0 . ) assert_allclose ( np_output [ : , : , : , dim2_offset , : ] , 0 . ) assert_allclose ( np_output [ : , : , : , right_offset ] , 0 . ) assert_allclose ( np_output [ : , 1 : -2 , 3 : -4 , : ] , 1 . ) assert_allclose ( np_output , expected_out ) for length_dim2 in [ 2 ] : elif data_format == 'channels_first ' : def test_upsampling_3d ( ) : assert_allclose ( np_output [ : , : , : , : , dim3_offset ] , 0 . )","['tests/keras/layers/advanced_activations_test.py', 'tests/keras/layers/convolutional_test.py', 'tests/keras/layers/core_test.py', 'tests/keras/layers/embeddings_test.py']",Various tests parametrizations ( # 12017 )
147,5c934bd646b0781eaf03e5b533ad2b6bfda401d5,2019-01-11 10:24:16-08:00,"def callbacks_factory ( histogram_freq , embeddings_freq=1 ) : callbacks=callbacks_factory ( histogram_freq=1 , write_images=False , write_grads=False , write_grads=False ) ) write_grads=write_grads , write_grads=False ) ) callbacks=callbacks_factory ( histogram_freq=1 ) ) fail_under = 87 callbacks=callbacks_factory ( histogram_freq=1 ) ) epochs=3 ) write_images=False , def callbacks_factory ( histogram_freq , embeddings_freq=1 , write_images=True , embeddings_freq=0 , write_images=True , write_grads=True , callbacks=callbacks_factory ( histogram_freq=1 ) , epochs=2 ) callbacks=callbacks_factory ( histogram_freq=0 , write_images=False , write_images=write_images , embeddings_freq=0 ) ) write_grads=False ) , callbacks=callbacks_factory ( histogram_freq=1 , callbacks=callbacks_factory ( histogram_freq=1 , fail_under = 85 epochs=2 ) callbacks=callbacks_factory ( histogram_freq=0 ) , epochs=2 ) write_grads=True ) : write_images=False ,","['.coveragerc', 'tests/keras/test_callbacks.py']",Reduce tests duration ( # 12009 )
148,ae30f36fbde83beb122a1eb1c3bf3cea2d490d5a,2019-01-10 12:28:41-08:00,"5.811451 , # output beam 0 # Add arbitrary offset - this is fine 0.584855 , # output beam 0 top_paths=top_paths , merge_repeated=False ) inputs = K.variable ( np.asarray ( inputs ) .transpose ( ( 1 , 0 , 2 ) ) ) ] , log_prob_truth = np.array ( [ 6.63339 # output beam 1 # batch_size length vector of negative log probabilities input_prob_matrix_0 = input_prob_matrix_0 + 2.0 decode_truth = [ np.array ( [ 1 , 0 ] ) , np.array ( [ 0 , 1 , 0 ] ) ] pip install tensorflow==1.9 -- progress-bar off # batch_size length vector of log probabilities pip install tensorflow==1.12 -- progress-bar off decode_truth = [ np.array ( [ 1 , 0 ] ) , np.array ( [ [ 1 ] ] ) ] inputs = K.variable ( inputs.transpose ( ( 1 , 0 , 2 ) ) ) 0.389139 # output beam 1 # change tensorflow order to keras backend order [ np.float32 ) [ np.newaxis , : ] # Take exponential as we directly apply ctc_decode_beam_search top_paths=top_paths ) inputs = np.exp ( inputs ) ] , np.float32 ) [ np.newaxis , : ] log_prob_truth = np.array (","['.travis.yml', 'keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",[ P ] Bump TF version to 1.12 ( # 11984 )
149,1336cdb14ff03de754aec6899794742ca91057b2,2019-01-09 16:03:33-08:00,"verbose=0 , return _call_and_count 'steps ' : steps , train_generator = data_generator ( X_train , y_train , batch_size=2 ) self.params = { } def predict_loop ( model , f , ins , num_test=4 ) outs_per_batch.append ( outs ) 'on_batch_begin ' : 25 , 'on_epoch_end ' : 5 , callbacks._call_end_hook ( 'test ' ) callbacks._call_batch_hook ( 'predict ' , 'begin ' , steps_done , batch_logs ) if batch_size == 0 : for method_name in methods_to_count : self._reset_batch_timing ( ) 'to the batch update ( % f ) . Check your callbacks . ' , hook_name , self.on_batch_begin ( batch , logs=logs ) `` `` '' `` `` '' Checks that the counts registered by ` counter ` are those expected . '' '' '' List of callbacks to apply during prediction . ' ( % f vs % f ) . Check your callbacks . ' steps=None ) : ` keras.callbacks.CallbackList ` to be called during prediction . 'on_test_batch_begin ' : 1 , batch_size = list ( x.values ( ) ) [ 0 ] .shape [ 0 ] batch_logs = { 'batch ' : steps_done , 'size ' : batch_size } logs : dict , Currently no data is passed to this argument for this method callbacks._call_end_hook ( 'test ' ) def _call_and_count ( * args , * * kwargs ) : 'on_predict_end ' : 1 , outs = model.test_on_batch ( x , y , sample_weight=sample_weight ) self._delta_ts_batch_end.append ( time.time ( ) - t_before_callbacks ) delta_t_median = np.median ( self._delta_ts_batch_end ) batch_size = 1 List of callbacks to apply during evaluation . } if hasattr ( model , 'metrics_names ' ) : self._delta_ts_batch_begin.append ( time.time ( ) - t_before_callbacks ) validation_steps=len ( X_test ) // 2 , callbacks= [ counter ] ) List of callbacks to apply during training and validation logs : dict , currently no data is passed to this argument for this method run . callback_metrics = list ( model.metrics_names ) def on_predict_batch_end ( self , batch , logs=None ) : callbacks._call_begin_hook ( 'train ' ) callback.on_predict_end ( logs ) 'Method ( % s ) is slow compared ' array [ i * batch_size : ( i + 1 ) * batch_size ] for array in x ] logs : dictionary of logs . batch_logs [ l ] = o if validation data is provided . List of callbacks to apply during training . `` `` '' Called at the beginning of a batch in ` predict ` methods . callback.on_batch_end ( batch , logs ) callback_model.stop_training = False batch_logs [ 'outputs ' ] = batch_outs self._delta_t_batch = time.time ( ) - self._t_enter_batch steps=None ) : batch_size=32 , callbacks._call_end_hook ( 'train ' ) callback_model = model._get_callback_model ( ) assert count == expected_count , \ if mode == _TRAIN : batch_logs [ 'batch ' ] = step_index x = to_list ( x ) batch_size=2 , epochs=5 , callbacks= [ counter ] ) def on_train_batch_end ( self , batch , logs=None ) : # Note that ` callbacks ` here is an instance of self.params = params def _check_counts ( self , counter , expected_counts ) : `` `` '' Called at the beginning of a training batch in ` fit ` methods . batch_logs [ 'size ' ] = batch_size 'verbose ' : verbose , 'at least one item . ' ) def test_loop ( model , f , ins , batch_size=None , verbose=0 , steps=None ) : callbacks=callbacks , 'on_train_batch_end ' : 0 , callbacks=None , This function should only be called during train mode . def on_predict_begin ( self , logs=None ) : 'on_batch_begin ' : 0 , model.compile ( optimizer='adam ' , loss='binary_crossentropy ' ) `` `` '' Called at the beginning of a batch in ` evaluate ` methods . self.wrap_with_counts ( method_name , getattr ( self , method_name ) ) ) if callbacks.model.stop_training : self._call_batch_hook ( _TRAIN , 'begin ' , batch , logs=logs ) def _get_model ( self ) : 'on_predict_batch_begin ' : 0 , `` `` '' Called at the end of a batch . epochs=5 , validation_data=validation_generator , callbacks=callbacks , raise ValueError ( 'Received an empty batch . ' for callback in self.callbacks : `` `` '' Calls the ` on_predict_batch_end ` methods of its callbacks . self.on_batch_end ( batch , logs=logs ) t_before_callbacks = time.time ( ) callback.on_test_begin ( logs ) def test_callback_hooks_are_called_in_evaluate ( self ) : def on_batch_end ( self , batch , logs=None ) : # Arguments def on_predict_end ( self , logs=None ) : epoch : integer , index of epoch . 'on_epoch_begin ' : 0 , i = i % max_batch_index else : `` `` '' Calls the ` on_predict_end ` methods of its callbacks . batch_size=None , % ( delta_t_median , self._delta_t_batch ) ) logs = logs or { } if hook == 'end ' : `` `` '' Calls the ` on_test_batch_begin ` methods of its callbacks . logs = logs or { } 'on_batch_end ' : 0 , t_before_callbacks = time.time ( ) Also called at the end of a validation batch in the ` fit ` methods , model.fit_generator ( train_generator , steps_per_epoch=len ( X_train ) // 2 , 'For method { } : expected { } , got : { } '.format ( batch_logs = { 'batch ' : batch_index , 'size ' : len ( batch_ids ) } 'on_batch_begin ' , 'on_batch_end ' , 'on_epoch_begin ' , 'on_epoch_end ' , `` `` '' Calls the ` on_test_batch_end ` methods of its callbacks . def _call_begin_hook ( self , mode ) : callbacks._call_batch_hook ( 'predict ' , 'end ' , step , batch_logs ) 'on_test_batch_end ' : 5 , 'on_predict_begin ' : 1 , callbacks._call_batch_hook ( 'predict ' , 'end ' , batch_index , batch_logs ) def test_callback_list_methods ( self ) : `` `` '' Calls the ` on_train_end ` methods of its callbacks . Subclasses should override for any actions to run . `` `` '' Calls the ` on_predict_batch_begin ` methods of its callbacks . verbose=0 , `` `` '' A backwards compatibility alias for ` on_train_batch_end ` . '' '' '' elif isinstance ( x , dict ) : # Arguments `` `` '' A backwards compatibility alias for ` on_train_batch_begin ` . '' '' '' ( delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1 ) ) : def on_batch_begin ( self , batch , logs=None ) : callbacks.on_predict_end ( ) validation epoch if validation is performed . Validation result keys else : return self.callback_model def on_predict_batch_begin ( self , batch , logs=None ) : 'is slow compared to a model step ' callback_list.on_test_batch_begin ( batch ) batch_hook ( batch , logs ) if not self.callbacks : 'on_batch_end ' : 25 , outs = to_list ( outs ) 'on_test_begin ' : 0 , self.on_test_end ( ) steps=len ( X_test ) // 2 , callbacks= [ counter ] ) if not hasattr ( self , '_t_enter_batch ' ) : model = self._get_model ( ) self._t_enter_batch = time.time ( ) callback_params = { 'on_predict_begin ' : 0 , warnings.warn ( 'In your callbacks , method ` on_batch_end ( ) ` ' self.method_counts = defaultdict ( int ) `` `` '' Returns the Callback Model for this Model . '' '' '' callbacks=callbacks , `` `` '' Counts the number of times each callback method was run . 'on_train_end ' : 1 , yield x_batch _TEST = 'test ' `` `` '' Calls the ` on_epoch_end ` methods of its callbacks . self._t_enter_batch = time.time ( ) `` `` '' Called at the beginning of training . callbacks=callbacks ) if hook == 'begin ' : callbacks : List of callbacks to be called during training _TRAIN = 'train ' 'Batches should contain ' count = counter.method_counts [ method_name ] batch_logs [ 'outputs ' ] = outs setattr ( self , method_name , 'on_test_end ' : 5 , 'batch_size ' : batch_size , Dense ( num_classes , activation='softmax ' ) callbacks._call_batch_hook ( 'train ' , 'end ' , batch_index , batch_logs ) callbacks._call_batch_hook ( 'test ' , 'begin ' , step , batch_logs ) `` `` '' Helper function for on_ { train|test|predict } _end methods . '' '' '' if not isinstance ( callbacks , cbks.CallbackList ) : but that may change in the future . 'on_train_batch_begin ' : 0 , 'on_test_begin ' : 5 , ( _ , _ ) , ( X_test , _ ) = get_data_callbacks ( num_test=10 ) `` `` '' Called at the end of training . 'on_train_batch_begin ' : 25 , batch_hook = getattr ( callback , hook_name ) for callback in self.callbacks : callbacks=callbacks , self._delta_ts_batch_end = deque ( [ ] , maxlen=self.queue_length ) self._delta_ts [ hook_name ] .append ( time.time ( ) - t_before_callbacks ) return counter = Counter ( ) for l , o in zip ( model.metrics_names , outs ) : self.on_predict_begin ( ) if not hasattr ( self , '_t_enter_batch ' ) : 'on_predict_batch_begin ' , 'on_predict_batch_end ' , def on_test_begin ( self , logs=None ) : return model method_counts : dict , contains the counts of time each callback method was validation_generator = data_generator ( X_test , y_test , batch_size=2 ) self._call_batch_hook ( _PREDICT , 'begin ' , batch , logs=logs ) callback_list.on_predict_batch_begin ( batch ) def __init__ ( self ) : callback.on_test_end ( logs ) hook_name = 'on_ { mode } _batch_ { hook } '.format ( mode=mode , hook=hook ) max_batch_index = len ( x [ 0 ] ) // batch_size method_name , expected_count , count ) callback_model.stop_training = False steps=steps , callbacks._call_batch_hook ( 'test ' , 'begin ' , steps_done , batch_logs ) callback_list = callbacks.CallbackList ( [ counter ] ) Dense ( 10 , activation='relu ' , input_dim=input_dim ) , 'on_test_end ' : 0 , `` `` '' Called at the beginning of prediction . self.method_counts [ method_name ] += 1 callbacks=None ) : callback_metrics = [ ] def wrap_with_counts ( self , method_name , method ) : be called during train mode . `` `` '' Called at the end of an epoch . class Counter ( callbacks.Callback ) : 'on_test_batch_end ' : 0 , 'on_epoch_end ' : 0 , 'on_train_begin ' : 0 , self.on_predict_end ( ) model.predict ( X_test , batch_size=2 , callbacks= [ counter ] ) self._delta_t_batch = 0 . are prefixed with ` val_ ` . `` `` '' Called at the end of an epoch . `` `` '' Called at the end of training . 'to the batch update ( % f ) . Check your callbacks . ' `` `` '' Calls the ` on_test_begin ` methods of its callbacks . callbacks.set_params ( callback_params ) # Handle data tensors support when no input given def _get_callback_model ( self ) : def predict_loop ( model , f , ins , batch_size=32 , verbose=0 , steps=None ) : callbacks._call_begin_hook ( 'predict ' ) def test_callback_hooks_are_called_in_predict_generator ( self ) : Subclasses should override for any actions to run . This function should only `` `` '' Calls the ` on_predict_begin ` methods of its callbacks . # ` keras.callbacks.CallbackList ` self._check_counts ( np.random.seed ( 1337 ) methods_to_count = [ i = 0 steps=None , def on_batch_begin ( self , batch , logs=None ) : i += 1 `` `` '' Calls the ` on_train_batch_begin ` methods of its callbacks . def on_test_end ( self , logs=None ) : # build batch logs self.on_train_begin ( ) 'on_test_batch_end ' : 10 , if ( self._delta_t_batch > 0. and batch_logs = { 'batch ' : step_index , 'size ' : 1 } self._call_batch_hook ( _TRAIN , 'end ' , batch , logs=logs ) callbacks.on_batch_begin ( batch_index , batch_logs ) def on_test_batch_begin ( self , batch , logs=None ) : 'on_predict_batch_end ' : 5 , 'on_test_end ' : 1 , self._call_batch_hook ( _TEST , 'end ' , batch , logs=logs ) steps=None , batch_logs = { 'batch ' : batch_index , 'size ' : len ( batch_ids ) } model.evaluate_generator ( data_generator ( X_test , y_test , batch_size=2 ) , if callback_model.stop_training : class TestCallbackCounts ( object ) : callbacks._call_batch_hook ( 'test ' , 'begin ' , batch_index , batch_logs ) # Batch is ending , calculate batch time See [ callbacks ] ( /callbacks ) . outs = to_list ( outs ) `` `` '' Called right before processing a batch . def on_test_batch_end ( self , batch , logs=None ) : `` `` '' Calls the ` on_epoch_begin ` methods of its callbacks . 'on_train_begin ' : 1 , delta_t_median ) `` `` '' Calls the ` on_train_batch_end ` methods of its callbacks . `` `` '' Called at the end of a batch in ` predict ` methods . self.model = model steps=len ( X_test ) // 2 , callbacks= [ counter ] ) self._call_batch_hook ( _PREDICT , 'end ' , batch , logs=logs ) callback_list.on_predict_batch_end ( batch ) 'on_test_batch_end ' : 1 , ] y_test = np_utils.to_categorical ( y_test ) `` `` '' Calls the ` on_test_end ` methods of its callbacks . callbacks=None ) : `` `` '' Called at the end of a batch in ` evaluate ` methods . callbacks._call_batch_hook ( 'test ' , 'end ' , steps_done , batch_logs ) 'on_test_begin ' : 1 , model.fit ( X_train , y_train , validation_data= ( X_test , y_test ) , self.on_train_end ( ) `` `` '' Helper function for all batch_ { begin | end } methods . '' '' '' `` `` '' Called at the start of an epoch . callbacks.on_batch_end ( batch_index , batch_logs ) def test_loop ( model , f , ins , logs : dict , has keys ` batch ` and ` size ` representing the current 'on_train_batch_end ' : 25 , 'metrics ' : callback_metrics , logs : dict , metric results for this batch . callback_model = model.callback_model `` `` '' Calls the ` on_train_begin ` methods of its callbacks . batch_logs = { 'batch ' : step , 'size ' : 1 } delta_t_median > 0.95 * self._delta_t_batch and steps=steps ) 'on_epoch_begin ' : 5 , callbacks=None ) : self.on_test_begin ( ) callbacks = cbks.CallbackList ( callbacks ) callbacks=None , `` `` '' Called at the end of prediction . counter , { `` `` '' Called at the beginning of training . callbacks.on_batch_end ( step_index , batch_logs ) steps=None , def on_batch_end ( self , batch , logs=None ) : % delta_t_median ) self.model = None batch_size = x [ 0 ] .shape [ 0 ] def _call_end_hook ( self , mode ) : warnings.warn ( 'Method on_batch_begin ( ) is slow compared ' callbacks._call_batch_hook ( 'predict ' , 'end ' , steps_done , batch_logs ) callbacks._call_batch_hook ( 'predict ' , 'begin ' , step , batch_logs ) callbacks : List of ` keras.callbacks.Callback ` instances . layers = [ callbacks=None , self._t_enter_batch = time.time ( ) ( X_train , y_train ) , ( X_test , y_test ) = get_data_callbacks ( num_train=10 , callbacks._call_begin_hook ( 'test ' ) logs : dict , metric results for this training epoch , and for the 'on_train_batch_begin ' , 'on_train_batch_end ' , callbacks._call_batch_hook ( 'train ' , 'begin ' , step_index , batch_logs ) callbacks.set_model ( callback_model ) return self def test_callback_hooks_are_called_in_predict ( self ) : batch_logs [ 'batch ' ] = batch_index callbacks=callbacks ) elif isinstance ( x , list ) : y_train = np_utils.to_categorical ( y_train ) def test_callback_hooks_are_called_in_fit ( self ) : if callbacks.model.stop_training : # For backwards compatibility callback_model = model if callback_model.stop_training : 'on_test_batch_begin ' , 'on_test_batch_end ' , self._delta_ts_batch_begin = deque ( [ ] , maxlen=self.queue_length ) for l , o in zip ( model.metrics_names , batch_outs ) : def on_train_batch_begin ( self , batch , logs=None ) : delta_t_median > 0.1 ) : callbacks._call_batch_hook ( 'predict ' , 'begin ' , batch_index , batch_logs ) def data_generator ( x , batch_size ) : ` keras.callbacks.CallbackList ` to be called during evaluation . callbacks._call_batch_hook ( 'test ' , 'end ' , batch_index , batch_logs ) delta_t_median = np.median ( self._delta_ts_batch_begin ) from collections import defaultdict if ( self._delta_t_batch > 0. and 'samples ' : num_samples , def _call_batch_hook ( self , mode , hook , batch , logs=None ) : callback.on_batch_begin ( batch , logs ) x_batch = [ x_batch = unpack_singleton ( x_batch ) if x is None or len ( x ) == 0 : outs_per_batch.append ( outs ) batch_logs = { 'batch ' : batch_index , 'size ' : batch_size } callbacks._call_batch_hook ( 'train ' , 'begin ' , batch_index , batch_logs ) callback.on_predict_begin ( logs ) batch number and the size of the batch . callbacks.on_train_begin ( ) outs = model.test_on_batch ( x , y , sample_weight=sample_weight ) batch : integer , index of batch within the current epoch . for method_name , expected_count in expected_counts.items ( ) : 'on_predict_batch_end ' : 1 , } ) if callback_model.stop_training : if hasattr ( self , 'callback_model ' ) and self.callback_model : self._delta_t_batch = time.time ( ) - self._t_enter_batch def test_callback_hooks_are_called_in_fit_generator ( self ) : _PREDICT = 'predict ' 'on_test_batch_begin ' : 10 , def test_callback_hooks_are_called_in_evaluate_generator ( self ) : else : delta_t_median > 0.1 ) : 'on_train_end ' : 0 , callbacks : List of callbacks to be called during training and validation callbacks : List of callbacks or an instance of if hasattr ( model , 'callback_model ' ) and model.callback_model : 'on_test_batch_begin ' : 5 , # build batch logs steps=steps , 'on_test_batch_begin ' : 0 , callbacks.model.stop_training = False batch_logs [ 'size ' ] = len ( batch_ids ) elif mode == _TEST : 'on_test_begin ' , 'on_test_end ' , Also called at the beginning of a validation batch in the ` fit ` methods , steps=steps ) model.predict_generator ( data_generator ( X_test , batch_size=2 ) , `` `` '' Called at the beginning of evaluation or validation . `` `` '' Helper function for on_ { train|test|predict } _begin methods . '' '' '' List of callbacks to apply during training . ( if ` val_function ` and ` val_inputs ` are not ` None ` ) . `` `` '' delta_t_median = np.median ( self._delta_ts [ hook_name ] ) # Check if callbacks have not been already configured self._delta_t_batch = 0 . callbacks.on_batch_begin ( step_index , batch_logs ) 'on_train_begin ' , 'on_train_end ' , 'on_predict_begin ' , 'on_predict_end ' , model.evaluate ( X_test , y_test , batch_size=2 , callbacks= [ counter ] ) 'on_predict_batch_end ' : 0 , if hasattr ( model , 'metrics_names ' ) : callback_list.on_test_batch_end ( batch ) ( if ) . `` `` '' Called at the start of an epoch . batch_logs = { } self._call_batch_hook ( _TEST , 'begin ' , batch , logs=logs ) callbacks._call_end_hook ( 'predict ' ) self._delta_ts = defaultdict ( lambda : deque ( [ ] , maxlen=self.queue_length ) ) while 1 : # Argument def _reset_batch_timing ( self ) : if callbacks.model.stop_training : 'on_predict_batch_begin ' : 1 , warnings.warn ( callbacks._call_batch_hook ( 'train ' , 'end ' , step_index , batch_logs ) pass batch_size = x.shape [ 0 ] model = Sequential ( layers=layers ) `` `` '' Called at the end of evaluation or validation . `` `` '' Called at the end of a training batch in ` fit ` methods . 'on_predict_end ' : 0 , callback_model = model._get_callback_model ( ) batch = 0 callbacks.on_train_end ( ) batch_logs [ 'size ' ] = 1 callbacks.model.stop_training = False ( _ , _ ) , ( X_test , y_test ) = get_data_callbacks ( num_test=10 ) delta_t_median > 0.95 * self._delta_t_batch and return method ( * args , * * kwargs ) callbacks=None , 'on_predict_batch_begin ' : 5 , self._t_enter_batch = time.time ( ) # step-size = 1 for data tensors callbacks._call_batch_hook ( 'test ' , 'end ' , step , batch_logs )","['keras/callbacks.py', 'keras/engine/training.py', 'keras/engine/training_arrays.py', 'keras/engine/training_generator.py', 'tests/keras/test_callbacks.py']",Sync the callback API with new tf.keras API ( # 11808 )
150,6096ded90df902e1f8a21f666748a03100d140c0,2019-01-09 15:57:18-08:00,"[ np.identity ( shape [ 1 ] ) , return self.gain * np.concatenate ( axis=1 return self.gain * np.eye ( shape [ 0 ] , shape [ 1 ] ) np.zeros ( ( shape [ 0 ] - shape [ 1 ] , shape [ 1 ] ) ) ] , else : ) axis=0 return self.gain * np.identity ( shape [ 0 ] ) [ np.identity ( shape [ 0 ] ) , if shape [ 0 ] == shape [ 1 ] : np.zeros ( ( shape [ 0 ] , shape [ 1 ] - shape [ 0 ] ) ) ] , elif shape [ 0 ] > shape [ 1 ] :",['keras/initializers.py'],Simplify identity initializer with zero padding ( # 11986 )
151,87a746878ba2359a7fb8135ed5829e6c5003b4db,2019-01-09 15:56:46-08:00,"'Dilated separable 1D convolution is currently not supported ' strides= ( 1 , 1 , 1 ) , if dilation_rate [ 0 ] ! = dilation_rate [ 1 ] : 'by CNTK backend . Please set ` dilation_rate ` to 1 . ' auto_padding= [ False ] ) raise ValueError ( 'Invalid strides for dilated convolution ' ) groups=x.shape [ 0 ] ) x = C.convolution ( depthwise_kernel , x , 'not supported . ' ) 'You passed : % s ' % ( dilation_rate , ) ) auto_padding= [ False , padding , padding ] , groups=x.shape [ 0 ] ) x = C.convolution ( pointwise_kernel , x , if dilation_rate == ( 1 , 1 ) : x = C.convolution ( depthwise_kernel , x , auto_padding= [ False , padding , padding ] , x = C.convolution ( pointwise_kernel , x , strides=strides , if strides ! = ( 1 , 1 ) : strides=strides , if dilation_rate ! = ( 1 , ) : strides= ( 1 , 1 , 1 ) , raise ValueError ( else : raise ValueError ( 'CNTK Backend : non-square dilation_rate is ' auto_padding= [ False ] )",['keras/backend/cntk_backend.py'],Clean up untouchable lines in ` separable_conv1d ` of CNTK ( # 12002 )
152,3821c77da30e420fb78231d306b18dbfa1131fb7,2019-01-09 13:33:18-08:00,"X = arr [ 'data ' ] [ start : end ] f.close ( ) start = batch_index x_test , y_test = f [ 'x_test ' ] , f [ 'y_test ' ] x_train , y_train = f [ 'x_train ' ] , f [ 'y_train ' ] start = batch_index while True : with np.load ( 'data_threads.npz ' ) as arr : X = arr [ 'data ' ] [ start : end ] y = arr [ 'labels ' ] [ start : end ] yield X , y with np.load ( 'data.npz ' ) as arr : f = np.load ( path ) arr = np.load ( 'data_threads.npz ' ) yield X , y x_train , y_train = f [ 'x_train ' ] , f [ 'y_train ' ] y = arr [ 'labels ' ] [ start : end ] batch_index = np.random.randint ( 0 , n_samples - batch_size ) with np.load ( path ) as f : while True : end = start + batch_size end = start + batch_size batch_index = np.random.randint ( 0 , n_samples - batch_size ) arr = np.load ( 'data.npz ' ) x_test , y_test = f [ 'x_test ' ] , f [ 'y_test ' ]","['keras/datasets/mnist.py', 'tests/test_multiprocessing.py']",[ P ] Make the file closing styles consistent ( # 12000 )
153,6c2c7613e673a847dc41ad6850dd446ddd3b7dd7,2019-01-09 13:32:10-08:00,"MobileNet v2 from keras.applications.resnet_v2 import ResNet152V2 keras.applications.resnet_v2.ResNet152V2 ( include_top=True , weights='imagenet ' , input_tensor=None , input_shape=None , pooling=None , classes=1000 ) keras.applications.resnet.ResNet152 ( include_top=True , weights='imagenet ' , input_tensor=None , input_shape=None , pooling=None , classes=1000 ) ResNet50 model , with weights pre-trained on ImageNet . ResNet50 from keras.applications.resnet_v2 import ResNet101V2 ` ResNet ` : [ Deep Residual Learning for Image Recognition ] ( https : //arxiv.org/abs/1512.03385 ) [ Deep Residual Learning for Image Recognition ] ( https : //arxiv.org/abs/1512.03385 ) | [ ResNet50 ] ( # resnet50 ) | 99 MB | 0.749 | 0.921 | 25,636,712 | 168 | These weights are ported from the following : from keras.applications.resnet import ResNet101 [ MobileNetV2 ] ( # mobilenetv2 ) keras.applications.resnet_v2.ResNet101V2 ( include_top=True , weights='imagenet ' , input_tensor=None , input_shape=None , pooling=None , classes=1000 ) MobileNet v2 [ ResNet , ResNetV2 , ResNeXt ] ( # resnet ) ` ResNet ` : [ The original repository of Kaiming He ] ( https : //github.com/KaimingHe/deep-residual-networks ) under the [ MIT license ] ( https : //github.com/KaimingHe/deep-residual-networks/blob/master/LICENSE ) . | [ ResNet152 ] ( # resnet ) | 232 MB | 0.766 | 0.931 | 60,419,944 | - | ResNeXt | [ ResNet101 ] ( # resnet ) | 171 MB | 0.764 | 0.928 | 44,707,176 | - | [ ResNet50 ] ( # resnet50 ) | [ ResNeXt101 ] ( # resnet ) | 170 MB | 0.787 | 0.943 | 44,315,560 | - | keras.applications.resnet.ResNet101 ( include_top=True , weights='imagenet ' , input_tensor=None , input_shape=None , pooling=None , classes=1000 ) ` ResNetV2 ` : [ Identity Mappings in Deep Residual Networks ] ( https : //arxiv.org/abs/1603.05027 ) from keras.applications.resnet import ResNet50 from keras.applications.resnet_v2 import ResNet50V2 ResNet , ResNetV2 , ResNeXt models , with weights pre-trained on ImageNet . ` ResNeXt ` : [ Aggregated Residual Transformations for Deep Neural Networks ] ( https : //arxiv.org/abs/1611.05431 ) from keras.applications.resnext import ResNeXt101 # # ResNet from keras.applications.resnet50 import ResNet50 from keras.applications.resnext import ResNeXt50 [ MobileNetV2 ] ( # mobilenetv2 ) keras.applications.resnet_v2.ResNet50V2 ( include_top=True , weights='imagenet ' , input_tensor=None , input_shape=None , pooling=None , classes=1000 ) keras.applications.resnext.ResNeXt101 ( include_top=True , weights='imagenet ' , input_tensor=None , input_shape=None , pooling=None , classes=1000 ) keras.applications.resnext.ResNeXt50 ( include_top=True , weights='imagenet ' , input_tensor=None , input_shape=None , pooling=None , classes=1000 ) from keras.applications.resnet import ResNet152 from keras.applications.mobilenet_v2 import MobileNetV2 | [ ResNet101V2 ] ( # resnet ) | 171 MB | 0.772 | 0.938 | 44,675,560 | - | # # ResNet50 | [ ResNeXt50 ] ( # resnet ) | 96 MB | 0.777 | 0.938 | 25,097,128 | - | ` ResNetV2 ` : [ Facebook ] ( https : //github.com/facebook/fb.resnet.torch ) under the [ BSD license ] ( https : //github.com/facebook/fb.resnet.torch/blob/master/LICENSE ) . keras.applications.resnet.ResNet50 ( include_top=True , weights='imagenet ' , input_tensor=None , input_shape=None , pooling=None , classes=1000 ) | [ ResNet50V2 ] ( # resnet ) | 98 MB | 0.760 | 0.930 | 25,613,800 | - | | [ ResNet152V2 ] ( # resnet ) | 232 MB | 0.780 | 0.942 | 60,380,648 | - | | [ ResNet50 ] ( # resnet ) | 98 MB | 0.749 | 0.921 | 25,636,712 | - | from keras.applications.mobilenet_v2 import MobileNetV2 ResNet keras.applications.resnet50.ResNet50 ( include_top=True , weights='imagenet ' , input_tensor=None , input_shape=None , pooling=None , classes=1000 ) ResNet v2 ` ResNeXt ` : [ Facebook AI Research ] ( https : //github.com/facebookresearch/ResNeXt ) under the [ BSD license ] ( https : //github.com/facebookresearch/ResNeXt/blob/master/LICENSE ) . These weights are ported from the ones [ released by Kaiming He ] ( https : //github.com/KaimingHe/deep-residual-networks ) under the [ MIT license ] ( https : //github.com/KaimingHe/deep-residual-networks/blob/master/LICENSE ) .","['docs/templates/applications.md', 'docs/templates/getting-started/faq.md']",Update docs for ResNet variants ( # 11988 )
154,0505393746d56ddacc34bb1c016dba79429c9ac9,2019-01-09 00:22:45+01:00,"ending_point - 1 ) ] in addition to the output . __return_sequences__ : Boolean . Whether to return the last output 'doc ' : `` '' '' Base class for recurrent layers . def test_doc_lists ( ) : return_sequences : Boolean . Whether to return the last output __Arguments__ # Arguments return_state : Boolean . Whether to return the last state block = docstring [ starting_point : ( ending_point - 1 if ending_point > -1 else block = docstring [ starting_point : ( None if ending_point == -1 else docstring = autogen.process_docstring ( docs_descriptor [ 'doc ' ] ) test_doc_with_arguments_as_last_block = { in the output sequence , or the full sequence . def test_doc_lists ( docs_descriptor ) : 'result ' : `` 'Base class for recurrent layers . assert markdown ( docstring ) == markdown ( docs_descriptor [ 'result ' ] ) assert markdown ( docstring ) == markdown ( test_doc1 [ 'result ' ] ) test_doc1 , in addition to the output . test_doc_with_arguments_as_last_block , `` `` '' , __return_state__ : Boolean . Whether to return the last state docstring = autogen.process_docstring ( test_doc1 [ 'doc ' ] ) in the output sequence , or the full sequence . section_end ) ] ] ) ' '' }","['docs/autogen.py', 'tests/test_doc_auto_generation.py']",Fix Arguments display in Docs ( # 12007 )
155,0cd3b07eb5de1aaaad84d1ff7f7c2ed7dab4b23c,2019-01-09 00:16:36+01:00,"export APP_CHANGED=False ; for entry in ` git diff -- name-only HEAD~1 ` ; do if [ [ `` $ entry '' == `` keras/applications/ '' * ] ] ; then export APP_CHANGED=True ; fi ; done os.environ.get ( 'CORE_CHANGED ' , 'True ' ) == 'False ' and export CORE_CHANGED=False ; pytestmark = pytest.mark.skipif ( # detect whether core files are changed or not for entry in ` git diff -- name-only HEAD~1 ` ; do if [ [ `` $ entry '' == `` keras/backend/ '' * ] ] || [ [ `` $ entry '' == `` keras/engine/ '' * ] ] || [ [ `` $ entry '' == `` keras/layers/ '' * ] ] ; then export CORE_CHANGED=True ; fi ; done reason='Runs only when the relevant files have been modified . ' ) os.environ.get ( 'APP_CHANGED ' , 'True ' ) == 'False ' ,","['.travis.yml', 'tests/integration_tests/applications_test.py']",Remove legacy environment variables ( # 11998 )
156,5a8c85f1261e926a7f4bb52fb72935c574476ac3,2019-01-08 15:19:25+05:18,the smaller the learning rate . # data . Image renderings and text are created on the fly the smaller the updates . # data . Image renderings are text are created on the fly,"['examples/image_ocr.py', 'keras/optimizers.py']",Fix typos ( # 11997 )
157,1d81a20292ca6926e595d06a6cd725dbb104a146,2019-01-05 15:22:04+01:00,"target_mean = ( 1 . * min ( tensor_shape ) ) / ( tensor_shape [ 0 ] * tensor_shape [ 1 ] ) multiple identity matrices are concatenated along the long side . axis=1 def recurrent_identity ( shape , gain=1 . ) : if len ( tensor_shape ) > 2 : if isinstance ( self.recurrent_initializer , initializers.Identity ) : [ np.identity ( shape [ 0 ] ) ] * ( shape [ 1 ] // shape [ 0 ] ) , axis=1 ) if type ( self.recurrent_initializer ) .__name__ == 'Identity ' : If the long side of the matrix is a multiple of the short side , [ np.identity ( shape [ 1 ] ) , target_mean=1 . / tensor_shape [ 0 ] , target_max=1 . ) If the desired matrix is not square , it pads with zeros on the if len ( tensor_shape ) > 2 or max ( tensor_shape ) % min ( tensor_shape ) ! = 0 : target_mean=target_mean , target_max=1 . ) np.zeros ( ( shape [ 0 ] , shape [ 1 ] - shape [ 0 ] ) ) ] , raise ValueError ( 'Long side should be multiple of short side . ' ) if max ( shape ) % min ( shape ) ! = 0 : target_mean=1 . / tensor_shape [ 0 ] , target_max=1 . ) additional rows/columns ) target_mean=target_mean , target_max=1 . ) [ np.identity ( shape [ 0 ] ) ] * ( shape [ 1 ] // shape [ 0 ] ) , axis=1 ) [ np.identity ( shape [ 1 ] ) ] * ( shape [ 0 ] // shape [ 1 ] ) , axis=0 ) [ np.identity ( shape [ 0 ] ) , return gain * np.concatenate ( axis=0 self.recurrent_initializer = recurrent_identity np.zeros ( ( shape [ 0 ] - shape [ 1 ] , shape [ 1 ] ) ) ] ,","['keras/initializers.py', 'keras/layers/recurrent.py', 'tests/keras/initializers_test.py']",[ P ] Identity initializer with zero padding ( # 11887 )
158,425e99a9e3d00c7d505d585e00e85ee716e3c350,2019-01-04 13:03:20+01:00,"'have one entry per model output . The model has \\d ' match='The model has \d outputs , but you passed a single ' 'have one entry per model output . The model has \d ' match='The model has \\d outputs , but you passed a single ' return [ x.strip ( ) for x in re.split ( r ' ( \W+ ) ? ' , sent ) if x.strip ( ) ] return [ x.strip ( ) for x in re.split ( ' ( \W+ ) ? ' , sent ) if x.strip ( ) ]","['examples/babi_memnn.py', 'examples/babi_rnn.py', 'tests/keras/engine/test_training.py']",Fix DeprecationWarning : invalid escape sequence in examples and tests ( # 11974 )
159,88cf9707530f9bb3246779aee0138e49efecc500,2019-01-04 13:01:10+01:00,f.close ( ) return json.load ( f ) return data data = json.load ( f ) f = open ( path ) with open ( path ) as f :,['keras/datasets/reuters.py'],[ P ] Use with to ensure no resource leak in get_word_index ( ) ( # 11975 )
160,e11c48d9ce3ee47bb8a966549b14cbd5b10ee70d,2019-01-01 18:17:19+01:00,"_keras_home = os.path.join ( os.path.abspath ( ' . ' ) , '.keras ' ) data_keras_home = os.path.dirname ( os.path.dirname ( os.path.abspath ( filepath ) ) ) os.environ.pop ( 'KERAS_HOME ' ) filepath = path + '.tar.gz ' cache_dir = os.path.join ( os.path.expanduser ( '~ ' ) , '.keras ' ) os.makedirs ( _keras_home ) reload_module ( K ) cache_dir = os.environ.get ( 'KERAS_HOME ' ) assert data_keras_home == os.path.dirname ( K._config_path ) else : shutil.rmtree ( _keras_home ) os.remove ( filepath ) path = get_file ( dirname , origin , untar=True ) os.environ [ 'KERAS_HOME ' ] = _keras_home from six.moves import reload_module os.remove ( os.path.join ( os.path.dirname ( path ) , 'test.txt ' ) ) cache_dir = os.path.join ( os.path.expanduser ( '~ ' ) , '.keras ' ) if 'KERAS_HOME ' in os.environ : import shutil if not os.path.exists ( _keras_home ) :","['keras/utils/data_utils.py', 'tests/keras/utils/data_utils_test.py']",Fix function 'get_file ( ) ' is inconsistent with keras backend when 'KERAS_HOME ' is not ` ~/.keras ` ( # 11924 )
161,2e0de19d629e2eb5be4f614049e5f0d32887d155,2019-01-01 13:41:32+01:00,from keras.utils import to_categorical from keras.utils import model_to_dot from keras.utils.vis_utils import model_to_dot from keras.utils.np_utils import to_categorical You can use the ` HDF5Matrix ` class from ` keras.utils ` . See [ the HDF5Matrix documentation ] ( /utils/ # hdf5matrix ) for details . The ` keras.utils.vis_utils ` module provides utility functions to plot from .vis_utils import model_to_dot You can use the ` HDF5Matrix ` class from ` keras.utils.io_utils ` . See [ the HDF5Matrix documentation ] ( /utils/ # hdf5matrix ) for details . Keras provides utility functions to plot a Keras model ( using ` graphviz ` ) . a Keras model ( using ` graphviz ` ) .,"['docs/templates/getting-started/faq.md', 'docs/templates/losses.md', 'docs/templates/visualization.md', 'keras/utils/__init__.py']",Make model_to_dot available in keras.utils and update API for HDF5Matrix and to_categorical ( # 11960 )
162,994c4bb338ce440a27df5db48b8f5044a3e8f611,2018-12-26 11:42:23-05:00,"for fname in fnames : layers.AveragePooling1D , 'all_module_functions ' : [ backend ] , layers.subtract , from keras import losses # For each class to document , it is possible to : models.Sequential.compile , Pooling Layers layers.GlobalAveragePooling3D , 'all_module_functions ' : [ activations ] , Getting started with the functional api from keras import activations explain compilation step utils.plot_model , Getting started with the sequential model 'page ' : 'layers/recurrent.md ' , from keras import callbacks layers.RNN , if fname [ -3 : ] == '.md ' : 'page ' : 'models/sequential.md ' , layers.SeparableConv2D , 'page ' : 'callbacks.md ' , layers.Add , explain input_shape < /details > layers.Conv2D , shutil.rmtree ( 'sources ' ) Advanced Activations Layers 'get ' , Constraints # [ classA , ( classB , [ `` method1 '' , `` method2 '' , ... ] ) , ... ] 'page ' : 'layers/convolutional.md ' , layers.Cropping2D , { layers.Conv3D , preprocessing.text.one_hot , Visualization layers.GlobalMaxPooling2D , Initializers from docs.structure import template_hidden_np_implementation if os.path.exists ( 'sources ' ) : from keras import layers # 1 ) Document only the class : [ classA , classB , ... ] layers.GlobalAveragePooling1D , preprocessing.sequence.make_sampling_table , layers.Subtract , Activations from keras import callbacks layers.Conv1D , 'set_session ' , # For each class to document , it is possible to : ROOT = 'http : //keras.io/ ' Normalization Layers preprocessing.sequence.skipgrams , Metrics layers.Dense , 'page ' : 'layers/noise.md ' , layers.RepeatVector , layers.SimpleRNN , 'all_module_classes ' : [ constraints ] , ( preprocessing.image.ImageDataGenerator , ' * ' ) 'classes ' : [ 'all_module_functions ' : [ activations ] , Model ( functional API ) 'page ' : 'layers/normalization.md ' , layers.maximum , 'classes ' : [ layers.GRUCell , models.Sequential.get_layer , Index 'all_module_classes ' : [ callbacks ] , models.Model.evaluate , layers.multiply , `` ` Datasets Getting started utils.normalize , layers.Reshape , from keras import metrics Home print ( 'Cleaning up existing sources directory . ' ) from keras import layers from keras.layers import wrappers Activations layers.LocallyConnected1D , 'CallbackList ' , 'page ' : 'optimizers.md ' , PAGES = [ General documentation architecture : explain common layer functions : get_weights , set_weights , get_config shutil.copy ( fpath , new_fpath ) from keras import preprocessing explain serialization , deserialization layers.Maximum , 'functions ' : [ ] , layers.Masking , models.Sequential.train_on_batch , Visualization 'page ' : 'preprocessing/text.md ' , for fname in fnames : new_fpath = fpath.replace ( 'templates ' , 'sources ' ) models.Model.get_layer , template_np_implementation = `` '' '' # Numpy implementation Scikit-learn API layers.ConvLSTM2D , # 3 ) Choose which methods to document ( methods listed as strings ) : Index layers.AveragePooling2D , 'all_module_classes ' : [ initializers ] , layers.SimpleRNN , 'page ' : 'initializers.md ' , layers.AveragePooling3D , 'page ' : 'utils.md ' , 'page ' : 'layers/normalization.md ' , 'set_image_dim_ordering ' , from keras import preprocessing models.Model.test_on_batch , 'Optimizer ' , Getting started Convolutional Layers Sequential models.Model.fit , layers.ZeroPadding1D , layers.SimpleRNNCell , from docs.structure import EXCLUDE layers.UpSampling3D , layers.AveragePooling1D , Text Preprocessing Losses layers.SeparableConv1D , Noise Layers preprocessing.sequence.skipgrams , 'page ' : 'losses.md ' , Preprocessing if __name__ == '__main__ ' : new_fpath = fpath.replace ( 'templates ' , 'sources ' ) 'get_session ' , 'all_module_classes ' : [ wrappers ] , FAQ layers.Conv3DTranspose , layers.ZeroPadding1D , ' '' 'methods ' : [ utils.Sequence ] , 'page ' : 'layers/merge.md ' , Optimizers 'page ' : 'layers/recurrent.md ' , layers.UpSampling3D , Utils layers.multiply , layers.SpatialDropout1D , layers.UpSampling2D , models.Sequential.predict , < details > models.Model.predict_on_batch , layers.LSTMCell , shutil.copy ( fpath , new_fpath ) < details > from keras import losses os.makedirs ( new_subdir ) os.makedirs ( new_subdir ) models.Sequential.test_on_batch , layers.BatchNormalization , 'functions ' : [ utils.to_categorical , layers.average , Noise Layers `` ` python utils.HDF5Matrix , models.Model.get_layer , layers.BatchNormalization , layers.Add , Sequence Preprocessing 'page ' : 'utils.md ' , layers.concatenate , explain when one should use Sequential or functional API 'Wrapper ' , explain weight saving , weight loading layers.GlobalMaxPooling3D , layers.SeparableConv2D , `` `` '' layers.ActivityRegularization , layers.Conv2DTranspose , Datasets 'page ' : 'backend.md ' , models.Sequential.evaluate , utils.print_summary , Core Layers ' '' models.Sequential.test_on_batch , layers.GlobalMaxPooling1D , layers.dot , preprocessing.sequence.make_sampling_table , preprocessing.text.text_to_word_sequence , layers.Conv2D , 'classes ' : [ utils.CustomObjectScope , 'page ' : 'layers/embeddings.md ' , models.Sequential.predict_generator , explain common layer functions : get_weights , set_weights , get_config Preprocessing layers.average , 'page ' : 'layers/wrappers.md ' , layers.LSTMCell , 'all_module_classes ' : [ advanced_activations ] , preprocessing.text.Tokenizer , 'Optimizer ' , Metrics from keras import initializers 'page ' : 'layers/merge.md ' , 'all_module_functions ' : [ backend ] , layers.Dot , < summary > Show the Numpy implementation < /summary > layers.Flatten , ROOT = 'http : //keras.io/ ' 'page ' : 'layers/wrappers.md ' , 'all_module_functions ' : [ initializers ] , 'page ' : 'activations.md ' , layers.Input , utils.get_file , layers.Multiply , 'methods ' : [ layers.MaxPooling3D , from keras import models 'page ' : 'losses.md ' , layers.concatenate , 'page ' : 'optimizers.md ' , layers.Conv3DTranspose , 'Constraint ' print ( 'Cleaning up existing sources directory . ' ) About Keras models layers.Input , models.Sequential.get_layer , ` KERAS_BACKEND=tensorflow python autogen.py ` 'image_dim_ordering ' , 'page ' : 'preprocessing/sequence.md ' , models.Model.predict_on_batch , } utils.get_file , FAQ PAGES = [ explain serialization , deserialization layers.CuDNNLSTM , preprocessing.text.Tokenizer , layers.Embedding , layers.add , 'page ' : 'backend.md ' , Callbacks new_subdir = subdir.replace ( 'templates ' , 'sources ' ) layers.SpatialDropout2D , layers.AveragePooling3D , layers.SpatialDropout3D , About Keras layers layers.CuDNNLSTM , 'with the TensorFlow backend because this ' Scikit-learn API layers.ActivityRegularization , models.Sequential.predict , layers.MaxPooling2D , layers.Permute , template_np_implementation = `` '' '' # Numpy implementation models.Model.test_on_batch , 'serialize ' , 'page ' : 'layers/pooling.md ' , from keras import constraints layers.Cropping3D , layers.DepthwiseConv2D , 'image_dim_ordering ' , layers.LSTM , models.Sequential.evaluate , 'functions ' : [ utils.to_categorical , layers.MaxPooling1D , models.Sequential.fit , explain input_shape Pooling Layers layers.MaxPooling3D , 'Wrapper ' , 'set_session ' , models.Model.train_on_batch , from keras import backend Layer Wrappers Embedding Layers ( preprocessing.image.ImageDataGenerator , ' * ' ) layers.LocallyConnected1D , layers.Cropping2D , layers.UpSampling1D , models.Model.evaluate , preprocessing.text.one_hot , preprocessing.sequence.pad_sequences , Writing your own Keras layers 'page ' : 'constraints.md ' , ] from keras.layers import advanced_activations 'page ' : 'layers/local.md ' , 'all_module_classes ' : [ noise ] , layers.UpSampling2D , models.Model.compile , # [ classA , ( classB , [ module.classB.method1 , module.classB.method2 , ... ] ) , ... ] { { code } } ] layers.Cropping1D , 'all_module_classes ' : [ optimizers ] , explain compilation step # 2 ) Document all its methods : [ classA , ( classB , `` * '' ) ] from keras import utils Constraints Initializers 'page ' : 'layers/local.md ' , layers.Multiply , ] explain usage on non-Keras tensors preprocessing.sequence.pad_sequences , models.Sequential.predict_on_batch , 'all_module_functions ' : [ metrics ] , layers.GlobalMaxPooling3D , if not os.path.exists ( new_subdir ) : from docs.structure import PAGES models.Model.fit_generator , layers.DepthwiseConv2D , Advanced Activations Layers generate ( ) layers.RNN , models.Model.evaluate_generator , 'CallbackList ' , Regularizers if os.path.exists ( 'sources ' ) : from keras import optimizers utils.Sequence ] , layers.Concatenate , layers.ZeroPadding3D , models.Sequential.predict_generator , `` ` layers.GlobalMaxPooling2D , 'page ' : 'activations.md ' , models.Sequential.fit , layers.Cropping3D , utils.multi_gpu_model ] , ] , layers.Masking , ` pip install -e . ` to make sure that Python will import your modified version of Keras . Layers Image Preprocessing `` ` python models.Model.compile , Layers models.Model.fit_generator , layers.Dropout , preprocessing.sequence.TimeseriesGenerator , Models models.Sequential.fit_generator , layers.GRUCell , 'page ' : 'layers/core.md ' , 'serialize ' , # 4 ) Choose which methods to document ( methods listed as qualified names ) : preprocessing.text.hashing_trick , 'page ' : 'constraints.md ' , 'normalize_data_format ' , layers.Conv3D , from keras.layers import wrappers 'get_session ' , # [ classA , ( classB , [ module.classB.method1 , module.classB.method2 , ... ] ) , ... ] utils.normalize , utils.plot_model , layers.ConvLSTM2D , Locally-connected Layers layers.LSTM , layers.SimpleRNNCell , from keras import metrics models.Sequential.train_on_batch , models.Sequential.compile , Layer Wrappers template_hidden_np_implementation = `` '' '' # Numpy implementation Getting started with the functional api utils.HDF5Matrix , layers.Cropping1D , explain when one should use Sequential or functional API 'all_module_classes ' : [ constraints ] , layers.LocallyConnected2D , models.Sequential.evaluate_generator , 'deserialize ' , 'Constraint ' # [ classA , ( classB , [ `` method1 '' , `` method2 '' , ... ] ) , ... ] if K.backend ( ) ! = 'tensorflow ' : layers.LocallyConnected2D , layers.subtract , 'page ' : 'metrics.md ' , } , Merge Layers layers.Activation , Normalization Layers layers.Permute , layers.add , from keras.layers import noise 'page ' : 'layers/pooling.md ' , for subdir , dirs , fnames in os.walk ( 'templates ' ) : Optimizers Models EXCLUDE = { 'page ' : 'layers/advanced-activations.md ' , layers.Conv1D , Text Preprocessing from keras.layers import advanced_activations layers.Lambda , layers.dot , Locally-connected Layers layers.RepeatVector , layers.SpatialDropout3D , 'all_module_classes ' : [ wrappers ] , layers.SpatialDropout2D , from keras import initializers 'all_module_functions ' : [ metrics ] , layers.Embedding , layers.GlobalAveragePooling3D , Image Preprocessing 'get_variable_shape ' , layers.ZeroPadding3D , from keras import activations from keras import backend # 1 ) Document only the class : [ classA , classB , ... ] 'page ' : 'layers/advanced-activations.md ' , Contributing 'page ' : 'layers/noise.md ' , General documentation architecture : layers.Subtract , 'all_module_classes ' : [ noise ] , ` python autogen.py ` preprocessing.text.text_to_word_sequence , 'all_module_functions ' : [ losses ] , models.Sequential.evaluate_generator , Applications Merge Layers from keras import optimizers Losses 'set_image_dim_ordering ' , layers.ZeroPadding2D , layers.GlobalAveragePooling2D , if __name__ == '__main__ ' : Applications if not os.path.exists ( new_subdir ) : 'all_module_classes ' : [ optimizers ] , 'all_module_classes ' : [ advanced_activations ] , 'page ' : 'layers/core.md ' , layers.UpSampling1D , Sequential 'get ' , layers.CuDNNGRU , # - * - coding : utf-8 - * - template_hidden_np_implementation = `` '' '' # Numpy implementation if fname [ -3 : ] == '.md ' : from docs.structure import template_np_implementation layers.GlobalAveragePooling2D , 'deserialize ' , 'functions ' : [ Getting started with the sequential model models.Model.predict , explain weight saving , weight loading models.Model.predict_generator , layers.Average , explain usage on non-Keras tensors layers.Lambda , < /details > 'normalize_data_format ' , shutil.rmtree ( 'sources ' ) < summary > Show the Numpy implementation < /summary > Sequence Preprocessing layers.Dropout , 'page ' : 'preprocessing/image.md ' , print ( 'Populating sources directory with templates . ' ) # 2 ) Document all its methods : [ classA , ( classB , `` * '' ) ] Callbacks layers.Reshape , 'page ' : 'preprocessing/sequence.md ' , layers.SpatialDropout1D , layers.GRU , Utils 'page ' : 'preprocessing/image.md ' , 'all_module_classes ' : [ initializers ] , layers.GRU , Backend Convolutional Layers layers.Conv2DTranspose , from keras import constraints 'is the only backend with docstrings . ' ) raise ModuleNotFoundError ( 'The documentation must be built ' utils.multi_gpu_model ] , utils.print_summary , from keras import backend as K } , Embedding Layers layers.SeparableConv1D , ] About Keras models { { code } } About Keras layers layers.Dense , layers.MaxPooling1D , layers.Dot , Recurrent Layers # 3 ) Choose which methods to document ( methods listed as strings ) : models.Sequential.predict_on_batch , { 'page ' : 'preprocessing/text.md ' , # 4 ) Choose which methods to document ( methods listed as qualified names ) : 'TFOptimizer ' , models.Model.predict_generator , 'all_module_classes ' : [ callbacks ] , Core Layers fpath = os.path.join ( subdir , fname ) `` `` '' Writing your own Keras layers layers.Maximum , 'page ' : 'initializers.md ' , from docs.structure import ROOT 'page ' : 'metrics.md ' , 'page ' : 'callbacks.md ' , 'all_module_functions ' : [ losses ] , layers.GlobalAveragePooling1D , layers.Concatenate , layers.Flatten , print ( 'Populating sources directory with templates . ' ) preprocessing.text.hashing_trick , Recurrent Layers layers.GlobalMaxPooling1D , Home 'get_variable_shape ' , Contributing for subdir , dirs , fnames in os.walk ( 'templates ' ) : 'page ' : 'layers/embeddings.md ' , 'page ' : 'models/sequential.md ' , Backend models.Model.evaluate_generator , fpath = os.path.join ( subdir , fname ) layers.AveragePooling2D , EXCLUDE = { layers.Average , layers.maximum , layers.ZeroPadding2D , 'all_module_functions ' : [ initializers ] , layers.CuDNNGRU , preprocessing.sequence.TimeseriesGenerator , 'classes ' : [ utils.CustomObjectScope , models.Model.train_on_batch , Model ( functional API ) from keras import utils new_subdir = subdir.replace ( 'templates ' , 'sources ' ) from keras import models 'page ' : 'layers/convolutional.md ' , layers.Activation , 'page ' : 'models/model.md ' , models.Model.predict , 'page ' : 'models/model.md ' , layers.MaxPooling2D , } models.Model.fit , models.Sequential.fit_generator , def generate ( ) : Regularizers from keras.layers import noise 'TFOptimizer ' ,","['docs/README.md', 'docs/autogen.py', 'docs/structure.py']",Moved the structure of the keras documentation into a separate file . ( # 11722 )
163,8461b0920a61a50d83fd147bb4e8c16c4e9126ff,2018-12-23 17:48:13+01:00,PYTHONPATH= $ PWD : $ PYTHONPATH py.test -- pep8 -m pep8 -n0 & & py.test tests/test_documentation.py ; ENV LC_ALL=C.UTF-8 ENV LANG=C.UTF-8 # install mkdocs pip install mkdocs -- progress-bar off theano \ theano & & \ mkdocs \ & & \ PYTHONPATH= $ PWD : $ PYTHONPATH py.test -- pep8 -m pep8 -n0 & & py.test tests/test_documentation.py & & cd docs & & python autogen.py & & mkdocs build ;,"['.travis.yml', 'docker/Dockerfile']",Add document generation test ( # 11913 )
164,a9cd4bfc8fcc7ed43e092c84415c261d30e9c79f,2018-12-21 13:13:49-08:00,"def _gcs_copy ( source_filepath , target_filepath , overwrite=True ) : `` `` '' Proxy for tensorflow.python.lib.io.file_io.file_exists class . Mocks the assert_allclose ( w , org_w ) raise IOError ( ' { } does not exist'.format ( filepath ) ) return tf_file_io.file_exists ( filename ) save_model ( model , gcs_filepath , overwrite=False ) `` `` '' Checks if ` filepath ` is referencing a google storage bucket . Since the bucket name can be provided using an environment variable , it is mock_fio.read = self.local_objects [ filepath ] .read elif filename.startswith ( self._gcs_prefix ) : if not self._is_started : save_function ( obj , filepath , overwrite , * args , * * kwargs ) `` `` '' Stop mocking of ` self.file_io_module ` if real bucket not y = np.random.random ( ( 1 , 3 , 3 ) ) raise ImportError ( if filename.startswith ( self._gcs_prefix ) : ask.return_value = False copy to . self.patched_file_io.stop ( ) copy from . self.local_objects = { } elif mode == 'wb ' : return load_function ( * args , * * kwargs ) from keras.utils.test_utils import tf_file_io_proxy specified using the GCS_TEST_BUCKET environment variable . from io import BytesIO from tensorflow.python.lib.io import file_io as tf_file_io # we should not use same filename in several tests to allow for parallel filepath , _args , _kwargs = extract_named_arg ( ask.return_value = True except : model.set_weights ( new_weights ) the function if a real GCS bucket is not available for testing . class tf_file_io_proxy ( object ) : custom_objects= { 'custom_opt ' : custom_opt , This decorator parses the ` filepath ` argument of the ` load_function ` and # test that new updates are the same with both models res = load_function ( * _args , * * _kwargs ) out2 = new_model.predict ( x ) else : bucket_name = bucket_name [ len ( self._gcs_prefix ) : ] mock_module = Mock ( ) `` `` '' If a ` bucket_name ` is provided , either as an input argument or by setting the os.path.basename ( filepath ) ) load_function : The function to wrap , with requirements : ask.assert_called_once ( ) def extract_named_arg ( f , name , args , kwargs ) : # will mock gcs locally for tests os.remove ( fname ) return res if argname == name : `` `` '' Copies a file to/from/within Google Cloud Storage ( GCS ) . 'custom_loss ' : custom_loss } ) raise AssertionError ( ' { } does not exist'.format ( filepath ) ) def test_saving_overwrite_option_gcs ( ) : try : transfers the file to GCS if ` filepath ` starts with `` gs : // '' . with tf_file_io.FileIO ( source_filepath , mode='rb ' ) as source_f : _test_bucket_env_key = 'GCS_TEST_BUCKET ' os.remove ( fname ) argnames = inspect.getargspec ( f ) [ 0 ] new_out = new_model.predict ( x ) if not proceed : for further details . self._check_started ( ) assert ask.call_count == 2 # test that new updates are the same with both models for w , new_w in zip ( new_model.get_weights ( ) , new_weights ) : 'custom_loss ' : custom_loss } } new_model = load_model ( fname ) _gcs_prefix = 'gs : // ' raise ValueError ( 'keras.engine.saving.tf_file_io ' save_model ( model , fname ) def assert_exists ( self , filepath ) : raise ValueError ( 'function { } has no argument { } '.format ( f , name ) ) return mock_fio def test_save_load_weights_gcs ( ) : recommended to use method ` get_filepath ( filename ) ` in tests to make them def _is_gcs_location ( filepath ) : Example : 'could not access provided bucket { } '.format ( self.bucket_path ) ) load_function , 'filepath ' , args , kwargs ) self.local_objects [ filepath ] = BytesIO ( ) self.patched_file_io = patched_file_io y2 = np.random.random ( ( 1 , 3 , 3 ) ) self.file_io_module = file_io_module mock_fio = MagicMock ( ) try : def file_exists ( self , filename ) : model = load_model ( fname ) mock_fio.__enter__ = Mock ( return_value=mock_fio ) new_model = load_model ( gcs_filepath ) import numpy as np if tf_file_io is None : mock_module.delete_file = self.delete_file _gcs_copy ( filepath , tmp_filepath ) ' { } only supports wrapping of FileIO for ` mode ` `` rb '' or `` wb '' ' ) from unittest.mock import patch if not self.file_exists ( filepath ) : model.add ( Dense ( 2 , input_shape= ( 3 , ) ) ) `` `` '' Proxy for tensorflow.python.lib.io.file_io.FileIO class . Mocks the class for w , org_w in zip ( model.get_weights ( ) , org_weights ) : gcs_filepath = file_io_proxy.get_filepath ( if self._is_started : third positional argument should be the ` overwrite ` option indicating self.local_objects [ filepath ] .seek ( 0 ) self._is_started = True raise RuntimeError ( 'start called on already started tf_file_io_proxy ' ) gcs_filepath = file_io_proxy.get_filepath ( filename=fname ) x = np.random.random ( ( 1 , 3 ) ) filename='test_saving_overwrite_option_gcs.h5 ' ) if name in kwargs : with tf_file_io.FileIO ( target_filepath , mode='wb ' ) as target_f : return tf_file_io.FileIO ( name , mode ) fetches the required object from GCS if ` filepath ` starts with `` gs : // '' . if _is_gcs_location ( filepath ) : _gcs_copy ( tmp_filepath , filepath , overwrite ) return arg , args , kwargs def stop ( self ) : credentials must be available , see : self.mock_gcs = False filepath = name model.save_weights ( gcs_filepath ) 'gs : // ' prefix ) . A bucket name provided with argument precedes what is if tf_file_io is None : self.local_objects = None mock_module.file_exists = self.file_exists os.remove ( tmp_filepath ) Google Cloud Storage , for witch the tensorflow ` file_io ` package is used . if self.mock_gcs : # will use real bucket for tests import tempfile def __enter__ ( self ) : assert_allclose ( w , new_w ) mock_fio.write = self.local_objects [ filepath ] .write `` `` '' Context manager for mock patching ` tensorflow.python.lib.io.file_io ` in tests . self.stop ( ) out = model.predict ( x ) ask.assert_called_once ( ) save_model ( model , gcs_filepath ) `` ` overwrite : Whether we should overwrite an existing file/object at the target if mode == 'rb ' : raise RuntimeError ( 'tf_file_io_proxy is not started ' ) from functools import wraps location , or instead ask the user with a manual prompt . model.load_weights ( gcs_filepath ) except : model = Sequential ( ) model.set_weights ( new_weights ) raise ImportError ( 'Google Cloud Storage file transfer requires TensorFlow . ' ) This decorator parses the ` filepath ` argument of the ` save_function ` and def allow_write_to_gcs ( save_function ) : patched_file_io = patch ( self.file_io_module , new=mock_module ) `` `` '' Returns filename appended to bucketpath '' '' '' finally : `` `` '' Function decorator to support loading from Google Cloud Storage ( GCS ) . out_2 = model.predict ( x2 ) os.remove ( filename ) for w , org_w in zip ( new_model.get_weights ( ) , org_weights ) : # execution source_filepath : String , path to the file on filesystem or object on GCS to assert_allclose ( out , new_out , atol=1e-05 ) def allow_read_from_gcs ( load_function ) : Note : the file is temporarily copied to local filesystem from GCS before loaded . self.bucket_name = bucket_name for w , org_w in zip ( new_model.get_weights ( ) , org_weights ) : new_model_gcs = load_model ( gcs_filepath ) import numpy as np self.patched_file_io = None target_f.write ( source_f.read ( ) ) def load_wrapper ( * args , * * kwargs ) : return return filename in self.local_objects new_model.train_on_batch ( x , y ) with patch ( 'keras.engine.saving.ask_to_proceed_with_overwrite ' ) as ask : new_model.train_on_batch ( x2 , y2 ) `` `` '' return open ( filepath , mode ) for new_model in [ new_model_disk , new_model_gcs ] : ask.return_value = False tf_file_io.delete_file ( filename ) if filepath.startswith ( self._gcs_prefix ) : available for testing '' '' '' Note : the file is temporarily writen to local filesystem before copied to GSC . return os.path.exists ( filename ) file_io_proxy.delete_file ( gcs_filepath ) # cleanup out2 = model.predict ( x ) for w , new_w in zip ( new_model.get_weights ( ) , new_weights ) : except ImportError : filename='test_save_load_weights_gcs.h5 ' ) def start ( self ) : tmp_filepath = os.path.join ( tempfile.gettempdir ( ) , whether we should overwrite an existing file/object at the target tf_file_io.is_directory ( self.bucket_path ) load_kwargs = { 'custom_objects ' : { 'custom_opt ' : custom_opt , from mock import patch filepath : The location to check . if not overwrite and tf_file_io.file_exists ( target_filepath ) : model.train_on_batch ( x2 , y2 ) save_function : The function to wrap , with requirements : new_model_gcs = load_model ( gcs_filepath , * * load_kwargs ) arg = kwargs.pop ( name ) # check that bucket exists and is accessible _kwargs [ 'filepath ' ] = tmp_filepath file_io_proxy.assert_exists ( gcs_filepath ) model.train_on_batch ( x , y ) assert_allclose ( out , out2 , atol=1e-05 ) second positional argument should indicate the location to save to . save_function ( obj , tmp_filepath , True , * args , * * kwargs ) def save_wrapper ( obj , filepath , overwrite=True , * args , * * kwargs ) : return save_wrapper def test_saving_overwrite_option ( ) : def __exit__ ( self , exc_type , exc_val , exc_tb ) : The purpose of this class is to be able to tests model saving/loading to/from return self._gcs_prefix + self.bucket_name return arg , args [ : i ] + args [ i + 1 : ] , kwargs assert_allclose ( w , new_w ) if a real GCS bucket is not available for testing . save_model ( model , fname , overwrite=False ) `` ` python bucket_name : String identifier of * a real * GCS bucket ( with or without the with tf_file_io_proxy ( 'keras.engine.saving.tf_file_io ' ) as file_io_proxy : NOTE that only part of the module is mocked and that the same Exceptions file_io_module : String identifier of the file_io module import to patch . E.g assert_allclose ( out_2 , new_out_2 , atol=1e-05 ) new_out_2 = new_model.predict ( x2 ) out = model.predict ( x ) proceed = ask_to_proceed_with_overwrite ( target_filepath ) def __init__ ( self , file_io_module=None , bucket_name=None ) : function if a real GCS bucket is not available for testing . raise ValueError ( ' ` file_io_module ` must be provided for mocking ' ) should have one _named_ argument ` filepath ` indicating the location to If a ` bucket_name ` is not provided , an identifier of the import of the file_io def bucket_path ( self ) : # Arguments model = load_model ( fname , if not self.mock_gcs : `` `` '' Proxy for tensorflow.python.lib.io.file_io.delete_file function . Mocks with patch ( 'keras.engine.saving.ask_to_proceed_with_overwrite ' ) as ask : new_model = load_model ( fname ) return os.path.join ( self.bucket_path , filename ) import inspect new_model_disk = load_model ( fname , * * load_kwargs ) else : new_weights = [ np.random.random ( w.shape ) for w in org_weights ] def _check_started ( self ) : from mock import patch , Mock , MagicMock def delete_file ( self , filename ) : `` `` '' Convenience method for verifying that a file exists after writing . '' '' '' `` `` '' Start mocking of ` self.file_io_module ` if real bucket not raise RuntimeError ( 'stop called on unstarted tf_file_io_proxy ' ) x2 = np.random.random ( ( 1 , 3 ) ) assert_allclose ( w , org_w ) Arguments : self.local_objects.pop ( filename ) self.patched_file_io.start ( ) def get_filepath ( self , filename ) : self.start ( ) import os load from . pass with and without a real GCS bucket during testing . See example below . if file_io_module is None : `` `` '' Returns the full GCS bucket path '' '' '' self.bucket_name = 'mock-bucket ' mock_module.FileIO = self.FileIO _ , fname = tempfile.mkstemp ( '.h5 ' ) assert ask.call_count == 2 from six import string_types new_model_disk = load_model ( fname ) model.set_weights ( [ np.random.random ( w.shape ) for w in org_weights ] ) module to mock must be provided , using the ` file_io_module ` argument . return isinstance ( filepath , string_types ) and filepath.startswith ( 'gs : // ' ) `` `` '' Function decorator to support saving to Google Cloud Storage ( GCS ) . for i , ( argname , arg ) in enumerate ( zip ( argnames , args ) ) : self.mock_gcs = True if bucket_name.startswith ( self._gcs_prefix ) : 'tensorflow must be installed to read/write to GCS ' ) raise IOError ( return self https : //cloud.google.com/video-intelligence/docs/common/auth environment variable GCS_TEST_BUCKET , * NO mocking * will be done and files will be transferred to the real GCS bucket . For this to work , valid Google application def FileIO ( self , name , mode ) : gcs_filepath = file_io_proxy.get_filepath ( filename='model.h5 ' ) if bucket_name is None : if filepath not in self.local_objects : self._is_started = False org_weights = model.get_weights ( ) assert not ( w == org_w ) .all ( ) return load_wrapper ask.return_value = True bucket_name = os.environ.get ( self._test_bucket_env_key , None ) are not raised in mock implementation . tf_file_io = None self.file_io_module = None from unittest.mock import patch , Mock , MagicMock target_filepath : String , path to the file on filesystem or object on GCS to","['keras/engine/network.py', 'keras/engine/saving.py', 'keras/utils/test_utils.py', 'tests/test_model_saving.py']",[ P ] [ RELNOTES ] Support seamless load/save of models and weights ( incl . checkpoints ) to Google Storage ( # 11636 )
165,0cfa5c2709906a7a76f552f71a562f899e408695,2018-12-21 13:09:45-08:00,"PREPROCESS_FN = resnet.preprocess_input def load_img ( fname , input_size , preprocess_fn ) : # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # model = model_class ( input_shape= ( input_size , input_size , 3 ) ) final_params = model.get_layer ( pred_layer ) .get_weights ( ) original_size = ( original_img.shape [ 1 ] , original_img.shape [ 0 ] ) # The following parameters can be changed to other models final_params = ( final_params [ 0 ] .reshape ( NETWORK_INPUT_SIZE , import keras.applications.resnet50 as resnet PRED_LAYER = `` fc1000 '' class_activation_map += cams [ 0 , : , : , i ] plt.show ( ) NETWORK_INPUT_SIZE = 224 img = cv2.resize ( original_img , ( input_size , input_size ) ) # 4. post processing input_size=NETWORK_INPUT_SIZE , plt.imshow ( original_img , alpha=0.5 ) imgs = np.expand_dims ( preprocess_fn ( img ) , axis=0 ) x = Conv2D ( filters=N_CLASSES , kernel_size= ( import cv2 original_img = cv2.imread ( fname ) [ : , : , : :-1 ] return cam_model import numpy as np N_CLASSES = 1000 model = get_cam_model ( resnet.ResNet50 , class_activation_map = postprocess ( preds , cams ) LAST_CONV_LAYER , # 5. plot image+cam to original size preds , cams = model.predict ( imgs ) cam_model.get_layer ( `` predictions_2 '' ) .set_weights ( final_params ) last_conv_output ) from keras.layers import UpSampling2D , Conv2D last_conv_layer= '' activation_49 '' , idxes = np.argsort ( preds [ 0 ] ) [ -top_k : ] # 1. load image x = UpSampling2D ( size= ( 32 , 32 ) , interpolation= '' bilinear '' ) ( input_size=224 , import matplotlib.pyplot as plt 1 , 1 , -1 , N_CLASSES ) , final_params [ 1 ] ) return imgs , original_img , original_size for i in idxes : from keras.models import Model # e.g . ) InceptionResnetV2 / NASNetLarge imgs , original_img , original_size = load_img ( INPUT_IMG_FILE , last_conv_output = model.get_layer ( last_conv_layer ) .output plt.imshow ( cv2.resize ( class_activation_map , original_size ) , cmap='jet ' , alpha=0.5 ) # - * - coding : utf-8 - * - def get_cam_model ( model_class , 1 , 1 ) , name= '' predictions_2 '' ) ( x ) class_activation_map = np.zeros_like ( cams [ 0 , : , : , 0 ] ) MODEL_CLASS = resnet.ResNet50 pred_layer= '' fc1000 '' ) : INPUT_IMG_FILE = `` dog.jpg '' # that use global average pooling . # number of imagenet classes def postprocess ( preds , cams , top_k=1 ) : # Please set an appropriate image file cam_model = Model ( inputs=model.input , return class_activation_map preprocess_fn=resnet.preprocess_input ) PRED_LAYER ) outputs= [ model.output , x ] ) # 2. prediction LAST_CONV_LAYER = `` activation_49 ''",['examples/class_activation_maps.py'],"Class activation maps using resnet50 , inception-resnet-v2 , and nasnet model ( # 11820 )"
166,c1c4afe60b1355a6c0e83577791a0423f37a3324,2018-12-21 13:08:55-08:00,"# Both 'sample_weights ` and 'class_weights ` . return class_sample_weight # Everything has weight 1 by default . weights = np.asarray ( [ class_weight [ cls ] for cls in y_classes ' ` class_weight ` argument will be ignored . ' ) def test_sample_weights ( ) : sample_weights = np.array ( [ 0.5 , 1. , 1. , 0. , 2 . ] ) class_weights = { 0 : 0.5 , 1 : 1. , 2 : 1.5 } # Only ` class_weights ` . if sample_weight_mode is None : elif y.shape [ 1 ] == 1 : warnings.warn ( 'Found both ` sample_weight ` and ` class_weight ` : ' return np.ones ( ( y.shape [ 0 ] , y.shape [ 1 ] ) , dtype=K.floatx ( ) ) return sample_weight if sample_weight is not None and class_weight is not None : if sample_weight is not None : return sample_weight weights = training_utils.standardize_weights ( y , sample_weights ) expected = sample_weights * np.array ( [ 0.5 , 1. , 0.5 , 0.5 , 1.5 ] ) if len ( class_sample_weight ) ! = len ( y_classes ) : y_classes = np.argmax ( y , axis=1 ) weights = training_utils.standardize_weights ( y , sample_weights , if sample_weight_mode is None : if y.shape [ 1 ] > 1 : weights = training_utils.standardize_weights ( y , class_weight=class_weights ) if cls in class_weight ] ) [ class_weight [ cls ] for cls in y_classes if cls in class_weight ] ) assert np.allclose ( weights , np.array ( [ 0.5 , 1. , 0.5 , 0.5 , 1.5 ] ) ) if class_sample_weight is not None : if y.shape [ 1 ] > 1 : assert np.allclose ( weights , expected ) weight array . If both ` sample_weights ` and ` class_weights ` are provided , if len ( weights ) ! = len ( y_classes ) : the weights are multiplied together . # Only ` sample_weights ` . assert np.allclose ( weights , sample_weights ) return weights return sample_weight * class_sample_weight class_weights ) return np.ones ( ( y.shape [ 0 ] , ) , dtype=K.floatx ( ) ) class_sample_weight = None elif y.shape [ 1 ] == 1 : if sample_weight is not None and class_sample_weight is not None : elif isinstance ( class_weight , dict ) : y_classes = np.reshape ( y , y.shape [ 0 ] ) y_classes = np.argmax ( y , axis=1 ) return np.ones ( ( y.shape [ 0 ] , y.shape [ 1 ] ) , dtype=K.floatx ( ) ) y = np.array ( [ 0 , 1 , 0 , 0 , 2 ] ) else : weight array . return np.ones ( ( y.shape [ 0 ] , ) , dtype=K.floatx ( ) ) y_classes = np.reshape ( y , y.shape [ 0 ] ) if len ( y.shape ) == 2 : if isinstance ( class_weight , dict ) : class_sample_weight = np.asarray (","['keras/engine/training_utils.py', 'tests/keras/engine/test_training.py']",Make ` sample_weights ` and ` class_weights ` multiplicative . ( # 11914 )
167,8f41e41eda6e8ea96403cae5798a5a89c8bb5605,2018-12-20 18:36:33+01:00,"`` `` '' Decorator which uses ` spawn ` when possible . def next ( x ) : yield X , y return wrapper yield X This is useful on Travis to avoid memory issues . use_multiprocessing=True ) workers=WORKERS , workers=0 , steps_per_epoch=STEPS_PER_EPOCH , workers=WORKERS , # - All worker threads share the SAME generator steps_per_epoch=samples , steps=STEPS , arr = np.load ( 'data_threads.npz ' ) # - All worker threads share the SAME generator verbose=1 , max_queue_size=10 , steps=good_batches + 1 , workers=WORKERS , use_multiprocessing=True ) # - Produce data on 1 worker thread , consume on main thread : if sys.version_info > ( 3 , 4 ) : # - Produce data on 4 worker threads , consume on main thread : use_multiprocessing=False ) workers=0 , import six max_queue_size=10 , # - Make sure ` RuntimeError ` exception bubbles up import multiprocessing as mp # - Produce and consume data without a queue on main thread steps=good_batches * WORKERS + 1 , return x.next ( ) workers=0 , # - For Sequence model.fit_generator ( custom_generator ( ) , max_queue_size=10 , workers=0 , steps=STEPS , out = func ( * args , * * kwargs ) max_queue_size=10 , start = batch_index return out max_queue_size=10 , y = arr_labels [ start : end ] good_batches = 3 os.remove ( 'data.npz ' ) model.predict_generator ( custom_generator ( ) , use_multiprocessing=False ) validation_steps=None , max_queue_size=10 , workers=WORKERS , use_multiprocessing=False ) steps=good_batches + 1 , `` `` '' Decorator to test both Unix ( fork ) and Windows ( spawn ) '' '' '' if sys.version_info > ( 3 , 4 ) and os.name ! = 'nt ' : steps=good_batches + 1 , max_queue_size=10 , use_multiprocessing=True ) steps=STEPS , steps=STEPS , steps=good_batches + 1 , steps=good_batches * WORKERS + 1 , with pytest.raises ( RuntimeError ) : steps_per_epoch=STEPS_PER_EPOCH , use_multiprocessing=True ) steps=good_batches * WORKERS + 1 , model.add ( Dense ( 1 , input_shape= ( 2 , ) ) ) while True : workers=0 , use_multiprocessing=True ) K.backend ( ) in { 'tensorflow ' , 'cntk ' } and 'TRAVIS_PYTHON_VERSION ' in os.environ , use_multiprocessing=True ) import warnings np.savez ( 'data_threads.npz ' , * * { 'data ' : arr_data , 'labels ' : arr_labels } ) n_samples = 50 workers=WORKERS , if sys.version_info < ( 3 , ) : def next ( x ) : batch_index = np.random.randint ( 0 , n_samples - batch_size ) model.predict_generator ( custom_generator ( ) , `` `` '' Raises an exception after a few good batches '' '' '' # - Make sure the value of ` use_multiprocessing ` is ignored # - Worker thread is the only thread running the generator validation_data=DummySequence ( ) , for i in range ( good_batches ) : workers=0 , model.evaluate_generator ( custom_generator ( ) , workers=WORKERS , max_queue_size=10 , os.remove ( 'data.npz ' ) # - Produce and consume data without a queue on main thread import sys model = Sequential ( ) workers=0 , workers=1 , model.predict_generator ( custom_generator ( ) , def test_multithreading_evaluate_error ( ) : model.evaluate_generator ( custom_generator ( ) , def test_multithreading_predict_error ( ) : use_multiprocessing=False ) use_multiprocessing=False ) validation_steps=None , use_multiprocessing=False ) os.remove ( 'data_threads.npz ' ) use_multiprocessing=True ) workers=WORKERS , reason='Generators do not work with ` spawn ` . ' ) mp.set_start_method ( 'spawn ' , force=True ) `` `` '' y = arr [ 'labels ' ] [ start : end ] model.fit_generator ( DummySequence ( ) , mp.set_start_method ( 'fork ' , force=True ) model.evaluate_generator ( custom_generator ( ) , X = arr_data [ start : end ] def test_multithreading_fit_error ( ) : use_multiprocessing=False ) def test_multithreading_from_file ( ) : X = arr [ 'data ' ] [ start : end ] arr_data = np.random.randint ( 0 , 256 , ( 50 , 2 ) ) raise RuntimeError use_multiprocessing=True ) use_multiprocessing=False ) model.fit_generator ( DummySequence ( ) , # - Produce data on 4 worker threads , consume on main thread : steps_per_epoch=samples , workers=WORKERS , validation_steps=1 , # - For Sequence batch_size = 10 workers=0 , n_samples = 50 steps=good_batches * WORKERS + 1 , model.fit_generator ( custom_generator ( ) , samples = batch_size * ( good_batches + 1 ) model.fit_generator ( custom_generator ( ) , use_multiprocessing=False ) validation_steps=None , model.add ( Dense ( 1 , input_shape= ( 2 , ) ) ) model.compile ( loss='mse ' , optimizer='adadelta ' ) arr_labels = np.random.randint ( 0 , 2 , 50 ) def use_spawn ( func ) : # - Make sure the value of ` use_multiprocessing ` is ignored def custom_generator ( ) : def test_multithreading_predicting ( ) : else : verbose=1 , workers=1 , workers=WORKERS , batch_size = 10 def test_multithreading_evaluating ( ) : max_queue_size=10 , return x.next ( ) end = start + batch_size # Build a NN epochs=1 , validation_steps=None , six.PY2 and 'TRAVIS_PYTHON_VERSION ' in os.environ , def wrapper ( * args , * * kwargs ) : with pytest.raises ( RuntimeError ) : skip_generators = pytest.mark.skipif ( K.backend ( ) in { 'tensorflow ' , 'cntk ' } and if sys.version_info < ( 3 , ) : max_queue_size=10 , workers=0 , workers=WORKERS , use_multiprocessing=True ) use_multiprocessing=False ) validation_data=custom_generator ( True ) , use_multiprocessing=True ) 'TRAVIS_PYTHON_VERSION ' in os.environ , # - Make sure ` RuntimeError ` exception bubbles up # - Main thread runs the generator without a queue epochs=1 , model.predict_generator ( custom_generator ( ) , workers=0 , model.evaluate_generator ( custom_generator ( ) , validation_steps=1 , model.fit_generator ( custom_generator ( ) ,","['tests/keras/utils/data_utils_test.py', 'tests/test_multiprocessing.py']",Split Generators tests from Sequence tests ( # 11901 )
168,f199d00ea5bb2230230a34193fbce7ecb5a4d4d2,2018-12-18 10:47:46-05:00,"optimizer = 'rmsprop ' loss_weights = [ 1. , 0.5 ] tracker_cb = LambdaCallback ( on_epoch_begin=on_epoch_begin , model = Model ( [ a , b ] , a_2 ) # TODO : resolve flakyness issue . Tracked with # 11560 assert trained_batches == list ( range ( 12 ) ) * 5 def test_fit_generator_shape ( ) : self.trained_batches.append ( batch ) self.trained_epochs = [ ] def on_batch_begin ( self , batch , logs ) : assert tracker_cb.trained_batches == list ( range ( 3 ) ) * 5 def test_model_methods ( ) : # test starting from non-zero initial epoch return model single_output_model = get_model ( num_outputs=1 ) trained_batches.append ( batch ) from keras.callbacks import Callback super ( TrackerCallback , self ) .__init__ ( ) def test_fit_generator ( ) : def on_epoch_begin ( self , epoch , logs ) : trained_epochs.append ( epoch ) model = get_model ( num_outputs=2 ) class TrackerCallback ( Callback ) : def on_batch_begin ( batch , logs ) : assert trained_epochs == [ 0 , 1 , 2 , 3 , 4 ] assert tracker_cb.trained_batches == list ( range ( 12 ) ) * 5 model = Model ( [ a , b ] , [ a_2 , b_2 ] ) # TODO : resolve flakyness issue . Tracked with # 11560 assert tracker_cb.trained_epochs == [ 2 , 3 , 4 ] single_output_model = Model ( [ a , b ] , a_2 ) self.trained_epochs.append ( epoch ) trained_batches = [ ] def __init__ ( self ) : def get_model ( num_outputs=1 ) : self.trained_batches = [ ] trained_epochs = [ ] # test starting from non-zero initial epoch if num_outputs == 1 : # define tracer callback model = Model ( [ a , b ] , [ a_2 , b_2 ] ) def on_epoch_begin ( epoch , logs ) : on_batch_begin=on_batch_begin ) else : @ flaky ( rerun_filter=lambda err , * args : issubclass ( err [ 0 ] , AssertionError ) ) assert trained_epochs == [ 2 , 3 , 4 ] def test_model_methods ( ) : assert tracker_cb.trained_epochs == [ 0 , 1 , 2 , 3 , 4 ] loss = 'mse ' tracker_cb = TrackerCallback ( ) # define tracer callback assert trained_batches == list ( range ( 3 ) ) * 5",['tests/keras/engine/test_training.py'],Splitting the test_model_method which had more than 400 LOC . ( # 11398 )
169,29e740e0965be5f460f67692a284a9a2deb6bb04,2018-12-17 22:37:32+01:00,"return np.stack ( x , axis=axis ) { { np_implementation } } assert_list_pairwise ( results ) axis=stack_axis , concat_args=True ) else : check_two_tensor_operation ( 'stack ' , ( 5 , 4 , 6 , 10 ) , stack_axis = 3 def test_stack ( self ) : tensor_list = [ np.random.randn ( 5 , 4 , 6 , 10 ) for _ in range ( 5 ) ] results = [ ] tensor_list_var = [ k.variable ( tensor ) for tensor in tensor_list ] ( 5 , 4 , 6 , 10 ) , WITH_NP , def stack ( x , axis=0 ) : for k in WITH_NP : results.append ( out ) out = k.eval ( k.stack ( tensor_list_var , axis=stack_axis ) ) if WITH_NP [ 0 ] == KC :","['keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",add stack to numpy backend ( # 11629 )
170,bfa87eb1b0ab53c5f1efab1ec1d91377678046af,2018-12-17 10:19:38-05:00,"module_name = module_name [ 6 : ] link = ROOT + module_name.replace ( ' . ' , '/ ' ) + ' # ' + cls.__name__.lower ( ) module_name = clean_module_name ( cls.__module__ ) return link def class_to_docs_link ( cls ) :",['docs/autogen.py'],Removed unused function in autogen.py ( # 11872 )
171,bf0f25f79a368aecc79515be6a7f20a87de2c436,2018-12-17 10:18:50-05:00,"return x / ( 1 + np.abs ( x ) ) check_single_tensor_operation ( 'softsign ' , ( 4 , 10 ) , WITH_NP ) { { np_implementation } } def softsign ( x ) :","['keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Add softsign in numpy backend ( # 11879 )
172,ca802e1b6b9fb47194fe62c2fd0df03de1002c84,2018-12-17 08:32:47+01:00,"missing_idx = 10 enqueuer = OrderedEnqueuer ( TimeOutSequence ( [ 3 , 2 , 2 , 3 ] ) , future = self.queue.get ( block=True ) def test_missing_inputs ( ) : self.queue.task_done ( ) return super ( TimeOutSequence , self ) .__getitem__ ( item ) TimeOutSequence ( [ 3 , 2 , 2 , 3 ] ) ) , use_multiprocessing=True ) warning_msg = `` The input { } could not be retrieved . `` .format ( missing_idx ) future.idx = i self.queue.task_done ( ) self.queue.put ( future , block=True ) ' It could be because a worker has died . '.format ( idx ) , try : inputs = self.sequence [ idx ] max_queue_size=1 ) reason='Generators do not work with ` spawn ` . ' ) 'We do not have any information on the lost sample . ' , callbacks= [ tracker_cb ] ) enqueuer = GeneratorEnqueuer ( create_finite_generator_from_sequence_pcs ( with pytest.warns ( UserWarning , match='An input could not be retrieved . ' ) : use_multiprocessing=True ) else : executor.apply_async ( get_index , ( self.uid , i ) ) , block=True ) class TimeOutSequence ( DummySequence ) : for _ in range ( 11 ) : self.queue.put ( reason='Generators can not use ` spawn ` . ' ) future = executor.apply_async ( get_index , ( self.uid , i ) ) ' It could be because a worker has died . ' out = func ( * args , * * kwargs ) next ( gen_output ) inputs = self.queue.get ( block=True ) .get ( ) K.backend ( ) in { 'tensorflow ' , 'cntk ' } and 'TRAVIS_PYTHON_VERSION ' in os.environ , for _ in range ( 4 * missing_idx ) : callbacks= [ tracker_cb ] , import os six.PY2 and 'TRAVIS_PYTHON_VERSION ' in os.environ , import warnings assert 12 * 5 < = len ( val_seq.logs ) < = ( 12 * 5 ) + 2 # the queue may be full . UserWarning ) skip_generators = pytest.mark.skipif ( K.backend ( ) in { 'tensorflow ' , 'cntk ' } and assert len ( val_seq.logs ) == 12 * 5 enqueuer.start ( 3 , 10 ) reason='Takes 150s to run ' ) warnings.warn ( 'The input { } could not be retrieved . ' gen_output = enqueuer.get ( ) and 'TRAVIS_PYTHON_VERSION ' in os.environ , except mp.TimeoutError : 'An input could not be retrieved . ' out = func ( * args , * * kwargs ) 'TRAVIS_PYTHON_VERSION ' in os.environ , def __getitem__ ( self , item ) : with pytest.warns ( UserWarning , match=warning_msg ) : func ( * args , * * kwargs ) inputs = future.get ( timeout=30 ) time.sleep ( 120 ) import keras.backend as K if item == missing_idx : idx = future.idx continue","['keras/utils/data_utils.py', 'tests/keras/engine/test_training.py', 'tests/keras/legacy/interface_test.py', 'tests/keras/utils/data_utils_test.py']",[ Do n't Merge ] [ Need discussion ] First-step toward a working mp on Travis ( # 11521 )
173,4b54657ab4806b0aaef8f8eeb973edb83c3d3483,2018-12-16 16:13:16-08:00,"model.compile ( loss='mse ' , optimizer='sgd ' ) model = create_masking_model ( ) assert loss == 0 return model x = y = np.array ( [ [ [ 0 ] , [ 0 ] ] ] ) model.add ( Masking ( mask_value=0 , input_shape= ( None , 1 ) ) ) model.add ( Masking ( mask_value=0 , input_shape= ( 2 , 1 ) ) ) def create_masking_model ( ) : model.add ( TimeDistributed ( Dense ( 1 , kernel_initializer='one ' ) ) ) score_array /= K.mean ( mask ) + K.epsilon ( ) def test_masking_is_all_zeros ( ) : loss = model.train_on_batch ( x , y ) model = Sequential ( ) score_array /= K.mean ( mask ) model.compile ( loss='mse ' , optimizer='sgd ' ) model.add ( TimeDistributed ( Dense ( 1 , kernel_initializer='one ' ) ) ) model = Sequential ( )","['keras/engine/training_utils.py', 'tests/test_loss_masking.py']",[ P ] Fixed NaN loss when given a mask with all zeros ( # 11643 )
174,88af7d0c97497b5c3a198ee9416b2accfbc72c36,2018-12-09 16:34:02-05:00,"return resnet_v2.ResNet101V2 ( * args , * * kwargs ) from __future__ import division return resnet_v2.preprocess_input ( * args , * * kwargs ) from keras_applications import resnet_v2 def ResNet50 ( * args , * * kwargs ) : def ResNet152V2 ( * args , * * kwargs ) : return resnext.preprocess_input ( * args , * * kwargs ) from __future__ import absolute_import def ResNet101V2 ( * args , * * kwargs ) : try : from .resnet_v2 import ResNet50V2 , ResNet101V2 , ResNet152V2 def decode_predictions ( * args , * * kwargs ) : from .resnext import ResNeXt50 , ResNeXt101 from .resnet import ResNet101 , ResNet152 return resnet.preprocess_input ( * args , * * kwargs ) resnext = None from . import keras_modules_injection return resnet.decode_predictions ( * args , * * kwargs ) from keras_applications import resnext return resnext.ResNeXt50 ( * args , * * kwargs ) from __future__ import print_function except : def preprocess_input ( * args , * * kwargs ) : return resnet.ResNet101 ( * args , * * kwargs ) def ResNeXt101 ( * args , * * kwargs ) : return resnet.ResNet152 ( * args , * * kwargs ) def ResNeXt50 ( * args , * * kwargs ) : return resnet_v2.decode_predictions ( * args , * * kwargs ) resnet_v2 = None def ResNet101 ( * args , * * kwargs ) : return resnext.decode_predictions ( * args , * * kwargs ) resnet = None def ResNet152 ( * args , * * kwargs ) : return resnet.ResNet50 ( * args , * * kwargs ) return resnet_v2.ResNet152V2 ( * args , * * kwargs ) return resnet_v2.ResNet50V2 ( * args , * * kwargs ) def ResNet50V2 ( * args , * * kwargs ) : from keras_applications import resnet return resnext.ResNeXt101 ( * args , * * kwargs )","['keras/applications/__init__.py', 'keras/applications/resnet.py', 'keras/applications/resnet_v2.py', 'keras/applications/resnext.py']","[ RELNOTES ] [ P ] Add ResNet , ResNetV2 , and ResNeXt variants ( # 11203 )"
175,4449be11edf1a6ac7298fd5c41c30a7c8cfdc292,2018-12-09 19:40:01+01:00,"in addition to the output . The returned elements of the each with shape ` ( batch_size , units ) ` . For example , the number of in addition to the output . state tensors is 1 ( for RNN and GRU ) or 2 ( for LSTM ) . each with shape ` ( batch_size , units ) ` . states list are the hidden state and the cell state , respectively .",['keras/layers/recurrent.py'],Updated docstring for LSTM layer - Specifying order of outputs ( # 11726 )
176,e90e17068eb0d9620aeeb90168cc222025ea119e,2018-12-09 18:39:57+01:00,"tft = K.constant ( npt ) expected = KNP.slice ( npt , x_start , x_size ) return x [ tuple ( slices ) ] [ 1 , 2 , 3 ] , ] ) [ 2 , 1 , 3 ] test_input = K.eval ( K.slice ( tft , x_start , x_size ) ) [ 1 , 1 , 3 ] , npt = np.array ( [ [ [ 1 , 1 , 1 ] , [ 2 , 2 , 2 ] ] , x_start = [ 1 , 0 , 0 ] slices = [ py_slice ( i , i + j ) for i , j in zip ( start , size ) ] [ [ 5 , 5 , 5 ] , [ 6 , 6 , 6 ] ] ] ) def slice ( x , start , size ) : 'only supported in tensorflow . ' ) py_slice = slice reason='tensorflow-way slice is ' [ [ 3 , 3 , 3 ] , [ 4 , 4 , 4 ] ] , assert np.allclose ( test_input , expected ) { { np_implementation } } def test_slice ( self , x_size ) :","['keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Add tensorflow-like slice in numpy backend ( # 11682 )
177,c05ef1fd95a6024155ab59656fef8dac5a45c335,2018-12-09 15:28:28+01:00,"def test_regression_predict_shape_correct_num_test_0 ( ) : reg.fit ( X_train , y_train , batch_size=batch_size , epochs=epochs ) def test_regression_predict_shape_correct_num_test_1 ( ) : return np.squeeze ( self.model.predict ( x , * * kwargs ) ) assert_regression_predict_shape_correct ( num_test=0 ) batch_size=batch_size , epochs=epochs ) build_fn=build_fn_reg , hidden_dims=hidden_dims , def assert_regression_predict_shape_correct ( num_test ) : reg = KerasRegressor ( assert preds.shape == ( num_test , ) return np.squeeze ( self.model.predict ( x , * * kwargs ) , axis=-1 ) assert_regression_predict_shape_correct ( num_test=1 ) preds = reg.predict ( X_test [ : num_test ] , batch_size=batch_size )","['keras/wrappers/scikit_learn.py', 'tests/keras/wrappers/scikit_learn_test.py']",Fixed KerasRegressor.predict ( ) squeezing length-1 output arrays . ( # 11658 )
178,7bee9a18d83899ce6dfd50c4883afc678139f4ad,2018-12-02 15:35:34+01:00,"if orig_x_ndim == 2 : if ndim ( result ) == 1 : for i in range ( a1 , 1 , -1 ) : y_squashed = False a0 += 1 x_squashed = True new_x_shape = tf.concat ( [ x_shape , tf.ones_like ( y_shape [ 2 : ] ) ] , 0 ) if None in y_shape [ 1 : ] : pattern [ 1 ] = a1 # if the inputs were originally rank 2 , we remove the added 1 dim . if y_batch_size is None : x_mid_dims = x_shape [ 1 : -1 ] result = tf.squeeze ( result , -1 ) pattern = list ( range ( y_ndim ) ) pattern [ 1 ] = a1 x_matmullabe = True # bring y 's dimension to be reduced to axis 1 . # squash trailing dimensions of y if y_squashed : do_reshape = True output_shape = [ output_shape [ 0 ] ] + x_mid_dims + [ output_shape [ -1 ] ] x_matmullabe = True y = tf.expand_dims ( y , 2 ) result = tf.squeeze ( result , 1 ) x_squashed_shape = tf.stack ( [ x_shape [ 0 ] , x_squashed_dim , x_shape [ -1 ] ] ) y_shape [ 2 : ] ] , 0 ) pattern [ -1 ] = a0 y_shape = tf.shape ( y ) y_matmullabe = False result = tf.expand_dims ( result , -1 ) # convert negative indices # bring the dimension to be reduced to axis 1 . result = tf.reduce_sum ( x * y , 1 ) # reshape to closest broadcastable shape . tf.ones_like ( x_shape [ 2 : ] ) , for i in range ( a0 , x_ndim - 1 ) : y_matmullabe = True # normalize both inputs to rank 3 . output_shape = tf.concat ( [ output_shape [ : -1 ] , y_trail_dims ] , 0 ) x_squashed_dim = tf.reduce_prod ( x_mid_dims ) # normalize both inputs to rank 3 . y_matmullabe = True output_shape [ -1 : ] ] , 0 ) new_y_shape = tf.concat ( [ y_shape [ :2 ] , y_ndim += 1 # bring x 's dimension to be reduced to last axis . if x_squashed : # 2 ) Using tf.matmul . This is more efficient but all dimensions except if y_ndim == 2 : y_squashed_dim = np.prod ( y_trail_dims ) y_batch_size = -1 if x_ndim == 2 : x_shape = list ( int_shape ( x ) ) pattern [ i ] = pattern [ i + 1 ] pattern [ i ] = pattern [ i - 1 ] # backup ndims . Need them later . if x_batch_size is None : # convert negative indices . output_shape = tf.shape ( result ) result = tf.matmul ( x , y ) if a0 ! = x_ndim - 1 : pattern = list ( range ( x_ndim ) ) x_squashed_dim = np.prod ( x_mid_dims ) x_mid_dims = x_shape [ 1 : -1 ] if do_reshape : a0 += 1 output_shape [ 0 ] = -1 else : y_trail_dims = y_shape [ 2 : ] x_squashed = True else : if x_ndim > 3 : if a0 ! = x_ndim - 1 : # bring y 's dimension to be reduced to axis 1 . elif orig_y_ndim == 2 : # if inputs were squashed , we have to reshape the matmul output . use_matmul = x_matmullabe and y_matmullabe # more memory but works with partial shape information . x = tf.transpose ( x , pattern ) if do_reshape : x_ndim += 1 if a0 ! = 1 : if x_ndim == 2 : x = reshape ( x , new_x_shape ) orig_y_ndim = y_ndim x = tf.reshape ( x , x_squashed_shape ) x = tf.expand_dims ( x , 1 ) pattern = list ( range ( x_ndim ) ) pattern = list ( range ( y_ndim ) ) output_shape = tf.concat ( [ output_shape [ :1 ] , if y_ndim > 3 : # There are 2 ways to perform theano 's batched_tensordot in tensorflow : x_mid_dims , y = tf.expand_dims ( y , 2 ) y_squashed = False orig_x_ndim = x_ndim # squash middle dimensions of x . y_squashed = True # if the inputs were originally rank 2 , we remove the added 1 dim . output_shape = output_shape [ : -1 ] + y_trail_dims x_squashed = False x_matmullabe = False pattern [ i ] = pattern [ i + 1 ] for i in range ( a0 , 1 , -1 ) : y_ndim += 1 x_ndim += 1 if output_shape [ 0 ] is None : elif orig_y_ndim == 2 : x_batch_size = -1 for i in range ( a0 , x_ndim - 1 ) : # squash trailing dimensions of y . result = tf.reshape ( result , output_shape ) # 1 ) Elementwise multiplication followed by tf.reduce_sum . This requires x = tf.expand_dims ( x , 1 ) x_shape = tf.shape ( x ) y = tf.transpose ( y , pattern ) result = tf.matmul ( x , y ) result = tf.squeeze ( result , 1 ) y = reshape ( y , new_y_shape ) orig_x_ndim = x_ndim if None in x_shape [ 1 : ] : y_squashed = True y_shape = shape ( y ) y_squashed_dim = tf.reduce_prod ( y_trail_dims ) # bring x 's dimension to be reduced to last axis . if use_matmul : y = tf.reshape ( y , y_squashed_shape ) do_reshape = False y_squashed_shape = tf.stack ( [ y_shape [ 0 ] , y_shape [ 1 ] , y_squashed_dim ] ) # if tuple , convert to list . result = tf.squeeze ( result , -1 ) pattern [ i ] = pattern [ i - 1 ] # if tuple , convert to list # backup ndims . Need them later . y_trail_dims = y_shape [ 2 : ] if orig_x_ndim == 2 : y = tf.transpose ( y , pattern ) # if rank is 2 , expand to 3 . # batch size should be known for input with rank > 3 . result = tf.reshape ( result , output_shape ) x_shape = shape ( x ) x = tf.reshape ( x , [ x_batch_size , x_squashed_dim , x_shape [ -1 ] ] ) y = tf.reshape ( y , [ y_batch_size , y_shape [ 1 ] , y_squashed_dim ] ) output_shape = list ( int_shape ( result ) ) x = tf.transpose ( x , pattern ) if a1 ! = 1 : if y_ndim == 2 : # if inputs were squashed , we have to reshape the matmul output . do_reshape = True # squash middle dimensions of x . for i in range ( a1 , 1 , -1 ) : if x_squashed : x_squashed = False if y_squashed : pattern [ 1 ] = a0 if a1 ! = 1 : pattern [ -1 ] = a0 orig_y_ndim = y_ndim assert z_shape [ 1 : ] == z_np.shape [ 1 : ] assert z_shape == z_np.shape y_shape = list ( int_shape ( y ) ) # if rank is 2 , expand to 3 . do_reshape = False","['keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Remove reduce sum impl in batch dot ( # 11768 )
179,7cd6c59789c8e469c0d3cab6bca7ae3d2d028002,2018-12-01 16:11:00-08:00,"assert markdown ( docstring ) == markdown ( test_doc1 [ 'result ' ] ) 'requests ' , from markdown import markdown 'requests ' ] , 'markdown ' ] , assert docstring == test_doc1 [ 'result ' ]","['setup.py', 'tests/test_doc_auto_generation.py']",Made the docstring processing test more robust by parsing with markdown ( # 11725 )
180,13b255dfd832dd78bca90fac142325df534d51be,2018-12-01 13:41:30-08:00,"QA2 - Two Supporting Facts | 20 | 50.0 encoded_question = RNN ( EMBED_HIDDEN_SIZE ) ( encoded_question ) encoded_question = layers.Dropout ( 0.3 ) ( encoded_question ) 52.1 % test accuracy on QA1 in 20 epochs ( 2 seconds per epoch on CPU ) 100 % test accuracy on QA1 in 20 epochs ( 2 seconds per epoch on CPU ) QA1 - Single Supporting Fact | 50 | 100.0 50 % test accuracy on QA2 in 20 epochs ( 16 seconds per epoch on CPU ) QA2 - Two Supporting Facts | 20 | 37.0 encoded_sentence = layers.Dropout ( 0.3 ) ( encoded_sentence ) The length and noise ( i.e . 'useless ' story components ) impact the ability for 37.0 % test accuracy on QA2 in 20 epochs ( 16 seconds per epoch on CPU ) print ( 'Evaluation ' ) merged = RNN ( EMBED_HIDDEN_SIZE ) ( merged ) encoded_sentence = RNN ( SENT_HIDDEN_SIZE ) ( encoded_sentence ) merged = layers.concatenate ( [ encoded_sentence , encoded_question ] ) EPOCHS = 40 print ( 'Test loss / test accuracy = { : .4f } / { : .4f } '.format ( loss , acc ) ) The length and noise ( i.e . 'useless ' story components ) impact the ability of encoded_question = layers.RepeatVector ( story_maxlen ) ( encoded_question ) print ( 'Test loss / test accuracy = { : .4f } / { : .4f } '.format ( loss , acc ) ) QA1 - Single Supporting Fact | 50 | 52.1 encoded_question = RNN ( QUERY_HIDDEN_SIZE ) ( encoded_question ) merged = layers.add ( [ encoded_sentence , encoded_question ] ) merged = layers.Dropout ( 0.3 ) ( merged ) EPOCHS = 20",['examples/babi_rnn.py'],Fix babi-rnn example ( # 11701 )
181,a8e235c35e23193f6cc68cd9d390ec37caeaec6e,2018-12-01 12:16:24-08:00,"Keras development is backed primarily by Google , and the Keras API comes packaged in TensorFlow as ` tf.keras ` . Additionally , Microsoft maintains the CNTK Keras backend . Amazon AWS is developing MXNet support . Other contributing companies include NVIDIA , Uber , and Apple ( with CoreML ) . Amazon is also currently working on developing a MXNet backend for Keras . Keras development is backed primarily by Google , and the Keras API comes packaged in TensorFlow as ` tf.keras ` . Additionally , Microsoft maintains the CNTK Keras backend . Amazon AWS is maintaining the Keras fork with MXNet support . Other contributing companies include NVIDIA , Uber , and Apple ( with CoreML ) . Amazon also has [ a fork of Keras which uses MXNet as backend ] ( https : //github.com/awslabs/keras-apache-mxnet ) .",['docs/templates/why-use-keras.md'],Some clarifications about the mxnet backend in the docs . ( # 11729 )
182,43b5e9993ab025bd07851d70de5de7fdfe461d24,2018-11-30 15:47:47-08:00,"value = tf.cast ( value , x.dtype ) gamma = tf.cast ( gamma , tf.float32 ) var = tf.cast ( var , tf.float32 ) if gamma.dtype ! = tf.float32 : if beta.dtype ! = tf.float32 : beta = tf.cast ( beta , tf.float32 ) gamma = tf.cast ( gamma , tf.float32 ) mean = tf.cast ( mean , tf.float32 ) beta = tf.cast ( beta , tf.float32 ) if var.dtype ! = tf.float32 : if K.backend ( ) == 'tensorflow ' and sample_size.dtype ! = 'float32 ' : sample_size = K.cast ( sample_size , dtype='float32 ' ) if mean.dtype ! = tf.float32 : if beta.dtype ! = tf.float32 : if gamma.dtype ! = tf.float32 : if value.dtype ! = x.dtype :","['keras/backend/tensorflow_backend.py', 'keras/layers/normalization.py']",Fixes for fused batch normalizationwith FP16 . ( # 11580 )
183,1c221934aaeab20ed5b6fa245b33b00a8b557533,2018-11-29 15:50:57+01:00,"h21 = h21 * np.expand_dims ( mask [ : , -1 ] , -1 ) assert_allclose ( last_y1 , last_y2 , atol=1e-05 ) last_output_k , output_k , last_states_k = K.rnn ( mask=mask_k , time_index = range ( input_length ) { 'go_backwards ' : True , 'mask ' : None } , initial_states_k = [ K.variable ( s ) for s in initial_states_np ] assert len ( h2 ) == 0 y = backend.dot ( inputs , w_i ) inputs_np , { 'go_backwards ' : True , 'mask ' : None , 'unroll ' : True , initial_states_np , check_rnn_operation ( step_function_k=get_step_function ( K , wi_k , wh_k ) , kwargs_list = [ if np_mask is not None : * * kwargs ) h_t1 [ np_mask [ : , t ] == 0 ] = prev [ np_mask [ : , t ] == 0 ] inputs_np=x , constants_k = None assert len ( h_k ) == 1 # test default setup initial_states_np , last_y1 = last_y1 * np.expand_dims ( mask [ : , -1 ] , -1 ) assert len ( h2 ) == 1 step_function_k , output_t , states_t = step_function ( inputs [ : , t ] , states_tm1 + constants ) for kwargs in kwargs_list : last_y1 , y1 , h1 = KNP.rnn ( x , [ wi , wh , None ] , h0 , * * kwargs ) if i % 2 == 0 : assert len ( states ) == 2 _ , x = parse_shape_or_val ( ( 32 , timesteps , input_dim ) ) if kwargs [ 'mask ' ] is not None : # implement a simple RNN def rnn ( step_function , inputs , initial_states , time_index = time_index [ : :-1 ] assert len ( last_states_k ) == len ( last_states_np ) return mask_ y1 = y1 * np.expand_dims ( mask , -1 ) h21 = K.eval ( h2 [ 0 ] ) _ , x = parse_shape_or_val ( ( num_samples , timesteps , input_dim ) ) last_output_np , output_np , last_states_np = KNP.rnn ( last_y2 , y2 , h2 = K.rnn ( rnn_fn , x_k , [ ] , output_k = K.eval ( output_k ) inputs_k = K.variable ( inputs_np ) constants=constants_k , h11 = h1 [ : , -1 ] if input_length is None : np_mask = None assert_allclose ( last_y1 , last_y2 , atol=1e-05 ) state_list = [ ] y = backend.dot ( inputs , w_i ) + backend.dot ( h , w_h ) step_function_np , for unroll in [ True , False ] : assert len ( states ) == 1 h2 = K.eval ( h2 [ 0 ] ) inputs_np , if go_backwards : mask = np.random.randint ( 2 , size= ( num_samples , timesteps ) ) output_sample , _ = step_function ( inputs [ : , 0 ] , initial_states + constants ) h1 = np.concatenate ( [ h0 , h0 ] , axis=-1 ) o_t = np.dot ( o_t , w_o ) assert_allclose ( h12 , h22 , atol=1e-05 ) if mask.dtype ! = np.bool : h22 = h22 * np.expand_dims ( mask [ : , -1 ] , -1 ) last_output_list.append ( last_y2 ) assert_allclose ( y1 , y2 , atol=1e-05 ) mask_np=None , y2 = K.eval ( y2 ) constants_np= [ c ] , atol=1e-05 ) unroll=False , input_length=None ) : assert_allclose ( h11 , h21 , atol=1e-05 ) for ( i , t ) in enumerate ( t_list ) : last_y2 = K.eval ( last_y2 ) last_y2 = last_y2 * np.expand_dims ( mask [ : , -1 ] , -1 ) 'mask should have ` shape= ( samples , time ) ` , ' h12 = h12 * np.expand_dims ( mask [ : , -1 ] , -1 ) return simple_rnn_add_constant raise ValueError ( go_backwards=False , mask=None ) if mask.shape ! = inputs.shape [ :2 ] : y2 = K.eval ( y2 ) h1 = h1 [ : , -1 ] assert len ( h_k ) == 2 h = [ ] def simple_no_states ( inputs , states ) : input_dim = 5 while mask_.ndim < x.ndim + 1 : assert_allclose ( last_output_list [ i - 1 ] , last_output_list [ i ] , def expand_mask ( mask_ , x ) : y2 = y2 * np.expand_dims ( mask , -1 ) h12 = np.concatenate ( [ h1 [ : , -1 ] , h1 [ : , -1 ] ] , axis=-1 ) h22 = K.eval ( h2 [ 1 ] ) output_tm1 = output_t assert_allclose ( state_list [ i - 1 ] [ 1 ] , state_list [ i ] [ 1 ] , atol=1e-05 ) return simple_rnn constants_np=None , # constants are appended to states in K.rnn if np_mask is not None : input_length = inputs.shape [ 1 ] last_states_k = [ K.eval ( s ) for s in last_states_k ] return y_k , [ y_k ] else : else : { 'go_backwards ' : False , 'mask ' : mask } , else : states_tm1 = initial_states # tm1 means `` t minus one '' as in `` previous timestep '' if mask_np is not None : h0_k = [ K.variable ( h0 ) ] mask_k = K.variable ( mask_np ) ] inputs_k , 'got { } '.format ( mask.shape ) ) output_tm1 = np.zeros ( output_sample.shape ) def get_step_function ( backend , w_i , w_h ) : def simple_rnn ( inputs , states ) : { 'go_backwards ' : False , 'mask ' : mask_k } , mask_ = np.expand_dims ( mask_ , axis=-1 ) if mask is not None : 'provide initial states ' ) step_function_np , last_y2 = K.eval ( last_y2 ) h0_k = [ K.variable ( h0 ) , K.variable ( np.concatenate ( [ h0 , h0 ] , axis=-1 ) ) ] def get_step_function ( backend , w_i ) : last_y1 , y1 , h1 = KNP.rnn ( x , [ wi , None , None ] , None , h = states [ 0 ] h11 = h11 * np.expand_dims ( mask [ : , -1 ] , -1 ) def rnn ( x , w , init , go_backwards=False , mask=None , unroll=False , input_length=None ) : for t in time_index : unroll=unroll , return y , [ y , backend.concatenate ( [ y , y ] , axis=-1 ) ] last_output_k = K.eval ( last_output_k ) prev = h [ i - 1 ] if i > 0 else init # expand mask so that ` mask [ : , t ] .ndim == x.ndim ` mask_k = K.variable ( mask ) * * kwargs ) h1 = h1 * np.expand_dims ( mask [ : , -1 ] , -1 ) w_i , w_h , w_o = w states_masks = [ expand_mask ( mask , state ) for state in initial_states ] if w_h is not None : num_samples = 3 go_backwards=False , mask=None ) return simple_rnn_with_extra_mock_state t_list = range ( x.shape [ 1 ] - 1 , -1 , -1 ) o.append ( o_t ) y_k = K.dot ( x_k , wi_k ) + K.dot ( h_k [ 0 ] , wh_k ) in zip ( states_masks , states_t , states_tm1 ) ] if not states : if constants is None : def check_rnn_operation ( step_function_k , outputs_list = [ ] _ , wh = parse_shape_or_val ( ( output_dim , output_dim ) ) [ h , c ] = states_and_constants outputs_list.append ( y2 ) input_length=inputs_np.shape [ 1 ] if unroll else None , output_mask = expand_mask ( mask , output_sample ) # note that numpy reference implementation is independent of ` unroll ` argument def rnn_fn ( x_k , h_k ) : from keras import backend as K assert_allclose ( state_list [ i - 1 ] , state_list [ i ] , atol=1e-05 ) return y , [ y ] return simple_no_states return y_k , [ ] h_t = np.dot ( x [ : , t ] , w_i ) state_list.append ( h2 ) outputs.append ( output_t ) states_tm1 = states_t { 'go_backwards ' : False , 'mask ' : mask_k , 'unroll ' : True , mask = mask.astype ( np.bool ) for ( i , kwargs ) in enumerate ( kwargs_list ) : np_mask = K.eval ( mask ) constants=constants_np , step_function_np=get_step_function ( KNP , wi , wh ) , { 'go_backwards ' : False , 'mask ' : None } , x_k = K.variable ( x ) { { np_implementation } } assert_allclose ( last_output_k , last_output_np , atol=1e-05 ) h2 = h2 * np.expand_dims ( mask [ : , -1 ] , -1 ) assert_allclose ( state_list [ i - 1 ] [ 0 ] , state_list [ i ] [ 0 ] , atol=1e-05 ) go_backwards=False , mask=None , constants=None , constants = [ ] y = backend.dot ( inputs , w_i ) + backend.dot ( h , w_h ) + c else : timesteps = 6 assert input_length == inputs.shape [ 1 ] assert_allclose ( output_k , output_np , atol=1e-05 ) * * kwargs ) assert_allclose ( s_k , s_np , atol=1e-05 ) last_output_list = [ ] h_t1 = 0 'input_length ' : timesteps } , initial_states_k , mask_np=kwargs.pop ( 'mask ' , None ) , h_t = h_t * np_mask [ : , t ] .reshape ( -1 , 1 ) t_list = range ( x.shape [ 1 ] ) return outputs [ -1 ] , np.stack ( outputs , axis=1 ) , states_tm1 check_rnn_operation ( step_function_k=get_step_function ( K , wi_k ) , h.append ( h_t + h_t1 ) * * kwargs ) : last_y2 , y2 , h2 = K.rnn ( rnn_fn , x_k , h0_k , * * kwargs ) assert len ( states ) == 0 output_t = np.where ( output_mask [ : , t ] , output_t , output_tm1 ) for s_k , s_np in zip ( last_states_k , last_states_np ) : def test_rnn_constants ( self ) : return y_k , [ y_k , K.concatenate ( [ y_k , y_k ] , axis=-1 ) ] raise ValueError ( 'No initial states provided ! ' state_list.append ( ( h21 , h22 ) ) assert_allclose ( h1 , h2 , atol=1e-05 ) wi_k = K.variable ( wi ) def simple_rnn_add_constant ( inputs , states_and_constants ) : o = [ ] return y , [ ] assert_allclose ( outputs_list [ i - 1 ] , outputs_list [ i ] , atol=1e-05 ) wh_k = K.variable ( wh ) states_t = [ np.where ( state_mask [ : , t ] , state_t , state_tm1 ) outputs = [ ] assert len ( h2 ) == 2 { 'go_backwards ' : True , 'mask ' : mask } , if go_backwards : if constants_np is not None : output_dim = 3 return o [ -1 ] , np.stack ( o , axis=1 ) , np.stack ( h , axis=1 ) initial_states_np= [ h0 ] , _ , c = parse_shape_or_val ( ( num_samples , output_dim ) ) assert_allclose ( y1 , y2 , atol=1e-05 ) constants_k = [ K.variable ( c ) for c in constants_np ] _ , h0 = parse_shape_or_val ( ( num_samples , output_dim ) ) num_samples = 4 mask_k = None _ , wi = parse_shape_or_val ( ( input_dim , output_dim ) ) step_function_np=get_step_function ( KNP , wi ) , y_k = K.dot ( x_k , wi_k ) if w_o is not None : { 'go_backwards ' : False , 'mask ' : None , 'unroll ' : True , 'When using masking in an RNN , you should ' for state_mask , state_t , state_tm1 def simple_rnn_with_extra_mock_state ( inputs , states ) : initial_states_np= [ ] , assert len ( h_k ) == 0 mask=mask_np , h_t1 = np.dot ( prev , w_h ) o_t = h_t + h_t1 initial_states_np= [ h0 , h1 ] ,","['keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Numpy backend implementation of rnn + cleanup of rnn tests ( # 11622 )
184,cd44c6e0aaff4bb52a060d778025b32dbde09b44,2018-11-27 14:21:27-08:00,"def clear_session_after_test ( ) : warnings.filterwarnings ( 'ignore ' , message=r ' ( .+ ) Keras 2 ' , `` `` '' import pytest yield with warnings.catch_warnings ( ) : `` `` '' This wrapper runs for all the tests in the legacy directory ( recursively ) . category=UserWarning ) import warnings",['tests/keras/legacy/conftest.py'],Suppressed the warnings about updating to keras 2 in the legacy tests . ( # 11668 )
185,920e8af34a43ad2cd11190a21200a2acbfd83e11,2018-11-25 15:27:38+01:00,"# TODO : resolve flakyness issue . Tracked with # 11560 # TODO : resolve flakyness issue . Tracked with # 11587 # TODO : resolve flakyness issue . Tracked with # 11064 # TODO : resolve flakyness issue . Tracked with # 11586 from flaky import flaky 'flaky ' ,","['setup.py', 'tests/keras/engine/test_training.py', 'tests/keras/metrics_test.py', 'tests/keras/utils/data_utils_test.py']",Added a decorator for flaky tests which rerun the test on specific errors . ( # 11660 )
186,edd0c0a4bd4cea027842bc8925048976d65b1ae9,2018-11-25 13:10:17+01:00,"output_shape = [ output_shape [ 0 ] ] + x_mid_dims + [ output_shape [ -1 ] ] y = reshape ( y , new_y_shape ) x = tf.transpose ( x , pattern ) y_trail_dims = y_shape [ 2 : ] # squash trailing dimensions of y output_shape [ 0 ] = -1 pattern [ 1 ] = a1 x = reshape ( x , new_x_shape ) pattern [ 1 ] = a1 pattern = list ( range ( y_ndim ) ) pattern [ i ] = pattern [ i + 1 ] y = reshape ( y , new_y_shape ) pattern = list ( range ( x_ndim ) ) y = permute_dimensions ( y , pattern ) # if the inputs were originally rank 2 , we remove the added 1 dim . tf.ones_like ( x_shape [ 2 : ] ) , if a0 ! = 1 : if z_shape is not None : use_matmul = x_matmullabe and y_matmullabe result = tf.squeeze ( result , 1 ) y = K.placeholder ( ndim=len ( y_shape ) ) if y_batch_size is None : y_shape = list ( int_shape ( y ) ) x_batch_size = -1 for i in range ( a0 , 1 , -1 ) : # 1 ) Elementwise multiplication followed by tf.reduce_sum . This requires y_matmullabe = True y_shape [ 2 : ] ] , 0 ) y_ndim += 1 if None in x_shape [ 1 : ] : if x_batch_size is None : if y_ndim > 3 : elif orig_y_ndim == 2 : y_shape = tf.shape ( y ) # bring x 's dimension to be reduced to last axis . z_shape = K.int_shape ( z ) y_matmullabe = False x_shape = tf.shape ( x ) if use_matmul : if a1 ! = 1 : y_squashed_dim = np.prod ( y_trail_dims ) orig_y_ndim = y_ndim # bring y 's dimension to be reduced to axis 1 . if y_squashed : # test with placeholders ( no shape info ) # batch size should be known for input with rank > 3 . # more memory but works with partial shape information . if a0 ! = 1 : pattern [ i ] = pattern [ i - 1 ] new_x_shape = tf.concat ( [ x_shape , tf.ones_like ( y_shape [ 2 : ] ) ] , 0 ) new_y_shape = tf.concat ( [ y_shape [ :2 ] , do_reshape = False y_matmullabe = True z = K.batch_dot ( x , y , axes ) x_matmullabe = False if x_ndim > 3 : # normalize both inputs to rank 3 . result = tf.reshape ( result , output_shape ) result = tf.expand_dims ( result , -1 ) assert len ( z_shape ) == z_np.ndim if ndim ( result ) == 1 : # if inputs were squashed , we have to reshape the matmul output . x = tf.reshape ( x , [ x_batch_size , x_squashed_dim , x_shape [ -1 ] ] ) if a1 ! = 1 : if orig_x_ndim == 2 : new_y_shape = tf.concat ( [ y_shape [ :2 ] , tf.ones_like ( x_shape [ 2 : ] ) , y_shape [ 2 : ] ] , 0 ) for i in range ( a0 , 1 , -1 ) : if None in y_shape [ 1 : ] : # bring the dimension to be reduced to axis 1 . x = K.placeholder ( ndim=len ( x_shape ) ) # There are 2 ways to perform theano 's batched_tensordot in tensorflow : a0 += 1 x_squashed = True y_batch_size = -1 else : x_matmullabe = True result = tf.reduce_sum ( x * y , 1 ) if y_ndim == 2 : if do_reshape : new_x_shape = tf.concat ( [ x_shape , tf.ones_like ( y_shape [ 2 : ] ) ] , 0 ) if output_shape [ 0 ] is None : # bring the dimensions to be reduced to axis 1 y = tf.expand_dims ( y , 2 ) pattern [ i ] = pattern [ i - 1 ] # reshape to closest broadcastable shape if x_squashed : y_shape = tf.shape ( y ) # reshape to closest broadcastable shape . x_shape = list ( int_shape ( x ) ) if K.backend ( ) ! = 'cntk ' : x_matmullabe = True # squash middle dimensions of x . pattern [ 1 ] = a0 x_squashed = False result = tf.matmul ( x , y ) x_mid_dims = x_shape [ 1 : -1 ] assert_allclose ( f ( [ x_np , y_np ] ) [ 0 ] , z_np , atol=1e-05 ) # backup ndims . Need them later . y = tf.transpose ( y , pattern ) x_squashed_dim = np.prod ( x_mid_dims ) for i in range ( a1 , 1 , -1 ) : if x_ndim == 2 : result = tf.expand_dims ( result , -1 ) if ndim ( result ) == 1 : output_shape = list ( int_shape ( result ) ) pattern [ 1 ] = a0 pattern [ -1 ] = a0 pattern = list ( range ( y_ndim ) ) if y_ndim > 3 : y = tf.reshape ( y , [ y_batch_size , y_shape [ 1 ] , y_squashed_dim ] ) x = reshape ( x , new_x_shape ) x = tf.expand_dims ( x , 1 ) for i in range ( a0 , x_ndim - 1 ) : assert set ( z_shape ) < = set ( ( None , 1 ) ) # 2 ) Using tf.matmul . This is more efficient but all dimensions except if a0 ! = x_ndim - 1 : else : orig_x_ndim = x_ndim x_shape = tf.shape ( x ) x_ndim += 1 for i in range ( a1 , 1 , -1 ) : result = tf.squeeze ( result , -1 ) f = K.function ( [ x , y ] , [ z ] ) if x_ndim > 3 : result = tf.reduce_sum ( x * y , 1 ) do_reshape = True x = permute_dimensions ( x , pattern ) y_squashed = True y_squashed = False output_shape = output_shape [ : -1 ] + y_trail_dims pattern = list ( range ( x_ndim ) ) # if rank is 2 , expand to 3 .","['keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Fix memory issue in tensorflow backend 's batch_dot ( # 11719 )
187,d2ebf181b603e7d03e65df941dd754df5de32913,2018-11-23 16:13:52+01:00,"If you need to load weights into a * different * architecture ( with some layers in common ) , for instance for fine-tuning or transfer-learning , you can load weights by * layer name * : If you need to save the * * weights of a model * * , you can do so in HDF5 with the code below . NASNet # model reconstruction from YAML If you need to load the weights into a * different * architecture ( with some layers in common ) , for instance for fine-tuning or transfer-learning , you can load them by * layer name * : If you need to save the * * weights of a model * * , you can do so in HDF5 with the code below : Note that if your model has a different behavior in training and testing phase ( e.g . if it uses ` Dropout ` , ` BatchNormalization ` , etc . ) , you will need to pass the learning phase flag to your function : When using ` validation_data ` or ` validation_split ` with the ` fit ` method of Keras models , evaluation will be run at the end of every * * epoch * * . ( unless the model was never compiled in the first place ) . from keras.applications.densenet import DenseNet169 Note that if your model has a different behavior in training and testing phase ( e.g . if it uses ` Dropout ` , ` BatchNormalization ` , etc . ) , you will need If it imports without error it is installed , otherwise you can find detailed MobileNet v2 # model reconstruction from YAML : DenseNet specify ` shuffle=False ` when calling fit ( ) . Please also see [ How can I install HDF5 or h5py to save my models in Keras ? ] ( # how-can-i-install-hdf5-or-h5py-to-save-my-models-in-keras ) for instructions on how to install ` h5py ` . For example : ` load_model ` will also take care of compiling the model using the saved training configuration A * * batch * * generally approximates the distribution of the input data better than a single input . The larger the batch , the better the approximation ; however , it is also true that the batch will take longer to process and will still result in only one update . For inference ( evaluate/predict ) , it is recommended to pick a batch size that is as large as you can afford without going out of memory ( since larger batches will usually result in faster evaluation/prediction ) . from keras.applications.nasnet import NASNetLarge The ` model.fit ` method returns a ` History ` callback , which has a ` history ` attribute containing the lists of successive losses and other metrics . to pass the learning phase flag to your function : Example : Please also see [ How can I install HDF5 or h5py to save my models in Keras ? ] ( # how-can-i-install-hdf5-or-h5py-to-save-my-models-in-keras ) for instructions on how to install ` h5py ` . The ` model.fit ` method returns an ` History ` callback , which has a ` history ` attribute containing the lists of successive losses and other metrics . from keras.applications.densenet import DenseNet121 A * * batch * * generally approximates the distribution of the input data better than a single input . The larger the batch , the better the approximation ; however , it is also true that the batch will take longer to process and will still result in only one update . For inference ( evaluate/predict ) , it is recommended to pick a batch size that is as large as you can afford without going out of memory ( since larger batches will usually result in faster evaluating/prediction ) . from keras.applications.densenet import DenseNet201 If it imports without error it is installed otherwise you can find detailed ` load_model ` will also take care of compiling the model using the saved training configuration ( unless the model was never compiled in the first place ) . When using ` evaluation_data ` or ` evaluation_split ` with the ` fit ` method of Keras models , evaluation will be run at the end of every * * epoch * * . from keras.applications.nasnet import NASNetMobile from keras.applications.mobilenet_v2 import MobileNetV2 specify ` shuffle=False ` when calling ` fit ( ) ` .",['docs/templates/getting-started/faq.md'],Revise Keras FAQ ( # 11679 )
188,dcb5e3cf152977209c7122f6b07d6c86fb4cf1e6,2018-11-19 13:25:56+09:00,"embeddings_initializer='glorot_normal ' ) ( image_class ) ) cls = Flatten ( ) ( Embedding ( num_classes , latent_size , cls = Embedding ( num_classes , latent_size , embeddings_initializer='glorot_normal ' ) ( image_class )",['examples/mnist_acgan.py'],Remove unnecessary Flatten layer from mnist_acgan ( # 11675 )
189,5e126ec99a4909efe45f2f32ae076fa4ac8c0e21,2018-11-19 13:25:47+09:00,"travis_retry conda install $ MKL pydot graphviz $ PIL pip install -e . [ tests ] pip install keras_applications keras_preprocessing pip install keras_applications keras_preprocessing -- progress-bar off travis_retry conda install -q $ MKL pydot graphviz $ PIL pip install tensorflow==1.9 -- progress-bar off pip install tensorflow==1.9 pip install -e . [ tests ] -- progress-bar off travis_retry pip install -- only-binary=numpy , scipy , pandas numpy nose scipy h5py theano pytest pytest-pep8 pandas -- progress-bar off travis_retry pip install -- only-binary=numpy , scipy , pandas numpy nose scipy h5py theano pytest pytest-pep8 pandas",['.travis.yml'],Hide progress bars of pip and conda on travis . ( # 11664 )
190,bd2968348c330dcb497d114f4b9f9783552113cb,2018-11-19 10:52:28+09:00,assert l2 [ l2 < m ] .size == 0 assert not l2 [ l2 > m * 2 + 1e-5 ] assert l2 [ l2 > m * 2 + 1e-5 ] .size == 0 assert not l2 [ l2 < m ],['tests/keras/constraints_test.py'],Supressed the depreciation warnings . ( # 11670 )
191,dc9e510192d0a8a6f6943cd46e9554364d4dcdd2,2018-11-18 21:29:19+01:00,"{ If an external backend is not valid due to missing a required entry , an error will be logged notifying which entry/entries are missing . `` backend '' : `` my_package.my_module '' `` ` `` image_data_format '' : `` channels_last '' , `` epsilon '' : 1e-07 , An external backend must be validated in order to be used , a valid backend must have the following functions : ` placeholder ` , ` variable ` and ` function ` . `` floatx '' : `` float32 '' , } In Keras it is possible to load more backends than ` `` tensorflow '' ` , ` `` theano '' ` , and ` `` cntk '' ` . Keras can use external backends as well , and this can be performed by changing the ` keras.json ` configuration file , and the ` `` backend '' ` setting . Suppose you have a Python module called ` my_module ` that you wanted to use as your external backend . The ` keras.json ` configuration file would be changed as follows :",['docs/templates/backend.md'],Fix Issue # 11384 - Explaining how to load external backends in the documentation ( # 11405 )
192,75a35032e194a2d065b0071a9e786adf6cee83ea,2018-11-16 09:20:54+09:00,"result = stack ( inner_prodcuts ) axes : int or tupe ( int , int ) . Target dimensions to be reduced . axes : int or tuple ( int , int ) . Target dimensions to be reduced . result = stack ( inner_products )",['keras/backend/tensorflow_backend.py'],fix small typo ( # 11631 )
193,63543a7e35639e8b0b8bce6bb3f80406bb925cf3,2018-11-16 09:20:35+09:00,https : //www.tensorflow.org/programmers_guide/embedding ) . [ TensorBoard ] ( https : //www.tensorflow.org/guide/summaries_and_tensorboard ) [ details ] ( https : //www.tensorflow.org/how_tos/embedding_viz/ # metadata_optional ) [ TensorBoard ] ( https : //www.tensorflow.org/get_started/summaries_and_tensorboard ) [ details ] ( https : //www.tensorflow.org/guide/embedding # metadata ) https : //www.tensorflow.org/guide/embedding ) .,['keras/callbacks.py'],update doc links for tensorboard/embeddings ( # 11609 )
194,a0e90bd845795a4e032a7ba82aadace76eddbcb3,2018-11-07 17:17:09+01:00,"A metric is a function that is used to judge the performance of your model . Metric functions are to be supplied in the ` metrics ` parameter when a model is compiled . A metric is a function that is used to judge the performance of your model . Metric functions are to be supplied in the ` metrics ` parameter when a model is compiled . A metric function is similar to a [ loss function ] ( /losses ) , except that the results from evaluating a metric are not used when training the model . You may use any of the loss functions as a metric function . A metric function is similar to a [ loss function ] ( /losses ) , except that the results from evaluating a metric are not used when training the model . In addition to the metrics above , you may use any of the loss functions described in the [ loss function ] ( /losses ) page as metrics .",['docs/templates/metrics.md'],Update metrics.md with references to losses.md ( # 11569 )
195,64737f23ea9500c248acf91b117c09ef7a43d470,2018-11-06 15:52:48-08:00,if [ [ `` $ TEST_MODE '' == `` INTEGRATION_TESTS '' ] ] || [ [ `` $ TEST_MODE '' == `` PEP8_DOC '' ] ] ; then env : KERAS_BACKEND=tensorflow TEST_MODE=PEP8_DOC env : KERAS_BACKEND=theano THEANO_FLAGS=optimizer=fast_compile export PIL=Pil ; env : KERAS_BACKEND=theano THEANO_FLAGS=optimizer=fast_compile MKL= '' mkl mkl-service '' fi # install PIL for preprocessing tests ( they are integration tests ) . export MKL= '' mkl mkl-service '' ; # install mkl only for Theano . env : KERAS_BACKEND=tensorflow TEST_MODE=INTEGRATION_TESTS PIL=Pil export PIL=Pillow ; env : KERAS_BACKEND=tensorflow TEST_MODE=PEP8_DOC PIL=Pillow env : KERAS_BACKEND=tensorflow TEST_MODE=INTEGRATION_TESTS else if [ [ `` $ KERAS_BACKEND '' == `` theano '' ] ] ; then if [ [ `` $ TRAVIS_PYTHON_VERSION '' == `` 2.7 '' ] ] ; then fi,['.travis.yml'],Simplifying travis . ( # 11585 )
196,fcf2ed7831185a282895dda193217c2a97e1e41d,2018-11-06 15:50:31-08:00,loss = K.variable ( 0.0 ) # build the VGG19 network with our 3 images as input # build the VGG16 network with our 3 images as input return K.sum ( K.square ( S - C ) ) / ( 4 . * ( channels * * 2 ) * ( size * * 2 ) ) return K.sum ( K.square ( S - C ) ) / ( 4.0 * ( channels * * 2 ) * ( size * * 2 ) ) loss = K.variable ( 0 . ),['examples/neural_style_transfer.py'],Change VGG16 to VGG19 an fix inconsistent float formatting . ( # 11593 )
197,09d721abf489cf20d9055e7163cf1762b7763bea,2018-11-05 15:37:35-08:00,"if max_value is None : return y max_value = np.inf return below_threshold + above_threshold above_threshold = x * ( x > = threshold ) if max_value is not None : y = x * ( x > = threshold ) y += alpha * ( x - threshold ) * ( x < threshold ) above_threshold = np.clip ( above_threshold , 0.0 , max_value ) { { np_implementation } } below_threshold = alpha * ( x - threshold ) * ( x < threshold ) y = np.clip ( y , 0.0 , max_value )","['keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py']",Displaying the numpy relu in the docs . ( # 11574 )
198,5a8f346e64cb432a445002230246ee042dbc278b,2018-11-05 15:33:48-08:00,"initial_states , states [ i ] ) for i in range ( len ( states ) ) ] # n times . # to be the same shape as its two 'as its first state at time ` t ` ' for use_mask in [ False , True ] : 'kernel_size ' : ( num_row , num_col ) , loop_vars= ( time , output_ta ) + states , # test for output shape : y_vals , s_final_vals = model.predict ( [ x_vals , s_initial_vals ] ) input_size = state_size = 7 # it just repeats the mask along its second dimension if mask.ndim == ndim - 1 : loop_vars= ( time , output_ta , initial_output ) + states , num_features = 5 outputs = T.switch ( output_mask , outputs , output_tm1 ) assert_allclose ( K.eval ( outputs ) , expected_outputs ) return K.any ( K.not_equal ( inputs , 0 . ) , axis= [ 2 , 3 , 4 ] ) model = Model ( x , states [ 0 ] ) s_initial_vals = np.random.random ( ( num_samples , state_size ) ) new_states = [ tf.where ( get_matching_mask ( mask_t , new_states [ i ] ) , return tf.tile ( mask_t , multiple ) dtype=output.dtype , swap_memory=True , 'padding ' : 'valid ' } , model = Model ( [ x , s_initial ] , [ y , s_final ] ) multiples = tf.stack ( [ 1 , tf.shape ( new_states [ i ] ) [ 1 ] ] ) 'stateful ' : True , output = layer_test ( convolutional_recurrent.ConvLSTM2D , # masking of two last timesteps for second sample only expected_outputs [ 1 , -mask_last_num_timesteps : ] = \ last_output , outputs , last_states = K.rnn ( num_samples = 2 expected_last_state [ 1 : ] += num_timesteps output = T.switch ( output_mask , output , prev_output ) `` `` '' Regular masking layer returns wrong shape of mask for RNN '' '' '' state_and_io_size = 5 'cond ' : lambda time , * _ : time < time_steps , state = model.predict ( inputs ) 'return_sequences ' : return_sequences , x_vals = np.random.random ( ( num_samples , num_timesteps , input_size ) ) expected_outputs = np.repeat ( inputs_vals [ ... , None ] , repeats=2 , axis=-1 ) # test for output shape : kwargs = { 'data_format ' : data_format , assert len ( states ) == 2 ' ` output == new_states [ 0 ] ` for ' num_timesteps = 4 'stateful ' : True , y_vals_expected [ 0 ] = x_vals [ 0 , -2 ] np.testing.assert_allclose ( tiled_mask_t = tf.tile ( mask_t , 'padding ' : 'valid ' } initial_output = zeros_like ( output ) def call ( self , inputs , states ) : ' ` output , new_states = step_function ( inputs , states ) ` ' ) if not inputs.shape [ 0 ] : ( num_samples , num_timesteps , state_and_io_size ) ) x_vals [ 0 , -1 , : ] = 0 mask = tf.transpose ( mask , [ 1 , 0 ] ) y , s_final = recurrent.RNN ( Cell ( ) , output = tf.where ( tiled_mask_t , output , prev_output ) # test for return state : def get_matching_mask ( mask_t , ref_tensor_t ) : input_shape=inputs.shape ) if len ( mask.get_shape ( ) ) == ndim - 1 : state_mask = get_matching_mask ( mask [ i ] , state ) self.output_size = input_shape [ -1 ] final_outputs = control_flow_ops.while_loop ( initial_states = [ K.variable ( initial_state_vals ) ] def test_masking_correctness_output_not_equal_to_first_state ( ) : self.output_size = None new_states = final_outputs [ 3 : ] # skip output_tm1 tf.stack ( [ 1 , tf.shape ( new_state ) [ 1 ] ] ) ) layer.build ( inputs.shape ) s_final_vals_expected = s_initial_vals.copy ( ) x_masked = Masking ( ) ( x ) # broadcast the mask to match the shape of A and B . state = model.predict ( inputs ) def test_masking_correctness_output_size_not_equal_to_first_state_size ( ) : ' ( and your step function should return ' num_samples = 5 expected_outputs = inputs_vals.copy ( ) from keras.layers import convolutional_recurrent , Input body=_step , for i in range ( len ( states ) ) : 'return_sequences ' : return_sequences , output , _ = step_function ( inputs [ 0 ] , initial_states + constants ) # last timestep masked for first sample ( all zero inputs masked by Masking layer ) return ( time + 1 , output_ta_t ) + tuple ( new_states ) # ( nsamples , 1 ) , and A and B are ( nsamples , ndimensions ) . layer = convolutional_recurrent.ConvLSTM2D ( * * kwargs ) s_final_vals_expected [ 0 ] += ( num_timesteps - 1 ) unroll=unroll , mask = mask.dimshuffle ( axes ) output_mask = get_matching_mask ( mask [ i ] , output ) mask = mask.dimshuffle ( [ 1 , 0 ] ) model.compile ( optimizer='sgd ' , loss='mse ' ) 'filters ' : filters , s_final_vals_expected [ 1 : ] += num_timesteps maximum_iterations=input_length ) err_msg= '' Unexpected state for unroll= { } '' .format ( unroll ) ) output_mask = get_matching_mask ( mask , outputs ) if not inputs.get_shape ( ) [ 0 ] : 'mask should have ` shape= ( samples , time ) ` , ' def _step ( time , output_ta_t , output_tm1 , * states ) : return_states.append ( T.switch ( state_mask , new_state , state ) ) # tf.where needs its condition tensor output = layer_test ( convolutional_recurrent.ConvLSTM2D , outputs , _ = step_function ( inputs [ 0 ] , initial_states + constants ) tiled_mask_t = tf.tile ( mask_t , expected_state = initial_state_vals.copy ( ) mask_vals [ 1 , -mask_last_num_timesteps : ] = 0 step_function , def build ( self , input_shape ) : raise ValueError ( return_states.append ( T.switch ( mask , new_state , state ) ) 'return_sequences ' : return_sequences , # final outputs equal to last inputs concatenated return_state=True , ndim = len ( inputs.shape ) return keras.layers.concatenate ( [ inputs ] * 2 ) , [ s + 1 for s in states ] # outputs expected to be same as inputs for the first sample mask = expand_dims ( mask ) if mask.ndim ! = 2 : state_mask = get_matching_mask ( mask , state ) add_shape = tf.shape ( ref_tensor_t ) [ 1 : ] inputs_vals = np.random.random ( 'return_state ' : True , assert_allclose ( y_vals , output = tf.where ( tiled_mask_t , output , states [ 0 ] ) warnings.warn ( tf.stack ( [ 1 , tf.shape ( output ) [ 1 ] ] ) ) # but increments states +1 per timestep cond=lambda time , * _ : time < time_steps , input_shape=inputs.shape ) def _step ( time , output_ta_t , * states ) : expected_state [ 0 ] += num_timesteps # result tensors if len ( mask.shape ) ! = 2 : for unroll in [ True , False ] : state_mask_t = get_matching_mask ( mask_t , new_state ) new_states = final_outputs [ 2 : ] # So we need to * * while_loop_kwargs ) # final outputs equal to last inputs ndim = len ( ref_tensor_t.shape ) 'the output at time ` t-1 ` ) . ' ) 'return_state ' : True , outputs = layer ( x ) np.testing.assert_allclose ( y_vals_expected = np.concatenate ( [ x_vals [ : , -1 ] ] * 2 , axis=-1 ) # test for return state : outputs = T.switch ( mask , outputs , output_tm1 ) self.output_size = input_shape [ -1 ] * 2 y_vals_expected [ 0 ] = np.concatenate ( [ x_vals [ 0 , -2 ] ] * 2 , axis=-1 ) outputs = K.tile ( K.expand_dims ( inputs ) , [ 1 , 1 , 2 ] ) # except for first sample , where it is equal to second to last value due to mask 'got { } '.format ( mask.shape ) ) return_states.append ( tf.where ( state_mask_t , # random inputs and state values outputs = layer ( x ) initial_state_vals = np.random.random ( ( num_samples , state_and_io_size ) ) multiple = tf.concat ( [ [ 1 ] , add_shape ] , 0 ) new_state.set_shape ( state.shape ) state_size = input_size = 3 # also equal to ` output_size ` ndim = len ( inputs.get_shape ( ) ) def __init__ ( self ) : inputs , loop_vars= ( time , output_ta ) + states , # not updated last timestep : 'maximum_iterations ' : input_length } 'provide initial states ' return T.tile ( mask_t , reps , ndim=ndim ) output , states = outputs [ 0 ] , outputs [ 1 : ] if not unroll and mask is not None : # same as the second to final output ( before masked region ) 'kernel_size ' : ( num_row , num_col ) , assert len ( states ) == 2 self.state_size = input_shape [ -1 ] # That 's what the tile call does , dtype=outputs.dtype , axes = [ 1 , 0 ] + list ( range ( 2 , len ( outputs.get_shape ( ) ) ) ) assert mask.ndim == ndim # as last output before masked region expected_outputs [ -1 , -1 ] = expected_outputs [ -1 , -2 ] num_samples = 3 if use_mask : kwargs= { 'data_format ' : data_format , x = Input ( ( num_timesteps , input_size ) , name= '' x '' ) for _ in range ( ndim - 1 ) : return ( time + 1 , output_ta_t , output ) + tuple ( new_states ) expected_outputs [ 1 , - ( mask_last_num_timesteps + 1 ) ] # but for the second sample all outputs in masked region should be the same K.eval ( layer.states [ 0 ] ) , state , atol=1e-4 ) s_initial = Input ( ( state_size , ) , name= '' s_initial '' ) output = T.switch ( mask [ i ] , output , prev_output ) mask_last_num_timesteps = 2 # for second sample only expected_last_state = initial_state_vals.copy ( ) for return_sequences in [ True , False ] : num_timesteps = 4 # tf.where needs its condition tensor 'CNTK Backend only supports accurate masking if ' mask_vals [ -1 , -1 ] = 0 # final timestep masked for last sample axes = [ 1 , 0 ] + list ( range ( 2 , len ( outputs.shape ) ) ) class Masking5D ( Masking ) : mask = K.variable ( mask_vals ) assert_allclose ( s_final_vals , new_states = tmp new_states = final_outputs [ 2 : ] kwargs = { 'data_format ' : data_format , 'kernel_size ' : ( num_row , num_col ) , 'filters ' : filters , ndim = ref_tensor_t.ndim err_msg= '' Unexpected output for unroll= { } '' .format ( unroll ) ) output = tf.where ( output_mask_t , output , prev_output ) 'padding ' : 'valid ' } initial_state_vals = np.random.random ( ( num_samples , 6 , 7 ) ) # states are incremented ` num_timesteps - 1 ` times for first sample def compute_mask ( self , inputs , mask=None ) : while_loop_kwargs = { # verify same expected output for ` unroll=true/false ` def test_rnn_output_num_dim_larger_than_2_masking ( self ) : x = Input ( batch_shape=inputs.shape ) x = Input ( batch_shape=inputs.shape ) # and ` num_timesteps - 1 ` times for remaining samples expected_state [ 1 ] += ( num_timesteps - mask_last_num_timesteps ) tmp.append ( tf.where ( tiled , new_states [ i ] , states [ i ] ) ) output , states = outputs [ 0 ] , outputs [ 1 : ] inputs = K.variable ( inputs_vals ) new_state.set_shape ( state.get_shape ( ) ) inputs_vals = np.random.random ( ( num_samples , num_timesteps , num_features ) ) # first state should be incremented for every timestep ( no masking ) num_timesteps = 6 class Cell ( keras.layers.Layer ) : 'swap_memory ' : True , # the condition ( mask ) tensor is # result tensors , but in our case new_states [ i ] , kept_states.append ( T.switch ( state_mask , new_state , state ) ) body=_step , def test_rnn_output_and_state_masking_independent ( self ) : expected_last_state [ 0 ] += ( num_timesteps - 2 ) 'filters ' : filters , mask_t = expand_dims ( mask_t ) input_length=num_timesteps if unroll else None ) layer = convolutional_recurrent.ConvLSTM2D ( * * kwargs ) s_final_vals_expected , tiled = tf.tile ( mask_t , multiples ) parallel_iterations=32 , for return_sequences in [ True , False ] : from keras.layers import convolutional_recurrent , Input , Masking , Lambda mask_vals = np.ones ( ( num_samples , num_timesteps ) ) kwargs= { 'data_format ' : data_format , K.eval ( layer.states [ 0 ] ) , state , atol=1e-4 ) # for the last sample , the final timestep ( in masked region ) should be the final_outputs = control_flow_ops.while_loop ( 'kernel_size ' : ( num_row , num_col ) , inputs_vals = np.random.random ( ( num_samples , num_timesteps , 5 ) ) else : initial_state_vals = np.random.random ( ( num_samples , 6 ) ) return outputs , states 'return_sequences ' : return_sequences , 'provide initial states ' ) layer.build ( inputs.shape ) # second state should not be incremented for last two timesteps # to be the same shape as its two output = tf.where ( output_mask_t , output , output_tm1 ) mask_vals [ 0 , -2 : ] = 0 # final two timesteps masked for first sample unroll=unroll ) ( x_masked , initial_state=s_initial ) return_states.append ( tf.where ( tiled_mask_t , output_mask_t = get_matching_mask ( mask_t , output ) model = Model ( x , states [ 0 ] ) self.state_size = None y_vals_expected = x_vals [ : , -1 ] .copy ( ) kept_states.append ( T.switch ( mask [ i ] , new_state , state ) ) outputs = layer ( Masking5D ( ) ( x ) ) add_shape = ref_tensor_t.shape [ 1 : ] assert_allclose ( K.eval ( last_states [ 0 ] ) , expected_last_state ) 'parallel_iterations ' : 32 , # a step function that just outputs inputs , 'filters ' : filters , assert_allclose ( K.eval ( last_states [ 0 ] ) , expected_state ) def step_function ( inputs , states ) : def test_rnn_state_num_dim_larger_than_2_masking ( self ) : tmp = [ ] reps = T.concatenate ( [ [ 1 ] , add_shape ] , 0 ) mask = tf.transpose ( mask , axes ) # ( see earlier comment for tile explanation ) 'padding ' : 'valid ' } , output_tm1 : output Tensor from previous timestep return inputs , [ s + 1 for s in states ] mask=mask , super ( Cell , self ) .__init__ ( ) for unroll in [ True , False ] : y_vals_expected ,","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py', 'tests/keras/layers/convolutional_recurrent_test.py', 'tests/keras/layers/recurrent_test.py']",[ P ] Fix masking issues in K.rnn ( tensorflow + theano ) ( # 11499 )
199,140605ee0f08b7996d69a61610782f48766776ad,2018-11-05 10:47:35-08:00,"# test serialization , weight setting at model level actual_output = _layer_in_model_test ( model ) actual_output_shape = actual_output.shape model.train_on_batch ( input_data , actual_output ) return actual_output model_config = model.get_config ( ) recovered_model = model.__class__.from_config ( model_config ) _output = recovered_model.predict ( input_data ) if expected_dim is not None : # different behavior at training and testing time ) . recovered_model.set_weights ( weights ) assert_allclose ( actual_output , expected_output , rtol=1e-3 ) model_config = model.get_config ( ) if expected_output is not None : if expected_dim is not None : if model.weights : assert_allclose ( _output , actual_output , rtol=1e-3 ) assert_allclose ( actual_output , expected_output , rtol=1e-3 ) if model.weights : if has_arg ( layer.call , 'training ' ) : assert expected_dim == actual_dim actual_output = model.predict ( input_data ) for expected_dim , actual_dim in zip ( expected_output_shape , assert expected_dim == actual_dim weights = model.get_weights ( ) recovered_model.set_weights ( weights ) model.compile ( 'rmsprop ' , 'mse ' ) for expected_dim , actual_dim in zip ( expected_output_shape , if expected_output is not None : # test training mode ( e.g . useful when the layer has a actual_output_shape ) : _output = recovered_model.predict ( input_data ) # different behavior at training and testing time ) . recovered_model = model.__class__.from_config ( model_config ) model.train_on_batch ( input_data , actual_output ) # test training mode ( e.g . useful when the layer has a # test serialization , weight setting at model level actual_output_shape ) : actual_output_shape = actual_output.shape weights = model.get_weights ( ) def _layer_in_model_test ( model ) : assert_allclose ( _output , actual_output , rtol=1e-3 ) actual_output = model.predict ( input_data ) model.compile ( 'rmsprop ' , 'mse ' ) if has_arg ( layer.call , 'training ' ) :",['keras/utils/test_utils.py'],Inlined nested function in layer_test ( # 11564 )
200,622540176d0943fcc3e361153ddac2ae08eb7e70,2018-11-05 10:39:05-08:00,"ztf = KTF.eval ( ztf ) else : zth , _ , _ = KTH.normalize_batch_in_training ( xtf , None , None , reduction_axes= [ 0 , 1 , 2 , 3 ] ) z = K.eval ( z ) sudo dpkg -i openmpi_1.10-3.deb pushd ~/mpi mkdir ~/mpi x = K.placeholder ( x_shape ) zc = KC.function ( [ xc ] , [ zc ] ) ( [ x_val ] ) [ 0 ] rm -rf ~/mpi shape = ( 2 , 3 ) reason='Specific to Theano . ' ) z , _ , _ = K.normalize_batch_in_training ( z = K.function ( [ x ] , [ z ] ) ( [ x_val ] ) [ 0 ] xc , None , None , reduction_axes= [ 0 , 1 , 2 , 3 ] ) popd for data_format in [ 'channels_first ' , 'channels_last ' ] : assert zth.shape == ztf.shape wget http : //cntk.ai/PythonWheel/ForKeras/depends/openmpi_1.10-3.zip if data_format == 'channels_first ' : assert z.shape == x_shape xth = KTH.variable ( x_val ) # open mpi is needed for cntk x_shape = ( 1 , ) + shape + ( 4 , ) pip install cntk def test_batchnorm ( self ) : def test_batchnorm_cntk ( self , x_shape ) : xc = KC.placeholder ( x_shape ) pip install cntk x_val = np.random.random ( x_shape ) .astype ( np.float32 ) x_shape = ( 1 , 4 ) + shape assert zth.shape == zc.shape pushd ~/mpi sudo dpkg -i openmpi_1.10-3.deb x = K.variable ( x_val ) x , None , None , reduction_axes= [ 0 , 1 , 2 , 3 ] ) # install open mpi def test_batchnorm_tf ( self , x_shape ) : if [ [ `` $ KERAS_BACKEND '' == `` cntk '' ] ] || [ [ `` $ TEST_MODE '' == `` PEP8_DOC '' ] ] ; then def test_batchnorm_th ( self , x_shape ) : wget http : //cntk.ai/PythonWheel/ForKeras/depends/openmpi_1.10-3.zip xth , None , None , reduction_axes='per-activation ' ) ./.travis/install_cntk.sh ; reason='Specific to Tensorflow . ' ) x_val = np.random.random ( x_shape ) .astype ( np.float32 ) fi rm -rf ~/mpi unzip ./openmpi_1.10-3.zip zth = KTH.eval ( zth ) unzip ./openmpi_1.10-3.zip x , None , None , reduction_axes='per-activation ' ) zc , _ , _ = KC.normalize_batch_in_training ( popd xtf = KTF.variable ( x_val ) mkdir ~/mpi set -e ztf , _ , _ = KTF.normalize_batch_in_training (","['.travis.yml', '.travis/install_cntk.sh', 'tests/keras/backend/backend_test.py']",Skipping the CNTK install in some travis jobs to speed up tests ( # 11562 )
201,314c4495a3a091867186c62b77b33877c238b0e1,2018-11-05 10:33:40-08:00,sed -i `` \/keras\/backend\/ $ { KERAS_BACKEND } _backend.py/d '' .coveragerc if [ [ `` $ KERAS_BACKEND '' ! = `` theano '' ] ] ; then keras/backend/theano_backend.py if [ [ `` $ KERAS_BACKEND '' ! = `` cntk '' ] ] ; then fi echo ' keras/backend/cntk_backend.py ' > > .coveragerc ; keras/backend/tensorflow_backend.py # Remove the current backend from the coverage exclusion . echo ' keras/backend/theano_backend.py ' > > .coveragerc ; echo ' keras/backend/tensorflow_backend.py ' > > .coveragerc ; # exclude different backends to measure a coverage for the designated backend only if [ [ `` $ KERAS_BACKEND '' ! = `` tensorflow '' ] ] ; then keras/backend/cntk_backend.py,"['.coveragerc', '.travis.yml']",Simplified travis build ( # 11572 )
202,9f2d6621bb8cdf8a5def7d5143a0faee993c8e13,2018-11-05 11:15:47+09:00,if [ [ `` $ KERAS_BACKEND '' == `` theano '' ] ] ; then fi travis_retry conda install mkl mkl-service pydot graphviz $ PIL # install mkl only for Theano . travis_retry conda install $ MKL pydot graphviz $ PIL export MKL= '' mkl mkl-service '' ;,['.travis.yml'],Removing mkl from travis to speed up the build ( # 11570 )
203,58432f2c78cd5bc1d9893c63b3cb8977dab31670,2018-11-03 23:24:08-07:00,"# Otherwise , something went wrong . 'pytest-timeout ' , # Running all tests should take less than 12 minutes . timeout = 720","['pytest.ini', 'setup.py']",Removed pytest-timout because it 's causing too many problems on travis . ( # 11559 )
204,1d10fe2d739c48f00404008fe5754d2807bb0fd6,2018-11-03 22:07:58-07:00,"# legacy layers return bad values from get_weights ( ) if has_arg ( layer_cls.__init__ , 'weights ' ) and len ( weights ) : layer = layer_cls ( * * kwargs ) # Checking for empty weights array to avoid a problem where some # test and instantiation from weights kwargs [ 'weights ' ] = weights",['keras/utils/test_utils.py'],Removed dead code in layer_tests ( # 11563 )
205,08a6dd04f9143a04ee77af1128eaedb3218c6147,2018-11-03 21:54:46-07:00,"return y y = np.minimum ( y , 1 . ) return np.clip ( y , 0 , 1 ) { { np_implementation } } y = np.maximum ( y , 0 . )","['keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py']",Added hard_sigmoid numpy implementation in the docs . ( # 11551 )
206,6207edd1477696f4a8a1761993544d72d766bd8e,2018-11-03 21:54:16-07:00,"BACKENDS.append ( KTH ) BACKENDS = [ ] # Holds a list of all available back-ends BACKENDS.append ( KTF ) BACKENDS.append ( KC ) # check_single_tensor_operation ( 'std ' , ( 4 , 2 , 3 ) , BACKENDS , axis= [ 1 , -1 ] )",['tests/keras/backend/backend_test.py'],Removed the variable BACKENDS as it is not used . ( # 11561 )
207,97bff7df815de472039d366728df8f610528d631,2018-11-03 23:16:58+09:00,"for a in axes : return np.flip ( x , axes ) if isinstance ( axes , int ) : x = np.flip ( x , a ) axes = tuple ( axes ) if isinstance ( axes , list ) : return x axes = [ axes ]",['keras/backend/numpy_backend.py'],Simplified the logic of reverse . ( # 11550 )
208,384d3bc35db3359cb7c1f560e053f2a510d9381a,2018-11-03 15:11:13+01:00,"u_m_o = u_m_o * np.expand_dims ( np_mask , -1 ) { 'go_backwards ' : False , 'mask ' : None , 'unroll ' : True , BACKENDS , cntk_dynamicity=True , outputs_list = [ [ ] , [ ] , [ ] , [ ] , [ ] , [ ] ] pool_size= ( 2 , 3 , 2 ) , strides= ( 1 , 1 , 1 ) , padding='valid ' ) W_o = k.variable ( W_o_val ) def legacy_test_pool2d ( self ) : # channels_last input shape : ( n , length , input_depth ) W_i_val = np.random.random ( ( input_dim , output_dim ) ) .astype ( np.float32 ) W_i = k.variable ( W_i_val ) # test default setup input_val = np.random.random ( for strides in [ 1 , 2 ] : pool_size= ( 3 , 3 , 3 ) , strides= ( 1 , 1 , 1 ) , padding='same ' , pool_mode='avg ' ) input_dim = 8 last_output , outputs , new_states = k.rnn ( rnn_fn , inputs , { 'go_backwards ' : True , 'mask ' : None } , input_dim = 5 ] initial_states , { 'go_backwards ' : False , 'mask ' : None } , state_list [ i ] .append ( k.eval ( new_states [ 0 ] ) ) m_o = m_o * np.expand_dims ( np_mask , -1 ) pool_size= ( 2 , 3 ) , strides= ( 1 , 1 ) , padding='valid ' ) u_m_l = u_m_l * np.expand_dims ( np_mask [ : , -1 ] , -1 ) assert_allclose ( l , u_l , atol=1e-04 ) assert_allclose ( b_o , b_u_o , atol=1e-04 ) m_l = m_l * np.expand_dims ( np_mask [ : , -1 ] , -1 ) initial_states = [ ] for m_s , u_m_s , k in zip ( state_list [ 4 ] , state_list [ 5 ] , BACKENDS ) : 'input_length ' : timesteps } , input_shape , kernel_shape , # TF input shape : ( samples , conv_dim1 , conv_dim2 , conv_dim3 , input_depth ) assert_list_pairwise ( last_output_list [ 2 ] , shape=False , atol=1e-04 ) timesteps = 5 mask=None ) pool_size= ( 2 , 2 , 2 ) , strides= ( 1 , 1 , 1 ) , pool_mode='avg ' ) pool_size= ( 3 , 3 ) , strides= ( 1 , 1 ) , last_output_list [ i ] .append ( k.eval ( last_output ) ) # implement a simple RNN without states # implement a simple RNN ( ( 2 , 3 , 4 , 5 ) , ( 2 , 2 , 3 , 4 ) , 'channels_first ' ) , last_output_list.append ( k.eval ( last_output ) ) assert len ( new_states ) == 0 kernel_shape = ( 3 , 2 , 3 ) outputs_list [ i ] .append ( k.eval ( outputs ) ) * * kwargs ) return output , [ ] inputs = k.variable ( input_val ) def rnn_step_fn ( k ) : for m_l , u_m_l , k in zip ( last_output_list [ 4 ] , last_output_list [ 5 ] , BACKENDS ) : ( ( 2 , 3 , 5 , 6 ) , ( 4 , 3 , 3 , 4 ) , 'channels_first ' ) , for ( i , kwargs ) in enumerate ( kwargs_list ) : { 'go_backwards ' : False , 'mask ' : mask , 'unroll ' : True , 'pool3d ' , ( 2 , 6 , 6 , 6 , 3 ) , [ KTH , KTF ] , ( num_samples , timesteps , input_dim ) ) .astype ( np.float32 ) # TH kernel shape : ( depth , input_depth , x , y , z ) 'pool3d ' , ( 5 , 9 , 11 , 5 , 3 ) , BACKENDS , cntk_dynamicity=True , return step_function def test_legacy_conv1d ( self ) : state_list = [ [ ] , [ ] , [ ] , [ ] , [ ] , [ ] ] check_two_tensor_operation ( 'conv1d ' , input_shape , kernel_shape , # TF kernel shape : ( rows , cols , input_depth , depth ) # TF kernel shape : ( rows , cols , input_depth , depth_multiplier ) def test_legacy_depthwise_conv_2d ( self ) : for k in BACKENDS : assert_allclose ( m_o , u_m_o , atol=1e-04 ) def test_legacy_conv2d ( self ) : initial_states = [ k.variable ( init_state_val ) ] # TH input shape : ( samples , input_depth , conv_dim1 , conv_dim2 , conv_dim3 ) { 'go_backwards ' : True , 'mask ' : None , 'unroll ' : True , check_single_tensor_operation ( output = k.dot ( x , W_i ) assert_list_pairwise ( outputs_list [ 2 ] , shape=False , atol=1e-04 ) if k == KTF : last_output_list = [ [ ] , [ ] , [ ] , [ ] , [ ] , [ ] ] for b_l , b_u_l in zip ( last_output_list [ 2 ] , last_output_list [ 3 ] ) : strides=strides , W_i_val = np.random.random ( ( input_dim , output_dim ) ) ( ( 1 , 6 , 5 , 3 ) , ( 3 , 3 , 3 , 2 ) , 'channels_last ' ) # channels_first input shape : ( n , input_depth , rows , cols ) assert_list_pairwise ( outputs_list [ 0 ] , shape=False , atol=1e-04 ) # TF kernel shape : ( x , y , z , input_depth , depth ) input_shape = ( 4 , 8 , 2 ) go_backwards=False , assert len ( new_states ) == 1 check_two_tensor_operation ( 'conv2d ' , input_shape , kernel_shape , for m_o , u_m_o , k in zip ( outputs_list [ 4 ] , outputs_list [ 5 ] , BACKENDS ) : def test_legacy_rnn_no_states ( self ) : outputs_list = [ ] assert len ( states ) == 1 data_format=data_format ) ( num_samples , output_dim ) ) .astype ( np.float32 ) for b_s , b_u_s in zip ( state_list [ 2 ] , state_list [ 3 ] ) : ] : for b_o , b_u_o in zip ( outputs_list [ 2 ] , outputs_list [ 3 ] ) : def step_function ( x , states ) : assert_list_pairwise ( last_output_list [ 0 ] , shape=False , atol=1e-04 ) output_dim = 4 num_samples = 4 assert_list_pairwise ( last_output_list , shape=False ) pool_size= ( 2 , 2 ) , strides= ( 1 , 1 ) , pool_mode='avg ' ) ( ( 2 , 3 , 4 , 5 , 4 ) , ( 2 , 2 , 2 , 3 , 4 ) , 'channels_first ' ) , pool_size= ( 2 , 2 ) , strides= ( 1 , 1 ) , padding='valid ' ) initial_states , def test_legacy_conv3d ( self ) : assert_list_pairwise ( outputs_list , shape=False ) last_output_list = [ ] check_two_tensor_operation ( 'conv3d ' , input_shape , kernel_shape , rnn_fn = rnn_step_fn ( k ) assert_allclose ( b_l , b_u_l , atol=1e-04 ) 'pool3d ' , ( 5 , 10 , 12 , 5 , 3 ) , BACKENDS , cntk_dynamicity=True , data_format='channels_last ' ) prev_output = states [ 0 ] kwargs_list = [ padding='same ' , pool_mode='avg ' ) assert_list_pairwise ( state_list [ 2 ] , shape=False , atol=1e-04 ) np_mask = np.random.randint ( 2 , size= ( num_samples , timesteps ) ) assert_allclose ( s , u_s , atol=1e-04 ) assert_allclose ( m_l , u_m_l , atol=1e-04 ) ( ( 1 , 2 , 2 , 2 , 1 ) , ( 2 , 2 , 2 , 1 , 1 ) , 'channels_last ' ) for o , u_o in zip ( outputs_list [ 0 ] , outputs_list [ 1 ] ) : assert_allclose ( m_s , u_m_s , atol=1e-04 ) output_dim = 3 def test_legacy_rnn ( self ) : for l , u_l in zip ( last_output_list [ 0 ] , last_output_list [ 1 ] ) : assert_allclose ( b_s , b_u_s , atol=1e-04 ) input_val = np.random.random ( ( 32 , timesteps , input_dim ) ) return output , [ output ] assert_allclose ( o , u_o , atol=1e-04 ) assert len ( states ) == 0 for ( input_shape , kernel_shape , data_format ) in [ W_o_val = np.random.random ( ( output_dim , output_dim ) ) .astype ( np.float32 ) 'pool2d ' , ( 5 , 10 , 12 , 3 ) , BACKENDS , cntk_dynamicity=True , ( ( 2 , 3 , 5 , 4 , 6 ) , ( 3 , 2 , 4 , 3 , 4 ) , 'channels_first ' ) , assert_list_pairwise ( state_list [ 0 ] , shape=False , atol=1e-04 ) check_two_tensor_operation ( 'depthwise_conv2d ' , timesteps = 6 { 'go_backwards ' : False , 'mask ' : mask } , last_output , outputs , new_states = k.rnn ( rnn_fn , inputs , mask = k.variable ( np_mask ) for s , u_s in zip ( state_list [ 0 ] , state_list [ 1 ] ) : def legacy_test_pool3d ( self ) : outputs_list.append ( k.eval ( outputs ) ) 'pool2d ' , ( 2 , 7 , 7 , 5 ) , BACKENDS , cntk_dynamicity=True , init_state_val = np.random.random ( output = k.dot ( x , W_i ) + k.dot ( prev_output , W_o ) 'pool2d ' , ( 5 , 9 , 11 , 3 ) , BACKENDS , cntk_dynamicity=True , pool_size= ( 2 , 2 , 2 ) , strides= ( 1 , 1 , 1 ) , padding='valid ' )",['tests/keras/backend/backend_test.py'],Remove legacy backend tests ( # 11558 )
209,90c90b69cf35c0457ae52e97e363b82562fe1920,2018-11-01 15:54:59-07:00,"raise ValueError ( 'Layer ' + layer_name + ' not found in model . ' ) if layer_name not in layer_dict : 'Layer ' + layer_name + ' not found in model . ' ) assert ( layer_name in layer_dict.keys ( ) ,",['examples/deep_dream.py'],Fix incorrect assertion syntax in example ( # 11542 )
210,8ef5dcfd3dfa385e00e7d2654825f7f79d2dec57,2018-11-01 12:29:45-07:00,"template_hidden_np_implementation = `` '' '' # Numpy implementation < summary > Show the Numpy implementation < /summary > code = inspect.getsource ( np_implementation ) code = '\n'.join ( code_lines [ : -1 ] ) `` ` python if code_lines [ i ] : code_lines = code.split ( '\n ' ) { { code } } `` `` '' if len ( code_lines ) < 10 : from keras.backend import numpy_backend code_lines [ i ] = ' ' + code_lines [ i ] import reference_operations as KNP def add_np_implementation ( function , docstring ) : for i in range ( len ( code_lines ) ) : # if there is something on the line , add 8 spaces . if ( 'backend ' in signature and section = template_hidden_np_implementation.replace ( ' { { code } } ' , code ) from keras.backend import numpy_backend as KNP docstring = add_np_implementation ( function , docstring ) template_np_implementation = `` '' '' # Numpy implementation `` ` else : < details > < /details > section = template_np_implementation.replace ( ' { { code } } ' , code ) return docstring.replace ( ' { { np_implementation } } ' , section ) np_implementation = getattr ( numpy_backend , function.__name__ ) { { np_implementation } } ' { { np_implementation } } ' in docstring ) :","['docs/autogen.py', 'tests/keras/backend/reference_operations.py => keras/backend/numpy_backend.py', 'keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Add the numpy implementation in the keras backend documentation ( # 11507 )
211,e47a003ed160d829300f6782a2e3ec014c739144,2018-10-31 15:51:27+01:00,"`` ` python return [ out1 , out2 ] x1 = Dense ( 32 ) ( input_1 ) x2 = Dense ( 32 ) ( input_2 ) out1 = tensors [ 0 ] * tensors [ 1 ] layer = Lambda ( hadamard_product_sum , hadamard_product_sum_output_shape ) # and sum of it from two input tensors return [ tuple ( shape1 ) , tuple ( shape2 [ : -1 ] ) ] Takes input tensor or list of tensors as first argument . assert shape1 == shape2 # else hadamard product is n't possible def hadamard_product_sum ( tensors ) : shape1 = list ( input_shapes [ 0 ] ) x_hadamard , x_sum = layer ( [ x1 , x2 ] ) Takes input tensor as first argument . `` ` out2 = K.sum ( out1 , axis=-1 ) def hadamard_product_sum_output_shape ( input_shapes ) : # add a layer that returns the hadamard product shape2 = list ( input_shapes [ 1 ] )",['keras/layers/core.py'],multiple input lambda doc example ( # 11529 )
212,b7fad0752779177e0968f93832d8a5b302236a7d,2018-10-31 16:24:10+09:00,"input_length , greedy=True ) decode_truth = np.array ( [ [ 0 , 1 , -1 ] , [ 1 , 1 , 0 ] ] ) inputs = K.variable ( inputs ) inputs = K.variable ( np.asarray ( inputs ) .transpose ( ( 1 , 0 , 2 ) ) ) length = input_length [ i ] np.sum ( -np.log ( [ 0.9 , 0.9 , 0.9 , 0.9 , 0.9 ] ) ) for i in range ( num_samples ) : decoded_dense = -np.ones_like ( y_pred [ ... , 0 ] ) log_prob_truth = np.array ( [ if greedy : inputs = np.asarray ( inputs ) .transpose ( ( 1 , 0 , 2 ) ) # batch_size length vector of negative log probabilities input_length , def ctc_decode ( y_pred , input_length , greedy=True , beam_width=100 , top_paths=1 ) : input_length , greedy=True ) input_length = K.variable ( np.array ( [ seq_len_0 , seq_len_1 ] , dtype=np.int32 ) ) ] , np.float32 ) [ : , np.newaxis ] input_length = K.variable ( input_length ) assert np.allclose ( log_prob_truth , log_prob_pred ) return inds [ inds < ( num_classes - 1 ) ] def _remove_repeats ( inds ) : input_length = np.array ( [ seq_len_0 , seq_len_1 ] , dtype=np.int32 ) decoded = np.argmax ( prob [ : length ] , axis=-1 ) assert np.allclose ( log_prob_pred_np , log_prob_pred ) decode_pred_np , log_prob_pred_np = KNP.ctc_decode ( inputs , num_samples = y_pred.shape [ 0 ] return decoded_dense [ : , : np.max ( decoded_length ) ] , log_prob assert np.alltrue ( decode_truth == decode_pred ) log_prob = np.zeros ( ( num_samples , 1 ) ) greedy=True ) decoded = _remove_blanks ( decoded , num_classes ) decoded_length [ i ] = len ( decoded ) decoded = _remove_repeats ( decoded ) assert np.alltrue ( decode_pred_np == decode_pred ) decoded_length = np.zeros ( ( num_samples , ) , dtype=np.int ) raise `` not supported yet '' log_prob [ i ] = -np.sum ( np.log ( prob [ np.arange ( length ) , decoded ] ) ) # keras output , unlike tensorflow , is a dense ( not sparse ) tensor num_classes = y_pred.shape [ -1 ] else : is_not_repeat = np.insert ( np.diff ( inds ) .astype ( np.bool ) , 0 , True ) return inds [ is_not_repeat ] decoded_dense [ i , : len ( decoded ) ] = decoded prob = y_pred [ i ] np.sum ( -np.log ( [ 1.0 , 0.6 , 0.6 , 0.9 ] ) ) , def _remove_blanks ( inds , num_classes ) :","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",add ctc_decode to numpy backend ( # 11504 )
213,3e772defb7781955be8c5490744532ccdfc1dd3f,2018-10-30 15:11:59-07:00,"f = H5Dict ( state , mode= ' r ' ) f = H5Dict ( d ) _serialize_model ( model , f ) def _deserialize_model ( f , custom_objects=None , compile=True ) : def _serialize_model ( model , f , include_optimizer=True ) : if 'optimizer_weights ' in f : model = _deserialize_model ( f , custom_objects , compile ) f = H5Dict ( filepath , mode= ' w ' ) h5dict : ` keras.utils.hdf5_utils.HFDict ` instance . f [ 'model_config ' ] = model_config group.attrs [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) h5dict = H5Dict ( d ) def _serialize_model ( model , h5dict , include_optimizer=True ) : h5dict = H5Dict ( filepath , mode= ' w ' ) # Arguments f.close ( ) def save_weights_to_hdf5_group ( group , layers ) : def _deserialize_model ( h5dict , custom_objects=None , compile=True ) : f : keras.utils.io_utils.HD5Dict instance . optimizer_weights_group = f [ 'optimizer_weights ' ] optimizer_weights_group = h5dict [ 'optimizer_weights ' ] h5dict = H5Dict ( filepath , ' r ' ) g = f.create_group ( layer.name ) `` `` '' Saves weights into the HDF5 group . model_config = f [ 'model_config ' ] `` `` '' h5dict [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) f : ` keras.utils.hdf5_utils.HFDict ` instance . group , 'layer_names ' , [ layer.name.encode ( 'utf8 ' ) for layer in layers ] ) g = group.create_group ( layer.name ) h5dict [ 'training_config ' ] = json.dumps ( { optimizer_weights_group = f [ 'optimizer_weights ' ] return _deserialize_model ( f ) training_config = f.get ( 'training_config ' ) f , 'layer_names ' , [ layer.name.encode ( 'utf8 ' ) for layer in layers ] ) h5dict [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) f [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) f = H5Dict ( filepath , ' r ' ) model = _deserialize_model ( h5dict , custom_objects , compile ) h5dict [ 'model_config ' ] = model_config group.attrs [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) _serialize_model ( model , h5dict ) _serialize_model ( model , h5dict , include_optimizer ) return _deserialize_model ( h5dict ) layers : Layers to load . h5dict = H5Dict ( state , mode= ' r ' ) f [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) model_config = h5dict [ 'model_config ' ] training_config = h5dict.get ( 'training_config ' ) model_weights_group = h5dict [ 'model_weights ' ] h5dict.close ( ) h5dict : keras.utils.io_utils.HD5Dict instance . if 'optimizer_weights ' in h5dict : model_weights_group = f [ 'model_weights ' ] optimizer_weights_group = h5dict [ 'optimizer_weights ' ] f [ 'training_config ' ] = json.dumps ( { f.attrs [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) f.attrs [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) group : A pointer to a HDF5 group . def save_weights_to_hdf5_group ( f , layers ) : _serialize_model ( model , f , include_optimizer )",['keras/engine/saving.py'],improve keras/engine/saving.py ( # 11305 )
214,2898da6cfcc893aafc0b9016f934ef08c196b9d4,2018-10-29 16:06:45-07:00,'Constraint ',"['docs/autogen.py', 'docs/templates/constraints.md']",Removed the base class of constraints from the documentation . ( # 11513 )
215,48c1c96ac4cfec5580a5feb7eb7ef7c25c6db234,2018-10-29 13:06:25-07:00,"if self.run_metadata : with shape ( batch_size , output_length , filters ) > > > K.is_keras_tensor ( keras_layer_output ) # Any Keras layer output is a Keras tensor . multiples = tf.stack ( [ 1 , tf.shape ( new_states [ i ] ) [ 1 ] ] ) > > > K.is_keras_tensor ( keras_layer_output ) session_has_key = has_arg ( tf.Session.run , key , True ) `` ` python In this case there will be soft-placing on the GPU device . Therefore we check if we are not explicitly put on if py_any ( is_tensor ( x ) for x in inputs ) : default_value=-1 ) new_x = x [ start [ 0 ] : start [ 0 ] + size [ 0 ] , ... , start [ -1 ] : start [ -1 ] + size [ -1 ] ] Tensor ` x [ start [ 0 ] : start [ 0 ] + size [ 0 ] , * W503 \ indices = concatenate ( [ batch_ind , label_ind ] , axis=0 ) and _has_nchw_support ( ) ) : > > > # A variable created with the keras backend is not a Keras tensor . TensorFlow does not support NCHW on CPU . Therefore we check if we are not explicitly put on output_shape , CPU , and have GPUs available . return self._legacy_call ( inputs ) A sliced tensor : tile_shape = tf.where ( shape_diff > 0 , expr_shape , zero_expr_shape ) decoded_dense.append ( dense_tensor ) 'expect to be 1 or % d dimensions ' if ( self.run_metadata and A boolean indicating if the current device if ( tf_data_format == 'NHWC ' keras/backend/tensorflow_backend.py E501 # so it may have extra axes with 1 , TensorFlow does not support NCHW on CPU . new_states = [ msg = 'Invalid argument `` % s '' passed to K.function with TensorFlow backend ' % key for i in range ( len ( states ) ) : st.values , tile_shape = tf.where ( shape_diff > 0 , expr_shape , tf.ones_like ( expr_shape ) ) ... , scope is explicitly set on the device type . current_version = StrictVersion ( tf.__version__.split ( '- ' ) [ 0 ] ) # it is not needed and should be removed StrictVersion ( tf.__version__.split ( '- ' ) [ 0 ] ) < StrictVersion ( ' 1.10.0 ' ) ) : ValueError : if ` data_format ` is if py_any ( is_tensor ( x ) for x in inputs ) : label_shape = tf.to_int64 ( label_shape ) `` ` > > > K.is_keras_tensor ( keras_var ) # A variable created with the keras backend is not a Keras tensor . return tf.SparseTensor ( tf.to_int64 ( indices ) , vals_sparse , tf.to_int64 ( label_shape ) ) the tensor after 1d conv with un-shared weights , with shape ( batch_size , output_length , filters ) > > > # A variable indirectly created outside of keras is not a Keras tensor . zero_expr_shape = tf.ones_like ( expr_shape ) 'to a Keras model and set ' raise ValueError ( 'Invalid argument `` % s '' passed to K.function ' * W503 max_num_labels_tns ) , reverse ( label_shape , 0 ) ) ) for st in decoded : return self._legacy_call ( inputs ) kernel , tmp = [ ] new_states [ i ] , states [ i ] ) for i in range ( len ( states ) ) tmp = tf.tile ( tf.range ( label_shape [ 0 ] ) , max_num_labels_tns ) ValueError : if ` data_format ` is neither ` `` channels_last '' ` or ` `` channels_first '' ` . 'with TensorFlow backend ' % key ) tmp.append ( tf.where ( tiled , new_states [ i ] , states [ i ] ) ) for st in decoded ] > > > # A placeholder is not a Keras tensor . def local_conv2d ( inputs , ' ` run_metadata ` , you need tensorflow 1.10 or higher . ' ) data_format=None ) : raise ValueError ( dense_tensor = tf.sparse_to_dense ( st.indices , st.dense_shape , indices = tf.to_int64 ( indices ) return tf.SparseTensor ( indices , vals_sparse , label_shape ) ' ` run_metadata ` , you need tensorflow 1.10 or higher . ' ) def local_conv2d ( inputs , kernel , kernel_size , strides , output_shape , data_format=None ) : ] if tf_data_format == 'NHWC ' or tf_data_format == 'NCHW ' and _has_nchw_support ( ) : batch_array = tf.transpose ( tf.reshape ( tmp , reverse ( label_shape , 0 ) ) ) new_states = tmp or tf_data_format == 'NCHW ' > > > K.is_keras_tensor ( keras_placeholder ) # A placeholder is not a Keras tensor . > > > K.is_keras_tensor ( keras_placeholder ) the tensor after 1d conv with un-shared weights , batch_array = tf.transpose ( tf.reshape ( tf.tile ( tf.range ( label_shape [ 0 ] ) , 'In order to feed symbolic tensors ' decoded_dense = [ tf.sparse_to_dense ( st.indices , st.dense_shape , st.values , default_value=-1 ) raise ValueError ( 'Unexpected bias dimensions % d , expect to be 1 or % d dimensions ' if not ( has_arg ( tf.Session.run , key , True ) or has_arg ( Function.__init__ , key , True ) ) : raise ValueError ( 'Unexpected bias dimensions % d , ' raise ValueError ( msg ) tiled = tf.tile ( mask_t , multiples ) if not ( session_has_key or function_has_key ) : kernel_size , indices = tf.transpose ( tf.reshape ( indices , [ 2 , -1 ] ) ) raise ValueError ( # so it may have extra axes with 1 , it is not needed and should be removed A boolean indicating if the current device scope is explicitly set on the device type . > > > K.is_keras_tensor ( k_var ) # A variable indirectly created outside of keras is not a Keras tensor . > > > K.is_keras_tensor ( keras_var ) strides , indices = tf.transpose ( tf.reshape ( concatenate ( [ batch_ind , label_ind ] , axis=0 ) , [ 2 , -1 ] ) ) CPU , and have GPUs available . In this case there will be soft-placing on the GPU device . > > > K.is_keras_tensor ( k_var ) neither ` `` channels_last '' ` or ` `` channels_first '' ` . function_has_key = has_arg ( Function.__init__ , key , True ) tf.where ( tf.tile ( mask_t , tf.stack ( [ 1 , tf.shape ( new_states [ i ] ) [ 1 ] ] ) ) , 'In order to feed symbolic tensors to a Keras model and set ' decoded_dense = [ ] start [ -1 ] : start [ -1 ] + size [ -1 ] ] ` > > > # Any Keras layer output is a Keras tensor . if current_version < StrictVersion ( ' 1.10.0 ' ) :","['keras/backend/tensorflow_backend.py', 'pytest.ini']",Enabled pep8 line lenght on the tensorflow backend . ( # 11514 )
216,a37b41d5ae799c338958b3eeeb884074dfba865f,2018-10-29 12:44:40-07:00,"def process_list_block ( docstring , starting_point , leading_spaces , marker ) : return dot ( x , y ) `` ` python next_section_idx = re.search ( section_regex , docstring [ shift : ] ) docstring_slice = docstring [ starting_point : section_end ] .replace ( block , marker ) generated = autogen.process_docstring ( dummy_docstring ) ( e.g . ` ( 2 , 3 ) * ( 4 , 3 , 5 ) - > ( 2 , 4 , 5 ) ` ) def test_doc_multiple_sections_code ( ) : # Numpy implementation docstring = ( docstring [ : starting_point ] docstring_slice # we have to recompute it docstring = docstring.replace ( block , marker ) # Examples # Theano-like behavior example When attempting to multiply a nD tensor else : `` `` '' docstring [ section_end : ] ) > > > xy = K.dot ( x , y ) > > > K.int_shape ( xy ) ( 2 , 4 , 5 ) section_end = -1 `` `` '' Checks that we can have code blocks in multiple sections . '' '' '' section_end = shift + next_section_idx.start ( ) if next_section_idx is None : dummy_docstring = `` '' '' Multiplies 2 tensors ( and/or variables ) and returns a * tensor * . `` ` assert 'def dot ( x , y ) : ' in generated section_end , def dot ( x , y ) : leading_spaces , marker ) : > > > y = K.ones ( ( 4 , 3 , 5 ) ) with a nD tensor , it reproduces the Theano behavior . # ` docstring ` has changed , so we ca n't use ` next_section_idx ` anymore > > > x = K.random_uniform_variable ( shape= ( 2 , 3 ) , low=0 , high=1 ) def process_list_block ( docstring , starting_point , section_end , assert ' # Theano-like behavior example ' in generated","['docs/autogen.py', 'tests/test_doc_auto_generation.py']",bug fix : ca n't use code blocks in multiple sections in docstrings . ( # 11508 )
217,aa0f12ce9eacd70932466d248fde746e0b0dfa0d,2018-10-29 09:51:46-07:00,signature = generic_utils.getargspec ( wrapped ) from keras.utils import generic_utils signature = inspect.getargspec ( function ) signature = generic_utils.getargspec ( function ) signature = inspect.getargspec ( wrapped ),['docs/autogen.py'],replace generic_utils.getargspec with inspect.getargspec ( # 11518 )
218,267ccbb4a76913680f4db6b400e05dea7aa84db7,2018-10-27 14:18:22+02:00,"def one_hot ( indices , num_classes ) : from keras.utils import to_categorical oh = np.eye ( num_classes ) [ indices ] oh = KNP.one_hot ( np.int32 ( indices ) , num_classes ) return to_categorical ( indices , num_classes )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",add one_hot to numpy backend ( # 11497 )
219,df8186833fe47a6fd4b7dbc52fe12f9df73609fb,2018-10-26 11:50:46-07:00,"path : Either a string ( path on disk ) , a dict , or a HDF5 Group . # We ca n't use isinstance here because it would require os.remove ( h5_path ) del f assert f [ ' x ' ] == 'abcd ' elif isinstance ( path , six.string_types ) : f = H5Dict ( Path ( h5_path ) , mode= ' w ' ) `` `` '' GitHub issue : 11459 '' '' '' _ , h5_path = tempfile.mkstemp ( '.h5 ' ) def test_H5Dict_accepts_pathlib_Path ( ) : return class_name == 'PosixPath ' or class_name == 'WindowsPath ' f.close ( ) f = H5Dict ( Path ( h5_path ) , mode= ' r ' ) def is_path_instance ( path ) : raise TypeError ( 'Required Group , str or dict . ' path : Either a string ( path on disk ) , a Path , a dict , or a HDF5 Group . f [ ' x ' ] = 'abcd ' class_name = type ( path ) .__name__ from pathlib import Path elif isinstance ( path , six.string_types ) or is_path_instance ( path ) : from pathlib2 import Path raise TypeError ( 'Required Group , str , Path or dict . ' # us to add pathlib2 to the Python 2 dependencies .","['keras/utils/io_utils.py', 'tests/keras/utils/io_utils_test.py']",[ P ] Make H5Dict accept pathlib Paths . ( # 11466 )
220,74264ea3cd5e8bdfe90325f98944297d7f409018,2018-10-26 11:47:16-07:00,"defaults=full_arg_spec.defaults ) arg_spec = inspect.getargspec ( fn ) signature = inspect.getargspec ( function ) from FullArgSpec . from keras.utils import generic_utils # Arguments signature = generic_utils.getargspec ( function ) signature = inspect.getargspec ( wrapped ) from .. utils import generic_utils arg_spec = inspect.ArgSpec ( `` `` '' full_arg_spec = inspect.getfullargspec ( fn ) if 'dtype ' in generic_utils.getargspec ( image.array_to_img ) .args : return arg_spec Calls ` getfullargspec ` and assigns args , varargs , ` ArgSpec ` struct . if 'dtype ' in inspect.getargspec ( if 'dtype ' in generic_utils.getargspec ( image.img_to_array ) .args : The parameter name 'varkw ' is changed to 'keywords ' to fit the if sys.version_info < ( 3 , ) : signature = generic_utils.getargspec ( wrapped ) `` `` '' Python 2/3 compatible ` getargspec ` . import inspect keywords=full_arg_spec.varkw , if 'dtype ' in inspect.getargspec ( image.img_to_array ) .args : else : # Returns varargs=full_arg_spec.varargs , def getargspec ( fn ) : if 'dtype ' in inspect.getargspec ( image.array_to_img ) .args : fn : the target function to inspect . An ArgSpec with args , varargs , keywords , and defaults parameters varkw , and defaults to a python 2/3 compatible ` ArgSpec ` . if 'dtype ' in generic_utils.getargspec ( args=full_arg_spec.args ,","['docs/autogen.py', 'keras/preprocessing/image.py', 'keras/utils/generic_utils.py']",Avoid deprecated getargspec ( # 11463 )
221,9148325b9ca9edf94aaebd453b3cf86dba2ef99a,2018-10-26 11:44:43+09:00,"# workaround because theano does n't accept axes x_batch_size = x_shape [ 0 ] if x.ndim > y.ndim : if x.shape [ 0 ] ! = y.shape [ 0 ] : return ( batch_size , ) + shape [ 1 : ] ' with both static and dynamic batch sizes . ' WITH_NP , axes= ( 2 , 2 ) ) test_cases.append ( [ ( None , 3 , 4 , 5 ) , ( None , 2 , 4 ) , 2 ] ) batch_size = 7 d2 = y.shape [ axes [ 1 ] ] if 0 in axes : # x : ( b_size , x1 , ... , d , ... , xn ) if x.ndim > y.ndim : result = squeeze ( result , -1 ) result = [ ] pattern = list ( range ( y_ndim ) ) test_cases.append ( [ ( None , 4 ) , ( None , 3 , 4 ) , ( 1 , 2 ) ] ) axes [ 0 ] = x.ndim - 1 y = C.swapaxes ( y , i , i - 1 ) pattern [ 1 ] = a1 result = np.array ( result ) x = K.variable ( x_np ) if axes [ 0 ] < 0 : permute_pattern [ i ] = permute_pattern [ i - 1 ] if d1 ! = d2 : if y.ndim == 2 : if len ( y.shape ) > 1 else 1 ) if ndim ( out ) == 1 : check_two_tensor_operation ( 'batch_dot ' , ( 4 , 2 ) , ( 4 , 2 , 3 ) , for i in range ( axes [ 1 ] , 1 , -1 ) : # batch size might be lost at this point # bring the dimensions to be reduced to axis 1 assert_allclose ( z , z_np , atol=1e-05 ) 'You probably attempted to permform the ' y_ndim = len ( y_shape ) axes = [ len ( x_shape ) - 1 , len ( y_shape ) - 2 ] y = permute_dimensions ( y , permute_pattern ) # reshape to closest broadcastable shape raise ValueError ( 'Can not perform batch dot on inputs ' out = tf.matmul ( x , y , adjoint_a=adj_x , adjoint_b=adj_y ) xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes= ( 0 , 1 ) ) axes = list ( axes ) xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes=1 ) permute_pattern = list ( range ( x_ndim ) ) for xi , yi in zip ( x , y ) : axes : int or tupe ( int , int ) . Target dimensions to be reduced . raise ValueError ( 'Can not perform batch_dot over axis 0 . ' ' If your inputs are not batched , ' str ( x_shape ) + ' and ' i += 1 axes : list of ( or single ) int with target dimensions . x_shape = tf.shape ( x ) ' with different batch sizes . ' ) while i < len ( x.shape ) - 1 : x = [ expand_dims ( t , axis ) for t in x ] for i in range ( a0 , 1 , -1 ) : x_shape = int_shape ( x ) pattern [ 1 ] = a0 # test with placeholders permute_pattern [ 1 ] = axes [ 1 ] # Input shapes : > > > xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes= [ 1 , 2 ] ) y_ndim = ndim ( y ) assert_allclose ( K.eval ( xy_batch_dot ) , np.ones ( ( 20 , 1 ) ) * 32 , atol=1e-05 ) return concatenate ( x , axis ) assert z_shape == z_np.shape d2 = y_shape [ a1 ] elif isinstance ( axes , tuple ) : adj_y = None # test with variables test_cases.append ( [ ( None , 3 , 4 ) , ( None , 2 , 3 , 4 ) , ( 2 , 3 ) ] ) if x_ndim > y_ndim : idx = x_ndim - 1 y = transpose ( y ) axes [ 1 ] = y.ndim - 1 if result.ndim == 1 : # Handle negative axes 'which is not yet supported on the CNTK backend . ' ) WITH_NP , axes= ( 1 , 1 ) ) # which contains the batch axis ( 0 ) WITH_NP , axes=1 ) return result if axes [ 0 ] == 1 else transpose ( result ) yi = squeeze ( yi , 0 ) if y_expanded : z_np = KNP.batch_dot ( x_np , y_np , axes ) out = np.squeeze ( out , tuple ( range ( idx , idx + diff ) ) ) assert_allclose ( f ( [ x_np , y_np ] ) [ 0 ] , z_np , atol=1e-05 ) result = tf.reduce_sum ( x * y , 1 ) axis_list = np.arange ( len ( out.shape ) - 1 ) .tolist ( ) if ndim ( xi ) == ndim ( x ) : # for older versions of CNTK d1 = x_shape [ axes [ 0 ] ] x_ndim = ndim ( x ) d1 = x_shape [ a0 ] ` x ` and ` y ` are data in batches , i.e . in a shape of raise ValueError ( 'Can not perform batch_dot on inputs ' result = stack ( inner_prodcuts ) result = sum ( x * y , axis=axes [ 0 ] , keepdims=True ) # y : ( b_size , d , ... ) y = expand_dims ( y ) axis_list.insert ( 0 , axis_list.pop ( axis ) ) z = K.eval ( z ) str ( x.shape ) + ' and ' + str ( y.shape ) test_cases.append ( [ ( None , 3 , 4 , 5 ) , ( None , 2 , 3 , 4 ) , ( 2 , 3 ) ] ) axes [ 0 ] += x.ndim x = reshape ( x , new_x_shape ) tuple ( axis_list ) ) x = permute_dimensions ( x , pattern ) pattern [ i ] = pattern [ i - 1 ] out = expand_dims ( out , 1 ) check_two_tensor_operation ( 'batch_dot ' , ( 4 , 2 , 3 ) , ( 4 , 5 , 3 ) , assert_allclose ( K.eval ( xy_batch_dot ) , np.ones ( ( 32 , 1 ) ) * 20 , atol=1e-05 ) if axes [ 1 ] < 0 : str ( x_shape ) + ' and ' adj_x = None if axes [ 0 ] == ndim ( x ) - 1 else True if x_ndim < 2 or y_ndim < 2 : y = permute_dimensions ( y , pattern ) if axes [ 0 ] == 0 : axes = [ x.ndim - 1 , y.ndim - 1 ] `` ` if x_batch_size ! = y_batch_size : new_y_shape = tf.concat ( [ y_shape [ :2 ] , tf.ones_like ( x_shape [ 2 : ] ) , y_shape [ 2 : ] ] , 0 ) axes [ 1 ] += y.ndim out = np.transpose ( np.diagonal ( out , axis1=0 , axis2=axis ) , x = C.swapaxes ( x , i , i + 1 ) dynamic_batch_size = False y_shape = int_shape ( y ) # y : ( b_size , y1 , ... , d , ... , yn ) if x_expanded : > > > xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes= ( 1 , 2 ) ) pattern = list ( range ( x_ndim ) ) check_two_tensor_operation ( 'batch_dot ' , ( 4 , 2 , 3 ) , ( 4 , 3 ) , # if tuple , convert to list i = normalized_axis [ 0 ] ` x ` and ` y ` are data in batch , i.e . in a shape of else : else : z_shape = K.int_shape ( z ) elif y_ndim > x_ndim : else : diff = 0 diff = x.ndim - y.ndim if y_ndim == 2 : result = C.times ( x , y , output_rank=y_ndim - 2 + int ( y_expanded ) ) result.append ( C.times ( xi , yi , output_rank=y_ndim - 2 + int ( y_expanded ) ) ) # convert negative indices return out x = transpose ( x ) # where d is the dimension to reduce . if x_ndim > y_ndim : if x_ndim == 2 : x = expand_dims ( x , 1 ) axes = [ x_ndim - 1 , y_ndim - 2 ] if len ( y_shape ) == 2 : # x : ( b_size , ... , d ) yi = y [ i ] result = tf.expand_dims ( result , -1 ) str ( y_shape ) + ' . ' ) Pseudocode : for i in range ( axes [ 0 ] , x_ndim - 1 ) : i = normalized_axis [ 1 ] # making sure swapping axes when ndim == 2 works ' y.shape [ % d ] ( % d ! = % d ) . ' % ( axes [ 0 ] , axes [ 1 ] , d1 , d2 ) ) normalized_axis.append ( _normalize_axis ( axes [ 1 ] , y ) [ 0 ] ) new_x_shape = tf.concat ( [ x_shape , tf.ones_like ( y_shape [ 2 : ] ) ] , 0 ) if z_shape is not None : x_expanded = False inner_products.append ( xi.dot ( yi ) ) if d1 is not None and d2 is not None and d1 ! = d2 : y = reshape ( y , new_y_shape ) out = np.sum ( np.multiply ( x , y ) , axes [ 0 ] ) y_batch = K.ones ( shape= ( 20 , 32 ) ) for axis in [ axes [ 0 ] ] : x_ndim = len ( x_shape ) axes = ( axes , axes ) 'Received inputs with shapes ' return result y = expand_dims ( y , -1 ) def random ( shape ) : test_cases.append ( [ ( None , 4 ) , ( None , 4 ) , None ] ) if diff : y = K.variable ( y_np ) if a0 ! = 1 : x = tf.reshape ( x , tf.concat ( [ tf.shape ( x ) , [ 1 ] * ( diff ) ] , axis=0 ) ) result = np.expand_dims ( result , -1 ) diff = y_ndim - x_ndim axes [ 0 ] += x_ndim idx = x.ndim + y.ndim - 3 check_two_tensor_operation ( 'batch_dot ' , ( 32 , 20 ) , ( 32 , 20 ) , x_np = random ( x_shape ) result = squeeze ( result , -1 ) if ndim ( result ) == 1 : if axes [ 0 ] == axes [ 1 ] : for x_shape , y_shape , axes in test_cases : z = K.batch_dot ( x , y , axes ) permute_pattern [ i ] = permute_pattern [ i + 1 ] axes = [ axes , axes ] y_batch = K.ones ( shape= ( 32 , 20 ) ) raise ValueError ( 'Can not perform batch dot over axis 0 . ' ) for xi , yi in zip ( x , y ) : x = K.placeholder ( shape=x_shape ) # Bring d to the last dimension in x ' add a dummy batch dimension to your ' axes = [ x.ndim - 1 , y.ndim - 2 ] xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes= ( 1 , 0 ) ) axes [ 1 ] += y_ndim y_expanded = True y = np.reshape ( y , np.concatenate ( [ np.shape ( y ) , [ 1 ] * diff ] , axis=0 ) ) if x.ndim < 2 or y.ndim < 2 : y_shape = tf.shape ( y ) normalized_axis = [ ] x_expanded = True adj_x = None raise ValueError ( 'Batch dot requires inputs of rank 2 or more . ' ) # Expand to rank 3 if needed 'Received inputs with shapes ' idx = x_ndim + y_ndim - 3 # Note : batch_dot implementation is different for for i in range ( x_batch_size ) : if ndim ( x ) == 2 and ndim ( y ) == 2 : adj_y = True if axes [ 1 ] == ndim ( y ) - 1 else None The lengths of ` axes [ 0 ] ` and ` axes [ 1 ] ` should be the same . result = stack ( result , 0 ) out = tf.reduce_sum ( tf.multiply ( tf.transpose ( x , [ 1 , 0 ] ) , y ) , axes [ 1 ] ) result.append ( np.tensordot ( xi , yi , axes ) ) return expand_dims ( result ) dynamic_batch_size = True result = C.times ( x , y , output_rank= ( len ( y.shape ) - 1 ) y_batch_size = y_shape [ 0 ] WITH_NP , axes= ( 2 , 1 ) ) 'operation on a placeholder and a variable , ' # behaves like tf.batch_matmul as default elif x_batch_size is not None and y_batch_size is not None : # transpose axes = [ x_ndim - 1 , y_ndim - 2 ] result = [ ] raise ValueError ( 'Can not do batch_dot on inputs ' while i > 0 : if axes is not None : str ( x_shape ) + ' and ' + str ( y_shape ) out = tf.reduce_sum ( tf.multiply ( x , y ) , axes [ 0 ] ) if x_batch_size is not None and y_batch_size is not None : @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , reason='Not supported . ' ) test_cases.append ( [ ( None , 4 , 3 ) , ( None , 3 , 5 ) , ( 2 , 1 ) ] ) if dynamic_batch_size : out = tf.squeeze ( out , list ( range ( idx , idx + diff ) ) ) out = np.sum ( np.multiply ( np.transpose ( x , [ 1 , 0 ] ) , y ) , axes [ 1 ] ) permute_pattern = list ( range ( y_ndim ) ) xi = x [ i ] def batch_shape ( shape ) : str ( y_shape ) + ' . ' ) 'with rank < 2 . ' ' with axes= ' + str ( axes ) + ' . x.shape [ % d ] ! = ' permute_pattern [ -1 ] = axes [ 0 ] diff = x_ndim - y_ndim else : x = permute_dimensions ( x , permute_pattern ) axes = [ axes [ 0 ] - 1 , axes [ 1 ] - 1 ] # ignore batch dimension out = np.tensordot ( x , y , axes=axes ) axes = [ x.ndim - 1 , y.ndim - 2 ] idx = x.ndim - 1 return np.random.random ( batch_shape ( shape ) ) f = K.function ( [ x , y ] , [ z ] ) y = tf.reshape ( y , tf.concat ( [ tf.shape ( y ) , [ 1 ] * ( diff ) ] , axis=0 ) ) raise ValueError ( 'Can not do batch_dot on inputs ' return result i -= 1 normalized_axis.append ( _normalize_axis ( axes [ 0 ] , x ) [ 0 ] ) d1 = x.shape [ axes [ 0 ] ] if axes [ 1 ] == 0 : # Bring d to the second dimension in y raise ValueError ( 'Can not do batch_dot on inputs with shapes ' if y_ndim == 2 : if x_batch_size is None and y_batch_size is None : x_batch = K.ones ( shape= ( 32 , 20 ) ) 'inputs using K.expand_dims ( x , 0 ) ' ) for i in range ( a1 , 1 , -1 ) : def stack ( x , axis=0 ) : axes = [ x_ndim - 1 , y_ndim - 1 ] d2 = y_shape [ axes [ 1 ] ] 'If your inputs are not batched , ' axes = list ( axes ) # sanity checks a0 , a1 = axes result = squeeze ( result , 1 ) 'with different batch sizes . ' if a1 ! = 1 : test_cases = [ ] y_expanded = False return sum ( x * transpose ( y ) , axis=axes [ 0 ] , keepdims=True ) xi = squeeze ( xi , 0 ) if len ( x_shape ) == 2 and len ( y_shape ) == 2 : y_np = random ( y_shape ) assert z_shape [ 1 : ] == z_np.shape [ 1 : ] # placeholders and variables in CNTK backend y = K.placeholder ( shape=y_shape ) inner_products = [ ] xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes=0 )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Bug fix : Batch dot ( # 11458 )
222,882302d2b0b6a5aad20dc08a98a9384165b3ab0a,2018-10-25 15:54:45-07:00,"vis_utils.plot_model ( model , to_file='model3.png ' , show_shapes=True , dpi : dot DPI . expand_nested=False , A ` pydot.Dot ` instance representing the Keras model or ` plot_model ` takes two optional arguments : dot.set ( 'dpi ' , dpi ) inner_input = Input ( shape= ( 2 , 3 ) , dtype='float32 ' , name='inner_input ' ) if not expand_nested or not ( assert dot.get_node ( inbound_layer_id ) dot.set ( 'concentrate ' , True ) dpi=96 , dot.add_edge ( pydot.Edge ( inbound_layer_id , layer_id ) ) assert dot.get_node ( layer_id ) dot.set_node_defaults ( shape='record ' ) else : # create any missing node . layer_name = ' { } ( { } ) '.format ( layer_name , layer.layer.name ) inbound_layer_id = str ( id ( inbound_layer ) ) assert dot.get_node ( layer_id ) preds = Dense ( 5 , activation='softmax ' , name='predictions ' ) ( lstm ) dot = pydot.Dot ( ) class_name = ' { } ( { } ) '.format ( class_name , child_class_name ) subgraph=False ) : isinstance ( inbound_layer.layer , Model ) ) : ` expand_nested ` ( defaults to False ) controls whether to expand nested models into clusters in the graph . from .. models import Model ` subgraph=True ` . dpi=96 ) : dot = pydot.Dot ( ) # Make sure that both nodes exist before connecting them with dot.set ( 'labeljust ' , ' l ' ) outer_input = Input ( shape= ( 5 , 2 , 3 ) , dtype='float32 ' , name='input ' ) model = Model ( outer_input , preds ) model_nodes = submodel.get_nodes ( ) ` dpi ` ( defaults to 96 ) controls image dpi . dot = model_to_dot ( model , show_shapes , show_layer_names , rankdir , model_nodes [ len ( model_nodes ) - 1 ] .get_name ( ) , expand_nested=False , rankdir='TB ' ) : rankdir='TB ' , expand_nested , dpi ) else : child_class_name = layer.layer.__class__.__name__ layer_name = ' { } ( { } ) '.format ( layer_name , layer.layer.name ) # an edge , as add_edge would otherwise create any missing node . subgraph=True ) subgraph : whether to return a pydot.Cluster instance . dot.add_subgraph ( submodel ) inner_encoder = TimeDistributed ( encoder , name='td_encoder ' ) ( outer_input ) child_class_name = layer.layer.__class__.__name__ dot.add_edge ( pydot.Edge ( class_name = ' { } ( { } ) '.format ( class_name , child_class_name ) dot = pydot.Cluster ( style='dashed ' ) inner_lstm = Bidirectional ( LSTM ( 16 , name='inner_lstm ' ) , name='bd ' ) ( inner_input ) rankdir='TB ' , encoder = Model ( inner_input , inner_lstm , name='Encoder_Model ' ) expand_nested=True , dpi=300 ) ` plot_model ` takes four optional arguments : from keras.layers import Conv2D , Bidirectional rankdir='TB ' ) : os.remove ( 'model3.png ' ) dot.set ( 'rankdir ' , rankdir ) if len ( layers ) > i + 1 : isinstance ( inbound_layer , Wrapper ) and expand_nested : whether to expand nested models into clusters . # Make sure that both nodes exist before connecting them with dot = model_to_dot ( model , show_shapes , show_layer_names , rankdir ) next_layer_id ) ) dot.set ( 'concentrate ' , True ) dot.set_node_defaults ( shape='record ' ) A ` pydot.Dot ` instance representing the Keras model . if subgraph : a ` pydot.Cluster ` instance representing nested model if assert dot.get_node ( inbound_layer_id ) dot.add_edge ( pydot.Edge ( inbound_layer_id , layer_id ) ) show_layer_names , rankdir , expand_nested , lstm = LSTM ( 16 , name='outer_lstm ' ) ( inner_encoder ) dot.add_edge ( pydot.Edge ( layer_id , model_nodes [ 0 ] .get_name ( ) ) ) from keras import Input , Model from keras.layers import Conv2D next_layer_id = str ( id ( layers [ i + 1 ] ) ) dot.set ( 'rankdir ' , rankdir ) submodel = model_to_dot ( layer.layer , show_shapes , inbound_layer_id = str ( id ( inbound_layer ) ) # an edge , as add_edge would otherwise for layer in layers : if expand_nested and isinstance ( layer.layer , Model ) : for i , layer in enumerate ( layers ) : dot.set ( 'label ' , model.name )","['docs/templates/visualization.md', 'keras/utils/vis_utils.py', 'tests/keras/utils/vis_utils_test.py']",[ P ] [ RELNOTES ] Add ability to visualize wrapped models with plot_model function ( # 11431 )
223,6272e662ae0c99ce7cad912992d4c55e22c8a369,2018-10-25 11:26:16-07:00,"enqueuer2 = OrderedEnqueuer ( DummySequence ( [ 3 , 10 , 10 , 3 ] , value=15 ) , import time DummySequence ( [ 3 , 10 , 10 , 3 ] ) ) , use_multiprocessing=True ) DummySequence ( [ 3 , 200 , 200 , 3 ] ) ) , use_multiprocessing=False ) DummySequence ( [ 3 , 10 , 10 , 3 ] ) ) , use_multiprocessing=False ) enqueuer = OrderedEnqueuer ( DummySequence ( [ 3 , 10 , 10 , 3 ] ) , time.sleep ( 0.05 ) DummySequence ( [ 3 , 200 , 200 , 3 ] ) ) , use_multiprocessing=True ) enqueuer2 = OrderedEnqueuer ( DummySequence ( [ 3 , 200 , 200 , 3 ] , value=15 ) , enqueuer = OrderedEnqueuer ( DummySequence ( [ 3 , 200 , 200 , 3 ] ) ,",['tests/keras/utils/data_utils_test.py'],Reduced the size of the numpy arrays used in the tests . ( # 11491 )
224,d6b5c5ebb410e3366c9d7aca41977a60134bfe10,2018-10-24 09:04:07-07:00,"seq : a possible Sequence object classification=True , if is_sequence : # TODO Dref360 : Decide which pattern to follow . First needs a new TF Version . y_train = to_categorical ( y_train ) layers.MaxPooling2D ( pool_size=2 ) , model.compile ( loss='categorical_crossentropy ' , val_use_sequence_api ) np.random.seed ( 1337 ) layers.Conv2D ( filters=8 , kernel_size=3 , use_sequence_api = True # Arguments assert history.history [ 'val_acc ' ] [ -1 ] > 0.75 ] ) activation='relu ' , boolean , whether the object follows the Sequence API . img_gen = ImageDataGenerator ( rescale=1 . ) # Dummy ImageDataGenerator num_classes=4 ) `` `` '' if ( val_gen and not val_use_sequence_api and from keras.preprocessing.image import ImageDataGenerator `` `` '' Determine if an object follows the Sequence API . input_shape=input_shape ) , if ( val_gen and not isinstance ( validation_data , Sequence ) and layers.Conv2D ( filters=4 , kernel_size= ( 3 , 3 ) , if use_sequence_api : y_test = to_categorical ( y_test ) activation='relu ' , padding='same ' ) , model = Sequential ( [ isinstance ( validation_data , Sequence ) ) or set ( dir ( Sequence ( ) ) ) .issubset ( set ( dir ( seq ) + [ 'use_sequence_api ' ] ) ) ) return ( getattr ( seq , 'use_sequence_api ' , False ) input_shape = ( 16 , 16 , 3 ) from .training_utils import is_sequence history = model.fit_generator ( img_gen.flow ( x_train , y_train , batch_size=16 ) , def is_sequence ( seq ) : from .. utils import Sequence if not use_sequence_api and use_multiprocessing and workers > 1 : if is_sequence ( val_data ) : validation_data=img_gen.flow ( x_test , y_test , def test_image_data_generator_training ( ) : ( x_train , y_train ) , ( x_test , y_test ) = get_test_data ( num_train=500 , if use_sequence_api : optimizer='rmsprop ' , metrics= [ 'accuracy ' ] ) batch_size=16 ) , if not is_sequence and use_multiprocessing and workers > 1 : epochs=10 , val_use_sequence_api = is_sequence ( validation_data ) use_sequence_api = is_sequence ( generator ) if is_sequence : # Returns if isinstance ( val_data , Sequence ) : input_shape=input_shape , layers.GlobalAveragePooling2D ( ) , K.backend ( ) == 'tensorflow ' , verbose=0 ) model.evaluate_generator ( img_gen.flow ( x_train , y_train , batch_size=16 ) ) num_test=200 , K.backend ( ) == 'tensorflow ' and 'TRAVIS_PYTHON_VERSION ' in os.environ , layers.Dense ( y_test.shape [ -1 ] , activation='softmax ' ) is_sequence = isinstance ( generator , Sequence )","['keras/engine/training_generator.py', 'keras/engine/training_utils.py', 'keras/utils/data_utils.py', 'tests/integration_tests/test_image_data_tasks.py', 'tests/keras/utils/data_utils_test.py', 'tests/test_multiprocessing.py']",Improve type-check for Sequence ( # 11468 )
225,36b9e4c055f32718a036cabaf767325b010c7485,2018-10-23 09:56:15-07:00,"'page ' : 'constraints.md ' , __non_neg ( ) __ : non-negativity constraint __max_norm ( max_value=2 , axis=0 ) __ : maximum-norm constraint } , from keras import constraints __min_max_norm ( min_value=0.0 , max_value=1.0 , rate=1.0 , axis=0 ) __ : minimum/maximum-norm constraint { { autogenerated } } __unit_norm ( axis=0 ) __ : unit-norm constraint { 'all_module_classes ' : [ constraints ] ,","['docs/autogen.py', 'docs/templates/constraints.md']",# # # Summary ( # 11456 )
226,b9a74813081303399e8fce0f9f5f37f6a8343b03,2018-10-22 10:12:14-07:00,"sequences of 10 time steps with 128 features per step in ( tuple of integers , does not include the sample axis ) , provide an ` input_shape ` argument ( tuple of integers or ` None ` , does not ` ( 10 , 128 ) ` for sequences of 10 vectors of 128-dimensional vectors , ( tuple of integers , does not include the batch axis ) , include the batch axis ) , e.g . ` input_shape= ( 10 , 128 ) ` for time series ` data_format= '' channels_last '' ` , or ` ( None , 128 ) ` for variable-length ( tuple of integers or ` None ` , e.g . or ` ( None , 128 ) ` for variable-length sequences of 128-dimensional vectors . sequences with 128 features per step . provide an ` input_shape ` argument",['keras/layers/convolutional.py'],Update Conv1D input_shape description ( # 11437 )
227,0a639230018d0c9e9f12476dc8b1c6e40c2dd0d0,2018-10-21 14:00:15-07:00,"x = K.variable ( [ -10. , -5. , 0. , 5. , 10 . ] ) min_value = -np.inf max_value = K.variable ( [ 5. , 4. , 1. , 4. , 9 . ] ) min_value : Python float or integer . if ( isinstance ( min_value , ( int , float ) ) and # GitHub issue : 11435 if max_value < min_value : if min_value is None : min_value : Python float , integer or tensor . max_value : Python float or integer . def test_clip_supports_tensor_arguments ( self ) : min_value = -np.inf max_value = min_value max_value = _to_tensor ( max_value , x.dtype.base_dtype ) if max_value is not None and max_value < min_value : min_value = K.variable ( [ -5. , -4. , 0. , 3. , 5 . ] ) np.asarray ( [ -5. , -4. , 0. , 4. , 9 . ] , dtype=np.float32 ) ) if min_value is None : assert np.allclose ( K.eval ( K.clip ( x , min_value , max_value ) ) , min_value = _to_tensor ( min_value , x.dtype.base_dtype ) max_value = min_value isinstance ( max_value , ( int , float ) ) ) : max_value : Python float , integer or tensor .","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py']",[ P ] Support tensor arguments in K.clip ( ) ( # 11442 )
228,c2244d2a4cb5f86968fb117f75469283a19b8a24,2018-10-21 13:45:02-07:00,"assert k_s_d.shape == k_d.shape t_W = K.variable ( W ) k_s = k.eval ( k.dot ( k.variable ( x_sparse ) , t_W ) ) assert K.is_sparse ( k_s ) k_d = k.eval ( k.concatenate ( [ k.variable ( x_dense_1 ) , # Theano has some dependency issues for sparse assert k_s.shape == k_d.shape assert k.is_sparse ( k_s ) assert_allclose ( k_s_d , k_d , atol=1e-05 ) reason='Sparse tensors are not supported in cntk . ' ) t = K.arange ( start ) k_s_d = k.eval ( k_s ) k_s = K.concatenate ( [ K.variable ( x_sparse_1 ) , K.variable ( x_sparse_2 ) ] ) k_s = k.concatenate ( [ k.variable ( x_sparse_1 ) , k.variable ( x_sparse_2 ) ] ) assert len ( k.eval ( t ) ) == 0 @ pytest.mark.skipif ( K.backend ( ) == 'cntk ' , assert len ( K.eval ( t ) ) == 1 k_s = K.eval ( K.dot ( K.variable ( x_sparse ) , t_W ) ) # cntk not support it yet start = K.constant ( -1 , dtype='int32 ' ) assert k_s_d.shape == k_d.shape if KTH.th_sparse_module : t = k.arange ( start ) or ( K.backend ( ) == 'theano ' and not K.th_sparse_module ) ) , for k in backends : start = K.constant ( 1 , dtype='int32 ' ) assert_allclose ( k_s_d , k_d , atol=1e-05 ) for k in WITH_NP : assert k_s.shape == k_d.shape reason='Sparse tensors are not supported in cntk ' k_d = k.eval ( k.dot ( k.variable ( x_dense ) , t_W ) ) 'and Theano has some dependency issues for sparse . ' ) assert_allclose ( k_s , k_d , atol=1e-05 ) assert len ( K.eval ( t ) ) == 0 k.variable ( x_dense_2 ) ] ) ) t_W = k.variable ( W ) assert_allclose ( k_s , k_d , atol=1e-05 ) assert len ( k.eval ( t ) ) == 1 backends.append ( KTH ) start = k.constant ( 1 , dtype='int32 ' ) k_d = K.eval ( K.dot ( K.variable ( x_dense ) , t_W ) ) k_s_d = K.eval ( k_s ) k_d = K.eval ( K.concatenate ( [ K.variable ( x_dense_1 ) , K.variable ( x_dense_2 ) ] ) ) start = k.constant ( -1 , dtype='int32 ' ) backends = [ KTF ]",['tests/keras/backend/backend_test.py'],Some tests were ran multiple times in backend_test.py . Fixed it . ( # 11394 )
229,37c0c1e1975e23bc96d31ed2c990412d0239a406,2018-10-21 14:33:08+02:00,"layers = model._layers from keras.layers import Embedding assert dot.get_node ( inbound_layer_id ) vis_utils.plot_model ( model , show_layer_names=True ) `` `` '' Fixes # 11376 '' '' '' assert dot.get_node ( layer_id ) show_shapes=True , # an edge , as add_edge would otherwise create any missing node . model.add ( Embedding ( 10000 , 256 , input_length=400 , name='embed ' ) ) # Make sure that both nodes exist before connecting them with layers = model.layers to_file='model1.png ' , model = Sequential ( ) def test_plot_sequential_embedding ( ) : os.remove ( 'model1.png ' )","['keras/utils/vis_utils.py', 'tests/keras/utils/vis_utils_test.py']",Draw input layers even when added implicitly ( # 11412 )
230,5ef0fc3215fb43f5772673b31c51ad8a6582bc39,2018-10-20 18:42:25+02:00,"new_states = [ C.element_select ( m , n , s ) for n , s in zip ( new_states , past_values ) ] if _LEARNING_PHASE in { 0 , 1 } : new_shape = tuple ( [ C.InferredDimension if _ == C.FreeDimension else _ for _ in new_shape ] ) return C.random.uniform ( new_states_temp.append ( C.element_select ( m , n , s ) ) ( _LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1 ) ) : 'You passed : % s ' % ( dilation_rate , ) ) 'CNTK can not take variable length inputs . Please ' for n , s in zip ( new_states , past_values ) : 'Please set ` dilation_rate ` to 1 . You passed : % s ' % ( dilation_rate , ) ) return C.times ( one_hot_matrix , reference , output_rank=len ( reference.shape ) - 1 ) x , C.variables.Parameter ) ) : 'requested permute on dynamic axis , ' elif ( isinstance ( x , C.variables.Constant ) or isinstance ( return _LEARNING_PHASE if _LEARNING_PHASE in { 0 , 1 } else _LEARNING_PHASE_PLACEHOLDER # static learning phase flag , if it is not 0 or 1 , we will go with dynamic seed=seed ) 'which is not supported . Please do permute ' rnn_constants.append ( C.sequence.broadcast_as ( constant , rnn_inputs ) ) shape_temp = [ ] scale=stddev , seed=seed , output_rank=len ( reference.shape ) - 1 ) 'to shape ` % s ` , but input shape is ` % s ` . Currently ' new_shape = tuple ( new_shape_temp ) return C.times ( constant , 'variable-length sequences . Please specify a ' dtype=dtype ) % ( str ( tensor.shape ) , str ( value.shape ) ) ) _LEARNING_PHASE == 1 ) ) : new_states = new_states_temp _LEARNING_PHASE_PLACEHOLDER = C.constant ( new_states_temp = [ ] 'CNTK can not take variable length inputs . Please ' if _ == C.FreeDimension : shape_temp.append ( C.InferredDimension ) np.asarray ( self.target_shape ) ) shape=shape , shape_temp.append ( _ ) [ _.output for _ in unrelated_updates ] ) dynamic_dimension = C.FreeDimension if _get_cntk_version ( ) > = 2.2 else C.InferredDimension num_element = root_gradients.shape ( ) [ 0 ] * np.prod ( np.asarray ( self.target_shape ) ) 'You passed : % s ' % ( dilation_rate , ) ) def resize_images ( x , height_factor , width_factor , data_format , interpolation='nearest ' ) : raise NotImplementedError ( place_holders = [ C.placeholder ( return C.random.uniform ( shape=shape , dtype=dtype , low=minval , high=maxval , seed=seed ) 'to shape ` % s ` , but input shape is ` % s ` . Currently ' return C.random.normal ( shape=shape , mean=mean , scale=stddev , seed=seed , dtype=dtype ) 'on static axis . ' % pattern ) dynamic_dimension = C.FreeDimension interpolation='nearest ' ) : else : 'Please set ` dilation_rate ` to ( 1 , 1 ) . ' 'variable-length sequences . Please specify a ' % ( str ( tensor.shape ) , str ( value.shape ) ) ) raise ValueError ( 'CNTK backend : the permute pattern % s ' new_shape_temp.append ( C.InferredDimension ) 'CNTK Backend : tensor with keras shape : ` % s ` has ' 'double check the keras shape history . ' % ( str ( shape ) , nones ) ) # learning phase tensor . 'Please set ` dilation_rate ` to ( 1 , 1 ) . ' 'Please set ` dilation_rate ` to ( 1 , 1 , 1 ) . ' raise ValueError ( num_element = root_gradients.shape ( ) [ 0 ] * np.prod ( 'static length for your sequences . ' ) raise ValueError ( 'Dilated convolution on CPU is not supported by CNTK backend . ' 'CNTK Backend : ` go_backwards ` is not supported with ' if ( num_dynamic_axis > 0 and shape= ( ) , dtype=np.float32 , def resize_images ( x , height_factor , width_factor , data_format , raise NotImplementedError ( 'CNTK Backend : ` go_backwards ` is not supported with ' raise ValueError ( 'CNTK backend : The placeholder has been resolved ' 'Please set ` dilation_rate ` to 1 . You passed : % s ' % ( dilation_rate , ) ) 'static length for your sequences . ' ) 'Dilated convolution on CPU is not supported by CNTK backend . ' for _ in shape : one_hot_matrix , reference , rnn_constants.append ( C.sequence.broadcast_as ( f_stats.append ( pattern [ : num_dynamic_axis ] ! = current_layout [ : num_dynamic_axis ] ) : self.unrelated_updates = C.combine ( return _LEARNING_PHASE_PLACEHOLDER 'pass inputs that have a static shape . ' shape=shape , mean=mean , dtype=dtype , 'on static axis . ' % pattern ) return C.random.normal ( raise ValueError ( 'Please set ` dilation_rate ` to ( 1 , 1 , 1 ) . ' shape = tuple ( shape_temp ) 'requested permute on dynamic axis , ' raise ValueError ( 'CNTK backend : the permute pattern % s ' name='_keras_learning_phase ' ) dynamic_axes=x.dynamic_axes ) for _ in states ] if _get_cntk_version ( ) > = 2.2 : rnn_inputs ) ) # static learning phase flag , if it is not 0 or 1 , we will go with dynamic learning phase tensor . dynamic_dimension = C.InferredDimension ( _LEARNING_PHASE_PLACEHOLDER.value == 1.0 or self.unrelated_updates = C.combine ( [ _.output for _ in unrelated_updates ] ) % ( str ( shape ) , nones ) ) shape = tuple ( [ C.InferredDimension if _ == C.FreeDimension else _ for _ in shape ] ) new_shape_temp.append ( _ ) if num_dynamic_axis > 0 and pattern [ : num_dynamic_axis ] ! = current_layout [ : num_dynamic_axis ] : elif isinstance ( x , C.variables.Constant ) or isinstance ( x , C.variables.Parameter ) : 'which is not supported . Please do permute ' 'double check the keras shape history . ' value=1.0 , ' % d cntk dynamic axis , this is not expected , please ' raise ValueError ( 'CNTK Backend : tensor with keras shape : ` % s ` has ' ' % d cntk dynamic axis , this is not expected , please ' new_shape_temp = [ ] else : 'CNTK backend : The placeholder has been resolved ' 'pass inputs that have a static shape . ' C.user_function ( ConvertToStatic ( l_s , batch_size=i_s.shape [ 0 ] ) ) ) _LEARNING_PHASE_PLACEHOLDER = C.constant ( shape= ( ) , dtype=np.float32 , value=1.0 , name='_keras_learning_phase ' ) low=minval , high=maxval , place_holders = [ C.placeholder ( dynamic_axes=x.dynamic_axes ) for _ in states ] return _LEARNING_PHASE keras/backend/cntk_backend.py E501 \ for _ in new_shape : f_stats.append ( C.user_function ( ConvertToStatic ( l_s , batch_size=i_s.shape [ 0 ] ) ) )","['keras/backend/cntk_backend.py', 'pytest.ini']",Line length reduced to 85 characters for cntk_backend.py ( # 11401 )
231,8fc8a1681332cc4c8b05260b20ee54fa1589e3d2,2018-10-20 16:45:52+02:00,"conv_out = conv_out [ : , : , : , : i , : ] smoothed_predict = ( 1 - alpha ) * predict [ : , Y ] + alpha * np.float32 ( 1 . ) / Y.shape [ 0 ] pointwise_kernel_shape = pointwise_kernel.eval ( ) .shape outputs_info= [ np.int32 ( 1 ) , log_first , np.int32 ( 1 ) , log_first ] ) > > > K.is_keras_tensor ( keras_layer_output ) # Any Keras layer output is a Keras tensor . i = ( x.shape [ 3 ] + strides [ 1 ] - 1 ) // strides [ 1 ] > > > K.is_keras_tensor ( keras_layer_output ) return _old_normalize_batch_in_training ( # for depthwise convolution . pointwise_kernel_shape = _preprocess_conv2d_filter_shape ( depthwise_kernel_shape , data_format ) imshp=None , or output of the softmax # 0 = test , 1 = train _LEARNING_PHASE = T.scalar ( dtype='uint8 ' , name='keras_learning_phase ' ) # 0 = test , 1 = train _raise_invalid_arg ( key ) conv_out = conv_out [ : , : , : , : i ] step , op = T.nnet.abstract_conv.AbstractConv2d_gradInputs ( pointwise_kernel_shape , data_format ) > > > # A variable created with the keras backend is not a Keras tensor . output_shape , conv_out = conv_out [ : , : , : , : ( x.shape [ 3 ] + strides [ 1 ] - 1 ) // strides [ 1 ] , : ] filter_dilation=dilation_rate ) slices = [ py_slice ( None , None , -1 ) if i in axes else py_slice ( None , None , None ) for i in range ( x.ndim ) ] conv_out = conv_out [ : , : , : ( x.shape [ 2 ] + strides [ 0 ] - 1 ) // strides [ 0 ] , : , : ] odd_pad_w = pool_size [ 0 ] > 2 and pool_size [ 0 ] % 2 == 1 ( idxs < b_active.dimshuffle ( 0 , ' x ' ) ) [ : :-1 , : :-1 ] ) i = ( x.shape [ 2 ] + strides [ 0 ] - 1 ) // strides [ 0 ] depthwise_kernel_shape = depthwise_kernel.eval ( ) .shape # in case of a shared variable step , sequences= [ L , L [ : :-1 , : :-1 ] ] , outputs_info= [ np.int32 ( 1 ) , log_first , np.int32 ( 1 ) , log_first ] ) # in case of a shared variable border_mode=th_padding , return _old_normalize_batch_in_training ( x , gamma , beta , reduction_axes , epsilon ) slices.append ( py_slice ( None , None , None ) ) depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape ( depthwise_kernel_shape , data_format ) L = T.log ( smoothed ) y_pred : tensor ( samples , time_steps , num_categories ) containing the # Theano has a built-in optimization for logsumexp ( see https : //github.com/Theano/Theano/pull/4736 ) * [ states_at_step [ i ] for states_at_step in successive_states ] ) ) total_log_prob = total_log_prob + common_factor slices.append ( py_slice ( None , None , -1 ) ) filter_dilation=dilation_rate ) w_pad = pool_size [ 0 ] - 2 if odd_pad_w else pool_size [ 0 ] - 1 # Theano has a built-in optimization for logsumexp depthwise_kernel_shape , strides , data_format ) conv_out = conv_out [ : , : , : ( x.shape [ 2 ] + strides [ 0 ] - 1 ) // strides [ 0 ] , : ] states.append ( T.stack ( * [ states_at_step [ i ] for states_at_step in successive_states ] ) ) _p_prev [ active_skip_idxs + 2 ] , p_prev [ active_skip_idxs ] ) sequences= [ L , L [ : :-1 , : :-1 ] ] , # there should be a shortcut to calculating this f_active_next , log_f_next = ctc_update_log_p ( f_skip_idxs , zeros , f_active , log_f_curr , log_f_prev ) raise ValueError ( msg ) for i in range ( x.ndim ) : def _raise_invalid_arg ( key ) : new_states = [ ] mask = ( idxs < f_active.dimshuffle ( 0 , ' x ' ) ) & ( idxs < b_active.dimshuffle ( 0 , ' x ' ) ) [ : :-1 , : :-1 ] _LEARNING_PHASE = T.scalar ( dtype='uint8 ' , name='keras_learning_phase ' ) > > > K.is_keras_tensor ( keras_var ) # A variable created with the keras backend is not a Keras tensor . total_log_prob = T.log ( T.sum ( T.exp ( log_probs - common_factor ) [ mask.nonzero ( ) ] ) ) msg = 'Invalid argument `` % s '' passed to K.function with Theano backend ' % key keras/backend/tensorflow_backend.py E501 \ depthwise_kernel = _preprocess_conv2d_depthwise_kernel ( use_cudnn = ndim ( x ) < 5 and reduction_axes == [ 0 , 2 , 3 ] and ( dev.startswith ( 'cuda ' ) or dev.startswith ( 'gpu ' ) ) if i in axes : > > > # A variable indirectly created outside of keras is not a Keras tensor . keras/backend/tensorflow_backend.py E501 kshp=kernel_shape , ( dev.startswith ( 'cuda ' ) or dev.startswith ( 'gpu ' ) ) ) L = T.log ( smoothed_predict ) states.append ( T.stack ( kernel , new_states.append ( states_at_step [ i ] ) normed , mean , stdinv = theano.sandbox.cuda.dnn.dnn_batch_normalization_train ( msg = 'Invalid argument `` % s '' passed to K.function with Theano backend ' % key w_pad = pool_size [ 0 ] - 2 if pool_size [ 0 ] > 2 and pool_size [ 0 ] % 2 == 1 else pool_size [ 0 ] - 1 conv_out = conv_out [ : , : , : , : ( x.shape [ 3 ] + strides [ 1 ] - 1 ) // strides [ 1 ] ] ValueError : if ` data_format ` is neither ` `` channels_last '' ` or ` `` channels_first '' ` . border_mode=th_padding , > > > # A placeholder is not a Keras tensor . def local_conv2d ( inputs , conv_out = conv_out [ : , : , : i , : , : ] normed , mean , stdinv = trained conv_out = conv_out [ : , : , : , : , : i ] initial_output = step_function ( inputs [ 0 ] , initial_states + constants ) [ 0 ] * 0 initial_output = initial_output [ 0 ] * 0 h_pad = pool_size [ 1 ] - 2 if pool_size [ 1 ] > 2 and pool_size [ 1 ] % 2 == 1 else pool_size [ 1 ] - 1 data_format=None ) : conv_out = _postprocess_conv2d_output ( _p_prev = T.inc_subtensor ( states.append ( T.stack ( * new_states ) ) _p_prev = T.inc_subtensor ( _p_prev [ active_skip_idxs + 2 ] , p_prev [ active_skip_idxs ] ) b_skip_idxs = ctc_create_skip_idxs ( Y [ : :-1 ] ) # there should be a shortcut to calculating this def local_conv2d ( inputs , kernel , kernel_size , strides , output_shape , data_format=None ) : prediction , or output of the softmax f_active_next , log_f_next = ctc_update_log_p ( > > > K.is_keras_tensor ( keras_placeholder ) # A placeholder is not a Keras tensor . y_pred : tensor ( samples , time_steps , num_categories ) containing the prediction , > > > K.is_keras_tensor ( keras_placeholder ) ValueError : if ` data_format ` is neither ` `` channels_last '' ` or # Theano expects ` ( input_depth * depth , 1 , rows , cols ) ` conv_out = _postprocess_conv2d_output ( conv_out , x , padding , filter_flip=not flip_filters , op = T.nnet.abstract_conv.AbstractConv2d_gradInputs ( imshp=None , subsample=strides , i = ( x.shape [ 4 ] + strides [ 2 ] - 1 ) // strides [ 2 ] slices = [ ] conv_out , x , padding , depthwise_kernel_shape , strides , data_format ) raise ValueError ( msg ) pointwise_kernel_shape = _preprocess_conv2d_filter_shape ( pointwise_kernel_shape , data_format ) b_skip_idxs = ctc_create_skip_idxs ( Y [ : :-1 ] ) odd_pad_h = pool_size [ 1 ] > 2 and pool_size [ 1 ] % 2 == 1 # Theano expects ` ( input_depth * depth , 1 , rows , cols ) ` for depthwise convolution . pointwise_kernel_shape = pointwise_kernel.eval ( ) .shape # in case of a shared variable kernel_size , initial_output = step_function ( inputs [ 0 ] , initial_states + constants ) h_pad = pool_size [ 1 ] - 2 if odd_pad_h else pool_size [ 1 ] - 1 b_active_next , log_b_next = ctc_update_log_p ( # ( see https : //github.com/Theano/Theano/pull/4736 ) depthwise_kernel_shape = depthwise_kernel.eval ( ) .shape use_cudnn = ( ndim ( x ) < 5 and subsample=strides , depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape ( for states_at_step in successive_states : else : conv_out = conv_out [ : , : , : , : , : ( x.shape [ 4 ] + strides [ 2 ] - 1 ) // strides [ 2 ] ] depthwise_kernel , depthwise_kernel_shape , data_format ) > > > K.is_keras_tensor ( k_var ) # A variable indirectly created outside of keras is not a Keras tensor . > > > K.is_keras_tensor ( keras_var ) strides , smoothed = ( 1 - alpha ) * predict [ : , Y ] + alpha * np.float32 ( 1 . ) / Y.shape [ 0 ] x , gamma , beta , reduction_axes , epsilon ) f_skip_idxs , zeros , f_active , log_f_curr , log_f_prev ) ` `` channels_first '' ` . > > > K.is_keras_tensor ( k_var ) depthwise_kernel = _preprocess_conv2d_depthwise_kernel ( depthwise_kernel , depthwise_kernel_shape , data_format ) b_skip_idxs , zeros , b_active , log_b_curr , log_b_prev ) conv_out = conv_out [ : , : , : i , : ] trained = theano.sandbox.cuda.dnn.dnn_batch_normalization_train ( total_log_prob = T.log ( T.sum ( T.exp ( log_probs - common_factor ) [ mask.nonzero ( ) ] ) ) + common_factor mask = ( ( idxs < f_active.dimshuffle ( 0 , ' x ' ) ) & keras/backend/theano_backend.py E501 > > > # Any Keras layer output is a Keras tensor . b_active_next , log_b_next = ctc_update_log_p ( b_skip_idxs , zeros , b_active , log_b_curr , log_b_prev ) reduction_axes == [ 0 , 2 , 3 ] and kshp=kernel_shape , filter_flip=not flip_filters ,","['keras/backend/theano_backend.py', 'pytest.ini']",Refactor theno_backend to shorten linelength ( # 11391 )
232,2fdbaa10ab0fd72f962923c4cf73e024424a3022,2018-10-19 19:50:33+09:00,"noise = np.random.choice ( [ 0 , 1 ] , noise_shape , noise_shape = x.shape if noise_shape is None : return x * noise / ( 1 - level ) else : for k in WITH_NP ] for k in BACKENDS ] replace=True , return x p= [ level , 1 - level ] ) if learning_phase ( ) : def dropout ( x , level , noise_shape=None , seed=None ) :","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Added dropout to reference_operations.py ( # 11390 )
233,dc6f00fa0d831e8583a5285f032cf583d4980c4f,2018-10-17 15:33:10-07:00,"input_shape , ( 'pool3d ' , ( 3 , 3 , 8 , 5 , 9 ) , ( 2 , 3 , 2 ) , ( 1 , 1 , 1 ) , { 'go_backwards ' : False , 'mask ' : mask_k , 'unroll ' : True , ( 'pool3d ' , ( 3 , 5 , 6 , 7 , 3 ) , ( 3 , 3 , 3 ) , ( 1 , 1 , 1 ) , 'same ' , 'channels_last ' , 'max ' ) , [ 0.20225059 , -0.38956559 ] , [ -0.13805378 , 0.08506755 ] ] , dtype=np.float32 ) WITH_NP , from_logits=False ) 'pool2d ' , ( 5 , 9 , 11 , 3 ) , BACKENDS , cntk_dynamicity=True , { 'go_backwards ' : False , 'mask ' : mask_k , 'unroll ' : True , 'input_length ' : timesteps } , 'pool2d ' , ( 2 , 7 , 7 , 5 ) , BACKENDS , cntk_dynamicity=True , check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP , from_logits=True ) pool_size= ( 2 , 2 ) , strides= ( 1 , 1 ) , pool_mode='avg ' ) check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , def legacy_test_pool3d ( self ) : y_shape_or_val , def assert_list_pairwise ( z_list , ( 'pool2d ' , ( 2 , 3 , 7 , 7 ) , ( 3 , 3 ) , ( 1 , 1 ) , 'same ' , 'channels_first ' , 'avg ' ) , check_single_tensor_operation ( 'zeros ' , ( 3 , 5 , 10 , 8 ) , WITH_NP , shape_or_val=False ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 , 3 ) , k_d = k.eval ( k.concatenate ( [ k.variable ( x_dense_1 ) , k.variable ( x_dense_2 ) ] ) ) 'op , input_shape , kernel_shape , padding , data_format , dilation_rate ' , [ WITH_NP = [ KTH if K.backend ( ) == 'theano ' else KC if K.backend ( ) == 'cntk ' else KTF , KNP ] op , padding , # ( k == 0 or k > num_classes ) does not raise an error but just return an unmeaningful tensor . def test_depthwise_conv ( self , op , input_shape , kernel_shape , padding , data_format ) : ( ( 2 , 3 , 5 , 4 , 6 ) , ( 3 , 2 , 4 , 3 , 4 ) , 'channels_first ' ) , ( 'separable_conv2d ' , ( 1 , 6 , 5 , 3 ) , ( 3 , 4 ) , 1 , 'valid ' , 'channels_last ' ) , assert_value_equality=False ) pool_size= ( 2 , 2 ) , strides= ( 1 , 1 ) , pool_mode='avg ' ) second_function_args , pool_size= ( 2 , 2 , 2 ) , strides= ( 1 , 1 , 1 ) , padding='valid ' ) def check_single_tensor_operation ( function_name , x_shape_or_val , backend_list , * * kwargs ) : 'valid ' , 'channels_last ' , ( 2 , 2 , 2 ) ) , init_state_val = np.random.random ( 'input_length ' : timesteps } , data_format , ( 'pool3d ' , ( 2 , 8 , 9 , 5 , 3 ) , ( 3 , 2 , 3 ) , ( 1 , 1 , 1 ) , 'valid ' , 'channels_last ' , 'avg ' ) , padding='same ' , pool_mode='avg ' ) data_format , check_two_tensor_operation ( 'binary_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP , from_logits=True ) 'same ' , 'channels_last ' , ( 2 , 2 ) ) , 'op , input_shape , pool_size , strides , padding , data_format , pool_mode ' , [ ( 'pool2d ' , ( 3 , 3 , 8 , 5 ) , ( 2 , 3 ) , ( 1 , 1 ) , WITH_NP , shape_or_val=False ) # due to the algo difference , we ca n't guarantee CNTK has the same result on the garbage input . pool_size= ( 2 , 3 , 2 ) , strides= ( 1 , 1 , 1 ) , padding='valid ' ) input_val = np.random.random ( for ( input_shape , pool_size ) in zip ( [ ( 5 , 10 , 12 , 3 ) , ( 5 , 10 , 12 , 6 , 3 ) ] , ztf , _ , _ = KTF.normalize_batch_in_training ( strides= ( 1 , 1 , 1 ) , padding='same ' , pool_mode='avg ' ) 'pool3d ' , ( 2 , 6 , 6 , 6 , 3 ) , [ KTH , KTF ] , ( 'conv2d ' , ( 2 , 3 , 9 , 8 ) , ( 4 , 3 , 3 , 4 ) , dilation_rate ) : depth_multiplier , 'valid ' , 'channels_first ' , 'max ' ) , ( ( 2 , 3 , 5 , 6 ) , ( 4 , 3 , 3 , 4 ) , 'channels_first ' ) , WITH_NP = [ KC , KNP ] check_single_tensor_operation ( 'ones_like ' , ( 3 , 5 , 10 , 8 ) , 'input_length ' : timesteps } , ( 'pool2d ' , ( 3 , 6 , 7 , 3 ) , ( 3 , 3 ) , ( 1 , 1 ) , 'same ' , 'channels_last ' , 'max ' ) , op , ( 'pool3d ' , ( 3 , 5 , 6 , 7 , 3 ) , ( 3 , 3 , 3 ) , ( 1 , 1 , 1 ) , shape_or_val=False , assert_value_equality=False ) pool_size= ( 2 , 3 , 2 ) , strides= ( 1 , 1 , 1 ) , padding='valid ' ) xc , None , None , reduction_axes= [ 0 , 1 , 2 , 3 ] ) else : input_shape , y_shape_or_val , backend_list , * * kwargs ) : ( 'pool2d ' , ( 2 , 9 , 5 , 3 ) , ( 3 , 2 ) , ( 1 , 1 ) , 'valid ' , 'channels_last ' , 'avg ' ) , def test_dilated_conv ( self , 'op , input_shape , kernel_shape , output_shape , padding , data_format , dilation_rate ' , def test_dilated_conv_transpose ( self , ( 'separable_conv1d ' , ( 2 , 8 , 2 ) , ( 3 , ) , 1 , 'same ' , 'channels_last ' ) , @ pytest.mark.parametrize ( 'op , input_shape , kernel_shape , padding , data_format , dilation_rate ' , [ check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP , from_logits=False ) backend_list , dilation_rate ) : ( 'pool2d ' , ( 2 , 9 , 5 , 3 ) , ( 3 , 2 ) , ( 1 , 1 ) , pool_size= ( 2 , 2 ) , strides= ( 1 , 1 ) , padding='valid ' ) padding , rand = K.eval ( K.truncated_normal ( ( 300 , 200 ) , mean=mean , stddev=std , seed=1337 ) ) check_single_tensor_operation ( 'ones ' , ( 3 , 5 , 10 , 8 ) , check_single_tensor_operation ( 'ones_like ' , ( 3 , 5 , 10 , 8 ) , WITH_NP , shape_or_val=True ) * * kwargs ) : [ 0.20225059 , -0.38956559 ] , [ -0.13805378 , 0.08506755 ] ] , def test_conv_transpose ( self , op , input_shape , kernel_shape , output_shape , 'op , input_shape , kernel_shape , output_shape , padding , data_format , dilation_rate ' , [ WITH_NP , shape_or_val=True ) ( 'conv2d ' , ( 2 , 8 , 9 , 3 ) , ( 3 , 3 , 3 , 2 ) , check_single_tensor_operation ( 'zeros_like ' , ( 3 , 5 , 10 , 8 ) , WITH_NP , shape_or_val=True ) check_single_tensor_operation ( 'pool3d ' , ( 5 , 10 , 12 , 5 , 3 ) , output_shape , ( 'pool3d ' , ( 2 , 3 , 7 , 7 , 7 ) , ( 3 , 3 , 3 ) , ( 1 , 1 , 1 ) , 'same ' , 'channels_first ' , 'avg ' ) , xth , None , None , reduction_axes='per-activation ' ) check_single_tensor_operation ( 'pool2d ' , ( 5 , 9 , 11 , 3 ) , ( 'conv1d ' , ( 2 , 8 , 3 ) , ( 4 , 3 , 2 ) , 'valid ' , 'channels_last ' , 2 ) , k.variable ( x_dense_2 ) ] ) ) WITH_NP , axis=1 , keepdims=True ) assert y._keras_shape == ( None , ) input_shape , 'same ' , 'channels_last ' , 'max ' ) , ( 'separable_conv2d ' , ( 1 , 7 , 6 , 3 ) , ( 3 , 3 ) , 2 , 'same ' , 'channels_last ' ) , keras/backend/theano_backend.py E501 ( 'pool2d ' , ( 2 , 3 , 7 , 7 ) , ( 3 , 3 ) , ( 1 , 1 ) , data_format ) : WITH_NP , from_logits=True ) kernel_shape , first_function_args , WITH_NP , cntk_two_dynamicity=True , from_logits=True ) ] ) kernel_shape , zc , _ , _ = KC.normalize_batch_in_training ( assert_allclose ( res [ : , 0 ] , ref , atol=1e-05 ) check_single_tensor_operation ( 'logsumexp ' , ( 4 , 2 ) , ( ( 2 , 3 , 5 , 4 , 6 ) , ( 3 , 2 , 4 , 3 , 4 ) , 'channels_first ' ) , WITH_NP , from_logits=True ) def test_conv_transpose ( self , 'pool2d ' , ( 5 , 10 , 12 , 3 ) , BACKENDS , cntk_dynamicity=True , pool_size= ( 2 , 2 , 2 ) , strides= ( 1 , 1 , 1 ) , pool_mode='avg ' ) second_function_name , 'same ' , 'channels_first ' , ( 2 , 2 , 2 ) ) , { 'go_backwards ' : True , 'mask ' : None , 'unroll ' : True , def test_pool ( self , pool_size= ( 2 , 2 , 2 ) , strides= ( 1 , 1 , 1 ) , pool_mode='avg ' ) # test that random_normal also generates different values when used def check_single_tensor_operation ( function_name , ( 'pool3d ' , ( 2 , 3 , 7 , 7 , 7 ) , ( 3 , 3 , 3 ) , ( 1 , 1 , 1 ) , ( num_samples , output_dim ) ) .astype ( np.float32 ) check_single_tensor_operation ( 'zeros_like ' , ( 3 , 5 , 10 , 8 ) , padding , ( ( 2 , 3 , 5 , 6 ) , ( 4 , 3 , 3 , 4 ) , 'channels_first ' ) , cntk_two_dynamicity=True , from_logits=True ) init_state_val = np.random.random ( ( num_samples , output_dim ) ) .astype ( np.float32 ) [ 0.64916514 , 0.35083486 ] , [ 0.47028078 , 0.52971922 ] ] , # on the garbage input . ( 'conv3d ' , ( 2 , 5 , 4 , 6 , 3 ) , ( 2 , 2 , 3 , 3 , 4 ) , ] ) { 'go_backwards ' : False , 'mask ' : None , 'unroll ' : True , ( ( 1 , 6 , 5 , 3 ) , ( 3 , 3 , 3 , 2 ) , 'channels_last ' ) WITH_NP , axis=-1 , keepdims=True ) allclose=True , ( num_samples , timesteps , input_dim ) ) .astype ( np.float32 ) ( 'separable_conv1d ' , ( 2 , 8 , 2 ) , ( 3 , ) , 1 , 'same ' , 'channels_last ' ) , ( input_depth , depth_multiplier ) ) ( 'conv2d ' , ( 2 , 8 , 9 , 3 ) , ( 3 , 3 , 3 , 2 ) , 'same ' , 'channels_last ' , ( 2 , 2 ) ) , pool_size= ( 3 , 3 ) , strides= ( 1 , 1 ) , ( 'conv1d ' , ( 2 , 3 , 8 ) , ( 4 , 3 , 2 ) , 'valid ' , 'channels_first ' , 2 ) , input_shape , pool_size= ( 2 , 3 ) , strides= ( 1 , 1 ) , padding='valid ' ) check_single_tensor_operation ( 'zeros ' , ( 3 , 5 , 10 , 8 ) , WITH_NP = [ KTH , KNP ] check_single_tensor_operation ( 'pool2d ' , ( 2 , 7 , 7 , 5 ) , _ , pointwise = parse_shape_or_val ( ( 1 , ) * len ( kernel_shape ) x_shape_or_val , data_format , dilation_rate ) : { 'go_backwards ' : False , 'mask ' : mask , 'unroll ' : True , 'input_length ' : timesteps } , idx_identical = np.random.choice ( num_classes , size=num_identical , replace=False ) def assert_list_pairwise ( z_list , shape=True , allclose=True , itself=False , atol=1e-05 ) : input_shape , backend_list ) : ( input_depth * depth_multiplier , 7 ) ) pool_size= ( 3 , 3 ) , strides= ( 1 , 1 ) , ( 'conv2d ' , ( 2 , 3 , 9 , 8 ) , ( 4 , 3 , 3 , 4 ) , 'valid ' , 'channels_first ' , ( 2 , 2 ) ) , check_single_tensor_operation ( 'pool3d ' , ( 5 , 9 , 11 , 5 , 3 ) , check_two_tensor_operation ( 'categorical_crossentropy ' , yval , xval , op , ( ( 1 , 2 , 2 , 2 , 1 ) , ( 2 , 2 , 2 , 1 , 1 ) , 'channels_last ' ) ] : backend_list ) : x_shape_or_val , input_depth = input_shape [ 1 ] atol=1e-05 ) itself=False , def check_composed_tensor_operations ( first_function_name , first_function_args , * * kwargs ) : check_single_tensor_operation ( 'ones ' , ( 3 , 5 , 10 , 8 ) , WITH_NP , shape_or_val=False ) size=num_identical , replace=False ) pool_mode ) : _ , depthwise = parse_shape_or_val ( kernel_shape + ( input_depth , depth_multiplier ) ) { 'go_backwards ' : False , 'mask ' : None , 'unroll ' : True , 'input_length ' : timesteps } , ( 'pool2d ' , ( 3 , 3 , 8 , 5 ) , ( 2 , 3 ) , ( 1 , 1 ) , 'valid ' , 'channels_first ' , 'max ' ) , rand = K.eval ( K.truncated_normal ( ( 300 , 200 ) , WITH_NP = [ KTF , KNP ] ( ( 2 , 3 , 4 , 5 ) , ( 2 , 2 , 3 , 4 ) , 'channels_first ' ) , ( 'conv1d ' , ( 2 , 8 , 3 ) , ( 4 , 3 , 2 ) , 'valid ' , 'channels_last ' , 2 ) , _ , depthwise = parse_shape_or_val ( kernel_shape pool_size= ( 3 , 3 , 3 ) , strides= ( 1 , 1 , 1 ) , padding='same ' , pool_mode='avg ' ) res = K.eval ( K.ctc_batch_cost ( k_labels , k_inputs , k_input_lens , reduction_axes= [ 0 , 1 , 2 , 3 ] ) { 'go_backwards ' : True , 'mask ' : None , 'unroll ' : True , 'input_length ' : timesteps } , ( 'conv3d ' , ( 2 , 5 , 4 , 6 , 3 ) , ( 2 , 2 , 3 , 3 , 4 ) , 'valid ' , 'channels_last ' , ( 2 , 2 , 2 ) ) , pool_size= ( 2 , 3 ) , strides= ( 1 , 1 ) , padding='valid ' ) atol=1e-05 ) : def test_depthwise_conv ( self , shape=True , ( ( 2 , 3 , 4 , 5 , 4 ) , ( 2 , 2 , 2 , 3 , 4 ) , 'channels_first ' ) , ( 'separable_conv2d ' , ( 2 , 3 , 4 , 5 ) , ( 3 , 3 ) , 1 , 'same ' , 'channels_first ' ) , elif K.backend ( ) == 'cntk ' : op , check_single_tensor_operation ( 'pool3d ' , ( 2 , 6 , 6 , 6 , 3 ) , [ KTH , KTF ] , pool_size= ( 3 , 3 , 3 ) , for ( input_shape , pool_size ) in zip ( [ ( 5 , 10 , 12 , 3 ) , ( 5 , 10 , 12 , 6 , 3 ) ] , [ ( 2 , 2 ) , ( 2 , 2 , 2 ) ] ) : def legacy_test_pool2d ( self ) : ztf , _ , _ = KTF.normalize_batch_in_training ( xtf , None , None , 'same ' , 'channels_first ' , 'avg ' ) , 'valid ' , 'channels_last ' , 'avg ' ) , ( 'separable_conv1d ' , ( 1 , 8 , 2 ) , ( 3 , ) , 2 , 'valid ' , 'channels_last ' ) , ( 'conv3d ' , ( 2 , 3 , 5 , 4 , 6 ) , ( 2 , 2 , 3 , 3 , 4 ) , 'same ' , 'channels_first ' , ( 2 , 2 , 2 ) ) , output_shape , input_val = np.random.random ( ( num_samples , timesteps , input_dim ) ) .astype ( np.float32 ) check_two_tensor_operation ( 'categorical_crossentropy ' , yval , xval , WITH_NP , assert_allclose ( res [ 0 , : ] if K.backend ( ) == 'theano ' else res [ : , 0 ] , ref , atol=1e-05 ) ( ( 1 , 6 , 5 , 3 ) , ( 3 , 3 , 3 , 2 ) , 'channels_last ' ) ] : data_format , ] : padding='same ' , pool_mode='avg ' ) check_single_tensor_operation ( 'logsumexp ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 , 3 ) , WITH_NP , axis=-1 , keepdims=True ) pool_size , input_shape , ( 'separable_conv2d ' , ( 1 , 6 , 5 , 3 ) , ( 3 , 4 ) , 1 , 'valid ' , 'channels_last ' ) , 'op , input_shape , kernel_shape , depth_multiplier , padding , data_format ' , [ check_two_tensor_operation ( 'binary_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , kernel_shape , input_depth = input_shape [ -1 ] ( 'separable_conv1d ' , ( 1 , 8 , 2 ) , ( 3 , ) , 2 , 'valid ' , 'channels_last ' ) , xtf , None , None , reduction_axes= [ 0 , 1 , 2 , 3 ] ) ( ( 2 , 3 , 4 , 5 , 4 ) , ( 2 , 2 , 2 , 3 , 4 ) , 'channels_first ' ) , mean=mean , stddev=std , seed=1337 ) ) mean=mean , stddev=std , seed=1337 ) ) { 'go_backwards ' : False , 'mask ' : mask , 'unroll ' : True , if K.backend ( ) == 'theano ' : shape_or_val=False , def test_dilated_conv_transpose ( self , op , input_shape , kernel_shape , output_shape , 'pool3d ' , ( 5 , 9 , 11 , 5 , 3 ) , BACKENDS , cntk_dynamicity=True , check_single_tensor_operation ( 'pool2d ' , ( 5 , 10 , 12 , 3 ) , ( 'pool3d ' , ( 2 , 8 , 9 , 5 , 3 ) , ( 3 , 2 , 3 ) , ( 1 , 1 , 1 ) , _ , pointwise = parse_shape_or_val ( ( 1 , ) * len ( kernel_shape ) + ( input_depth * depth_multiplier , 7 ) ) ( 'separable_conv2d ' , ( 2 , 3 , 5 , 6 ) , ( 4 , 3 ) , 2 , 'valid ' , 'channels_first ' ) , ( 'conv3d ' , ( 2 , 3 , 5 , 4 , 6 ) , ( 2 , 2 , 3 , 3 , 4 ) , ( ( 1 , 2 , 2 , 2 , 1 ) , ( 2 , 2 , 2 , 1 , 1 ) , 'channels_last ' ) assert y._keras_shape == ( None , ) padding , data_format ) : # due to the algo difference , we ca n't guarantee CNTK has the same result idx_identical = np.random.choice ( num_classes , strides , @ pytest.mark.parametrize ( 'op , input_shape , kernel_shape , depth_multiplier , padding , data_format ' , [ BACKENDS , cntk_dynamicity=True , second_function_name , second_function_args , keras/backend/theano_backend.py E501 \ # but just return an unmeaningful tensor . zth , _ , _ = KTH.normalize_batch_in_training ( xth , None , None , zc , _ , _ = KC.normalize_batch_in_training ( xc , None , None , { 'go_backwards ' : False , 'mask ' : None , 'unroll ' : True , rand = K.eval ( K.random_normal ( ( 300 , 200 ) , mean=mean , stddev=std , seed=1337 ) ) reduction_axes='per-activation ' ) k_d = k.eval ( k.concatenate ( [ k.variable ( x_dense_1 ) , def check_two_tensor_operation ( function_name , x_shape_or_val , zth , _ , _ = KTH.normalize_batch_in_training ( k_label_lens ) ) else : check_single_tensor_operation ( def check_composed_tensor_operations ( first_function_name , def test_dilated_conv ( self , op , input_shape , kernel_shape , padding , ( 'separable_conv2d ' , ( 2 , 3 , 4 , 5 ) , ( 3 , 3 ) , 1 , 'same ' , 'channels_first ' ) , # within a function pool_size= ( 2 , 2 , 2 ) , strides= ( 1 , 1 , 1 ) , padding='valid ' ) if K.backend ( ) == 'theano ' : assert_allclose ( last_output_list [ i - 1 ] , last_output_list [ i ] , ( 'conv1d ' , ( 2 , 3 , 8 ) , ( 4 , 3 , 2 ) , 'valid ' , 'channels_first ' , 2 ) , padding , data_format , dilation_rate ) : ( 'separable_conv2d ' , ( 1 , 7 , 6 , 3 ) , ( 3 , 3 ) , 2 , 'same ' , 'channels_last ' ) , [ 0.64916514 , 0.35083486 ] , [ 0.47028078 , 0.52971922 ] ] , dtype=np.float32 ) res = K.eval ( K.ctc_batch_cost ( k_labels , k_inputs , k_input_lens , k_label_lens ) ) # ( k == 0 or k > num_classes ) does not raise an error input_depth = input_shape [ 1 ] if data_format == 'channels_first ' else input_shape [ -1 ] tests/keras/backend/backend_test.py E501 dtype=np.float32 ) [ check_two_tensor_operation ( 'binary_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP , from_logits=False ) 'pool3d ' , ( 5 , 10 , 12 , 5 , 3 ) , BACKENDS , cntk_dynamicity=True , assert_allclose ( last_output_list [ i - 1 ] , last_output_list [ i ] , atol=1e-05 ) { 'go_backwards ' : True , 'mask ' : None , 'unroll ' : True , padding , ( 'pool3d ' , ( 3 , 3 , 8 , 5 , 9 ) , ( 2 , 3 , 2 ) , ( 1 , 1 , 1 ) , 'valid ' , 'channels_first ' , 'max ' ) , ( 'separable_conv2d ' , ( 2 , 3 , 5 , 6 ) , ( 4 , 3 ) , 2 , 'valid ' , 'channels_first ' ) , def test_legacy_pool2d ( self ) : if data_format == 'channels_first ' : rand = K.eval ( K.random_normal ( ( 300 , 200 ) , ( 'pool2d ' , ( 3 , 6 , 7 , 3 ) , ( 3 , 3 ) , ( 1 , 1 ) , backend_list , 'valid ' , 'channels_first ' , ( 2 , 2 ) ) , @ pytest.mark.parametrize ( 'op , input_shape , pool_size , strides , padding , data_format , pool_mode ' , [ def check_two_tensor_operation ( function_name , ( ( 2 , 3 , 4 , 5 ) , ( 2 , 2 , 3 , 4 ) , 'channels_first ' ) , { 'go_backwards ' : True , 'mask ' : None , 'unroll ' : True , 'input_length ' : timesteps } , def test_separable_conv ( self , { 'go_backwards ' : False , 'mask ' : None , 'unroll ' : True , 'input_length ' : timesteps } , def test_legacy_pool3d ( self ) : pool_size= ( 2 , 2 ) , strides= ( 1 , 1 ) , padding='valid ' ) [ ( 2 , 2 ) , ( 2 , 2 , 2 ) ] ) : assert_allclose ( res [ 0 , : ] , ref , atol=1e-05 ) def test_pool ( self , op , input_shape , pool_size , strides , padding , data_format , pool_mode ) : check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , # test that random_normal also generates different values when used within a function def test_separable_conv ( self , op , input_shape , kernel_shape , depth_multiplier , padding , data_format ) : reduction_axes= [ 0 , 1 , 2 , 3 ] )","['pytest.ini', 'tests/keras/backend/backend_test.py']",# 11383 Making backend_tests pass PEP8 test ( # 11411 )
234,e15533e6c725dca8c37a861aacb13ef149789433,2018-10-16 17:58:36-07:00,"from keras.utils.generic_utils import transpose_shape spatial_axes= ( 1 , 2 ) ) check_single_tensor_operation ( 'spatial_3d_padding ' , x_shape , BACKENDS , all_dims_padding = ( ( 0 , 0 ) , ) + padding + ( ( 0 , 0 ) , ) def temporal_padding ( x , padding= ( 1 , 1 ) ) : check_single_tensor_operation ( 'spatial_3d_padding ' , x_shape , WITH_NP , WITH_NP ) spatial_axes= ( 1 , 2 , 3 ) ) check_single_tensor_operation ( 'spatial_2d_padding ' , x_shape , BACKENDS , def spatial_3d_padding ( x , padding= ( ( 1 , 1 ) , ( 1 , 1 ) , ( 1 , 1 ) ) , data_format=None ) : check_single_tensor_operation ( 'spatial_2d_padding ' , x_shape , WITH_NP , all_dims_padding = transpose_shape ( all_dims_padding , data_format , BACKENDS ) return np.pad ( x , [ ( 0 , 0 ) , padding , ( 0 , 0 ) ] , mode='constant ' ) BACKENDS , padding= ( 1 , 2 ) ) def spatial_2d_padding ( x , padding= ( ( 1 , 1 ) , ( 1 , 1 ) ) , data_format=None ) : return np.pad ( x , all_dims_padding , mode='constant ' ) WITH_NP , padding= ( 1 , 2 ) )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Added the padding operations in reference_operations.py ( # 11392 )
235,7d5d4a6368e45cb9939acb04d089cb46adbca32a,2018-10-16 15:23:32-07:00,"def int_shape ( x ) : def count_params ( x ) : else : return x.size 'int_shape ' , 'get_variable_shape ' ] ) 'int_shape ' , 'get_variable_shape ' ] : for k in WITH_NP ] def test_value_manipulation ( self , function_name ) : def get_variable_shape ( x ) : v_list = [ getattr ( k , function_name ) ( k.variable ( val ) ) assert_list_pairwise ( v_list ) assert_list_pairwise ( v_list ) [ 'get_value ' , 'count_params ' , if function_name == 'get_value ' : def test_value_manipulation ( self ) : for k in BACKENDS ] if function_name == 'get_value ' : # print_tensor return x else : assert_list_pairwise ( v_list , shape=False , allclose=False , itself=True ) def test_print_tensor ( self ) : def get_value ( x ) : for function_name in [ 'get_value ' , 'count_params ' , v_list = [ getattr ( k , function_name ) ( k.variable ( val ) ) assert_list_pairwise ( v_list , shape=False , allclose=False , itself=True ) return x.shape return int_shape ( x )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']","Added get_value , count_params , int_shape and get_variable_shape to ( # 11388 )"
236,885ba629acba8128fd40b0af92c960b99ac5b827,2018-10-16 15:18:46-07:00,def test_legacy_rnn_no_states ( self ) : def legacy_test_rnn ( self ) : def legacy_test_conv1d ( self ) : def test_legacy_conv1d ( self ) : def legacy_test_pool3d ( self ) : def legacy_test_rnn_no_states ( self ) : def legacy_test_depthwise_conv_2d ( self ) : def test_legacy_pool3d ( self ) : def test_legacy_conv2d ( self ) : def test_legacy_depthwise_conv_2d ( self ) : def legacy_test_conv3d ( self ) : def test_legacy_conv3d ( self ) : def legacy_test_conv2d ( self ) : def test_legacy_pool2d ( self ) : def legacy_test_pool2d ( self ) : def test_legacy_rnn ( self ) :,['tests/keras/backend/backend_test.py'],Activated disabled tests in the backend tests . ( # 11389 )
237,2f12e96d8bfa22c2303ed0482544fb0ab6721a10,2018-10-16 15:17:50-07:00,"return reference [ indices ] for k in WITH_NP ] for k in BACKENDS ] def gather ( reference , indices ) :","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Added the gather operation in reference_operations.py . ( # 11387 )
238,f2b261bc2555773bd88cbbeda976f98e244d02c1,2018-10-14 11:49:08-07:00,"input_log_prob_matrix_0 = np.log ( input_prob_matrix_0 ) o = K.sum ( o , axis=1 ) embeddings_metadata = { } if K.backend ( ) == 'theano ' : except Exception : except Exception as e : substory = None","['examples/babi_memnn.py', 'examples/babi_rnn.py', 'examples/cifar10_cnn_capsule.py', 'keras/callbacks.py', 'keras/utils/data_utils.py', 'tests/keras/backend/backend_test.py']",Removed some unused variables . ( # 11395 )
239,b2d1dea24c8b369b3a4411f0ede9518cef1db7db,2018-10-14 11:48:17-07:00,"if isinstance ( axes , int ) : if any ( [ isinstance ( a , ( list , tuple ) ) for a in axes ] ) : if ndim ( x ) == 2 and ndim ( y ) == 2 : out = np.tensordot ( x , y , axes=axes ) def batch_dot ( x , y , axes=None ) : if axes is None : if x.ndim > y.ndim : axis_list.insert ( 0 , axis_list.pop ( axis ) ) 'Provided : ' + str ( axes ) ) BACKENDS , cntk_two_dynamicity=True , axes= ( 1 , 1 ) ) if x.ndim > y.ndim : 'Expected : None , int , ( int , int ) , ' idx = x.ndim - 1 out = np.sum ( np.multiply ( np.transpose ( x , [ 1 , 0 ] ) , y ) , axes [ 1 ] ) diff = 0 out = np.transpose ( np.diagonal ( out , axis1=0 , axis2=axis ) , else : BACKENDS , cntk_two_dynamicity=True , axes= ( 2 , 2 ) ) raise ValueError ( 'Multiple target dimensions are not supported . ' return out out = np.sum ( np.multiply ( x , y ) , axes [ 0 ] ) tuple ( axis_list ) ) if diff : for axis in [ axes [ 0 ] ] : BACKENDS , cntk_two_dynamicity=True , axes=1 ) out = np.squeeze ( out , tuple ( range ( idx , idx + diff ) ) ) out = expand_dims ( out , 1 ) if axes [ 0 ] == axes [ 1 ] : axes = [ x.ndim - 1 , y.ndim - 2 ] # behaves like tf.batch_matmul as default diff = x.ndim - y.ndim axis_list = np.arange ( len ( out.shape ) - 1 ) .tolist ( ) if ndim ( out ) == 1 : WITH_NP , cntk_two_dynamicity=True , axes= ( 1 , 1 ) ) BACKENDS , cntk_two_dynamicity=True , axes= ( 2 , 1 ) ) else : WITH_NP , cntk_two_dynamicity=True , axes= ( 2 , 1 ) ) WITH_NP , cntk_two_dynamicity=True , axes=1 ) WITH_NP , cntk_two_dynamicity=True , axes= ( 2 , 2 ) ) idx = x.ndim + y.ndim - 3 y = np.reshape ( y , np.concatenate ( [ np.shape ( y ) , [ 1 ] * diff ] , axis=0 ) ) axes = ( axes , axes )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Added the tensor_dot operation in the references_operations.py . ( # 11386 )
240,874f6db89d70b3c0221034943b2a427d88b63e81,2018-10-14 23:24:47+05:18,"y = K.spatial_3d_padding ( x , padding=padding , data_format='channels_last ' ) y = k.spatial_2d_padding ( x , padding=padding , data_format='channels_last ' ) assert k.int_shape ( y ) == ( 1 , None , None , 1 ) result = K.eval ( K.logsumexp ( K.variable ( x_np ) , axis=0 ) ) # Check handling of dynamic shapes . assert k.int_shape ( y ) == ( 1 , None , None , None , 1 ) y = K.spatial_2d_padding ( x , padding=padding , data_format='channels_last ' ) x_np = np.array ( [ 1e+4 , 1e-4 ] ) assert K.int_shape ( y ) == ( 1 , None , None , 1 ) y = k.spatial_3d_padding ( x , padding=padding , data_format='channels_last ' ) x = K.placeholder ( shape= ( 1 , None , None , 1 ) ) 1e4 , assert_allclose ( k.eval ( k.logsumexp ( k.variable ( x_np ) , axis=0 ) ) , if K in [ KTF , KTH ] : x = K.placeholder ( shape= ( 1 , None , None , None , 1 ) ) assert K.int_shape ( y ) == ( 1 , None , None , None , 1 ) rtol=1e-5 ) reason='The optimization is applied only with TensorFlow . ' ) for k in [ KTF ] : x = k.placeholder ( shape= ( 1 , None , None , 1 ) ) assert_allclose ( result , 1e4 , rtol=1e-5 ) for k in [ KTF , KTH ] : # Check handling of dynamic shapes . x = k.placeholder ( shape= ( 1 , None , None , None , 1 ) ) x_np = np.array ( [ 1e+4 , 1e-4 ] )",['tests/keras/backend/backend_test.py'],De-duped some test functions . ( # 11393 )
241,02c1f8865684a055416a93bdc8a7bd07c988a09b,2018-10-14 10:57:50-07:00,"def test_metrics ( ) : assert K.eval ( output ) .shape == ( 6 , ) y_a = K.variable ( np.random.randint ( 0 , 7 , ( 6 , ) ) , dtype=K.floatx ( ) ) assert K.eval ( metric ( y_a , y_b ) ) .shape == ( 6 , ) y_b = K.variable ( np.random.random ( ( 6 , 7 ) ) , dtype=K.floatx ( ) ) assert K.eval ( metric ( y_a , y_b ) ) .shape == ( 6 , ) def test_metrics ( metric ) : assert K.eval ( output ) .shape == ( 6 , ) output = metric ( y_a , y_b ) def test_sparse_metrics ( ) : y_a = K.variable ( np.random.randint ( 0 , 7 , ( 6 , ) ) , dtype=K.floatx ( ) ) for metric in all_metrics : print ( metric.__name__ ) output = metric ( y_a , y_b ) y_b = K.variable ( np.random.random ( ( 6 , 7 ) ) , dtype=K.floatx ( ) ) def test_sparse_metrics ( metric ) : for metric in all_sparse_metrics :",['tests/keras/metrics_test.py'],Parametrizing the metric tests to get one test per metric . ( # 11378 )
242,161f0a876db4f8aa9d57161c13d366f081369601,2018-10-14 12:20:51+02:00,"https : //arxiv.org/abs/1212.5701 ) http : //yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf [ Adadelta - an adaptive learning rate method ] ( ( https : //arxiv.org/abs/1609.03499 ) . [ Dropout : A Simple Way to Prevent Neural Networks from Overfitting ] ( [ Delving Deep into Rectifiers : Surpassing Human-Level Performance on ImageNet Classification ] ( http : //arxiv.org/abs/1502.01852 ) https : //arxiv.org/abs/1609.03499 ) . [ Zero-Bias Autoencoders and the Benefits of Co-Adapting Features ] ( https : //arxiv.org/abs/1603.07285v1 ) [ Efficient Object Localization Using Convolutional Networks ] ( ( https : //arxiv.org/abs/1212.5701 ) [ Understanding the difficulty of training deep feedforward neural https : //arxiv.org/abs/1412.6980v8 ) [ On the Convergence of Adam and Beyond ] [ Dropout : A Simple Way to Prevent Neural Networks from Overfitting ] Linear Units ( ELUs ) ] ( https : //arxiv.org/abs/1511.07289 ) ( https : //www.tensorflow.org/programmers_guide/embedding ) . https : //ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf ) ( https : //ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf ) networks ] ( http : //jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf ) [ rmsprop : Divide the gradient by a running average of its recent magnitude ] [ Adam - A Method for Stochastic Optimization ] ( ( http : //www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf ) [ WaveNet : A Generative Model for Raw Audio , section 2.1 ] http : //www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf ) ( https : //arxiv.org/abs/1412.6980v8 ) [ Adam - A Method for Stochastic Optimization ] Glorot & Bengio , AISTATS 2010 http : //www.cs.toronto.edu/~fritz/absps/momentum.pdf ) Saxe et al. , http : //arxiv.org/abs/1312.6120 [ WaveNet : A Generative Model for Raw Audio , section 2.1 ] ( [ Adadelta - an adaptive learning rate method ] Learn [ more about embeddings ] ( Learn [ more about embeddings ] [ Efficient Object Localization Using Convolutional Networks ] [ Rectifier Nonlinearities Improve Neural Network Acoustic Models ] ( [ On the importance of initialization and momentum in deep learning ] ( https : //www.tensorflow.org/programmers_guide/embedding ) . [ Exact solutions to the nonlinear dynamics of learning in deep [ On the Convergence of Adam and Beyond ] ( https : //openreview.net/forum ? id=ryQu7f-RZ ) [ Zero-Bias Autoencoders and the Benefits of Co-Adapting Features ] ( [ A guide to convolution arithmetic for deep learning ] ( http : //jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf ( http : //www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf ) http : //www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf ) [ rmsprop : Divide the gradient by a running average of its recent magnitude ] ( http : //www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf ) [ Deconvolutional Networks ] ( LeCun 98 , Efficient Backprop , https : //arxiv.org/abs/1411.4280 ) [ Deconvolutional Networks ] linear neural networks ] ( http : //arxiv.org/abs/1312.6120 ) https : //arxiv.org/abs/1402.3337 ) [ A guide to convolution arithmetic for deep learning ] http : //www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf ) [ On the importance of initialization and momentum in deep learning ] ( http : //www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf ) https : //arxiv.org/abs/1603.07285v1 ) ( https : //arxiv.org/abs/1402.3337 ) [ Efficient BackProp ] ( http : //yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf ) ( http : //www.cs.toronto.edu/~fritz/absps/momentum.pdf ) ( https : //openreview.net/forum ? id=ryQu7f-RZ ) [ Rectifier Nonlinearities Improve Neural Network Acoustic Models ] ( http : //www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf ) He et al. , http : //arxiv.org/abs/1502.01852 Linear Units ( ELUs ) ] ( https : //arxiv.org/abs/1511.07289 ) ( https : //arxiv.org/abs/1411.4280 )","['keras/activations.py', 'keras/callbacks.py', 'keras/constraints.py', 'keras/initializers.py', 'keras/layers/advanced_activations.py', 'keras/layers/convolutional.py', 'keras/layers/core.py', 'keras/layers/noise.py', 'keras/optimizers.py']",Fix badly rendered references in docs ( # 11380 )
243,6dff721a3a8755356b2e89d02ef63ad8ab38ec95,2018-10-13 23:39:27+05:18,"K.floatx ( ) ) y_a_dense_labels = K.cast ( K.one_hot ( K.cast ( y_a , dtype='int32 ' ) , 7 ) , # convert dense predictions to labels y_a = K.variable ( np.random.randint ( 0 , 7 , shape ) , dtype=K.floatx ( ) ) def test_sparse_categorical_accuracy_correctness ( ) : y_b = K.variable ( np.random.random ( y_b_shape ) , dtype=K.floatx ( ) ) if K.ndim ( y_true ) == K.ndim ( y_pred ) : y_b_shape = shape + ( 7 , ) y_pred_labels = K.argmax ( y_pred , axis=-1 ) # reshape in case it 's in shape ( num_samples , 1 ) instead of ( num_samples , ) K.cast ( K.argmax ( y_pred , axis=-1 ) , K.floatx ( ) ) ) , y_true = K.squeeze ( y_true , -1 ) # flatten y_true in case it 's in shape ( num_samples , 1 ) instead of ( num_samples , ) y_b = K.variable ( np.random.random ( ( 6 , 7 ) ) , dtype=K.floatx ( ) ) y_a_dense_labels = K.cast ( K.one_hot ( K.cast ( y_a , dtype='int32 ' ) , num_classes=7 ) , y_pred_labels = K.cast ( y_pred_labels , K.floatx ( ) ) return K.cast ( K.equal ( y_true , y_pred_labels ) , K.floatx ( ) ) return K.cast ( K.equal ( K.flatten ( y_true ) , def test_sparse_categorical_accuracy_correctness ( shape ) : y_a = K.variable ( np.random.randint ( 0 , 7 , ( 6 , ) ) , dtype=K.floatx ( ) )","['keras/metrics.py', 'tests/keras/metrics_test.py']",Fix bug in sparse_categorical_accuracy ( # 11373 )
244,d7b04e6fc6ad57c58323f16687c366e19bdd0fd0,2018-10-11 13:23:24+02:00,"`` `` '' Checks if batch axes are the same for numpy arrays . first input numpy array . When steps is not ` None ` and `` `` '' Checks if batch axes are the same for Numpy arrays . A numpy array of target weights , one entry per sample to weight . A Numpy array of target weights , one entry per sample to weight . When ` steps ` is ` None ` , returns the number of samples to be first input Numpy array . When ` steps ` is not ` None ` and When steps is ` None ` , returns the number of samples to be",['keras/engine/training_utils.py'],Fix doc ( # 11352 )
245,38749e552bacf13a9fb4444ebb153255519ab871,2018-10-09 14:35:19+02:00,"# accuracy of the auxiliary classifier on generated images , so we # accuracy of the auxilary classifier on generated images , so we # To preserve sum of sample weights for the auxilary classifier , # To preserve sum of sample weights for the auxiliary classifier ,",['examples/mnist_acgan.py'],Fix typos ( # 11340 )
246,c7d04b70cd15dd47ae95dd2b1ccda64d293321c5,2018-10-08 10:26:12-07:00,"# Example # Consider an array of 5 labels out of a set of 3 classes { 0 , 1 , 2 } : # ` to_categorical ` converts this into a matrix with as many > labels `` ` python array ( [ 0 , 2 , 1 , 2 , 0 ] ) # stays the same . array ( [ [ 1. , 0. , 0 . ] , [ 0. , 1. , 0 . ] , [ 1. , 0. , 0 . ] ] , dtype=float32 ) `` ` [ 0. , 0. , 1 . ] , # columns as there are classes . The number of rows > to_categorical ( labels )",['keras/utils/np_utils.py'],added example to illustrate effects of ` to_categorical ` ( # 11327 )
247,d059890d0342955e968fdf97b5a90d19c9d68b4e,2018-10-07 16:33:12+02:00,"parallel_model = multi_gpu_model ( model , cpu_merge=False ) # Examples except : # Example 1 - Training models with weights merge on CPU parallel_model = multi_gpu_model ( model , cpu_relocation=True ) Example 1 - Training models with weights merge on CPU model = multi_gpu_model ( model , cpu_relocation=True ) model.compile ( .. ) # Example 3 - Training models with weights merge on GPU ( recommended for NV-link ) Example 3 - Training models with weights merge on GPU ( recommended for NV-link ) parallel_model.compile ( .. ) except ValueError : Example 2 - Training models with weights merge on CPU using cpu_relocation # Example 2 - Training models with weights merge on CPU using cpu_relocation parallel_model = model model = multi_gpu_model ( model , cpu_merge=False )",['keras/utils/multi_gpu_utils.py'],DOC : Closes # 11306 multi_gpu_model examples do not show properly ( # 11310 )
248,7875e856f8dc09372a83d8c5dc0af74a50d17d30,2018-10-07 16:32:04+02:00,This is recommended in [ Jozefowicz et al . ] ( http : //www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015 ) This is recommended in [ Jozefowicz et al . ( 2015 ) ] ( cells output [ Learning to forget : Continual prediction with LSTM ] ( The current implementation does not include the feedback loop on the http : //www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015 ) [ Learning to forget : Continual prediction with LSTM ] [ Long short-term memory ] ( http : //www.cs.toronto.edu/~graves/preprint.pdf ) cells output [ Supervised sequence labeling with recurrent neural networks ] [ Long short-term memory ] http : //www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf ) . ( http : //www.bioinf.jku.at/publications/older/2604.pdf ) ( http : //www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf ) . http : //www.bioinf.jku.at/publications/older/2604.pdf ) Precipitation Nowcasting ] ( http : //arxiv.org/abs/1506.04214v1 ) The current implementation does not include the feedback loop on the Precipitation Nowcasting ] ( http : //arxiv.org/abs/1506.04214v1 ) [ Supervised sequence labeling with recurrent neural networks ] ( ( http : //www.cs.toronto.edu/~graves/preprint.pdf ),"['keras/layers/convolutional_recurrent.py', 'keras/layers/cudnn_recurrent.py', 'keras/layers/recurrent.py']",fix the broken format of links and references ( # 11319 )
249,33902e53eb7312fbf2f17b7b700bdb7683441a24,2018-10-04 10:14:36-07:00,"ins = x + y + sample_weights + [ 1 . ] f : Keras function returning a list of tensors val_function = None fit_inputs : List of tensors to be fed to ` fit_function ` cbk.validation_data = val_ins ins_batch = slice_arrays ( fit_inputs , batch_ids ) if val_function and val_inputs : val_function=val_function , if ( verbose and fit_inputs and hasattr ( fit_inputs [ 0 ] , 'shape ' ) and hasattr ( val_inputs [ 0 ] , 'shape ' ) ) : f = self.train_function val_inputs = [ ] val_inputs=None , `` `` '' Abstract fit function for ` fit_function ( fit_inputs ) ` . if isinstance ( ins [ -1 ] , float ) : val_ins = [ ] the outputs of ` f ` val_f = self.test_function val_ins : List of tensors to be fed to ` val_f ` fit_function = self.train_function return training_arrays.fit_loop ( self , fit_function , fit_inputs , def fit_loop ( model , f , ins , val_f=None , val_ins = [ 0 . ] ins = x + y + sample_weights val_ins = val_x + val_y + val_sample_weights ins : List of tensors to be fed to ` f ` Assumes that fit_function returns a list , labeled by out_labels . val_outs = test_loop ( model , val_function , val_inputs , val_inputs = val_x + val_y + val_sample_weights + [ 0 . ] val_f = None if val_f and val_ins : fit_inputs = x + y + sample_weights ` fit_function ` and the list of display names of the outputs of ` fit_inputs ` . ins [ : -1 ] , batch_ids ) + [ ins [ -1 ] ] ins_batch = slice_arrays ( ins , batch_ids ) num_train_samples = check_num_samples ( ins , val_outs = test_loop ( model , val_f , val_ins , val_ins=val_ins , hasattr ( ins [ 0 ] , 'shape ' ) and hasattr ( val_ins [ 0 ] , 'shape ' ) ) : outs = f ( ins_batch ) val_inputs : List of tensors to be fed to ` val_function ` if ( verbose and ins and val_function = self.test_function if issparse ( ins [ i ] ) and not K.is_sparse ( feed [ i ] ) : ( fit_inputs [ 0 ] .shape [ 0 ] , val_inputs [ 0 ] .shape [ 0 ] ) ) fit_inputs = x + y + sample_weights + [ 1 . ] def fit_loop ( model , fit_function , fit_inputs , val_function : Keras function to call for validation val_function=None , val_ins = val_x + val_y + val_sample_weights + [ 0 . ] outs = fit_function ( fit_inputs ) val_ins=None , val_inputs = val_x + val_y + val_sample_weights val_f=val_f , cbk.validation_data = val_inputs if isinstance ( fit_inputs [ -1 ] , float ) : outs = fit_function ( ins_batch ) the outputs of ` fit_function ` fit_function : Keras function returning a list of tensors outs = f ( ins ) ` f ` and the list of display names of the outputs of ` f_val ` . ( ins [ 0 ] .shape [ 0 ] , val_ins [ 0 ] .shape [ 0 ] ) ) val_inputs = [ 0 . ] Assumes that f returns a list , labeled by out_labels . return training_arrays.fit_loop ( self , f , ins , val_f : Keras function to call for validation if issparse ( fit_inputs [ i ] ) and not K.is_sparse ( feed [ i ] ) : `` `` '' Abstract fit function for ` f ( ins ) ` . num_train_samples = check_num_samples ( fit_inputs , val_inputs=val_inputs , val_outs = test_loop ( model , val_f , val_ins , fit_inputs [ : -1 ] , batch_ids ) + [ fit_inputs [ -1 ] ] val_outs = test_loop ( model , val_function , val_inputs ,","['keras/engine/training.py', 'keras/engine/training_arrays.py']",Refactor keras/engine/training.py and keras/engine/training_arrays.py ( # 11226 )
250,6e004306dfe70ab499afc04c31808fdac594ae08,2018-10-04 10:13:35-07:00,"' ( % f vs % f ) . Check your callbacks . ' % ( delta_t_median , self._delta_t_batch ) ) warnings.warn ( 'In your callbacks , method ` on_batch_end ( ) ` ' 'to the batch update ( % f ) . Check your callbacks . ' % delta_t_median ) 'is slow compared to a model step ' warnings.warn ( 'Method on_batch_end ( ) is slow compared '",['keras/callbacks.py'],Show time spent in the warning of slow progress ( # 11278 )
251,6939d91768543d953412a321983915eaf013ebd9,2018-10-04 10:10:57-07:00,"num_rows : Number of rows in the returned one hot encoding . This is `` `` '' One hot encode given string C . model.add ( layers.TimeDistributed ( layers.Dense ( len ( chars ) ) ) ) C : string , to be encoded . model.add ( layers.TimeDistributed ( layers.Dense ( len ( chars ) , activation='softmax ' ) ) ) calc_argmax : Whether to find the character index with maximum # As the decoder RNN 's input , repeatedly provide with the last hidden state of `` `` '' One-hot encode given string C . `` `` '' Decode the given vector or 2D array to their character output . # Arguments x : A vector or a 2D array of probabilities or one-hot representations ; # As the decoder RNN 's input , repeatedly provide with the last output of `` `` '' model.add ( layers.Activation ( 'softmax ' ) ) Decode the one hot integer representation to their character output Encode them to a one-hot integer representation Decode the one-hot or integer representation to their character output Encode them to a one hot integer representation num_rows : Number of rows in the returned one-hot encoding . This is or a vector of character indices ( used with ` calc_argmax=False ` ) . probability , defaults to ` True ` .",['examples/addition_rnn.py'],Improve addition_rnn example 's code and comments ( # 11296 )
252,2b963e5ab64166ecde96c2828b937414fa9bf4e5,2018-10-04 10:07:57-07:00,"var = tf.reshape ( var , [ -1 ] ) var = tf.reshape ( var , ( -1 ) ) mean = tf.reshape ( mean , ( -1 ) ) gamma = tf.reshape ( gamma , ( -1 ) ) gamma = tf.reshape ( gamma , [ -1 ] ) beta = tf.reshape ( beta , ( -1 ) ) mean = tf.reshape ( mean , [ -1 ] ) beta = tf.reshape ( beta , [ -1 ] )",['keras/backend/tensorflow_backend.py'],[ P ] Update tensorflow_backend.py ( # 11294 )
253,5a24ebbb432dee986d6bf1c3442da5955a9bfb73,2018-10-03 11:13:43-07:00,"if attr in self : save_model ( model , fname , overwrite=True ) metrics= [ metrics.categorical_accuracy ] ) new_model = load_model ( fname ) if mode == ' w ' : model.add ( Dense ( 2 , input_shape= ( 3 , ) ) ) self.data = h5py.File ( path , mode=mode ) out = model.predict ( x ) assert_allclose ( out , out2 , atol=1e-05 ) model.train_on_batch ( x , y ) x = np.random.random ( ( 1 , 3 ) ) def test_loop_model_saving ( ) : model.compile ( loss=losses.MSE , _ , fname = tempfile.mkstemp ( '.h5 ' ) y = np.random.random ( ( 1 , 2 ) ) for _ in range ( 3 ) : optimizer=optimizers.RMSprop ( lr=0.0001 ) , if isinstance ( self.data , h5py.Group ) and attr in self.data : self.data.clear ( ) os.remove ( fname ) out2 = new_model.predict ( x ) self.data = h5py.File ( path , ) model = Sequential ( )","['keras/utils/io_utils.py', 'tests/test_model_saving.py']",Bug fix : model save when file already exists ( # 11289 )
254,a07253d8269e1b750f0a64767cc9a07da8a3b7ea,2018-10-03 08:51:08-04:00,"assert len ( val_seq.logs ) == 12 * 5 trained_batches = [ ] validation_data=val_seq , callbacks= [ tracker_cb ] , val_enqueuer_gen = iter_sequence_infinite ( generator ) epochs=5 , validation_steps = validation_steps or len ( val_data ) assert trained_batches == list ( range ( 12 ) ) * 5 # test for workers = 0 out = model.fit_generator ( generator=RandomSequence ( 3 ) , assert trained_epochs == [ 0 , 1 , 2 , 3 , 4 ] trained_epochs = [ ] val_seq = RandomSequence ( 4 ) workers=0 ) val_enqueuer_gen = iter_sequence_infinite ( val_data )","['keras/engine/training_generator.py', 'tests/keras/engine/test_training.py']",Fix ` fit_generator ` for ` workers=0 ` ( # 11285 )
255,2bfd1f2c950df5fc3f40b903c1966f1b0a48bee4,2018-10-03 09:39:48+02:00,"w = np.insert ( w , 2 * j + 1 , 0 , axis=i ) if KTH is not None : for ( i , d ) in enumerate ( dilation_rate ) : def test_conv_transpose ( self , op , input_shape , kernel_shape , output_shape , backend_list.append ( KTF ) def get_dilated_conv_backends ( ) : dilation_rate = ( dilation_rate , ) * ( x.ndim - 2 ) backend_list.append ( KC ) padding , data_format , dilation_rate ) : w = np.fliplr ( np.flipud ( w ) ) 'same ' , 'channels_first ' ) , op , input_shape , kernel_shape , WITH_NP , DILATED_CONV_BACKENDS = get_dilated_conv_backends ( ) backend_list.append ( KTH ) 'same ' , 'channels_last ' ) , check_two_tensor_operation ( if isinstance ( dilation_rate , int ) : op , input_shape , kernel_shape , WITH_NP , output_shape=output_shape , def conv_transpose ( x , w , output_shape , padding , data_format , dilation_rate=1 ) : output_shape=output_shape , padding=padding , data_format=data_format , # CNTK only supports dilated convolution on GPU backend_list = [ ] w = np.transpose ( w , ( 0 , 1 , 2 , 4 , 3 ) ) padding , data_format ) : def test_dilated_conv_transpose ( self , op , input_shape , kernel_shape , output_shape , for j in range ( w.shape [ i ] - 1 ) : if x.ndim == 4 : 'op , input_shape , kernel_shape , output_shape , padding , data_format ' , [ w = np.transpose ( w , ( 0 , 1 , 3 , 2 ) ) op , input_shape , kernel_shape , DILATED_CONV_BACKENDS , output_shape=output_shape , w = np.flip ( np.fliplr ( np.flipud ( w ) ) , axis=2 ) conv3d_transpose = conv_transpose if KC is not None and KC.dev.type ( ) == 1 : ( 'conv2d_transpose ' , ( 2 , 5 , 6 , 3 ) , ( 3 , 3 , 2 , 3 ) , ( 2 , 5 , 6 , 2 ) , ] ) ( 'conv2d_transpose ' , ( 2 , 3 , 8 , 9 ) , ( 3 , 3 , 2 , 3 ) , ( 2 , 2 , 8 , 9 ) , return conv ( x , w , padding=padding , data_format=data_format ) else : cntk_dynamicity=True ) return backend_list def test_conv_transpose_dilation ( self , op , input_shape , kernel_shape , output_shape , if KTF is not None : conv2d_transpose = conv_transpose padding , data_format , dilation_rate ) : if d > 1 :","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Add ` conv_transpose ` into reference operations ( # 11258 )
256,ecbf73f72b59f8f5c8746de63270aa1fb3ad7524,2018-10-01 12:00:01-07:00,"batch_size=5 ) ] self.samples_seen += logs [ 'size ' ] 10000 samples . Note that writing too frequently to TensorBoard batch_size=5 , def test_TensorBoard ( tmpdir ) : self._write_logs ( logs , self.samples_seen ) self.samples_seen_at_last_write = self.samples_seen update_freq : ` 'batch ' ` or ` 'epoch ' ` or integer . When using ` 'batch ' ` , writes update_freq=update_freq ) ] self.samples_seen = 0 def on_batch_end ( self , batch , logs=None ) : if update_freq == 'batch ' : applies for ` 'epoch ' ` . If using an integer , let 's say ` 10000 ` , self.update_freq = update_freq def test_TensorBoard ( tmpdir , update_freq ) : if self.update_freq ! = 'epoch ' : else : self.writer.add_summary ( summary , epoch ) self._write_logs ( logs , index ) the losses and metrics to TensorBoard after each batch . The same can slow down your training . index = epoch if self.update_freq == 'epoch ' : self.update_freq = 1 # It is the same as writing as frequently as possible . update_freq='epoch ' ) : embeddings_data=None , self.writer.add_summary ( summary , index ) if samples_seen_since > = self.update_freq : samples_seen_since = self.samples_seen - self.samples_seen_at_last_write self.samples_seen_at_last_write = 0 embeddings_data=None ) : def _write_logs ( self , logs , index ) : index = self.samples_seen the callback will write the metrics and losses to TensorBoard every","['keras/callbacks.py', 'tests/keras/test_callbacks.py']",[ RELNOTES ] [ P ] Write to TensorBoard every x samples . ( # 11152 )
257,ae6474df5ee2072cc056e02803c3b58efde1f5d0,2018-10-01 11:56:02-07:00,"timeout = 720 # Running all tests should take less than 12 minutes . # Otherwise , something went wrong .",['pytest.ini'],Added a global timeout for the test suite . ( # 11263 )
258,f8e80beb96a88b3ff8679ae16489502590d614b4,2018-10-01 10:48:16+09:00,PYTHONPATH= $ PWD : $ PYTHONPATH py.test -- pep8 -m pep8 -n0 & & py.test tests/test_documentation.py ; 'Or specify input_shape or batch_input_shape in the first layer for automatic build . ' ) 'Or specify input_shape or batch_input_shape ' PYTHONPATH= $ PWD : $ PYTHONPATH py.test -- pep8 -m pep8 -n0 ; raise ValueError ( 'Unrecognized value for argument ' PYTHONPATH= $ PWD : $ PYTHONPATH py.test tests/test_documentation.py ; 'Build the model first by calling build ( ) or calling fit ( ) with some data . ' 'Build the model first by calling build ( ) ' 'merge_mode : % s ' % ( self.merge_mode ) ) 'in the first layer for automatic build . ' ) raise ValueError ( 'Unrecognized value for argument merge_mode : % s ' % ( self.merge_mode ) ) 'or calling fit ( ) with some data . ',"['.travis.yml', 'keras/engine/network.py', 'keras/layers/wrappers.py']",Travis was ignoring PEP8 failures . Now fixed . ( # 11260 )
259,af85b95bb8b0c82aabe0df0ab24420e31fb057a2,2018-09-30 20:35:23+02:00,"return np.zeros ( shape , dtype=dtype ) return np.eye ( size , dtype=dtype ) def test_zeros ( self ) : return np.eye ( size , dtype=dtype ) def ones_like ( x , dtype=floatx ( ) , name=None ) : check_single_tensor_operation ( 'zeros_like ' , ( 3 , 5 , 10 , 8 ) , WITH_NP , shape_or_val=True ) def eye ( size , dtype=None , name=None ) : check_single_tensor_operation ( 'zeros ' , ( 3 , 5 , 10 , 8 ) , WITH_NP , shape_or_val=False ) def test_ones ( self ) : def ones ( shape , dtype=floatx ( ) , name=None ) : return np.ones ( shape , dtype=dtype ) return np.ones_like ( x , dtype=dtype ) def test_zeros_like ( self ) : def zeros_like ( x , dtype=floatx ( ) , name=None ) : def zeros ( shape , dtype=floatx ( ) , name=None ) : def test_ones_like ( self ) : return np.zeros_like ( x , dtype=dtype ) def eye ( size , dtype=None , name=None ) : check_single_tensor_operation ( 'ones ' , ( 3 , 5 , 10 , 8 ) , WITH_NP , shape_or_val=False ) check_single_tensor_operation ( 'ones_like ' , ( 3 , 5 , 10 , 8 ) , WITH_NP , shape_or_val=True )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",add more variable initializers to numpy backend ( # 11261 )
260,4dea0f867aa29c4e6bda2a66cdf1254adfa20400,2018-09-30 00:27:19+02:00,"cos = np.cos check_single_tensor_operation ( 'cos ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'sin ' , ( 4 , 2 ) , WITH_NP ) sin = np.sin","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",add trig funcs for numpy backend ( # 11254 )
261,3c899337c0d4c895308239a04cead2761822bee2,2018-09-29 22:04:08+02:00,"for j in range ( w.shape [ 2 + i ] - 1 ) : if isinstance ( dilation_rate , int ) : for ( i , d ) in enumerate ( dilation_rate ) : op , input_shape , kernel_shape , DILATED_CONV_BACKENDS , padding=padding , w = np.insert ( w , 2 * j + 1 , 0 , axis=2 + i ) def test_conv_dilation ( self , op , input_shape , kernel_shape , padding , dilation_rate=dilation_rate , cntk_dynamicity=True ) dilation_rate = ( dilation_rate , ) * ( x.ndim - 2 ) padding=padding , data_format=data_format , def test_dilated_conv ( self , op , input_shape , kernel_shape , padding , data_format=data_format , dilation_rate=dilation_rate , cntk_dynamicity=True ) dilation_rate = kwargs.pop ( 'dilation_rate ' , 1 ) data_format , dilation_rate ) : op , input_shape , kernel_shape , WITH_NP , if d > 1 : data_format , dilation_rate ) :","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Add ` dilated_conv ` into reference operations ( # 11244 )
262,67d563c455072dd26e4612a0b050b9f8363385ca,2018-09-28 12:09:48-07:00,"if ndim == 2 : x - x.max ( ) ) .sum ( axis=axis , keepdims=True ) expected = softmax ( test_values , axis=1 ) result = f ( [ test_values ] ) [ 0 ] def softmax ( values , axis ) : assert_allclose ( result , expected , rtol=1e-05 ) m = np.max ( values , axis=axis , keepdims=True ) f = K.function ( [ x ] , [ activations.softmax ( x , axis=1 ) ] ) raise ValueError ( ' Can not apply softmax to a tensor that is 1D ' ) if ndim == 1 : if ( axis == -1 or axis == x.ndim - 1 ) and x.ndim == 2 : e = np.exp ( values - m ) if axis == -1 or axis == x.ndim - 1 : `` `` '' x - xm ) .sum ( axis=axis , keepdims=True ) x = K.placeholder ( ndim=3 ) check_single_tensor_operation ( 'softmax ' , ( 4 , 5 , 3 ) , WITH_NP , axis=1 ) def test_softmax_3d ( ) : test_values = get_standard_values ( ) [ : , : , np.newaxis ] .copy ( ) return T.exp ( x - x.max ( ) ) / T.exp ( `` `` '' Test using a reference implementation of softmax . return T.exp ( x - xm ) / T.exp ( xm = x.max ( axis=axis , keepdims=True ) elif ndim == 2 : return e / np.sum ( e , axis=axis , keepdims=True )","['keras/activations.py', 'keras/backend/theano_backend.py', 'tests/keras/activations_test.py', 'tests/keras/backend/backend_test.py']",[ updated ] improve softmax implementation ( # 11189 )
263,6fb506979a4074e3b935ce22999311c22f6dce6a,2018-09-28 11:56:31-07:00,"config = { 'max_value ' : self.max_value } raise ValueError ( 'max_value of ReLU layer ' if alpha == 0 and max_value == 6 : def __init__ ( self , max_value=None , negative_slope=0. , input_shape= ( 2 , 3 , 4 ) ) negative_part = tf.nn.relu ( -x + threshold ) max_value : Saturation threshold . x = T.nnet.relu ( x ) ` f ( x ) = alpha * ( x - threshold ) ` otherwise . y = x * ( x > = threshold ) ( 0.1 , 0.0 , 0.8 ) , # max_value is zero clip_max = max_value is not None x = x * tf.cast ( tf.greater ( x , threshold ) , floatx ( ) ) ( 0.1 , 5.0 , 0.0 ) , # set alpha and max_value Otherwise , it follows : # computes x for x > threshold else 0 def test_relu ( self , alpha , max_value , threshold ) : y = np.minimum ( y , max_value ) layer_test ( layers.ReLU , raise ValueError ( 'negative_slope of ReLU layer can not be ' for max_value in [ None , 1. , 6 . ] : zero = _to_tensor ( 0. , x.dtype.base_dtype ) max_value : float . Saturation threshold . if threshold ! = 0. : x = T.nnet.relu ( x , alpha ) max_value : float > = 0 . Maximum activation value . ( 0.0 , 5.0 , 0.0 ) , # set max_value only threshold=0. , * * kwargs ) : return activations.relu ( inputs , max_value=self.max_value ) self.threshold = K.cast_to_floatx ( threshold ) x = tf.minimum ( x , max_value ) max_value=self.max_value , negative_part = T.nnet.relu ( -x ) The ( leaky ) rectified linear unit activation : ` x ` if ` x > 0 ` , ( 0.1 , 5.0 , 0.8 ) , # set all ( 0.1 , 9.0 , 0.8 ) , # max_value > 6 clip_max = False if threshold ! = 0. : ( 0.1 , None , 0.8 ) , # set alpha and threshold } x = x * T.cast ( T.gt ( x , threshold ) , floatx ( ) ) ( 0.0 , None , 0.0 ) , # standard relu layer_test ( layers.ReLU , kwargs= { 'negative_slope ' : -2.0 } , # negative_slope of ReLU layer can not be negative value check_single_tensor_operation ( 'relu ' , ( 4 , 2 ) , WITH_NP , alpha=0.1 , max_value=0.5 ) ] ) kwargs= { 'max_value ' : 10 , x = tf.clip_by_value ( x , zero , max_value ) x = C.relu ( x ) 'negative_slope ' : 0.2 , return tf.nn.relu6 ( x ) ` f ( x ) = max_value ` for ` x > = max_value ` , ( 0.0 , 5.0 , 0.8 ) , # set max_value and threshold if threshold ! = 0 : check_single_tensor_operation ( 'relu ' , ( 4 , 2 ) , WITH_NP , alpha=alpha , ( 0.1 , 5.0 , -2.8 ) , # threshold is negative x = tf.nn.leaky_relu ( x , alpha ) layer_test ( layers.ReLU , kwargs= { 'max_value ' : max_value } , ' can not be negative value : % s ' % str ( max_value ) ) else : negative_part = C.relu ( -x + threshold ) x -= alpha * negative_part input_shape= ( 2 , 3 , 4 ) ) if clip_max : 'threshold ' : 3.0 } , ( 0.0 , None , 0.8 ) , # set threshold only max_value=max_value , threshold=threshold ) 'max_value ' : self.max_value , alpha=self.negative_slope , max_value : Maximum value for the output . def __init__ ( self , max_value=None , * * kwargs ) : def relu ( x , alpha=0. , max_value=None ) : return K.relu ( x , alpha=alpha , max_value=max_value , threshold=threshold ) 'threshold ' : self.threshold if negative_slope < 0. : negative_part = C.relu ( -x ) negative_part = tf.nn.relu ( -x ) config = { is truncated to this value . y = x * ( x > 0 ) + alpha * x * ( x < 0 ) negative_part = C.relu ( -x ) x = T.minimum ( x , max_value ) if alpha ! = 0 : negative_slope : float > = 0 . Negative slope coefficient . 'negative value : % s ' % str ( negative_slope ) ) if max_value is not None : return K.relu ( x , alpha=alpha , max_value=max_value ) with pytest.raises ( ValueError ) : threshold : float . Threshold value for thresholded activation . # max_value of ReLU layer can not be negative value def relu ( x , alpha=0. , max_value=None , threshold=0 . ) : if max_value is None and threshold == 0. : if max_value is not None : if alpha ! = 0. : ` alpha * x ` if ` x < 0 ` . If ` max_value ` is defined , the result ` f ( x ) = negative_slope * ( x - threshold ) ` otherwise . x = C.relu ( x ) With default values , it returns element-wise ` max ( x , 0 ) ` . alpha : float . Slope of the negative part . Defaults to zero . elif max_value == 6 : x = T.clip ( x , 0.0 , max_value ) y += alpha * ( x - threshold ) * ( x < threshold ) return K.relu ( inputs , threshold=self.threshold ) x = tf.nn.relu6 ( x ) max_value : Float , the maximum output value . else : return tf.nn.leaky_relu ( x , alpha=alpha ) 'negative_slope ' : self.negative_slope , # if no threshold , then can use nn.relu6 native TF op for performance y = np.clip ( y , 0.0 , max_value ) self.negative_slope = K.cast_to_floatx ( negative_slope ) alpha : Slope of the negative part . Defaults to zero . A tensor . max_value = K.cast_to_floatx ( max_value ) alpha = _to_tensor ( alpha , x.dtype.base_dtype ) layer_test ( layers.ReLU , kwargs= { 'max_value ' : -2.0 } , ( 0.1 , None , 0.0 ) , # set alpha only ` f ( x ) = x ` for ` threshold < = x < max_value ` , negative_part = T.nnet.relu ( -x + threshold ) if max_value is not None and max_value < 0. : x = x * C.greater ( x , threshold )","['keras/activations.py', 'keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/layers/advanced_activations.py', 'tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py', 'tests/keras/layers/advanced_activations_test.py']",Consolidate functionality of ThresholdedReLU and LeakyReLU layers into ReLU layer ( # 11233 )
264,b9ee83cc227ac0719a0de937ae65392473fe007f,2018-09-28 11:43:07-07:00,"elif isinstance ( val , list ) : f : keras.utils.io_utils.HD5Dict instance . array = np.random.random ( ( 3 , 4 , 5 ) ) f : keras.utils.hdf5.HD5Dict instance . if isinstance ( val , list ) : array = np.random.random ( ( 4 , 5 , 512 ) )","['keras/engine/saving.py', 'keras/utils/io_utils.py', 'tests/keras/utils/io_utils_test.py']",Fix h5py error `` Unable to create attribute ( object header message is too large ) '' ( # 11242 )
265,a9e77534c889e378eda67e6aba10632a9d30d629,2018-09-27 11:38:46-07:00,"input_shapes = unpack_singleton ( path , 'layer { } '.format ( layer.name ) + ' due to ' keras/datasets/reuters.py E501 \ uses_learning_phase = any ( shape_key += ' _ % s_ % s ' % ( node_index , tensor_index ) original_backend ) 'in number of weights ( { } vs { } ) . '.format ( output_masks ) : input_shapes = unpack_singleton ( [ x._keras_shape for x in computed_tensors ] ) warnings.warn ( 'Skipping loading of weights for ' original_keras_version , for x , y , mask in zip ( reference_output_tensors , output_tensors , output_masks ) : warnings.warn ( 'Skipping loading of weights for layer { } '.format ( layer.name ) ' ' + str ( len ( output_tensors ) ) + ' output tensors ' xs = [ [ w if ( skip_top < = w < num_words ) else oov_char for w in x ] origin='https : //s3.amazonaws.com/keras-datasets/boston_housing.npz ' , `` `` '' Converts layers nested in ` TimeDistributed ` wrapper . kernels = transform_kernels ( weights [ 0 ] , transpose_input ( from_cudnn ) , n_gates ) uses_learning_phase = any ( [ x._uses_learning_phase for x in computed_tensors ] ) outputs : Optional output tensors ( if already computed by running the model ) . path = get_file ( keras/engine/training.py E501 \ output_tensors = to_list ( layer.call ( computed_tensor , * * kwargs ) ) keras/engine/saving.py E501 \ origin='https : //s3.amazonaws.com/text-datasets/reuters_word_index.json ' , 'and ' + str ( len ( output_masks ) ) + ' output masks . ' ) ' ' + str ( len ( output_masks ) ) + ' output masks . ' ) [ x._uses_learning_phase for x in computed_tensors ] ) kernels = transform_kernels ( weights [ 0 ] , use_multiprocessing=use_multiprocessing ) original_keras_version , `` `` '' Converts layers nested in ` Model ` or ` Sequential ` by ` preprocess_weights_for_loading ( ) ` . symbolic_shape = K.int_shape ( symbolic_weights [ i ] ) warnings.warn ( 'Skipping loading of weights for ' keras/engine/network.py E501 \ assert ( shape [ 0 ] == layer.filters and val_data , shape [ 2 : ] == ( layer.kernel_size [ 0 ] , 1 ) ) n_gates ) assert shape [ 0 ] == layer.filters and shape [ 2 : ] == ( layer.kernel_size [ 0 ] , 1 ) model_weights_group [ 'layer_names ' ] = [ layer.name.encode ( 'utf8 ' ) for layer in model_layers ] `` `` '' Trains the model on data generated batch-by-batch by a Python generator ' ( { } vs { } ) . '.format ( ' due to mismatch in number of weights ' metric_fn = ( if K.int_shape ( symbolic_weights [ i ] ) ! = weight_values [ i ] .shape : xs = [ [ w if ( skip_top < = w < num_words ) else oov_char for w in x ] for x in xs ] target_tensors ) forward_weights = preprocess_weights_for_loading ( `` `` '' Converts layers nested in ` TimeDistributed ` wrapper by ` preprocess_weights_for_loading ( ) ` . `` `` '' Trains the model on data generated batch-by-batch by a Python generator ( or an instance of ` Sequence ` ) . ' a list of tensors , or dict of tensors , but got : ' , target_tensors ) original_backend ) x._uses_learning_phase = getattr ( x , '_uses_learning_phase ' , False ) or uses_learning_phase weights [ num_weights_per_layer : ] , use_multiprocessing=use_multiprocessing ) weights [ num_weights_per_layer : ] , keras/datasets/imdb.py E501 \ shape_key = inbound_layer.name So the channel axis needs to be flipped when we 're loading TF weights onto a TH model , ( or an instance of ` Sequence ` ) . keras/datasets/boston_housing.py E501 \ _u = getattr ( x , '_uses_learning_phase ' , False ) ' ( { } vs { } ) . '.format ( len ( symbolic_weights ) , len ( weight_values ) ) ) the model ) . output_tensors , output_tensors = to_list ( if symbolic_shape ! = weight_values [ i ] .shape : x._uses_learning_phase = _u or uses_learning_phase xs = [ [ w for w in x if skip_top < = w < num_words ] for x in xs ] weights [ : num_weights_per_layer ] , outputs : Optional output tensors ( if already computed by running metric_fn = metrics_module.sparse_categorical_crossentropy `` `` '' Converts layers weights from Keras 1 format to Keras 2 . val_enqueuer = GeneratorEnqueuer ( len ( symbolic_weights ) , len ( weight_values ) ) ) val_enqueuer = GeneratorEnqueuer ( val_data , ' due to mismatch in shape ' path = get_file ( path , unpack_singleton ( input_shapes ) ) warnings.warn ( 'Skipping loading of weights for layer { } '.format ( layer.name ) `` `` '' Converts layers nested in ` Bidirectional ` wrapper by ` preprocess_weights_for_loading ( ) ` . origin='https : //s3.amazonaws.com/text-datasets/imdb_word_index.json ' , forward_weights = preprocess_weights_for_loading ( layer.forward_layer , 'layer { } '.format ( layer.name ) + ' due to mismatch ' losses.sparse_categorical_crossentropy ) : xs = [ [ w for w in x if skip_top < = w < num_words ] `` `` '' Converts layers nested in ` Model ` or ` Sequential ` . ' has shape { } '.format ( K.int_shape ( symbolic_weights [ i ] ) ) file_hash='4d44cc38712099c9e383dc6e5f11a921 ' ) # If there is no bias we skip the conversion since CuDNNGRU always has biases . layer.forward_layer , transpose_input ( from_cudnn ) , elif ( self.loss_functions [ i ] == # since CuDNNGRU always has biases . val_enqueuer = OrderedEnqueuer ( for x , y , mask in zip ( reference_output_tensors , val_enqueuer = OrderedEnqueuer ( val_data , for x in xs ] output_shape = layer.compute_output_shape ( elif self.loss_functions [ i ] == losses.sparse_categorical_crossentropy : if hasattr ( layer , 'activity_regularizer ' ) and layer.activity_regularizer is not None : ' has shape { } '.format ( symbolic_shape ) `` `` '' Converts layers nested in ` Bidirectional ` wrapper . `` `` '' Converts layers weights from Keras 1 format to Keras 2 and also weights of CuDNN layers in Keras 2 . backward_weights = preprocess_weights_for_loading ( origin='https : //s3.amazonaws.com/keras-datasets/boston_housing.npz ' , [ x._keras_shape for x in computed_tensors ] ) original_keras_version , layer.backward_layer , origin='https : //s3.amazonaws.com/text-datasets/imdb_word_index.json ' , for layer in model_layers ] file_hash='f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5 ' ) keras/engine/training_generator.py E501 \ model_weights_group [ 'layer_names ' ] = [ layer.name.encode ( 'utf8 ' ) use_multiprocessing=use_multiprocessing ) layer.call ( computed_tensor , * * kwargs ) ) # If there is no bias we skip the conversion So the channel axis needs to be flipped when TF weights are loaded on a TH model , ' ' + str ( len ( output_tensors ) ) + ' output tensors and ' layer.activity_regularizer is not None ) : output_shape = layer.compute_output_shape ( unpack_singleton ( input_shapes ) ) metrics_module.sparse_categorical_crossentropy ) original_backend ) backward_weights = preprocess_weights_for_loading ( layer.backward_layer , file_hash='bfafd718b763782e994055a2d397834f ' ) file_hash='bfafd718b763782e994055a2d397834f ' ) file_hash='4d44cc38712099c9e383dc6e5f11a921 ' ) 'mismatch in shape ( { } vs { } ) . '.format ( weights [ : num_weights_per_layer ] , file_hash='f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5 ' ) if ( hasattr ( layer , 'activity_regularizer ' ) and origin='https : //s3.amazonaws.com/text-datasets/reuters_word_index.json ' , ' a list of tensors , or dict of tensors , but got : ' , shape_key = inbound_layer.name + ' _ % s_ % s ' % ( node_index , tensor_index )","['keras/datasets/boston_housing.py', 'keras/datasets/imdb.py', 'keras/datasets/reuters.py', 'keras/engine/network.py', 'keras/engine/saving.py', 'keras/engine/training.py', 'keras/engine/training_generator.py', 'pytest.ini']",Style fixes for enabling PEP8 501 ( # 11234 )
266,32fee92df5fa21e3d1ff6375f7a5a923636c311b,2018-09-27 17:25:36+02:00,"' ( e.g . by calling it on some test data ) . ' ) 'Build the model first ' 'Build the model first by calling build ( ) or calling fit ( ) with some data . ' 'This model has never been called , thus its weights ' 'have not yet been created , so no summary can be displayed . ' 'Or specify input_shape or batch_input_shape in the first layer for automatic build . ' ) 'This model has not yet been built . '",['keras/engine/network.py'],Add better error message for model.summary ( ) ( # 11222 )
267,621ec292c2609b82fd3f992b5a66f3d6611a4477,2018-09-26 14:23:03-07:00,env : KERAS_BACKEND=tensorflow TEST_MODE=PEP8_DOC if [ [ `` $ TEST_MODE '' == `` INTEGRATION_TESTS '' ] ] || [ [ `` $ TEST_MODE '' == `` PEP8 '' ] ] ; then elif [ [ `` $ TEST_MODE '' == `` PEP8 '' ] ] ; then env : KERAS_BACKEND=tensorflow TEST_MODE=DOC env : KERAS_BACKEND=tensorflow TEST_MODE=PEP8 if [ [ `` $ TEST_MODE '' == `` INTEGRATION_TESTS '' ] ] || [ [ `` $ TEST_MODE '' == `` PEP8_DOC '' ] ] ; then elif [ [ `` $ TEST_MODE '' == `` DOC '' ] ] ; then elif [ [ `` $ TEST_MODE '' == `` PEP8_DOC '' ] ] ; then python : 2.7,['.travis.yml'],Merge test environments for PEP8 and DOC ( # 11163 )
268,630a3a877c3c96ce39be7b0611570466c8c9f447,2018-09-26 14:22:12-07:00,"num_words = min ( MAX_NUM_WORDS , len ( word_index ) + 1 ) num_words = min ( MAX_NUM_WORDS , len ( word_index ) ) + 1 if i > = MAX_NUM_WORDS : if i > MAX_NUM_WORDS :",['examples/pretrained_word_embeddings.py'],Use all top MAX_NUM_WORDS words for the embedding matrix . ( # 11202 )
269,d337273d2e1be823c5e3e1b5e757e2426cba8004,2018-09-25 14:58:25-07:00,"def tile ( x , n ) : check_single_tensor_operation ( 'tile ' , ( 2 , 5 ) , BACKENDS , n= [ 5 , 2 ] ) check_single_tensor_operation ( 'tile ' , arr , BACKENDS , n= [ 2 , 1 ] ) check_single_tensor_operation ( 'tile ' , arr , WITH_NP , n= [ 2 , 1 ] ) check_single_tensor_operation ( 'tile ' , ( 2 , 5 ) , WITH_NP , n= [ 5 , 2 ] ) return np.tile ( x , n )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Added np.tile to reference_operations.py . ( # 11205 )
270,02bc5010a04bb11c8e91835cc9775c8149dec754,2018-09-24 12:19:38+02:00,"# If the shape of y_true is ( num_samples , 1 ) , flatten to ( num_samples , ) def test_sparse_top_k_categorical_accuracy ( y_pred , y_true ) : return K.mean ( K.in_top_k ( y_pred , K.cast ( K.flatten ( y_true ) , 'int32 ' ) , k ) , return K.mean ( K.in_top_k ( y_pred , K.cast ( K.max ( y_true , axis=-1 ) , 'int32 ' ) , k ) , y_pred = K.variable ( np.array ( [ [ 0.3 , 0.2 , 0.1 ] , [ 0.1 , 0.2 , 0.7 ] ] ) ) # Test correctness if the shape of y_true is ( num_samples , 1 ) # Test correctness if the shape of y_true is ( num_samples , ) y_true = K.variable ( y_true ) def test_sparse_top_k_categorical_accuracy ( ) : y_true = K.variable ( np.array ( [ [ 1 ] , [ 0 ] ] ) ) ( np.array ( [ [ 0.3 , 0.2 , 0.1 ] , [ 0.1 , 0.2 , 0.7 ] ] ) , np.array ( [ [ 1 ] , [ 0 ] ] ) ) , y_pred = K.variable ( y_pred ) ] ) ( np.array ( [ [ 0.3 , 0.2 , 0.1 ] , [ 0.1 , 0.2 , 0.7 ] ] ) , np.array ( [ 1 , 0 ] ) ) ,","['keras/metrics.py', 'tests/keras/metrics_test.py']",Fix bug in sparse_top_k_categorical_accuracy ( # 11196 )
271,98465b85d020f1326bcef7632f1261a9a7a84e92,2018-09-23 10:44:23+02:00,"momentum_cache_t_1 = self.beta_1 * ( output_shape = [ output_shape ] + [ ( input_shape [ 0 ] , self.filters , rows , cols ) for _ in range ( 2 ) ] [ Dropout : A Simple Way to Prevent Neural Networks from Overfitting Srivastava , Hinton , et al . 2014 ] ( http : //www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf ) stacklevel=3 ) warnings.warn ( 'Update your ` ' + object_name K.pow ( K.cast_to_floatx ( 0.96 ) , ( t + 1 ) * self.schedule_decay ) ) ) K.pow ( K.cast_to_floatx ( 0.96 ) , t * self.schedule_decay ) ) ) field : String ; JSON field under which the data will be stored . 'right after the ` Embedding ` layer to get the same behavior . ' , 'activity_regularizer ' : [ Adadelta - an adaptive learning rate method ] ( http : //arxiv.org/abs/1212.5701 ) for idx , model_input in enumerate ( self.model.input ) } 'rate to % s . ' % ( epoch + 1 , new_lr ) ) ( https : //arxiv.org/abs/1412.6980v8 ) [ rmsprop : Divide the gradient by a running average of its recent magnitude ] ( http : //www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf ) for _original , _cloned in zip ( model._input_layers , input_layers ) : ' ` padding= ( ( top_pad , bottom_pad ) , ( left_pad , right_pad ) ) ` ' , # ( samples , output_dim ) logs = dict ( [ ( k , logs [ k ] ) if k in logs else ( k , 'NA ' ) for k in self.keys ] ) [ Adaptive Subgradient Methods for Online Learning and Stochastic ( http : //www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf ) 'learning rate to % s . ' % ( epoch + 1 , new_lr ) ) tf.summary.histogram ( ' { } _out_ { } '.format ( layer.name , i ) , [ On the Convergence of Adam and Beyond ] keras/callbacks.py E501 \ application/json . axis=-1 ) 'use TensorBoard . ' ) 1 . - 0.5 * ( K.pow ( K.cast_to_floatx ( 0.96 ) , ( t + 1 ) * self.schedule_decay ) ) ) if all ( [ x in kwargs for x in [ 'kernel_dim2 ' , 'kernel_dim3 ' ] ] ) : application/json . Otherwise the serialized JSON will be send within a form os.path.join ( self.log_dir , 'keras_embedding.ckpt ' ) , output_shape = [ output_shape ] + [ ( input_shape [ 0 ] , rows , cols , self.filters ) for _ in range ( 2 ) ] within a form ( i.e . send_as_json is set to False ) . and len ( inputs ) > 1 and initial_state is None ) : [ On the importance of initialization and momentum in deep learning ] ( http : //www.cs.toronto.edu/~fritz/absps/momentum.pdf ) feed_dict = { _input : embeddings_data [ idx ] [ batch ] keras/optimizers.py E501 \ 'the best epoch ' ) m_t_bar = ( 1 . - momentum_cache_t ) * g_prime + momentum_cache_t_1 * m_t_prime self.dilation_rate = conv_utils.normalize_tuple ( dilation_rate , 2 , layer_map [ _original ] = _cloned state_shape = ( input_shape [ 0 ] , self.filters , rows , cols ) keras/legacy/interfaces.py E501 \ 1 . - 0.5 * ( K.pow ( K.cast_to_floatx ( 0.96 ) , t * self.schedule_decay ) ) ) [ Adaptive Subgradient Methods for Online Learning and Stochastic Optimization ] ( http : //www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf ) self.embeddings_data = standardize_input_data ( self.embeddings_data , model.input_names ) m_t_bar = ( 1 . - momentum_cache_t ) * g_prime + ( self.embeddings_data = standardize_input_data ( self.embeddings_data , keras/metrics.py E501 \ warnings.warn ( 'Update your ` ' + object_name + ' ` call to the ' Learn [ more about embeddings ] ( https : //www.tensorflow.org/programmers_guide/embedding ) feed_dict = { model_input : embeddings_data [ idx ] [ batch ] [ Adadelta - an adaptive learning rate method ] grads ) ' ` padding= ( top_pad , bottom_pad , left_pad , right_pad ) ` . ' , ' ` padding= ( top_pad , bottom_pad , left_pad , right_pad ) ` . ' , stacklevel=3 ) tf.summary.histogram ( ' { } _out_ { } '.format ( layer.name , i ) , output ) Otherwise the serialized JSON will be send within a form os.path.join ( self.log_dir , keras1_args = { 'samples_per_epoch ' , 'val_samples ' , tf.summary.histogram ( ' { } _grad'.format ( mapped_weight_name ) , grads ) keras/legacy/layers.py E501 \ self.dilation_rate = conv_utils.normalize_tuple ( dilation_rate , 2 , 'dilation_rate ' ) for original_input_layer , cloned_input_layer in zip ( model._input_layers , input_layers ) : [ On the Convergence of Adam and Beyond ] ( https : //openreview.net/forum ? id=ryQu7f-RZ ) 'right after the ` Embedding ` layer to get the same behavior . ' , stacklevel=3 ) ( https : //arxiv.org/abs/1212.5701 ) send_as_json : Boolean ; whether the request should be send as application/json . 'Keras 2 API : ' + signature , stacklevel=2 ) [ rmsprop : Divide the gradient by a running average of its recent magnitude ] model.input_names ) keras1_args = { 'samples_per_epoch ' , 'val_samples ' , 'nb_epoch ' , 'nb_val_samples ' , 'nb_worker ' } ( http : //www.cs.toronto.edu/~fritz/absps/momentum.pdf ) [ Dropout : A Simple Way to Prevent Neural Networks from Overfitting ] field : String ; JSON field under which the data will be stored . The field is used only if the payload is sent [ Adam - A Method for Stochastic Optimization ] ( http : //arxiv.org/abs/1412.6980v8 ) ' ` call to the Keras 2 API : ' + signature , stacklevel=2 ) initial_state = K.tile ( initial_state , [ 1 , self.units ] ) # ( samples , output_dim ) send_as_json : Boolean ; whether the request should be send as ( i.e . send_as_json is set to False ) . If send_as_json is set to True , the content type of the request will be application/json . 'keras_embedding.ckpt ' ) , If send_as_json is set to True , the content type of the request will be raise ImportError ( 'You need the TensorFlow module installed to ' ' ` padding= ( ( top_pad , bottom_pad ) , ( left_pad , right_pad ) ) ` ' , stacklevel=3 ) [ Adam - A Method for Stochastic Optimization ] 'nb_epoch ' , 'nb_val_samples ' , 'nb_worker ' } keras/models.py E501 \ state_shape = ( input_shape [ 0 ] , rows , cols , self.filters ) momentum_cache_t_1 * m_t_prime ) keras/constraints.py E501 \ print ( '\nEpoch % 05d : ReduceLROnPlateau reducing learning ' if 'kernel_dim1 ' in kwargs and 'kernel_dim2 ' in kwargs and 'kernel_dim3 ' in kwargs : return K.mean ( K.in_top_k ( y_pred , K.cast ( K.max ( y_true , axis=-1 ) , 'int32 ' ) , k ) , axis=-1 ) Learn [ more about embeddings ] raise ImportError ( 'You need the TensorFlow module installed to use TensorBoard . ' ) The field is used only if the payload is sent within a form if 'kernel_dim2 ' in kwargs and 'kernel_dim3 ' in kwargs : output_shape = [ output_shape , state_shape , state_shape ] if isinstance ( args [ 2 ] , int ) and isinstance ( args [ 3 ] , int ) and isinstance ( args [ 4 ] , int ) : stacklevel=3 ) logs = dict ( [ ( k , logs [ k ] if k in logs else 'NA ' ) for k in self.keys ] ) for idx , _input in enumerate ( self.model.input ) } layer_map [ original_input_layer ] = cloned_input_layer ( https : //www.tensorflow.org/programmers_guide/embedding ) . Optimization ] ( http : //www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf ) tf.summary.histogram ( ' { } _grad'.format ( mapped_weight_name ) , return K.mean ( K.in_top_k ( y_pred , K.cast ( K.max ( y_true , axis=-1 ) , 'int32 ' ) , k ) , if ( isinstance ( inputs , ( list , tuple ) ) 'dilation_rate ' ) momentum_cache_t = self.beta_1 * ( if isinstance ( inputs , ( list , tuple ) ) and len ( inputs ) > 1 and initial_state is None : momentum_cache_t_1 = self.beta_1 * ( 1 . - 0.5 * ( if all ( [ isinstance ( x , int ) for x in args [ 2:5 ] ] ) : initial_state = K.tile ( initial_state , [ 1 , self.units ] ) ( http : //www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf ) if all ( [ x in kwargs for x in [ 'kernel_dim1 ' , 'kernel_dim2 ' , 'kernel_dim3 ' ] ] ) : regularizers.serialize ( self.activity_regularizer ) , [ On the importance of initialization and momentum in deep learning ] ( https : //openreview.net/forum ? id=ryQu7f-RZ ) print ( 'Restoring model weights from the end of ' momentum_cache_t = self.beta_1 * ( 1 . - 0.5 * ( output ) 'activity_regularizer ' : regularizers.serialize ( self.activity_regularizer ) , print ( '\nEpoch % 05d : ReduceLROnPlateau reducing ' print ( `` Restoring model weights from the end of the best epoch '' )","['keras/callbacks.py', 'keras/constraints.py', 'keras/legacy/interfaces.py', 'keras/legacy/layers.py', 'keras/metrics.py', 'keras/models.py', 'keras/optimizers.py', 'pytest.ini']",Style fixes for enabling PEP8 501 ( # 11199 )
272,adfec1e8440b3af64b642ea75bdb21bbee70cc80,2018-09-20 12:16:45-07:00,"y_train [ i * batch_size : ( i + 1 ) * batch_size ] ) yield ( [ X_train [ i * batch_size : ( i + 1 ) * batch_size ] ] * 2 , def data_generator ( ) : else : y_test [ i * batch_size : ( i + 1 ) * batch_size ] ) model.fit_generator ( data_generator ( True ) , len ( X_train ) , epochs=2 , y_train [ i * batch_size : ( i + 1 ) * batch_size ] ) yield ( [ X_test [ i * batch_size : ( i + 1 ) * batch_size ] ] * 2 , max_batch_index = len ( x [ 0 ] ) // batch_size train_generator = data_generator ( [ X_train ] * 2 , [ y_train ] * 2 , batch_size ) history = model.fit_generator ( data_generator ( ) , from keras.utils.generic_utils import to_list x_batch = unpack_singleton ( x_batch ) model.fit_generator ( train_generator , model.fit_generator ( data_generator ( True ) , len ( X_train ) , epochs=1 , history = model.fit_generator ( data_generator ( X_train , y_train , batch_size ) , validation_data=validation_generator , y_batch = [ array [ i * batch_size : ( i + 1 ) * batch_size ] for array in y ] max_batch_index = len ( X_test ) // batch_size max_batch_index = len ( X_train ) // batch_size y = to_list ( y ) model.fit_generator ( train_generator , len ( X_train ) , epochs=2 , # case 2 fit_generator i = 0 yield ( X_train [ i * batch_size : ( i + 1 ) * batch_size ] , max_batch_index = len ( X_train ) // batch_size def data_generator ( x , y , batch_size ) : x = to_list ( x ) i = 0 yield ( X_train [ i * batch_size : ( i + 1 ) * batch_size ] , yield ( X_test [ i * batch_size : ( i + 1 ) * batch_size ] , i = i % max_batch_index validation_data=data_generator ( False ) , from keras.utils.generic_utils import unpack_singleton len ( X_train ) , epochs=2 , i = i % max_batch_index i += 1 model.fit_generator ( data_generator ( True ) , len ( X_train ) , epochs=2 , model.fit_generator ( train_generator , len ( X_train ) , epochs=1 , # simulate multi-input/output models y_batch = unpack_singleton ( y_batch ) validation_generator = data_generator ( X_test , y_test , batch_size ) [ y_train [ i * batch_size : ( i + 1 ) * batch_size ] ] * 2 ) model.fit_generator ( train_generator , len ( X_train ) , epochs=2 , def data_generator ( train ) : x_batch = [ array [ i * batch_size : ( i + 1 ) * batch_size ] for array in x ] yield x_batch , y_batch else : train_generator = data_generator ( X_train , y_train , batch_size ) if train : while 1 : if train : [ y_test [ i * batch_size : ( i + 1 ) * batch_size ] ] * 2 ) while 1 : i += 1",['tests/keras/test_callbacks.py'],Refactoring : Added a data_generator to the test_utils.py . ( # 11153 )
273,f130fa1bca288cb566b083620ba70cb89e2e5fdd,2018-09-20 09:10:34-04:00,"recurrent.LSTM recurrent.SimpleRNNCell , `` `` '' @ keras_test recurrent.LSTM ] ) recurrent.SimpleRNN , recurrent.GRU , output = func ( * args , * * kwargs ) A function wrapping the input function . # Returns return wrapper return pytest.mark.parametrize ( 'cell_class ' , [ def keras_test ( func ) : # Arguments recurrent.GRUCell , [ recurrent.SimpleRNN , func : test function to clean up after . recurrent.LSTMCell `` `` '' Function wrapper to clean up after TensorFlow tests . so we can run through them with a single function . @ six.wraps ( func ) `` `` '' All the recurrent layers share the same interface , [ recurrent.SimpleRNNCell , recurrent.GRU , recurrent.LSTMCell ] ) from keras.utils.test_utils import keras_test def rnn_test ( f ) : ] ) ( f ) rnn_cell_test = pytest.mark.parametrize ( 'cell_class ' , K.clear_session ( ) def rnn_cell_test ( f ) : return output def wrapper ( * args , * * kwargs ) : if K.backend ( ) == 'tensorflow ' or K.backend ( ) == 'cntk ' : `` `` '' Test wrapper to clean up after TensorFlow and CNTK tests . rnn_test = pytest.mark.parametrize ( 'layer_class ' , recurrent.GRUCell , This wrapper runs for all the tests in the keras test suite . f = keras_test ( f ) return pytest.mark.parametrize ( 'layer_class ' , [","['keras/utils/test_utils.py', 'tests/conftest.py', 'tests/keras/layers/convolutional_recurrent_test.py', 'tests/keras/layers/recurrent_test.py']",Removed completely the keras_test decorator . ( # 11182 )
274,9a9abbac2ab66ec2fb6dfffea3249b32fdbbc894,2018-09-20 11:31:57+02:00,"and width and height should be no smaller than 139 . or ` ( 3 , 224 , 224 ) ` ( with ` channels_first ` data format ) . and width and height should be no smaller than 32 . has to be ` ( 224 , 224 , 3 ) ` ( with ` 'channels_last ' ` data format ) It should have exactly 3 inputs channels . and width and height should be no smaller than 48 . and width and height should be no smaller than 75 . or ` ( 3 , 224 , 224 ) ` ( with ` 'channels_first ' ` data format ) . It should have exactly 3 inputs channels , E.g . ` ( 200 , 200 , 3 ) ` would be one valid value . has to be ` ( 224 , 224 , 3 ) ` ( with ` channels_last ` data format )",['docs/templates/applications.md'],Fix incorrect minimum input size for applications ( # 11181 )
275,bb8cc405f3ea3b74a9f3908b606a35f08d8d0be8,2018-09-18 10:17:15-07:00,"def clear_session_after_test ( ) : from keras.utils.test_utils import layer_test if K.backend ( ) == 'tensorflow ' or K.backend ( ) == 'cntk ' : from keras.utils.test_utils import layer_test , keras_test from keras.utils.test_utils import get_test_data , keras_test import pytest @ keras_test from keras.utils.test_utils import keras_test , layer_test from keras.utils.test_utils import keras_test from keras.utils.test_utils import get_test_data K.clear_session ( ) yield from keras import backend as K","['tests/conftest.py', 'tests/integration_tests/applications_test.py', 'tests/integration_tests/test_image_data_tasks.py', 'tests/integration_tests/test_temporal_data_tasks.py', 'tests/integration_tests/test_tensorflow_integration.py', 'tests/integration_tests/test_vector_data_tasks.py', 'tests/keras/backend/backend_test.py', 'tests/keras/constraints_test.py', 'tests/keras/engine/test_topology.py', 'tests/keras/engine/test_training.py', 'tests/keras/layers/advanced_activations_test.py', 'tests/keras/layers/convolutional_test.py', 'tests/keras/layers/core_test.py', 'tests/keras/layers/cudnn_recurrent_test.py', 'tests/keras/layers/embeddings_test.py', 'tests/keras/layers/local_test.py', 'tests/keras/layers/merge_test.py', 'tests/keras/layers/noise_test.py', 'tests/keras/layers/normalization_test.py', 'tests/keras/layers/pooling_test.py', 'tests/keras/layers/recurrent_test.py', 'tests/keras/layers/wrappers_test.py', 'tests/keras/legacy/interface_test.py', 'tests/keras/legacy/layers_test.py', 'tests/keras/metrics_test.py', 'tests/keras/optimizers_test.py', 'tests/keras/test_callbacks.py', 'tests/keras/test_sequential_model.py', 'tests/keras/utils/generic_utils_test.py', 'tests/keras/utils/layer_utils_test.py', 'tests/keras/utils/multi_gpu_test.py', 'tests/test_dynamic_trainability.py', 'tests/test_loss_masking.py', 'tests/test_loss_weighting.py', 'tests/test_model_pickling.py', 'tests/test_model_saving.py', 'tests/test_multiprocessing.py']",Used pytest 's autouse to get rid of the @ keras_test decorator . ( # 11170 )
276,fed304bcedfed737fa540fc0b6ce0bf27187d4b9,2018-09-18 06:13:42+02:00,"travis_retry conda create -q -n test-environment python= $ TRAVIS_PYTHON_VERSION travis_retry pip install -- only-binary=numpy , scipy numpy nose scipy matplotlib h5py theano travis_retry pip install -- only-binary=numpy , scipy , pandas numpy nose scipy matplotlib h5py theano pytest pytest-pep8 pandas travis_retry conda create -q -n test-environment python= $ TRAVIS_PYTHON_VERSION pytest pandas",['.travis.yml'],Move package installation on Travis ( # 11162 )
277,3184efd620840fb6032526df32a09185b7ae18e2,2018-09-17 15:13:01-07:00,"'return_sequences ' : return_sequences , layer.compute_output_shape ( inputs.shape ) ) assert len ( layer.losses ) == 4 'dropout ' : 0.1 , # ( even though the model itself did n't change ) 'filters ' : filters , def test_convolutional_recurrent_statefulness ( ) : kwargs= { 'data_format ' : data_format , 'return_sequences ' : return_sequences , 'padding ' : 'same ' } 'stateful ' : True , from keras.utils.test_utils import keras_test model.compile ( optimizer='sgd ' , loss='mse ' ) # check that output changes after states are reset 'padding ' : 'same ' } 'bias_regularizer ' : 'l2 ' , 'batch_input_shape ' : inputs.shape , output = layer ( K.variable ( np.ones ( inputs.shape ) ) ) 'return_sequences ' : return_sequences , assert ( out4.max ( ) ! = out5.max ( ) ) assert layer.activity_regularizer kwargs = { 'data_format ' : data_format , 'padding ' : 'same ' , # check that the call to ` predict ` updated the states filters = 2 # cntk does n't support eval convolution with static 'kernel_size ' : ( num_row , num_col ) , model = Sequential ( ) assert ( model.predict ( inputs ) .shape == layer.reset_states ( ) y = layer ( x , initial_state=initial_state ) K.eval ( output ) out4 = model.predict ( np.ones_like ( inputs ) ) # train once so that the states change out1 = model.predict ( np.ones_like ( inputs ) ) # variable , will enable it later if K.backend ( ) ! = 'cntk ' : kwargs= { 'data_format ' : data_format , model = Model ( x , y ) kwargs = { 'data_format ' : data_format , input_channel ) layer.build ( inputs.shape ) 'padding ' : 'same ' } model.reset_states ( ) if data_format == 'channels_first ' or return_sequences : layer_test ( convolutional_recurrent.ConvLSTM2D , 'kernel_size ' : ( num_row , num_col ) , input_num_col = 5 layer = convolutional_recurrent.ConvLSTM2D ( layer_test ( convolutional_recurrent.ConvLSTM2D , assert ( out1.max ( ) ! = out2.max ( ) ) # check regularizers num_col = 3 # Tests for statefulness 'return_sequences ' : return_sequences , out1 = model.predict ( np.ones_like ( inputs ) ) assert ( out1.max ( ) ! = out2.max ( ) ) filters=filters , kernel_size= ( num_row , num_col ) , return_sequences = False 'return_sequences ' : return_sequences , 'kernel_size ' : ( num_row , num_col ) , model = Model ( x , y ) layer = convolutional_recurrent.ConvLSTM2D ( * * kwargs ) inputs = np.random.rand ( num_samples , sequence_len , # if the state is not reset , output should be different num_row = 3 data_format=data_format , return_sequences=return_sequences ) 'recurrent_dropout ' : 0.1 } , 'padding ' : 'same ' } assert ( model.predict ( inputs ) .shape == 'recurrent_dropout ' : 0.1 } , model.add ( layer ) out2 = model.predict ( np.ones_like ( inputs ) ) # variable , will enable it later 'filters ' : filters , # if the state is not reset , output should be different input_channel = 2 'recurrent_regularizer ' : regularizers.L1L2 ( l1=0.01 ) , out3 = model.predict ( np.ones_like ( inputs ) ) 'filters ' : filters , layer.build ( inputs.shape ) out4 = model.predict ( np.ones_like ( inputs ) ) model = Sequential ( ) initial_state = layer.get_initial_state ( x ) initial_state = layer.get_initial_state ( x ) num_samples = 1 # check dropout model.add ( layer ) # train once so that the states change output = layer ( K.variable ( np.ones ( inputs.shape ) ) ) x = Input ( batch_shape=inputs.shape ) 'batch_input_shape ' : inputs.shape , 'kernel_regularizer ' : regularizers.L1L2 ( l1=0.01 ) , y = layer ( x , initial_state=initial_state ) 'recurrent_constraint ' : 'max_norm ' , np.random.random ( out1.shape ) ) input_channel = 2 out3 = model.predict ( np.ones_like ( inputs ) ) assert layer.activity_regularizer 'filters ' : filters , assert_allclose ( out3 , out4 , atol=1e-5 ) continue 'stateful ' : True , out5 = model.predict ( np.ones_like ( inputs ) ) 'kernel_constraint ' : 'max_norm ' , 'batch_input_shape ' : inputs.shape , assert_allclose ( out3 , out4 , atol=1e-5 ) 'kernel_size ' : ( num_row , num_col ) , num_samples = 1 'bias_constraint ' : 'max_norm ' , # check state initialization assert ( out4.max ( ) ! = out5.max ( ) ) sequence_len = 2 layer = convolutional_recurrent.ConvLSTM2D ( * * kwargs ) model.train_on_batch ( np.ones_like ( inputs ) , # check dropout 'bias_regularizer ' : 'l2 ' , assert ( out2.max ( ) ! = out3.max ( ) ) layer.compute_output_shape ( inputs.shape ) ) 'return_sequences ' : return_sequences , assert ( out2.max ( ) ! = out3.max ( ) ) input_shape=inputs.shape ) 'stateful ' : True , x = Input ( batch_shape=inputs.shape ) # cntk does n't support eval convolution with static if K.backend ( ) ! = 'cntk ' : 'activity_regularizer ' : 'l2 ' , 'bias_constraint ' : 'max_norm ' , # check regularizers K.eval ( output ) layer.reset_states ( ) num_col = 3 input_num_row = 5 # check that container-level reset_states ( ) works 'recurrent_constraint ' : 'max_norm ' , 'padding ' : 'same ' , 'filters ' : filters , layer = convolutional_recurrent.ConvLSTM2D ( * * kwargs ) # check that output changes after states are reset out5 = model.predict ( np.ones_like ( inputs ) ) out2 = model.predict ( np.ones_like ( inputs ) ) 'kernel_constraint ' : 'max_norm ' , 'stateful ' : True , data_format = 'channels_last ' # check that the call to ` predict ` updated the states 'kernel_size ' : ( num_row , num_col ) , assert len ( layer.losses ) == 3 sequence_len = 2 assert len ( layer.losses ) == 4 kwargs = { 'data_format ' : data_format , 'kernel_size ' : ( num_row , num_col ) , # Tests for statefulness layer = convolutional_recurrent.ConvLSTM2D ( layer = convolutional_recurrent.ConvLSTM2D ( * * kwargs ) # No need to check following tests for both data formats 'dropout ' : 0.1 , assert len ( layer.losses ) == 3 input_num_row , input_num_col , input_shape=inputs.shape ) # check that container-level reset_states ( ) works # ( even though the model itself did n't change ) layer.build ( inputs.shape ) kwargs = { 'data_format ' : data_format , 'kernel_regularizer ' : regularizers.L1L2 ( l1=0.01 ) , # check state initialization 'activity_regularizer ' : 'l2 ' , np.random.random ( out1.shape ) ) model.reset_states ( ) filters = 2 num_row = 3 data_format=data_format , return_sequences=return_sequences ) 'filters ' : filters , model.compile ( optimizer='sgd ' , loss='mse ' ) 'batch_input_shape ' : inputs.shape , input_num_row = 5 model.train_on_batch ( np.ones_like ( inputs ) , layer.build ( inputs.shape ) input_num_col = 5 'recurrent_regularizer ' : regularizers.L1L2 ( l1=0.01 ) , filters=filters , kernel_size= ( num_row , num_col ) ,",['tests/keras/layers/convolutional_recurrent_test.py'],Splitted the convolutional recurrent tests . ( # 11154 )
278,03bd8709778eefb611e470c76af44b2ce90fbff8,2018-09-17 11:34:38-07:00,"out_height , out_width , self.filters ) output_shape = self._compute_elemwise_op_output_shape ( output_shape , shape ) depthwise_kernel_shape = ( input_dim , self.depth_multiplier ) new_rows = ( ( rows - 1 ) * strides [ 1 ] + kernel_size [ 1 ] self.moving_variance_initializer = ( [ Dropout : A Simple Way to Prevent Neural Networks from Overfitting Srivastava , Hinton , et al . 2014 ] ( http : //www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf ) # If inputs have been transposed , depthwise_kernel_shape = self.kernel_size + depthwise_kernel_shape ` ( batch , depth , first_axis_to_pad , second_axis_to_pad , third_axis_to_pad ) ` new_cols = ( cols - 1 ) * strides [ 2 ] + kernel_size [ 2 ] - 2 * padding [ 2 ] + output_padding [ 2 ] [ On the Properties of Neural Machine Translation : Encoder-Decoder Approaches ] ( https : //arxiv.org/abs/1409.1259 ) ( http : //www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf ) . keras/layers/wrappers.py E501 \ 'recurrent_constraint ' : constraints.serialize ( self.recurrent_constraint ) , pointwise_kernel_shape = ( 1 , ) * self.rank + ( self.depth_multiplier * input_dim , self.filters ) first_cropped_axis , second_cropped_axis , third_cropped_axis ) ` config [ 'pointwise_regularizer ' ] = ( ( https : //arxiv.org/abs/1609.03499 ) . config [ 'depthwise_constraint ' ] = ( self.recurrent_kernel_c = ( new_depth = ( depth - 1 ) * strides [ 0 ] + kernel_size [ 0 ] - 2 * padding [ 0 ] + output_padding [ 0 ] ' ( symmetric_dim1_crop , symmetric_dim2_crop , symmetric_dim3_crop ) , ' 'activity_regularizer ' : activations.serialize ( self.recurrent_activation ) , [ Learning Phrase Representations using RNN Encoder-Decoder for 'recurrent_activation ' : activations.serialize ( self.recurrent_activation ) , 2 * padding [ 1 ] + output_padding [ 1 ] ) added = keras.layers.Add ( ) ( [ x1 , x2 ] ) [ On the Properties of Neural Machine Translation : 'or a tuple of 3 tuples of 2 ints ' 'recurrent_initializer ' : initializers.serialize ( self.recurrent_initializer ) , [ Learning to forget : Continual prediction with LSTM ] ( http : //www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015 ) [ A Theoretically Grounded Application of Dropout in Recurrent Neural Networks ] ( http : //arxiv.org/abs/1512.05287 ) Encoder-Decoder Approaches ] ( https : //arxiv.org/abs/1409.1259 ) 'moving_mean_initializer ' : initializers.serialize ( self.moving_mean_initializer ) , initializers.serialize ( self.moving_mean_initializer ) , [ Supervised sequence labeling with recurrent neural networks ] ( http : //www.cs.toronto.edu/~graves/preprint.pdf ) config [ 'pointwise_initializer ' ] = initializers.serialize ( self.pointwise_initializer ) [ A guide to convolution arithmetic for deep learning ] ( https : //arxiv.org/abs/1603.07285v1 ) keras/layers/merge.py E501 \ output_shape = ( batch_size , self.filters , ' ` padding ` should be either an int , a tuple of 3 ints ' [ Deconvolutional Networks ] 'Found : ' + str ( padding ) ) ' ( ( left_dim1_crop , right_dim1_crop ) , ' [ Batch Normalization : Accelerating Deep Network Training by ` ( batch , first_padded_axis , second_padded_axis , third_axis_to_pad , ( left_dim2_pad , right_dim2_pad ) , 'kernel_regularizer ' : ' ( left_dim2_pad , right_dim2_pad ) , ' # equivalent to added = keras.layers.add ( [ x1 , x2 ] ) pointwise_kernel_shape = ( 1 , ) * self.rank + pointwise_kernel_shape config [ 'depthwise_constraint ' ] = constraints.serialize ( self.depthwise_constraint ) ` ( ( left_dim1_pad , right_dim1_pad ) , ( left_dim2_pad , right_dim2_pad ) , ( left_dim3_pad , right_dim3_pad ) ) ` constraints.serialize ( self.recurrent_constraint ) , 'recurrent_regularizer ' : ' ( left_dim2_crop , right_dim2_crop ) , ' This is recommended in [ Jozefowicz et al . ] ( http : //www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf ) 2 * padding [ 0 ] + output_padding [ 0 ] ) Recurrent Neural Networks ] ( https : //arxiv.org/abs/1512.05287 ) ' ( ( left_dim1_pad , right_dim1_pad ) , ' ' ( left_dim3_crop , right_dim2_crop ) ) . ' keras/layers/noise.py E501 \ [ Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift ] ( https : //arxiv.org/abs/1502.03167 ) [ WaveNet : A Generative Model for Raw Audio , section 2.1 ] ( https : //arxiv.org/abs/1609.03499 ) . [ Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling ] ( http : //arxiv.org/abs/1412.3555v1 ) the input mask is not ` None ` : ` ( batch , depth , first_cropped_axis , second_cropped_axis , third_cropped_axis ) ` ` ( batch , first_cropped_axis , second_cropped_axis , third_cropped_axis , y_shape [ : y_ndim - 1 ] ] ) self.moving_variance_initializer = initializers.get ( moving_variance_initializer ) new_shape = K.concatenate ( [ x_shape [ 1 : ] , K.expand_dims ( batch_size ) ] ) ' ( left_dim3_pad , right_dim2_pad ) ) . ' first_axis_to_crop , second_axis_to_crop , third_axis_to_crop ) ` [ Long short-term memory ] ( http : //www.bioinf.jku.at/publications/older/2604.pdf ) ( original 1997 paper ) initializers.serialize ( self.pointwise_initializer ) ) config [ 'depthwise_regularizer ' ] = ( * * kwargs ) : def __init__ ( self , size= ( 2 , 2 ) , data_format=None , interpolation='nearest ' , * * kwargs ) : initializers.get ( moving_variance_initializer ) ) [ A Theoretically Grounded Application of Dropout in config [ 'pointwise_constraint ' ] = ( first_axis_to_pad , second_axis_to_pad , third_axis_to_pad ) ` changed due to padding . raise ValueError ( ' ( symmetric_dim1_pad , symmetric_dim2_pad , symmetric_dim3_pad ) , ' ' ( left_dim2_pad , right_dim2_pad ) , ' new_depth = ( ( depth - 1 ) * strides [ 0 ] + kernel_size [ 0 ] initializers.serialize ( self.kernel_initializer ) , self.recurrent_bias [ self.units : self.units * 2 ] ) 'moving_mean_initializer ' : self.recurrent_bias_r = self.recurrent_bias [ self.units : self.units * 2 ] ' ( left_dim2_crop , right_dim2_crop ) , ' normalized_padding = 3 * ( ( padding , padding ) , ) K.prod ( x_shape [ 1 : ] ) ] ) ) x_transposed = K.reshape ( x , K.stack ( [ batch_size , ` new_conv_dim1 ` , ` new_conv_dim2 ` and ` new_conv_dim3 ` values might have 2 * padding [ 0 ] + output_padding [ 0 ] ) ' ( left_dim3_pad , right_dim2_pad ) ) . ' 2 * padding [ 2 ] + output_padding [ 2 ] ) # 1D vectors or scalars . new_shape = K.concatenate ( [ x_shape [ 1 : ] , 'recurrent_constraint ' : 'activity_regularizer ' : # We do n't transpose inputs if they are config [ 'depthwise_initializer ' ] = ( self.kernel_size = conv_utils.normalize_tuple ( kernel_size , rank , 'kernel_size ' ) output_shape = ( batch_size , self.filters , out_depth , out_height , out_width ) 'kernel_initializer ' : initializers.serialize ( self.kernel_initializer ) , shape ) depth ) ` added = keras.layers.Add ( ) ( [ x1 , x2 ] ) # equivalent to added = keras.layers.add ( [ x1 , x2 ] ) ' ( left_dim3_crop , right_dim2_crop ) ) . ' new_shape = K.concatenate ( [ K.expand_dims ( batch_size ) , y_shape [ : y_ndim - 1 ] ] ) ` new_conv_dim1 ` , ` new_conv_dim2 ` and ` new_conv_dim3 ` values might have changed due to padding . ' a tuple of 3 ints ' regularizers.serialize ( self.kernel_regularizer ) , ( http : //www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015 ) ' ( ( left_dim1_pad , right_dim1_pad ) , ' ` ( batch , first_axis_to_crop , second_axis_to_crop , third_axis_to_crop , depth ) ` # If inputs have been transposed , we have to transpose the output too . raise ValueError ( ' ` padding ` should be either an int , ' self.recurrent_kernel [ : , self.units * 2 : self.units * 3 ] ) Sequence Modeling ] ( https : //arxiv.org/abs/1412.3555v1 ) config [ 'pointwise_constraint ' ] = constraints.serialize ( self.pointwise_constraint ) x_transposed = K.reshape ( x , K.stack ( [ batch_size , K.prod ( x_shape [ 1 : ] ) ] ) ) new_cols = ( ( cols - 1 ) * strides [ 1 ] + kernel_size [ 1 ] ( left_dim3_pad , right_dim3_pad ) ) ` # We do n't transpose inputs if they are 1D vectors or scalars . new_rows = ( rows - 1 ) * strides [ 1 ] + kernel_size [ 1 ] - 2 * padding [ 1 ] + output_padding [ 1 ] out_depth , out_height , out_width ) config [ 'pointwise_regularizer ' ] = regularizers.serialize ( self.pointwise_regularizer ) ( left_dim2_crop , right_dim2_crop ) , ( https : //arxiv.org/abs/1603.07285v1 ) initializers.serialize ( self.depthwise_initializer ) ) regularizers.serialize ( self.recurrent_regularizer ) , ` ( ( left_dim1_crop , right_dim1_crop ) , ' ( symmetric_dim1_crop , symmetric_dim2_crop , symmetric_dim3_crop ) , ' keras/layers/recurrent.py E501 \ raise ValueError ( ' ` cropping ` should be either an int , ' 'moving_variance_initializer ' : initializers.serialize ( self.moving_variance_initializer ) , [ Dropout : A Simple Way to Prevent Neural Networks from Overfitting ] new_rows = ( ( rows - 1 ) * strides [ 0 ] + kernel_size [ 0 ] output_shape = ( batch_size , out_depth , output_shape = self._compute_elemwise_op_output_shape ( output_shape , ` ( batch , first_axis_to_pad , second_axis_to_pad , third_axis_to_pad , 'recurrent_regularizer ' : regularizers.serialize ( self.recurrent_regularizer ) , K.expand_dims ( batch_size ) ] ) regularizers.serialize ( self.depthwise_regularizer ) ) first_padded_axis , second_padded_axis , third_axis_to_pad ) ` ( http : //www.cs.toronto.edu/~graves/preprint.pdf ) keras/layers/convolutional.py E501 \ ' ` cropping ` should be either an int , a tuple of 3 ints ' ` ( batch , depth , first_padded_axis , second_padded_axis , third_axis_to_pad ) ` ' ( ( left_dim1_crop , right_dim1_crop ) , ' 'Found : ' + str ( cropping ) ) # we have to transpose the output too . ` ( batch , first_axis_to_crop , second_axis_to_crop , third_axis_to_crop , [ Empirical Evaluation of Gated Recurrent Neural Networks on 'activity_regularizer ' : regularizers.serialize ( self.activity_regularizer ) , initializers.serialize ( self.recurrent_initializer ) , 'Found : ' + str ( padding ) ) 'kernel_size ' ) output_shape = ( batch_size , out_depth , out_height , out_width , self.filters ) [ WaveNet : A Generative Model for Raw Audio , section 2.1 ] ` ( ( left_dim1_crop , right_dim1_crop ) , ( left_dim2_crop , right_dim2_crop ) , ( left_dim3_crop , right_dim3_crop ) ) ` ( left_dim3_crop , right_dim3_crop ) ) ` [ A guide to convolution arithmetic for deep learning ] 'moving_variance_initializer ' : 'kernel_regularizer ' : regularizers.serialize ( self.kernel_regularizer ) , 'recurrent_initializer ' : Reducing Internal Covariate Shift ] ( https : //arxiv.org/abs/1502.03167 ) keras/layers/normalization.py E501 \ self.recurrent_kernel_f = self.recurrent_kernel [ : , self.units : self.units * 2 ] self.recurrent_kernel_c = self.recurrent_kernel [ : , self.units * 2 : self.units * 3 ] If the output mask at each time step is ` None ` and the input mask is not ` None ` : def __init__ ( self , size= ( 2 , 2 ) , data_format=None , interpolation='nearest ' , If the output mask at each time step is ` None ` and new_cols = ( cols - 1 ) * strides [ 1 ] + kernel_size [ 1 ] - 2 * padding [ 1 ] + output_padding [ 1 ] self.kernel_size = conv_utils.normalize_tuple ( kernel_size , rank , ( http : //www.bioinf.jku.at/publications/older/2604.pdf ) Statistical Machine Translation ] ( https : //arxiv.org/abs/1406.1078 ) [ Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation ] ( https : //arxiv.org/abs/1406.1078 ) ` ( ( left_dim1_pad , right_dim1_pad ) , [ Supervised sequence labeling with recurrent neural networks ] 'recurrent_activation ' : 'dilation_rate ' ) normalized_padding = ( ( padding , padding ) , ( padding , padding ) , ( padding , padding ) ) ` ( batch , first_padded_axis , second_padded_axis , third_axis_to_pad , depth ) ` self.recurrent_kernel_f = ( constraints.serialize ( self.depthwise_constraint ) ) pointwise_kernel_shape = ( self.depth_multiplier * input_dim , self.filters ) ` ( batch , depth , first_axis_to_crop , second_axis_to_crop , third_axis_to_crop ) ` ` ( batch , first_cropped_axis , second_cropped_axis , third_cropped_axis , depth ) ` regularizers.serialize ( self.activity_regularizer ) , [ Deconvolutional Networks ] ( http : //www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf ) self.dilation_rate = conv_utils.normalize_tuple ( dilation_rate , rank , new_cols = ( ( cols - 1 ) * strides [ 2 ] + kernel_size [ 2 ] ( http : //www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf ) [ Long short-term memory ] 'or a tuple of 3 tuples of 2 ints ' depthwise_kernel_shape = self.kernel_size + ( input_dim , self.depth_multiplier ) regularizers.serialize ( self.activity_regularizer ) , ( http : //www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf ) self.recurrent_bias_r = ( regularizers.serialize ( self.pointwise_regularizer ) ) config [ 'depthwise_initializer ' ] = initializers.serialize ( self.depthwise_initializer ) [ Learning to forget : Continual prediction with LSTM ] new_shape = K.concatenate ( [ K.expand_dims ( batch_size ) , 'kernel_initializer ' : 'activity_regularizer ' : regularizers.serialize ( self.activity_regularizer ) , new_rows = ( rows - 1 ) * strides [ 0 ] + kernel_size [ 0 ] - 2 * padding [ 0 ] + output_padding [ 0 ] config [ 'pointwise_initializer ' ] = ( self.dilation_rate = conv_utils.normalize_tuple ( dilation_rate , rank , 'dilation_rate ' ) constraints.serialize ( self.pointwise_constraint ) ) config [ 'depthwise_regularizer ' ] = regularizers.serialize ( self.depthwise_regularizer ) ` ( batch , first_axis_to_pad , second_axis_to_pad , third_axis_to_pad , depth ) ` ` ( batch , depth , 'Found : ' + str ( cropping ) ) initializers.serialize ( self.moving_variance_initializer ) , self.recurrent_kernel [ : , self.units : self.units * 2 ] ) This is recommended in [ Jozefowicz et al . ] ' ( symmetric_dim1_pad , symmetric_dim2_pad , symmetric_dim3_pad ) , '","['keras/layers/convolutional.py', 'keras/layers/merge.py', 'keras/layers/noise.py', 'keras/layers/normalization.py', 'keras/layers/recurrent.py', 'keras/layers/wrappers.py', 'pytest.ini']",Style fixes for enabling PEP8 501 ( # 11161 )
279,6773d96ee495e205552551b49b08f2d927d36ebe,2018-09-17 13:11:16-04:00,export PIL=Pillow ; export PIL=Pil ; # install pydot for visualization tests travis_retry conda install mkl mkl-service pydot graphviz $ PIL # install pydot for visualization tests conda install pydot graphviz conda install Pillow ; conda install pil ; conda install mkl mkl-service,['.travis.yml'],Grouped conda installations together to speed up the travis build . ( # 11166 )
280,d6388ed6457465ab35da8a34ae4abf5847a50cd0,2018-09-17 13:09:17-04:00,"input_shape= ( input_dim , input_dim ) ) def get_data_callbacks ( num_train=train_samples , ( X_train , y_train ) , ( X_test , y_test ) = get_data_callbacks ( classification=True , input_shape=input_shape ) input_shape= ( input_dim , ) , num_test=test_samples , input_shape=input_shape , num_classes=num_classes ) ( x_train , y_train ) , ( x_test , y_test ) = get_data_callbacks ( ( x_train , y_train ) , _ = get_test_data ( num_train=10 , input_shape= ( input_dim , ) , input_shape= ( input_dim , ) , input_shape= ( input_dim , input_dim ) , num_test=200 , input_shape=input_shape , input_shape=input_shape , classification=True , num_test=0 , # Changing the default arguments of get_test_data . classification=True , num_train=train_samples , ( x_train , y_train ) , ( x_test , y_test ) = get_test_data ( num_train=500 , ( X_train , y_train ) , ( X_test , y_test ) = get_data_callbacks ( ) return get_test_data ( num_train=num_train , num_train=500 , classification=classification , num_test=test_samples , ( x_train , y_train ) , _ = get_data_callbacks ( num_train=10 , num_test=num_test , input_shape=input_shape ) num_classes=num_classes ) num_test=200 , num_classes=num_classes ) ( X_train , y_train ) , ( X_test , y_test ) = get_test_data ( num_train=train_samples , num_classes=num_classes ) num_test=test_samples , num_classes=num_classes ) : classification=True , ( X_train , y_train ) , ( X_test , y_test ) = get_test_data ( num_test=0 ,",['tests/keras/test_callbacks.py'],Grouped common function calls together in the test_callbacks.py . ( # 11167 )
281,ef0c95aca2fd19666711422c14a6b16f8025ed84,2018-09-17 09:06:16+02:00,"regularizers.serialize ( self.embeddings_regularizer ) , ( http : //www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf ) . ` ( samples , timesteps , # notice that this layer will consume ( 30 * 30 ) * ( 3 * 3 * 3 * 64 ) + ( 30 * 30 ) * 64 parameters ( ` state_size [ 0 ] ` ) should be the same as 'recurrent_constraint ' : constraints.serialize ( self.recurrent_constraint ) , keras/layers/convolutional_recurrent.py E501 \ ( https : //arxiv.org/abs/1402.3337 ) 'embeddings_regularizer ' : initializers.serialize ( self.embeddings_initializer ) , self.recurrent_kernel_c = ( ( one size per state ) . In this case , the first entry ( ` state_size [ 0 ] ` ) activations.serialize ( self.recurrent_activation ) , 'activity_regularizer ' : 'recurrent_activation ' : activations.serialize ( self.recurrent_activation ) , output_row * output_col , 'recurrent_initializer ' : initializers.serialize ( self.recurrent_initializer ) , # + ( 30 * 30 ) * 64 parameters [ Fast and Accurate Deep Network Learning by Exponential Linear Units [ A Theoretically Grounded Application of Dropout in Recurrent Neural Networks ] ( http : //arxiv.org/abs/1512.05287 ) # notice that this layer will consume ( 30 * 30 ) * ( 3 * 3 * 3 * 64 ) 'embeddings_regularizer ' : regularizers.serialize ( self.embeddings_regularizer ) , the number of channels of the recurrent state 'but it received ' + str ( len ( states ) ) 'kernel_regularizer ' : [ Fast and Accurate Deep Network Learning by Exponential Linear Units ( ELUs ) ] ( https : //arxiv.org/abs/1511.07289v1 ) 'kernel_constraint ' : constraints.serialize ( self.kernel_constraint ) , 'embeddings_initializer ' : section `` Note on passing external constants '' below . constraints.serialize ( self.recurrent_constraint ) , 'recurrent_regularizer ' : This is recommended in [ Jozefowicz et al . ] ( http : //www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf ) 'recurrent_constraint ' : # the largest integer ( i.e . word index ) in the input should be no larger than 999 ( vocabulary size ) . 'The same thing goes for the number of rows and columns . ' ) raise ValueError ( [ Delving Deep into Rectifiers : Surpassing Human-Level Performance on ( one size per state ) . In this case , the first entry regularizers.serialize ( self.recurrent_regularizer ) , new_rows , new_cols , filters ) ` if data_format='channels_last ' . # the largest integer ( i.e . word index ) in the input should be # apply a 3x3 unshared weights convolution with 64 output filters self.kernel_size [ 0 ] * self.kernel_size [ 1 ] * input_filter , self.dilation_rate = conv_utils.normalize_tuple ( dilation_rate , 2 , [ Rectifier Nonlinearities Improve Neural Network Acoustic Models ] ( https : //web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf ) [ A Theoretically Grounded Application of Dropout in Recurrent Neural Networks ] ( http : //arxiv.org/abs/1512.05287 ) `` `` '' Fast LSTM implementation backed by [ CuDNN ] ( https : //developer.nvidia.com/cudnn ) . This can also be a list/tuple of integers raise ValueError ( ' '' input_length '' is % s , but received input has shape % s ' % ` ( output_at_t , states_at_t_plus_1 ) ` . The call method of the initializers.serialize ( self.kernel_initializer ) , ' { } '.format ( [ spec.shape for spec in self.state_spec ] , self.cell.state_size ) ) 'recurrent_initializer ' : initializers.serialize ( self.recurrent_initializer ) , raise ValueError ( ' '' input_length '' is % s , but received input has shape % s ' % ' '' input_length '' is % s , but received input has shape % s ' % 'recurrent_constraint ' : 'activity_regularizer ' : [ Rectifier Nonlinearities Improve Neural Network Acoustic Models ] ( ELUs ) ] ( https : //arxiv.org/abs/1511.07289v1 ) 'kernel_initializer ' : initializers.serialize ( self.kernel_initializer ) , ( https : //arxiv.org/abs/1411.4280 ) 'embeddings_constraint ' : constraints.serialize ( self.embeddings_constraint ) , keras/layers/cudnn_recurrent.py E501 \ `` `` '' Fast LSTM implementation with [ CuDNN ] ( https : //developer.nvidia.com/cudnn ) . ( http : //www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf ) 'kernel_constraint ' : self.filters ) self.recurrent_kernel_c = self.recurrent_kernel [ : , : , : , self.filters * 2 : self.filters * 3 ] self.kernel_shape = ( a ` state_size ` attribute . This can be a single integer self.dilation_rate = conv_utils.normalize_tuple ( dilation_rate , 2 , 'dilation_rate ' ) ( which should be the same as the number of channels of the cell regularizers.serialize ( self.kernel_regularizer ) , constraints.serialize ( self.kernel_constraint ) , self.recurrent_kernel [ : , self.units * 2 : self.units * 3 ] ) 'but it received ' + str ( len ( states ) ) ( single state ) in which case it is # apply a 3x3 unshared weights convolution with 64 output filters on a 32x32 image ( which should be the same as the number of channels of the cell output ) . initializers.serialize ( self.recurrent_initializer ) , section `` Note on passing external constants '' below . keras/layers/core.py E501 \ regularizers.serialize ( self.recurrent_regularizer ) , raise ValueError ( [ Dropout : A Simple Way to Prevent Neural Networks from Overfitting ] the size of the cell output . output ) . This can also be a list/tuple of integers in which case it is the number of channels of the recurrent state ( https : //ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf ) 'recurrent_regularizer ' : regularizers.serialize ( self.recurrent_regularizer ) , [ Efficient Object Localization Using Convolutional Networks ] # with ` data_format= '' channels_last '' ` : self.recurrent_kernel [ : , : , : , self.filters * 2 : self.filters * 3 ] ) self.filters ) keras/layers/local.py E501 \ 'embeddings_constraint ' : ImageNet Classification ] ( https : //arxiv.org/abs/1502.01852 ) ( str ( self.input_length ) , str ( input_shape ) ) ) [ Delving Deep into Rectifiers : Surpassing Human-Level Performance on ImageNet Classification ] ( https : //arxiv.org/abs/1502.01852 ) [ Efficient Object Localization Using Convolutional Networks ] ( https : //arxiv.org/abs/1411.4280 ) initializers.serialize ( self.recurrent_initializer ) , ' { } '.format ( [ spec.shape for spec in self.state_spec ] , 'activity_regularizer ' : regularizers.serialize ( self.activity_regularizer ) , 'recurrent_regularizer ' : ` ( output_at_t , states_at_t_plus_1 ) ` . The call method of the ` ( samples , timesteps , new_rows , new_cols , filters ) ` if data_format='channels_last ' . [ Zero-Bias Autoencoders and the Benefits of Co-Adapting Features ] self.kernel_size [ 0 ] * self.kernel_size [ 1 ] * input_filter , raise ValueError ( ' ` output_shape ` function must return a tuple or a list of tuples . ' ) raise ValueError ( ' ` output_shape ` function must return a tuple or ' 'The same thing goes for the number of rows ' 'kernel_regularizer ' : regularizers.serialize ( self.kernel_regularizer ) , 'embeddings_initializer ' : initializers.serialize ( self.embeddings_initializer ) , 'recurrent_initializer ' : ( str ( self.input_length ) , str ( input_shape ) ) ) cell can also take the optional argument ` constants ` , see ( str ( self.input_length ) , str ( input_shape ) ) ) # on a 32x32 image with ` data_format= '' channels_last '' ` : self.recurrent_kernel_f = self.recurrent_kernel [ : , self.units : self.units * 2 ] self.recurrent_kernel_c = self.recurrent_kernel [ : , self.units * 2 : self.units * 3 ] constraints.serialize ( self.embeddings_constraint ) , 'and columns . ' ) 'recurrent_initializer ' : self.cell.state_size ) ) This is recommended in [ Jozefowicz et al . ] keras/layers/advanced_activations.py E501 \ [ Zero-Bias Autoencoders and the Benefits of Co-Adapting Features ] ( http : //arxiv.org/abs/1402.3337 ) self.kernel_shape = ( output_row * output_col , constraints.serialize ( self.recurrent_constraint ) , 'recurrent_activation ' : 'dilation_rate ' ) cell can also take the optional argument ` constants ` , see filters , new_rows , new_cols ) ` if data_format='channels_first ' self.recurrent_kernel_f = ( 'recurrent_regularizer ' : regularizers.serialize ( self.recurrent_regularizer ) , keras/layers/embeddings.py E501 \ self.recurrent_kernel_f = self.recurrent_kernel [ : , : , : , self.filters : self.filters * 2 ] regularizers.serialize ( self.activity_regularizer ) , should be the same as the size of the cell output . ` ( samples , timesteps , filters , new_rows , new_cols ) ` if data_format='channels_first ' regularizers.serialize ( self.activity_regularizer ) , 'recurrent_constraint ' : constraints.serialize ( self.recurrent_constraint ) , ' '' input_length '' is % s , but received input has shape % s ' % 'kernel_initializer ' : 'activity_regularizer ' : regularizers.serialize ( self.activity_regularizer ) , ( str ( self.input_length ) , str ( input_shape ) ) ) ' a list of tuples . ' ) self.recurrent_kernel [ : , : , : , self.filters : self.filters * 2 ] ) a ` state_size ` attribute . This can be a single integer ( single state ) self.recurrent_kernel [ : , self.units : self.units * 2 ] ) [ Dropout : A Simple Way to Prevent Neural Networks from Overfitting ] ( http : //www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf ) # no larger than 999 ( vocabulary size ) .","['keras/layers/advanced_activations.py', 'keras/layers/convolutional_recurrent.py', 'keras/layers/core.py', 'keras/layers/cudnn_recurrent.py', 'keras/layers/embeddings.py', 'keras/layers/local.py', 'pytest.ini']",Style fixes for enabling PEP8 501 ( # 11160 )
282,cd22c5a53bf738ea678cb5ce0e799093d527f2a6,2018-09-17 08:22:42+02:00,"if KTH is not None : @ pytest.mark.skipif ( ( K.backend ( ) == 'cntk ' ) , # As of Keras 2.0.0 , all kernels are normalized ( 'conv1d ' , ( 2 , 3 , 8 ) , ( 4 , 3 , 2 ) , 'valid ' , 'channels_first ' , 2 ) , padding=padding , data_format=data_format , dilation_rate=dilation_rate , return backend_list ( 'conv3d ' , ( 2 , 3 , 5 , 4 , 6 ) , ( 2 , 2 , 3 , 3 , 4 ) , 'same ' , 'channels_first ' , ( 2 , 2 , 2 ) ) , 'dilation_rate ' : ( 2 , 2 ) } , if KC is not None and KC.dev.type ( ) == 1 : ( 'conv1d ' , ( 2 , 8 , 3 ) , ( 4 , 3 , 2 ) , 'valid ' , 'channels_last ' , 2 ) , input_shape= ( num_samples , num_row , num_col , stack_size ) ) 'op , input_shape , kernel_shape , output_shape , padding , data_format , dilation_rate ' , [ 'kernel_size ' : kernel_size , check_two_tensor_operation ( kwargs= { 'filters ' : filters , ] ) 'same ' , 'channels_first ' , ( 2 , 2 ) ) , # on the format ` ( steps , input_depth , depth ) ` , kwargs= { 'filters ' : filters , # cntk only support dilated conv on GPU padding , data_format , dilation_rate ) : 'kernel_size ' : kernel_size , 'padding ' : padding , 'padding ' : padding , if KTF is not None : ( 'conv2d ' , ( 2 , 8 , 9 , 3 ) , ( 3 , 3 , 3 , 2 ) , 'same ' , 'channels_last ' , ( 2 , 2 ) ) , reason='cntk only supports dilated conv transpose on GPU ' ) data_format=data_format , dilation_rate=dilation_rate , cntk_dynamicity=True ) ( 'conv1d ' , ( 1 , 2 , 8 ) , ( 3 , 2 , 3 ) , 'valid ' , 'channels_first ' ) , input_shape= ( num_samples , num_row , num_col , stack_size ) ) layer_test ( convolutional.Conv2D , # Test dilation backend_list.append ( KTH ) op , input_shape , kernel_shape , DILATED_CONV_BACKENDS , padding=padding , kernel = C.swapaxes ( kernel , 0 , 2 ) 'same ' , 'channels_last ' , ( 2 , 2 ) ) , backend_list.append ( KTF ) def test_conv_dilation ( self , op , input_shape , kernel_shape , padding , dilation_rate = ( 1 , dilation_rate ) # CNTK expects ` ( depth , input_depth , steps ) ` . ( 'conv2d ' , ( 2 , 3 , 9 , 8 ) , ( 4 , 3 , 3 , 4 ) , 'valid ' , 'channels_first ' , ( 2 , 2 ) ) , DILATED_CONV_BACKENDS = get_dilated_conv_backends ( ) layer_test ( convolutional.Conv2D , ( 'conv2d_transpose ' , ( 2 , 5 , 6 , 3 ) , ( 3 , 3 , 2 , 3 ) , ( 2 , 5 , 6 , 2 ) , def get_dilated_conv_backends ( ) : ] ) 'dilation_rate ' : ( 2 , 2 ) } , ( 'conv3d ' , ( 2 , 5 , 4 , 6 , 3 ) , ( 2 , 2 , 3 , 3 , 4 ) , 'valid ' , 'channels_last ' , ( 2 , 2 , 2 ) ) , ( 'conv2d_transpose ' , ( 2 , 3 , 8 , 9 ) , ( 3 , 3 , 2 , 3 ) , ( 2 , 2 , 8 , 9 ) , reason='cntk only supports dilated conv on GPU ' ) if K.backend ( ) ! = 'cntk ' : # CNTK only supports dilated convolution on GPU # independently of ` data_format ` . def test_conv_transpose_dilation ( self , op , input_shape , kernel_shape , output_shape , cntk_dynamicity=True ) kernel = C.swapaxes ( kernel , 0 , 2 ) data_format , dilation_rate ) : op , input_shape , kernel_shape , DILATED_CONV_BACKENDS , output_shape=output_shape , dilation_rate = ( 1 , ) + dilation_rate backend_list.append ( KC ) backend_list = [ ]","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py', 'tests/keras/layers/convolutional_test.py']",Fix bug in dilated conv for CNTK ( # 11125 )
283,40a9ecc684d2c5e7fdfc09f46d0912940b74ab63,2018-09-15 14:00:39-07:00,x = f [ ' x ' ] with np.load ( path ) as f : f.close ( ) y = f [ ' y ' ] y = f [ ' y ' ] f = np.load ( path ) x = f [ ' x ' ],['keras/datasets/boston_housing.py'],Used a with statement instead of a close statement for robustness . ( # 11147 )
284,f60313e29657b2afb6a02f28dba5936bc0dd09e6,2018-09-14 11:14:13-07:00,"mode = ' a ' dialect=CustomDialect ) self.file_flags = ' b ' assert len ( list_lines ) == 5 for line in list_lines : self.csv_file = io.open ( self.filename , assert line.count ( sep ) == 4 validation_data= ( X_test , y_test ) , callbacks=cbks , epochs=1 ) mode = ' w ' if six.PY2 : self.file_flags = `` self._open_args = { 'newline ' : '\n ' } self._open_args = { } fieldnames=fieldnames , if six.PY2 : validation_data= ( X_test , y_test ) , callbacks=cbks , epochs=2 ) * * self._open_args ) else : fieldnames= [ 'epoch ' ] + self.keys , dialect=CustomDialect ) mode + self.file_flags , self.file_flags = ' b ' if six.PY2 and os.name == 'nt ' else `` output = `` `` .join ( list_lines ) output = `` `` .join ( csvfile.readlines ( ) ) fieldnames = [ 'epoch ' ] + self.keys self.csv_file = open ( self.filename , ' w ' + self.file_flags ) list_lines = csvfile.readlines ( ) fieldnames = [ unicode ( x ) for x in fieldnames ] import io import sys self.csv_file = open ( self.filename , ' a ' + self.file_flags )","['keras/callbacks.py', 'tests/keras/test_callbacks.py']",Fixed the windows line endings in the CSVCallback . ( # 11124 )
285,5b6243485acc20cc36f2db4f258512c332d691ec,2018-09-14 10:19:10-04:00,"iterable = [ iterable_or_element ] allow_tuple : If False and x is a tuple , y = to_list ( y , allow_tuple=True ) else : initial_state = to_list ( initial_state , allow_tuple=True ) outputs = list ( outputs ) self.inputs = [ inputs ] initial_state = [ initial_state ] return list ( x ) from .. utils.generic_utils import to_list def to_list ( x , allow_tuple=False ) : inputs = to_list ( inputs , allow_tuple=True ) if isinstance ( inputs , ( list , tuple ) ) : all_inputs += list ( y ) outputs = [ outputs ] if isinstance ( self.input_length , ( list , tuple ) ) : self.shared_axes = [ shared_axes ] in_lens = [ self.input_length ] all_inputs += to_list ( y , allow_tuple=True ) if isinstance ( y , ( list , tuple ) ) : if isinstance ( outputs , ( list , tuple ) ) : elif not isinstance ( shared_axes , ( list , tuple ) ) : if not isinstance ( iterable_or_element , ( list , tuple ) ) : with a single element ( the tuple ) . self.outputs = [ outputs ] iterable = to_list ( iterable_or_element , allow_tuple=True ) states = to_list ( states , allow_tuple=True ) states = list ( states ) self.shared_axes = to_list ( shared_axes , allow_tuple=True ) in_lens = to_list ( self.input_length , allow_tuple=True ) self.shared_axes = list ( shared_axes ) y = [ y ] in_lens = list ( self.input_length ) inputs = list ( inputs ) if not isinstance ( states , ( list , tuple ) ) : self.outputs = to_list ( outputs , allow_tuple=True ) states = [ states ] self.inputs = list ( inputs ) # Tensor or list of tensors . Else converts the tuple to a list . if not isinstance ( y , ( list , tuple ) ) : def to_list ( x ) : else : self.outputs = list ( outputs ) all_inputs.append ( y ) it will be converted into a list self.inputs = to_list ( inputs , allow_tuple=True ) if allow_tuple and isinstance ( x , tuple ) : else : iterable = iterable_or_element inputs = [ inputs ] outputs = to_list ( outputs , allow_tuple=True ) else : if not isinstance ( initial_state , ( list , tuple ) ) :","['keras/engine/network.py', 'keras/engine/training.py', 'keras/layers/advanced_activations.py', 'keras/layers/convolutional_recurrent.py', 'keras/layers/embeddings.py', 'keras/layers/recurrent.py', 'keras/legacy/layers.py', 'keras/utils/generic_utils.py']",Breaking down the attention API PR : part 2 ( # 11140 )
286,842d360e4388f255ae69055136909525cb66b276,2018-09-14 16:15:26+05:18,def exponential ( x ) : `` `` '' Exponential ( base e ) activation function . `` `` '' return K.exp ( x ),['keras/activations.py'],Added the exponential activation . ( # 11136 )
287,fe38f9dfc8c732a77ac03507b63c79b1d2acfba2,2018-09-13 14:49:33-07:00,"assert 'name ' in config layer_configs = config [ 'layers ' ] return config 'layers ' : copy.deepcopy ( layer_configs ) self._build_input_shape = None if self._layers : } build_input_shape = config.get ( 'build_input_shape ' ) layer_configs.append ( { self._build_input_shape = input_shape # Test serialization model = cls ( name=name ) config [ 'build_input_shape ' ] = self._build_input_shape config = { # test serialization assert new_model.weights # Model should be built . model = cls ( ) model.build ( build_input_shape ) for conf in layer_configs : self._layers [ 0 ] .batch_input_shape = batch_shape Sequential.from_config ( config ) layer_configs = [ ] if not model.inputs and build_input_shape : 'name ' : self.name , config = [ ] config.append ( { new_model = keras.models.Sequential.from_config ( config ) return copy.deepcopy ( config ) if 'name ' in config : if self._build_input_shape : name = config [ 'name ' ] for conf in config : new_model = Sequential.from_config ( config )","['keras/engine/sequential.py', 'tests/keras/test_sequential_model.py']",Modify Sequential config to include model name ( breaking change ) . Matches tf.keras behavior as of TF 1.11.0 . ( # 11133 )
288,ba7ab2ffe62143949b9730a55291c474352f31e5,2018-09-13 11:55:47-07:00,"for a in axis_list : else : keras_shape_list.pop ( a ) y._keras_shape = tuple ( keras_shape_list ) keras_shape_list = ( 1 , ) for a in axis_list [ : :-1 ] : axis_list = [ axis ] if not keras_shape_list : if axis is None : keras_shape_list [ a ] = 1 if hasattr ( x , '_keras_shape ' ) : def _set_keras_shape_for_reduction ( x , y , axis , keepdims ) : y = _set_keras_shape_for_reduction ( x , y , axis , keepdims ) return y axis_list = list ( set ( int ( a ) for a in axis ) ) if keepdims : if isinstance ( axis , int ) : else : keras_shape_list = list ( x._keras_shape ) y._keras_shape = ( 1 , ) * len ( x._keras_shape ) if keepdims else ( 1 , )",['keras/backend/theano_backend.py'],[ Refactoring ] Removed code duplication in the theano backend . ( # 11131 )
289,06c3a804e7d56cc77f76a28ee34aa67a34daedee,2018-09-12 09:29:27-07:00,"beta = tf.reshape ( beta , ( -1 ) ) var = tf.squeeze ( var ) beta = tf.squeeze ( beta ) var = tf.reshape ( var , ( -1 ) ) mean = tf.reshape ( mean , ( -1 ) ) gamma = tf.squeeze ( gamma ) input_shape= ( 3 , 4 , 2 ) ) mean = tf.squeeze ( mean ) gamma = tf.reshape ( gamma , ( -1 ) ) input_shape= ( 1 , 4 , 1 ) )","['keras/backend/tensorflow_backend.py', 'tests/keras/layers/normalization_test.py']",Fix bug when channel=1 ( # 11123 )
290,9149ff85210c1cc8b294b26ea37b8f6d7d51c8f6,2018-09-12 09:28:27-07:00,"padding=padding , data_format=data_format ) ( [ x ] ) [ 0 ] padding=padding , data_format=data_format ) variables = [ ] xc = KC.placeholder ( x_shape ) placeholders.append ( KC.placeholder ( shape ) ) def cntk_func_single_tensor ( function_name , x_shape , * * kwargs ) : _ , cntk_func = cntk_func_tensors ( output_cntk = getattr ( KC , function_name ) ( * ( placeholders + variables ) , * * kwargs ) y2 = cntk_func ( [ x ] ) [ 0 ] shape = shape_or_val if isinstance ( shape_or_val , tuple ) : output_cntk = getattr ( KC , function_name ) ( xc , * * kwargs ) return output_cntk , KC.function ( [ xc , yc ] , [ output_cntk ] ) depthwise , pointwise , def cntk_func_tensors ( function_name , shapes_or_vals , * * kwargs ) : xc = KC.placeholder ( x_shape ) if isinstance ( y , ( np.generic , np.ndarray ) ) : output_cntk = getattr ( KC , function_name ) ( xc , yc , * * kwargs ) value = shape_or_val def cntk_func_three_tensor ( function_name , x_shape , y , z , * * kwargs ) : t , f = cntk_func_tensors ( function_name , [ x_shape , y_shape ] , * * kwargs ) y2 = cntk_func_three_tensor ( else : output_cntk = getattr ( KC , function_name ) ( xc , KC.variable ( y ) , KC.variable ( z ) , * * kwargs ) placeholders = [ ] op , input_shape , t , f = cntk_func_tensors ( function_name , [ x_shape ] , * * kwargs ) return output_cntk , KC.function ( [ xc ] , [ output_cntk ] ) xc = KC.placeholder ( ndim=len ( x_shape ) ) for shape_or_val in shapes_or_vals : def cntk_func_two_tensor ( function_name , x_shape , y , * * kwargs ) : return output_cntk , cntk_func output_cntk = getattr ( KC , function_name ) ( xc , KC.variable ( y ) , * * kwargs ) cntk_func = KC.function ( placeholders , [ output_cntk ] ) return KC.function ( [ xc ] , [ output_cntk ] ) t , f = cntk_func_two_tensor ( function_name , x_shape , y=y_val , * * kwargs ) return output_cntk , KC.function ( [ xc ] , [ output_cntk ] ) t , f = cntk_func_single_tensor ( function_name , x_shape , * * kwargs ) else : t , f = cntk_func_two_tensor ( function_name , x_shape , y=y_shape , * * kwargs ) yc = KC.placeholder ( y ) variables.append ( KC.variable ( value ) ) op , [ input_shape , depthwise , pointwise ] , t , f = cntk_func_tensors ( function_name , [ x_shape , y_val ] , * * kwargs )",['tests/keras/backend/backend_test.py'],Simplified the cntk backend tests and made them more readable . ( # 11116 )
291,cb9d79efa87043f76989ed530de2bdb4b1fe8d19,2018-09-11 15:53:33-07:00,_layer_in_model_test ( model ) actual_output = _layer_in_model_test ( model ) # check with the sequential API actual_output = _layer_in_model_test ( model ) # test instantiation from layer config # test as first layer in Sequential API model.add ( layer ) model = Sequential ( ),['keras/utils/test_utils.py'],Removed the tests of each layers in the sequential model . ( # 11115 )
292,9a4c5d8f10667e571c23f1b2f2e0397a85368bea,2018-09-11 10:34:46-07:00,"} , default=get_json_type ) .encode ( 'utf8 ' ) custom_loss = losses.mse ' in the save file . ' val = [ x.encode ( 'utf-8 ' ) for x in val ] chunk_attr = ' % s % d ' % ( attr , chunk_id ) raise ValueError ( 'Can not set item in read only mode . ' ) weight_names = [ ] assert f [ ' y ' ] == [ b'efg ' , b'hij ' , b'klmn ' ] 'loss ' : model.loss , out2 = model.predict ( x ) f.attrs [ 'model_config ' ] = json.dumps ( { val = self.data.attrs [ attr ] from keras.utils.test_utils import keras_test chunked_data = np.array_split ( data_npy , num_chunks ) if isinstance ( self.data , h5py.Group ) : training_config = json.loads ( training_config.decode ( 'utf-8 ' ) ) self.data [ '_is_group ' ] = True if K.backend ( ) == 'theano ' or K.backend ( ) == 'cntk ' : self.data.attrs [ attr ] = val model_weights_group [ 'layer_names ' ] = [ layer.name.encode ( 'utf8 ' ) for layer in model_layers ] elif isinstance ( path , dict ) : model = Model ( inputs , outputs ) name = str ( w.name ) name = str ( w.name ) h5py.File or h5py.Group object where to save the model model._make_train_function ( ) f = filepath if len ( bad_attributes ) > 0 : weight_values ) ) : [ How can I install HDF5 or h5py to save my models in Keras ? ] ( considered during deserialization . model.compile ( loss=losses.MSE , opened_new_file = not isinstance ( filepath , h5py.Group ) model.optimizer.set_weights ( optimizer_weight_values ) string , path where to save the model , or if compile : try : chunk_id += 1 except ValueError : val = H5Dict ( val ) dataset [ ( ) ] = val in the FAQ for instructions on how to install ` h5py ` . original_backend , ' Received { } . '.format ( type ( path ) ) ) else : else : 'optimizer attributes or optimizer state ' proceed = ask_to_proceed_with_overwrite ( filepath ) val = H5Dict ( self.data.create_group ( attr ) ) } f : keras.utils.hdf5.HD5Dict instance . val.shape , num_chunks += 1 idx += 1 return out def get ( self , key , default=None ) : optimizer=optimizers.Adam ( ) , from .. utils.io_utils import h5dict 'config ' : model.optimizer.get_config ( ) f [ ' z ' ] = array for i , ( w , val ) in enumerate ( zip ( symbolic_weights , f = h5dict ( path , mode= ' w ' ) model.add ( TimeDistributed ( Dense ( 3 ) ) ) return saving.pickle_model ( self ) layer_group [ name ] = val b = Input ( shape= ( 256 , 512 , 1 ) ) original_keras_version = model_weights_group [ 'keras_version ' ] .decode ( 'utf8 ' ) if symbolic_weights : from numpy.testing import assert_allclose optimizer_config = training_config [ 'optimizer_config ' ] def h5wrapper ( * args , * * kwargs ) : model.compile ( loss=loss , optimizer='sgd ' , metrics=metrics ) else : group2 = group1 [ 'group2 ' ] def unpickle_model ( state ) : _serialize_model ( model , f ) 'config ' : model.get_config ( ) filtered_layers = [ ] model = Sequential ( ) f [ 'model_config ' ] = model_config raise ImportError ( ' ` save_model ` requires h5py . ' ) loss = { 'output1 ' : 'mse ' , 'output2 ' : 'mse ' } a = Input ( shape= ( 256 , 512 , 6 ) ) f = h5py.File ( filepath , mode= ' w ' ) 'class_name ' : model.__class__.__name__ , 'the model was * not * compiled . ' if not val.shape : paths = [ h5_path , dict ( ) ] the exact same state , without any of the code param_dset = optimizer_weights_group.create_dataset ( K.batch_set_value ( weight_value_tuples ) self.data.attrs [ ' % s % d ' % ( attr , chunk_id ) ] = chunk_data output1 = Dense ( 1 , name='output1 ' ) ( x ) model_config = model_config.encode ( 'utf-8 ' ) if h5py is None : dataset [ : ] = val ask the user with a manual prompt . if isinstance ( path , h5py.Group ) : self.data [ attr ] = pickle.dumps ( val ) loss=loss , sample_weight_mode=sample_weight_mode ) pytest.main ( [ __file__ ] ) assert_allclose ( group4 [ ' z ' ] , array ) ' elements . ' ) self.data = path # Raises # Set optimizer weights . a warning will be displayed . When ` compile ` is set ImportError : if h5py is not available . original_backend = None ) val = H5Dict ( val ) return iter ( self.data ) if include_optimizer and model.optimizer : 'optimizer . ' ) `` `` '' Test pickling model without compiling . def load_model ( filepath , custom_objects=None , compile=True ) : sample_weight_mode=sample_weight_mode ) model._make_train_function ( ) # Recover loss functions and metrics . loss_weights=loss_weights , loss_weights = training_config [ 'loss_weights ' ] if 'backend ' in model_weights_group : return model # If file exists and should not be overwritten . optimizer = optimizers.deserialize ( optimizer_config , as well as pickling . This is achieved via a sample_weight_mode = training_config [ 'sample_weight_mode ' ] # because in that case even chunking the array would not make the saving chunk_attr = ' % s % d ' % ( attr , 0 ) try : ' % d bytes : % s ' % ( HDF5_OBJECT_HEADER_LIMIT , optimizer_weights_group [ 'weight_names ' ] ] 'starting with a freshly initialized ' if w.name.split ( '/ ' ) [ -1 ] == 'variable ' : # Arguments model.add ( Dense ( 3 ) ) weight_values = K.batch_get_value ( symbolic_weights ) for name in layer_names : if attr in self.data : else : optimizer_weight_names ] # test with custom optimizer , loss model_layers = model.layers model_config = f.attrs.get ( 'model_config ' ) 'optimizer_weights ' ) else : for path in paths : used for model definition or training . # for Theano and CNTK model = pickle.loads ( pickle.dumps ( model ) ) f.attrs [ 'training_config ' ] = json.dumps ( { 'class_name ' : model.optimizer.__class__.__name__ , ' , '.join ( bad_attributes ) ) ) 'optimizer_config ' : { model_weights_group [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) is_chunked = chunk_attr in self.data.attrs if val and sys.version_info [ 0 ] == 3 and isinstance ( val [ 0 ] , str ) : new_model.train_on_batch ( x , y ) model._make_train_function ( ) weight_values = [ layer_weights [ weight_name ] for weight_name in weight_names ] if isinstance ( model.optimizer , optimizers.TFOptimizer ) : def _deserialize_model ( f , custom_objects=None , compile=True ) : return val def __contains__ ( self , key ) : self.__dict__.update ( model.__dict__ ) else : name = str ( w.name ) def test_pickling_unused_layers_is_ok ( ) : filepath : one of the following : warnings.warn ( 'Error in loading the saved optimizer ' idx = 2 attr = attr.decode ( 'utf-8 ' ) skipif_no_tf_gpu = pytest.mark.skipif ( 'loss_weights ' : model.loss_weights , name = str ( w.name ) opened_new_file = False data_npy = np.asarray ( val ) f = h5dict ( path , mode= ' r ' ) return def test_pickling_without_compilation ( ) : if __name__ == '__main__ ' : 'optimizer attributes or optimizer state ' name = 'param_ ' + str ( i ) elif ' _ { } _pickled'.format ( attr ) in self.data : if isinstance ( val , list ) : 'config ' : model.optimizer.get_config ( ) d = { } model.train_on_batch ( x , y ) Thus the saved model can be reinstantiated in 'TensorFlow optimizers do not ' training_config = json.loads ( training_config.decode ( 'utf-8 ' ) ) group1 = f [ 'group1 ' ] return val loss_weights=loss_weights , f = h5dict ( d ) optimizer_weights_group [ name ] = val else : ' '' in the current model ) was found to ' 'after instantiation . ' self.data [ ' _ { } _pickled'.format ( attr ) ] = True 'loss_weights ' : model.loss_weights , if h5py is None : training_config = f.attrs.get ( 'training_config ' ) raise KeyError ( 'Can not set attribute . ' 'Prefer using a Keras optimizer instead ' optimizer_weight_names = [ metrics = convert_custom_objects ( training_config [ 'metrics ' ] ) def _serialize_model ( model , f , include_optimizer=True ) : model_weights_group = f.create_group ( 'model_weights ' ) model_config = json.loads ( model_config.decode ( 'utf-8 ' ) ) weight_values , string , path to the saved model , or h5py.File or h5py.Group object from which to load the model include_optimizer : If True , save optimizer 's state together . opened_new_file = False the model 's configuration ( topology ) layer_group = model_weights_group [ layer.name ] layers = model.layers if hasattr ( w , 'name ' ) and w.name : # could be chunked # for Theano and CNTK for name , val in zip ( weight_names , weight_values ) : original_keras_version , original_keras_version = ' 1 ' if 'optimizer_weights ' in f : /getting-started/faq/ 'sample_weight_mode ' : model.sample_weight_mode , for both pickling and saving to disk . 'weight_names ' ] = weight_names warnings.warn ( 'No training configuration found in save file : ' optimizer_weights_group = f [ 'optimizer_weights ' ] return H5Dict ( out ) x = Dense ( 2 ) ( inputs ) deserialization . raise ValueError ( 'No model found in config file . ' ) name = str ( w.name ) + ' _ ' + str ( i ) group2 [ ' x ' ] = 'abcd ' if not isinstance ( filepath , h5py.Group ) : f [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) self.data.file.flush ( ) opened_new_file = not isinstance ( filepath , h5py.Group ) model = saving.unpickle_model ( state ) name = str ( w.name ) assert 'group2 ' in group1 model_config = f [ 'model_config ' ] 'Compile it manually . ' ) def test_pickling_right_after_compilation ( ) : 'the model was * not * compiled . ' import sys from keras.layers import Input `` `` '' Save a model to a HDF5 file . for layer in model_layers : ( not K.tensorflow_backend._get_available_gpus ( ) ) , f [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) is_too_big = lambda x : x.nbytes > HDF5_OBJECT_HEADER_LIMIT warnings.warn ( 'TensorFlow optimizers do not ' 'As a result , we can not save the optimizer ' custom_objects : Optional dictionary mapping names reason='Requires TensorFlow backend and a GPU ' ) model : Keras model instance to be serialized . unique_name = name + ' _ ' + str ( idx ) to False , the compilation is omitted without any model = Model ( inputs=inputs , outputs= [ output1 , output2 ] ) return from keras.utils.io_utils import h5dict weight_names.append ( name.encode ( 'utf8 ' ) ) return custom_objects=custom_objects ) return len ( self.data ) if len ( layer_names ) ! = len ( filtered_layers ) : symbolic_weights = layer.weights weight_names.append ( name.encode ( 'utf8 ' ) ) if weight_names : used for model definition or training . for name , val in zip ( weight_names , weight_values ) : 'after loading it . ' val = np.asarray ( val ) return key in self.data self.data.update ( * args ) elif isinstance ( path , str ) : unique_name = name + '_1 ' val = { '_is_group ' : True } assert_allclose ( f [ ' z ' ] , array ) raise RuntimeError ( 'The following attributes can not be saved to ' out2 = new_model.predict ( x ) _serialize_model ( model , f , include_optimizer ) optimizer = optimizers.deserialize ( optimizer_config , 'state . As a result , your model is ' } , default=get_json_type ) .encode ( 'utf8 ' ) optimizer_weights_group [ 'weight_names ' ] = weight_names str ( len ( weight_values ) ) finally : 'However the new layer ' + layer.name the exact same state , without any of the code 'after instantiation . ' training_config = f.get ( 'training_config ' ) def test_sequential_model_pickling_2 ( ) : raise ImportError ( ' ` load_model ` requires h5py . ' ) idx = 2 } , if 'optimizer_weights ' in f : f [ 'training_config ' ] = json.dumps ( { ( K.backend ( ) ! = 'tensorflow ' ) or if hasattr ( w , 'name ' ) : if val.dtype.type == np.string_ : import os n.decode ( 'utf8 ' ) for n in sample_weight_mode = training_config [ 'sample_weight_mode ' ] } , default=get_json_type ) .encode ( 'utf8 ' ) name = 'param_ ' + str ( i ) ValueError : In case of an invalid savefile . h5py.File or h5py.Group object from which to load the model if isinstance ( self.data , type ( out ) ) : model_layers = model.layers if is_np : # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) model_weights_group = f [ 'model_weights ' ] val = self.data [ attr ] layer_names = model_weights_group [ 'layer_names ' ] `` `` '' Model serialization logic . weights = layer.weights in the FAQ for instructions on how to install ` h5py ` . # Flag to check if a dict is user defined data or a sub group : model_config [ 'config ' ] = model.get_config ( ) if self.read_only : .format ( len ( layer_names ) , len ( filtered_layers ) ) return H5Dict ( val ) save_weights_to_hdf5_group ( model_weights_group , model_layers ) assert 'group3 ' in group2 after loading . HDF5_OBJECT_HEADER_LIMIT = 64512 model = model_from_config ( model_config , custom_objects=custom_objects ) for name , val in zip ( weight_names , weight_values ) : if 'keras_version ' in model_weights_group : 'As a result , we can not save the optimizer ' import cPickle as pickle include_optimizer : If True , save optimizer 's state together . metrics= [ metrics.categorical_accuracy ] ) output2 = Dense ( 1 , name='output2 ' ) ( x ) # set weights weight_names = layer_weights [ 'weight_names ' ] model = model_from_config ( model_config , custom_objects=custom_objects ) # Compile model . A Keras model instance . If an optimizer was found while unique_name in weight_names : 'after loading it . ' Note : Please also see return model raise ValueError ( 'No model found in config . ' ) # Recover loss functions and metrics . 'metrics ' : model.metrics , weight_values = K.batch_get_value ( symbolic_weights ) if opened_new_file : def save_model ( model , filepath , overwrite=True , include_optimizer=True ) : def test_sequential_model_pickling ( ) : out = f ( * args , * * kwargs ) model.add ( RepeatVector ( 3 ) ) assert f [ ' x ' ] == 'abcd ' group3 [ ' y ' ] = [ b'efg ' , b'hij ' , b'klmn ' ] # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) 'make it possible to access ' loss_weights = training_config [ 'loss_weights ' ] the model 's weights if is_np : if attr in self.data.attrs : def test_functional_model_pickling ( ) : # test both HDF5 and dict implementations # list < bytes > weight_values = preprocess_weights_for_loading ( layer , model at the target location , or instead def __len__ ( self ) : elif attr in self.data : dataset = self.data.create_dataset ( attr , val.shape , dtype=val.dtype ) assert group2 [ ' x ' ] == 'abcd ' f.attrs [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) # assure that model is working if hasattr ( w , 'name ' ) and w.name : overwrite : Whether we should overwrite any existing # If file exists and should not be overwritten . 'metrics ' : model.metrics , # Save optimizer weights . if opened_new_file : model_config = json.loads ( model_config.decode ( 'utf-8 ' ) ) # This will never loop forever thanks to the test above . state = pickle.dumps ( model ) if hasattr ( w , 'name ' ) and w.name : name = 'param_ ' + str ( i ) def test_pickling_multiple_metrics_outputs ( ) : def iter ( self ) : def load_model ( filepath , custom_objects=None , compile=True ) : 'starting with a freshly initialized ' return _deserialize_model ( f ) if compile : ' ( see keras.io/optimizers ) . ' ) new_model = pickle.loads ( state ) def __getitem__ ( self , attr ) : optimizer_weights_group.attrs [ 'weight_names ' ] ] # Raises `` `` '' Loads a model saved via ` save_model ` . if model_config is None : filtered_layers.append ( layer ) from keras.layers import Dense , Lambda , RepeatVector , TimeDistributed 'HDF5 file because they are larger than ' and makes sense only in the context of model serialization/ f [ ' x ' ] = 'abcd ' ask the user with a manual prompt . x = np.random.random ( ( 1 , 3 ) ) opened_new_file = True layer_weights = model_weights_group [ name ] filepath : one of the following : # Build train function ( to get weight updates ) . raise TypeError ( 'Required Group , str or dict . ' self.data = h5py.File ( path , ) if isinstance ( attr , bytes ) : if isinstance ( val , h5py.Dataset ) : if w.name.split ( '/ ' ) [ -1 ] == 'variable ' : layer_names = filtered_layer_names for k , name in enumerate ( layer_names ) : from numpy.testing import assert_raises The saved model contains : layer = filtered_layers [ k ] filtered_layer_names.append ( name ) while chunk_attr in self.data.attrs : if not overwrite and os.path.isfile ( filepath ) : raise ValueError ( 'Can not create group in read only mode . ' ) for chunk_id , chunk_data in enumerate ( chunked_data ) : if not isinstance ( filepath , h5py.Group ) : This method is used for both writing to HDF5 file/group , self._is_file = True model.compile ( loss=custom_loss , optimizer=custom_opt ( ) , metrics= [ 'acc ' ] ) model : Keras model instance to be saved . overwrite : Whether we should overwrite any existing load_weights_from_hdf5_group ( f [ 'model_weights ' ] , model.layers ) bad_attributes = [ x for x in val if len ( x ) > HDF5_OBJECT_HEADER_LIMIT ] num_chunks = 1 chunk = self.data.attrs [ chunk_attr ] loss = convert_custom_objects ( training_config [ 'loss ' ] ) symbolic_weights = getattr ( model.optimizer , 'weights ' ) model.compile ( loss='mse ' , optimizer='sgd ' , metrics= [ 'acc ' ] ) f : ` keras.utils.hdf5_utils.HFDict ` instance . `` `` '' De-serializes a model serialized via _serialize_model # Expecting this to never be true . param_dset [ : ] = val The saved model contains : metrics= [ metrics.categorical_accuracy ] , model.compile ( optimizer=optimizer , name = str ( w.name ) + ' _ ' + str ( i ) model_config = json.dumps ( model_config , default=get_json_type ) h5py.File or h5py.Group object where to save the model raise ValueError ( 'Layer # ' + str ( k ) f.close ( ) except ValueError : ' containing { } layers into a model with { } layers ' # instantiate optimizer import numpy as np f.file.flush ( ) ValueError : In case of an invalid savefile . val = val.tolist ( ) assert 'group4 ' in group3 if name in weight_names : def __getattr__ ( self , attr ) : weight_values = K.batch_get_value ( symbolic_weights ) optimizer_weights_group = f [ 'optimizer_weights ' ] assert_allclose ( out , out2 , atol=1e-05 ) warnings.warn ( 'Error in loading the saved optimizer ' def test_h5dict_attrs ( h5_path='test.h5 ' ) : the model 's optimizer 's state ( if any ) if symbolic_weights : metrics = convert_custom_objects ( training_config [ 'metrics ' ] ) # We have to remember to unpickle in __getitem__ n.decode ( 'utf8 ' ) for n in return model f = h5dict ( state , mode= ' r ' ) opened_new_file = True import pickle if self.read_only : chunked_data = np.array_split ( data_npy , num_chunks ) raise ValueError ( 'You are trying to load a weight file ' unique_name = name + ' _ ' + str ( idx ) dtype=val.dtype ) proceed = ask_to_proceed_with_overwrite ( filepath ) # scalar 'as part of the model save file . ' 'sample_weight_mode ' : model.sample_weight_mode , group4 = group3 [ 'group4 ' ] 'loss ' : model.loss , 'as part of the model save file . ' `` `` '' A dict-like wrapper around h5py groups ( or dicts ) . # str if isinstance ( self.data , dict ) : array = np.random.random ( ( 3 , 4 , 5 ) ) if num_chunks > 1 : if not overwrite and os.path.isfile ( filepath ) : if not proceed : if hasattr ( w , 'name ' ) : group3 = group2 [ 'group3 ' ] optimizer_weights_group = f [ 'optimizer_weights ' ] f.close ( ) if isinstance ( val , dict ) and val.get ( '_is_group ' ) : raise NotImplementedError x = Dense ( 5 ) ( inputs ) 'class_name ' : model.optimizer.__class__.__name__ , def save_model ( model , filepath , overwrite=True , include_optimizer=True ) : name = unique_name # We batch weight value assignments in a single backend call # instantiate model def __init__ ( self , path , mode= ' a ' ) : def __getstate__ ( self ) : val = [ ] 'You will have to compile your model again ' if isinstance ( model.optimizer , optimizers.TFOptimizer ) : if training_config is None : string , path to the saved model , or f [ ' y ' ] = [ b'efg ' , b'hij ' , b'klmn ' ] loss = convert_custom_objects ( training_config [ 'loss ' ] ) # Check that no item in ` data ` is larger than ` HDF5_OBJECT_HEADER_LIMIT ` weight_names = [ ] val = self.data [ attr ] # Default values of symbolic_weights is /variable metrics=metrics , inputs = Input ( shape= ( 3 , ) ) def __iter__ ( self ) : def __setstate__ ( self , state ) : for i , ( w , val ) in enumerate ( zip ( symbolic_weights , weight_values ) ) : # Set optimizer weights . self.data.close ( ) custom_objects=custom_objects ) [ How can I install HDF5 or h5py to save my models in Keras ? ] ( from .io_utils import h5dict return wrapper ( getattr ( self.data , attr ) ) f.attrs [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) the model 's optimizer 's state ( if any ) val = pickle.loads ( val ) warning . weight_value_tuples = [ ] return ( key in self.data ) or ( key in self.data.attrs ) Note : Please also see the model 's weights y = np.random.random ( ( 1 , 3 ) ) ' ( named `` ' + layer.name optimizer_weight_names = [ reshape=False ) inputs = Input ( shape= ( 5 , ) ) layer_group [ 'weight_names ' ] = weight_names class H5Dict ( object ) : return self [ key ] from keras.models import Model , Sequential def wrapper ( f ) : 'correspond to layer ' + name self.read_only = mode == ' r ' weight_names.append ( name.encode ( 'utf8 ' ) ) ( strings ) to custom classes or functions to be `` `` '' Loads a model saved via ` save_model ` . model = Model ( inputs= [ a , b ] , outputs=c ) 'You will have to compile your model again ' if K.backend ( ) == 'theano ' or K.backend ( ) == 'cntk ' : metrics=metrics , model.optimizer.set_weights ( optimizer_weight_values ) x = np.array ( [ [ 1 , 1 , 1 , 1 , 1 ] ] ) optimizer=optimizers.RMSprop ( lr=0.0001 ) , if attr in self : y = np.random.random ( ( 1 , 3 , 3 ) ) This allows us to have a single serialization logic model_config = { } assert 'group1 ' in f ImportError : if h5py is not available . for i , ( w , val ) in enumerate ( zip ( symbolic_weights , if not proceed : for layer in layers : def update ( self , * args ) : There are lot of edge cases which have been hardcoded , self._is_file = False def close ( self ) : val.extend ( [ x.decode ( 'utf8 ' ) for x in chunk ] ) metrics = { 'output1 ' : [ 'mse ' , 'binary_accuracy ' ] , weight_value_tuples += zip ( symbolic_weights , weight_values ) unique_name = name + '_1 ' as part of the saved model , the model is already 'Compile it manually . ' ) # ndarray string , path where to save the model , or model_config [ 'class_name ' ] = model.__class__.__name__ h5dict = H5Dict model.add ( Dense ( 2 , input_shape= ( 3 , ) ) ) while unique_name in weight_names : from keras import losses def __setitem__ ( self , attr , val ) : if include_optimizer and model.optimizer : # test that new updates are the same with both models finally : optimizer_weights_group.attrs [ try : try : self.data.attrs [ attr ] = val 'optimizer_config ' : { is_np = type ( val ) .__module__ == np.__name__ # Default values of symbolic_weights is /variable warnings.warn ( 'No training configuration found in save file : ' import tempfile c = Lambda ( lambda x : x [ : , : , : , :1 ] ) ( a ) 'make it possible to access ' f = h5py.File ( filepath , mode= ' r ' ) include_optimizer : If True , serialize optimizer 's state together . else : from keras import backend as K from keras import optimizers 'output2 ' : [ 'mse ' , 'binary_accuracy ' ] /getting-started/faq/ weight_names = [ ] f = h5dict ( filepath , ' r ' ) return h5wrapper optimizer_weight_values = [ optimizer_weights_group [ n ] for n in ` keras.utils.hdf5_utls.H5Dict ` object , which can wrap HDF5 if len ( weight_values ) ! = len ( symbolic_weights ) : # possible . return default Thus the saved model can be reinstantiated in model = _deserialize_model ( f , custom_objects , compile ) outputs = Dense ( 3 ) ( x ) optimizer_weight_values = [ optimizer_weights_group [ n ] for n in model = pickle.loads ( state ) while any ( map ( is_too_big , chunked_data ) ) : if not val.shape : original_backend = model_weights_group [ 'backend ' ] .decode ( 'utf8 ' ) the model 's configuration ( topology ) `` `` '' custom_opt = optimizers.rmsprop if opened_new_file : 'optimizer . ' ) if is_chunked : param_dset [ ( ) ] = val assert group3 [ ' y ' ] == [ b'efg ' , b'hij ' , b'klmn ' ] files , groups and dicts with a common API . # scalar idx += 1 # Compile model . model.compile ( optimizer=optimizer , group4 [ ' z ' ] = array model at the target location , or instead name , chunk_id = 0 if weights : optimizer_weights_group = f.create_group ( if training_config is None : 'Prefer using a Keras optimizer instead ' if model_config is None : self.data [ attr ] = val # Returns model : Keras model instance to be saved . ' expects ' + str ( len ( symbolic_weights ) ) raise ImportError ( ' ` save_model ` requires h5py . ' ) ' Group with name { } exists . '.format ( attr ) ) f = h5dict ( filepath , mode= ' w ' ) model_weights_group [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) raise ImportError ( ' ` load_model ` requires h5py . ' ) # convert to bytes Note : This is not intended to be a generic wrapper . `` `` '' Save a model to a HDF5 file . if self._is_file : name = unique_name ' ( see keras.io/optimizers ) . ' ) out = model.predict ( x ) warnings.warn ( optimizer_config = training_config [ 'optimizer_config ' ] f.close ( ) from keras import metrics # which provides a speedup in TensorFlow . sample_weight_mode='temporal ' ) 'state . As a result , your model is ' weight_values ) ) : def test_h5dict_groups ( h5_path='test.h5 ' ) : def pickle_model ( model ) : return d optimizer_weight_names ] compiled . Otherwise , the model is uncompiled and symbolic_weights = getattr ( model.optimizer , 'weights ' ) # Build train function ( to get weight updates ) . if name in weight_names : if sys.version_info [ 0 ] == 3 : else : filtered_layer_names = [ ] import pytest compile : Boolean , whether to compile the model if type ( val ) .__module__ == np.__name__ : } , loss=loss , if key in self : ' weights , but the saved weights have '","['keras/engine/network.py', 'keras/engine/saving.py', 'keras/utils/__init__.py', 'keras/utils/io_utils.py', 'tests/keras/utils/io_utils_test.py', 'tests/test_model_pickling.py']",[ RELNOTES ] [ P ] Make Keras models pickle-able ( refactored ) ( # 11030 )
293,5a6af4bc6d44e9adbc2a21804bfcd18c4ce849ef,2018-09-11 10:32:43-07:00,"dtype=K.floatx ( ) ) # flatten y_true in case it 's in shape ( num_samples , 1 ) instead of ( num_samples , ) y_a = K.variable ( np.random.randint ( 0 , 7 , ( 6 , ) ) , dtype=K.floatx ( ) ) categorical_acc = metrics.categorical_accuracy ( y_a_dense_labels , y_b ) return K.cast ( K.equal ( K.max ( y_true , axis=-1 ) , def test_sparse_categorical_accuracy_correctness ( ) : return K.cast ( K.equal ( K.flatten ( y_true ) , # use one_hot embedding to convert sparse labels to equivalent dense labels assert np.allclose ( K.eval ( sparse_categorical_acc ) , K.eval ( categorical_acc ) ) y_a_dense_labels = K.cast ( K.one_hot ( K.cast ( y_a , dtype='int32 ' ) , num_classes=7 ) , sparse_categorical_acc = metrics.sparse_categorical_accuracy ( y_a , y_b ) y_b = K.variable ( np.random.random ( ( 6 , 7 ) ) , dtype=K.floatx ( ) )","['keras/metrics.py', 'tests/keras/metrics_test.py']",fix sparse categorical acc ( # 11100 )
294,c913b6da92f6ab9a3f4c897caa4085e782a14680,2018-09-11 18:48:27+02:00,"return np.sum ( x , axis=axis , keepdims=keepdims ) axis : An integer , the axis to find maximum values . axis : An integer or list of integers in [ -rank ( x ) , rank ( x ) ) , check_single_tensor_operation ( 'all ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) return np.mean ( x , axis=axis , keepdims=keepdims ) the mean over all dimensions . the logical and over all dimensions . return np.max ( x , axis=axis , keepdims=keepdims ) return x x = np.sum ( x , axis=a , keepdims=keepdims ) for a in axis : x = np.mean ( x , axis=a , keepdims=keepdims ) the axes to compute the standard deviation . If ` None ` ( default ) , the axes to compute the variance . If ` None ` ( default ) , computes the axes to compute the mean . If ` None ` ( default ) , computes x = np.std ( x , axis=a , keepdims=keepdims ) return np.min ( x , axis=axis , keepdims=keepdims ) check_single_tensor_operation ( 'logsumexp ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) the axes to find maximum values . If ` None ` ( default ) , finds the check_single_tensor_operation ( 'any ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) return np.var ( x , axis=axis , keepdims=keepdims ) return np.std ( x , axis=axis , keepdims=keepdims ) the axes to compute the product . If ` None ` ( default ) , computes return np.mean ( x , axis=axis , keepdims=keepdims ) axis : axis along which to perform the reduction . check_single_tensor_operation ( 'var ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) check_single_tensor_operation ( 'logsumexp ' , ( 4 , 2 ) , WITH_NP ) x = np.prod ( x , axis=a , keepdims=keepdims ) axis : An integer , the axis to sum over . axis : An integer , the axis to reduce over . the axes to sum over . If ` None ` ( default ) , sums over all return np.prod ( x , axis=axis , keepdims=keepdims ) return np.prod ( x , axis=axis , keepdims=keepdims ) axis : An integer , the axis to compute the product . axis : An integer , the axis to compute the standard deviation . axis : A list of integer . Axes to compute the mean . check_single_tensor_operation ( 'max ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) return sp.misc.logsumexp ( x , axis=axis , keepdims=keepdims ) minimum over all dimensions . check_single_tensor_operation ( 'std ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) import scipy as sp computes the standard deviation over all dimensions . def var ( x , axis=None , keepdims=False ) : axis : An integer , the axis to compute the variance . the axes to compute the logsumexp . If ` None ` ( default ) , computes the product over all dimensions . x = np.max ( x , axis=a , keepdims=keepdims ) x = np.min ( x , axis=a , keepdims=keepdims ) check_single_tensor_operation ( 'var ' , ( 4 , 2 ) , WITH_NP ) def logsumexp ( x , axis=None , keepdims=False ) : axis = tuple ( axis ) dimensions . check_single_tensor_operation ( 'logsumexp ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) the axes to find minimum values . If ` None ` ( default ) , finds the axis : An integer , the axis to find minimum values . the axes to compute the logical and . If ` None ` ( default ) , computes the axes to compute the logical or . If ` None ` ( default ) , computes the variance over all dimensions . axis : axis : An integer or list of integers in [ -rank ( x ) , rank ( x ) ) , else : return np.min ( x , axis=axis , keepdims=keepdims ) the logsumexp over all dimensions . return np.sum ( x , axis=axis , keepdims=keepdims ) maximum over all dimensions . return np.max ( x , axis=axis , keepdims=keepdims ) the logical or over all dimensions . return np.std ( x , axis=axis , keepdims=keepdims ) check_single_tensor_operation ( 'var ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) if isinstance ( axis , list ) :","['keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Support multiple axes for some operations ( tests and docs ) ( # 11114 )
295,244546c2fe5165b6770eb456afd5fac8878473c5,2018-09-10 10:50:17-07:00,"self.session_kwargs = session_kwargs.copy ( ) options=run_options , return self._legacy_call ( inputs ) outputs= [ x_placeholder + y_placeholder ] , StrictVersion ( tf.__version__.split ( '- ' ) [ 0 ] ) < StrictVersion ( ' 1.10.0 ' ) ) : raise ValueError ( 'In order to feed symbolic tensors to a Keras model and set ' # ` run_metadata ` keyword argument since TF 1.10 assert output == [ 30 . ] from tensorflow.core.protobuf import config_pb2 def test_function_tf_run_options_with_run_metadata ( self ) : fetched = self._callable_fn ( * array_vals ) # callable generated by Session._make_callable_from_options accepts self.session_kwargs = session_kwargs if py_any ( is_tensor ( x ) for x in inputs ) : self.run_metadata = session_kwargs.pop ( 'run_metadata ' , None ) output = f ( [ 10. , 20 . ] ) else : x_placeholder = K.placeholder ( shape= ( ) ) # enable run_options . if self.run_options : fetched = self._callable_fn ( * array_vals , run_metadata=self.run_metadata ) ' ` run_metadata ` , you need tensorflow 1.10 or higher . ' ) run_options = config_pb2.RunOptions ( output_partition_graphs=True ) if ( self.run_metadata and # Handle run_options . # disable run_options . # self.session_kwargs is used for _legacy_call assert len ( run_metadata.partition_graphs ) > 0 run_metadata=run_metadata ) fetched = self._callable_fn ( * array_vals ) run_metadata = config_pb2.RunMetadata ( ) reason='Uses the ` options ` and ` run_metadata ` arguments . ' ) y_placeholder = K.placeholder ( shape= ( ) ) assert len ( run_metadata.partition_graphs ) == 0 if self.run_metadata : callable_opts.run_options.CopyFrom ( self.run_options ) f = K.function ( inputs= [ x_placeholder , y_placeholder ] , self.run_options = session_kwargs.pop ( 'options ' , None )","['keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Fix TF Function accepts session_kwargs : options and run_metadata ( # 11107 )
296,9400be98783135a1d42dd238f4e6c3aa048eceea,2018-09-06 14:49:22-07:00,"from keras.models import Sequential raise ValueError ( ' '' input_length '' is % s , but received input has shape % s ' % model = Sequential ( [ Embedding ( raise ValueError ( ' '' input_length '' is % s , but received input has shape % s ' % # len ( input_length ) should be equal to len ( input_shape ) - 1 input_shape= ( 3 , 4 , 5 ) ) ] ) # input_length should be equal to input_shape [ 1 : ] ( str ( self.input_length ) , str ( input_shape ) ) ) ( str ( self.input_length ) , str ( input_shape ) ) ) ( str ( self.input_length ) , str ( input_shape ) ) ) with pytest.raises ( ValueError ) : ValueError ( ' '' input_length '' is % s , but received input has shape % s ' % input_length=2 , input_dim=10 , ValueError ( ' '' input_length '' is % s , but received input has shape % s ' % input_shape= ( 3 , 5 ) ) ] ) def test_embedding_invalid ( ) : ( str ( self.input_length ) , str ( input_shape ) ) ) output_dim=4 ,","['keras/layers/embeddings.py', 'tests/keras/layers/embeddings_test.py']",Add missing raise for ValueError at Embedding compute_output_shape ( # 11091 )
297,c2a6caae852adb0c832da86d5815c26b2e7d24c6,2018-09-05 16:09:09-07:00,"class UpSampling1D ( _UpSampling ) : def compute_output_shape ( self , input_shape ) : 'data_format ' : self.data_format } dim3 ) class _UpSampling ( Layer ) : def __init__ ( self , size , data_format=None , * * kwargs ) : inputs with shape ` ( batch , channels , ... ) ` . self.input_spec = InputSpec ( ndim=5 ) self.size = size base_config = super ( UpSampling1D , self ) .get_config ( ) width = self.size [ 1 ] * input_shape [ 2 ] if input_shape [ 2 ] is not None else None spatial_axes ) spatial_axes = list ( range ( 1 , 1 + self.rank ) ) # self.rank is 1 for UpSampling1D , 2 for UpSampling2D . config [ 'size ' ] = self.size [ 0 ] self.data_format = K.normalize_data_format ( data_format ) return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) config = { 'size ' : self.size , data_format : A string , output = K.repeat_elements ( inputs , self.size [ 0 ] , axis=1 ) dim2 , class UpSampling1D ( Layer ) : one of ` `` channels_last '' ` or ` `` channels_first '' ` . self.input_spec = InputSpec ( ndim=4 ) width , self.data_format = K.normalize_data_format ( data_format ) config = super ( UpSampling2D , self ) .get_config ( ) dim1 = self.size [ 0 ] * input_shape [ 2 ] if input_shape [ 2 ] is not None else None super ( _UpSampling , self ) .__init__ ( * * kwargs ) class UpSampling3D ( _UpSampling ) : dim3 , super ( UpSampling3D , self ) .__init__ ( normalized_size , data_format , * * kwargs ) output_shape [ dim ] * = size_all_dims [ dim ] The ordering of the dimensions in the inputs . elif self.data_format == 'channels_last ' : dim3 = self.size [ 2 ] * input_shape [ 3 ] if input_shape [ 3 ] is not None else None height = self.size [ 0 ] * input_shape [ 1 ] if input_shape [ 1 ] is not None else None height , output = K.repeat_elements ( inputs , self.size , axis=1 ) input_shape [ 3 ] ) config = { 'size ' : self.size } self.size = conv_utils.normalize_tuple ( size , 2 , 'size ' ) normalized_size = conv_utils.normalize_tuple ( size , 3 , 'size ' ) super ( UpSampling2D , self ) .__init__ ( * * kwargs ) Keras config file at ` ~/.keras/keras.json ` . width = self.size [ 1 ] * input_shape [ 3 ] if input_shape [ 3 ] is not None else None def get_config ( self ) : return ( input_shape [ 0 ] , dim1 = self.size [ 0 ] * input_shape [ 1 ] if input_shape [ 1 ] is not None else None input_shape [ 1 ] , config = super ( UpSampling1D , self ) .get_config ( ) self.input_spec = InputSpec ( ndim=self.rank + 2 ) return tuple ( output_shape ) ` `` channels_last '' ` corresponds to inputs with shape dim2 = self.size [ 1 ] * input_shape [ 3 ] if input_shape [ 3 ] is not None else None def call ( self , inputs ) : dim3 = self.size [ 2 ] * input_shape [ 4 ] if input_shape [ 4 ] is not None else None output_shape = list ( input_shape ) size : Tuple of ints . class UpSampling2D ( Layer ) : ` ( batch , ... , channels ) ` while ` `` channels_first '' ` corresponds to if output_shape [ dim ] is not None : class UpSampling2D ( _UpSampling ) : for dim in range ( len ( output_shape ) ) : self.rank = len ( size ) `` `` '' config.pop ( 'data_format ' ) def compute_output_shape ( self , input_shape ) : base_config = super ( UpSampling3D , self ) .get_config ( ) self.size = conv_utils.normalize_tuple ( size , 3 , 'size ' ) config [ 'interpolation ' ] = self.interpolation if self.data_format == 'channels_first ' : super ( UpSampling1D , self ) .__init__ ( * * kwargs ) If you never set it , then it will be `` channels_last '' . dim1 , base_config = super ( UpSampling2D , self ) .get_config ( ) It defaults to the ` image_data_format ` value found in your normalized_size = conv_utils.normalize_tuple ( size , 2 , 'size ' ) config = { 'size ' : self.size , return config return ( input_shape [ 0 ] , size , input_shape [ 2 ] ) raise NotImplementedError self.data_format , dim2 = self.size [ 1 ] * input_shape [ 2 ] if input_shape [ 2 ] is not None else None size_all_dims = ( 1 , ) + self.size + ( 1 , ) # Arguments super ( UpSampling2D , self ) .__init__ ( normalized_size , data_format , * * kwargs ) class UpSampling3D ( Layer ) : super ( UpSampling3D , self ) .__init__ ( * * kwargs ) base_config = super ( _UpSampling , self ) .get_config ( ) height = self.size [ 0 ] * input_shape [ 2 ] if input_shape [ 2 ] is not None else None self.size = int ( size ) `` `` '' Abstract nD UpSampling layer ( private , used as implementation base ) . 'data_format ' : self.data_format } size_all_dims = transpose_shape ( size_all_dims , self.input_spec = InputSpec ( ndim=3 ) def get_config ( self ) : return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) size = self.size * input_shape [ 1 ] if input_shape [ 1 ] is not None else None super ( UpSampling1D , self ) .__init__ ( ( int ( size ) , ) , 'channels_last ' , * * kwargs ) width ) input_shape [ 4 ] )",['keras/layers/convolutional.py'],Made an abstract class for upsampling . ( # 11065 )
298,23c20f756be07a9e92d9513181eca850fcf2af62,2018-09-05 16:00:49-07:00,"# Plot training & validation loss values plt.plot ( history.history [ 'val_acc ' ] ) history = model.fit ( x , y , validation_split=0.25 , epochs=50 , batch_size=16 , verbose=1 ) plt.title ( 'Model loss ' ) plt.xlabel ( 'Epoch ' ) plt.ylabel ( 'Accuracy ' ) # Plot training & validation accuracy values # # Training history visualization plt.title ( 'Model accuracy ' ) plt.show ( ) `` ` `` ` python The ` fit ( ) ` method on a Keras ` Model ` returns a ` History ` object . The ` History.history ` attribute is a dictionary recording training loss values and metrics values at successive epochs , as well as validation loss values and validation metrics values ( if applicable ) . Here is a simple example using ` matplotlib ` to generate loss & accuracy plots for training & validation : plt.plot ( history.history [ 'loss ' ] ) plt.legend ( [ 'Train ' , 'Test ' ] , loc='upper left ' ) plt.ylabel ( 'Loss ' ) plt.plot ( history.history [ 'val_loss ' ] ) plt.plot ( history.history [ 'acc ' ] ) import matplotlib.pyplot as plt",['docs/templates/visualization.md'],Section for Training history visualization ( # 11076 )
299,2780ab0c9712b41c34693758141f30ae3d3c2e7f,2018-09-05 15:57:14-07:00,"Skip the data if it is ` None ` . shuffle=False ) : if _GENERATOR_COUNTER is None : Generator yielding tuples ` ( inputs , targets ) ` ( when full , workers could block on ` put ( ) ` ) # For new processes that may spawn # For new processes that may spawn `` `` '' initializer=init_pool , self.executor_fn = self._get_executor_init ( workers ) self.run_thread = None # Arguments self.uid = _GENERATOR_COUNTER.value _SHARED_GENERATOR [ self.uid ] = None super ( GeneratorEnqueuer , self ) .__init__ ( sequence , use_multiprocessing ) raise NotImplementedError `` `` '' self.queue.queue.clear ( ) self.queue = None def _send_generator ( self ) : # We use a Value to provide unique id to different processes . use_multiprocessing : use multiprocessing if True , otherwise threading workers : number of worker threads shuffle : whether to shuffle the data at the beginning of each epoch Should be called by the same thread which called ` start ( ) ` . self.stop_signal.set ( ) _SHARED_SEQUENCES = gens def __init__ ( self , sequence , use_multiprocessing=False , shuffle=False ) : `` `` '' Start the handler 's workers . super ( OrderedEnqueuer , self ) .__init__ ( sequence , use_multiprocessing ) # We do not need the init since it 's threads . self.queue.not_full.notify ( ) wait_time=None , timeout : maximum time to wait on thread.join ( ) `` `` '' Submits request to the executor and queue the ` Future ` objects . '' '' '' _GENERATOR_COUNTER = mp.Value ( ' i ' , 0 ) Skip the data if it is ` None ` . initializer=init_pool_generator , with closing ( self.executor_fn ( _SHARED_SEQUENCES ) ) as executor : if isinstance ( _GENERATOR_COUNTER , int ) : initargs= ( seqs , ) ) self.stop_signal = None def get ( self ) : _GENERATOR_COUNTER.value += 1 def is_running ( self ) : `` `` '' Stop running threads and wait for them to exit , if necessary . timeout : maximum time to wait on ` thread.join ( ) ` def __init__ ( self , generator , return lambda seqs : mp.Pool ( workers , initargs= ( seqs , ) ) `` `` '' Get the Pool initializer for multiprocessing . or ` ( inputs , targets , sample_weights ) ` . initializer=init_pool_generator , return six.next ( _SHARED_SEQUENCES [ uid ] ) Used in ` fit_generator ` , ` evaluate_generator ` , ` predict_generator ` . self.queue.unfinished_tasks = 0 def get ( self ) : shuffle : whether to shuffle the data at the beginning of each epoch sequence : A ` keras.utils.data_utils.Sequence ` object . _SHARED_GENERATOR [ self.uid ] = self.generator self.run_thread.join ( timeout ) return self.stop_signal is not None and not self.stop_signal.is_set ( ) # multiprocessing . We resort to an int initargs= ( seqs , self.random_seed ) ) with closing ( self.executor_fn ( _SHARED_GENERATOR ) ) as executor : Used in ` fit_generator ` , ` evaluate_generator ` , ` predict_generator ` . _GENERATOR_COUNTER = 0 else : self.queue.unfinished_tasks = 0 initargs= ( gens , global _SHARED_GENERATOR _GENERATOR_COUNTER += 1 self.use_multiprocessing = use_multiprocessing # Arguments Function , a Function to initialize the pool _SHARED_SEQUENCES [ self.uid ] = self.sequence use_multiprocessing=False ) : with self.queue.mutex : global _GENERATOR_COUNTER self.workers = 0 class OrderedEnqueuer ( SequenceEnqueuer ) : `` `` '' Send current Sequence to all workers . '' '' '' self.queue = queue.Queue ( max_queue_size ) class OrderedEnqueuer ( SequenceEnqueuer ) : sequence : A ` keras.utils.data_utils.Sequence ` object . def _get_executor_init ( self , workers ) : `` `` '' Creates a generator to extract data from the queue . `` `` '' Builds a Enqueuer from a Sequence . `` `` '' Send current Iterable to all workers . '' '' '' max_queue_size : queue size `` `` '' self.shuffle = shuffle `` `` '' Stops running threads and wait for them to exit , if necessary . # Arguments self.executor_fn = lambda _ : ThreadPool ( workers ) if self.use_multiprocessing : _SHARED_SEQUENCES [ self.uid ] = None `` `` '' # Returns _SHARED_SEQUENCES [ self.uid ] = self.sequence def stop ( self , timeout=None ) : use_multiprocessing=False , self.generator = generator ( when full , threads could block on ` put ( ) ` ) . def _send_sequence ( self ) : self.executor_fn = lambda seqs : mp.Pool ( workers , def start ( self , workers=1 , max_queue_size=10 ) : self.queue.queue.clear ( ) # Global variables to be shared across processes self.random_seed ) ) # In this case the OS does not allow us to use _SHARED_GENERATOR = { } # Doing Multiprocessing.Value += x is not process-safe . def stop ( self , timeout=None ) : def _send_sequence ( self ) : self.run_thread = threading.Thread ( target=self._run ) initializer=init_pool , `` `` '' Builds a Enqueuer from a Sequence . self.uid = _GENERATOR_COUNTER self.executor_fn = lambda gens : mp.Pool ( workers , raise NotImplementedError with _GENERATOR_COUNTER.get_lock ( ) : self.stop_signal.set ( ) Should be called by the same thread which called start ( ) . # Arguments self.workers = workers `` `` '' Stops running threads and wait for them to exit , if necessary . Should be called by the same thread which called ` start ( ) ` . self.run_thread.start ( ) Generator yielding tuples ` ( inputs , targets ) ` except OSError : return six.next ( _SHARED_GENERATOR [ uid ] ) self.run_thread.daemon = True self.shuffle = shuffle use_multiprocessing : use multiprocessing if True , otherwise threading def __init__ ( self , sequence , use_multiprocessing=False , wait_time=None , self._send_generator ( ) # Share the initial generator _SHARED_GENERATOR = gens # for enqueuer indexing . self._send_sequence ( ) # Share the initial generator _GENERATOR_COUNTER = None try : `` `` '' Send current generator to all workers . '' '' '' `` `` '' Creates a generator to extract data from the queue . _SHARED_SEQUENCES [ self.uid ] = None or ` ( inputs , targets , sample_weights ) ` . self.queue.not_full.notify ( ) # Returns with self.queue.mutex : global _SHARED_SEQUENCES @ abstractmethod self.executor_fn = None `` `` '' Starts the handler 's workers . def _run ( self ) : self.run_thread.join ( timeout ) self.stop_signal = threading.Event ( ) timeout : maximum time to wait on ` thread.join ( ) `",['keras/utils/data_utils.py'],Refactor Enqueuers ( # 11079 )
300,9e1a0df3cd1d1bbbe7f9010c4c943bedfdfc3487,2018-09-05 15:51:27-07:00,"model.compile ( 'rmsprop ' , 'mse ' ) if has_arg ( layer.call , 'training ' ) : model.train_on_batch ( input_data , actual_output ) model.compile ( 'rmsprop ' , 'mse ' ) model.train_on_batch ( input_data , actual_output ) # test training mode ( e.g . useful for dropout tests ) # test training mode ( e.g . useful when the layer has a # different behavior at training and testing time ) .",['keras/utils/test_utils.py'],Skipped the training tests when the layer has the same behavior at training and testing time . ( # 11089 )
301,a9d9595101f7e38780f197dfde643fcfcc0814d3,2018-09-05 14:04:59-07:00,"| [ VGG19 ] ( # vgg19 ) | 549 MB | 0.713 | 0.900 | 143,667,240 | 26 | | [ NASNetMobile ] ( # nasnet ) | 23 MB | 0.744 | 0.919 | 5,326,716 | - | | [ MobileNet ] ( # mobilenet ) | 17 MB | 0.665 | 0.871 | 4,253,864 | 88 | [ ResNet50 ] ( # resnet50 ) | 99 MB | 0.749 | 0.921 | 25,636,712 | 168 | | [ InceptionV3 ] ( # inceptionv3 ) | 92 MB | 0.779 | 0.937 | 23,851,784 | 159 | | [ VGG16 ] ( # vgg16 ) | 528 MB | 0.713 | 0.901 | 138,357,544 | 23 | | [ MobileNet ] ( # mobilenet ) | 16 MB | 0.704 | 0.895 | 4,253,864 | 88 | | [ DenseNet121 ] ( # densenet ) | 33 MB | 0.750 | 0.923 | 8,062,504 | 121 | | [ DenseNet169 ] ( # densenet ) | 57 MB | 0.759 | 0.928 | 14,307,880 | 169 | [ DenseNet201 ] ( # densenet ) | 80 MB | 0.773 | 0.936 | 20,242,984 | 201 | | [ Xception ] ( # xception ) | 88 MB | 0.790 | 0.945| 22,910,480 | 126 | | [ DenseNet121 ] ( # densenet ) | 33 MB | 0.745 | 0.918 | 8,062,504 | 121 | [ VGG16 ] ( # vgg16 ) | 528 MB| 0.715 | 0.901 | 138,357,544 | 23 | [ ResNet50 ] ( # resnet50 ) | 99 MB | 0.759 | 0.929 | 25,636,712 | 168 | [ VGG19 ] ( # vgg19 ) | 549 MB | 0.727 | 0.910 | 143,667,240 | 26 | [ InceptionResNetV2 ] ( # inceptionresnetv2 ) | 215 MB | 0.803 | 0.953 | 55,873,736 | 572 | | [ DenseNet169 ] ( # densenet ) | 57 MB | 0.762 | 0.932 | 14,307,880 | 169 | | [ DenseNet201 ] ( # densenet ) | 80 MB | 0.770 | 0.933 | 20,242,984 | 201 | [ InceptionResNetV2 ] ( # inceptionresnetv2 ) | 215 MB | 0.804 | 0.953 | 55,873,736 | 572 | | [ NASNetLarge ] ( # nasnet ) | 343 MB | 0.825 | 0.960 | 88,949,818 | - | | [ InceptionV3 ] ( # inceptionv3 ) | 92 MB | 0.788 | 0.944 | 23,851,784 | 159 | | [ Xception ] ( # xception ) | 88 MB | 0.790 | 0.945 | 22,910,480 | 126 | | [ MobileNetV2 ] ( # mobilenetv2 ) | 14 MB | 0.713 | 0.901 | 3,538,984 | 88 |",['docs/templates/applications.md'],Revise the performance table for applications ( # 11085 )
302,66f8cc7ac4942f7f9fe0164a2a854a6264b87735,2018-09-05 14:03:54-07:00,"a ` output_size ` attribute . This can be a single integer or a output_dim = state_size [ 0 ] nested_states = nested_states [ : :-1 ] stacked_cell = recurrent.StackedRNNCells ( elif hasattr ( cell.state_size , '__len__ ' ) : # States are a flat list ( None , 6 ) ] ( ` state_size [ 0 ] ` ) should be the same as ( one size per state ) . for cell in self.cells [ : :-1 ] if self.reverse_state_order else self.cells : # e.g . states of a 2-layer LSTM would be ` [ h1 , c1 , h2 , c2 ] ` . def call ( self , inputs , states ) : states = [ ] self.reverse_state_order = kwargs.pop ( 'reverse_state_order ' , False ) self.output_size = input_shape [ -1 ] stacked_cell , return_state=True , return_sequences=True ) np.zeros ( ( batch , input_size ) ) ) # theano does not support static shape inference . assert cell.state_size == state_size new_nested_states = new_nested_states [ : :-1 ] # in reverse order of the cell stack . if getattr ( self.cells [ -1 ] , 'output_size ' , None ) is not None : states += cell_states assert layer.cell.state_size == ( 8 , 8 , 16 , 16 , 32 , 32 ) for cell_states in new_nested_states [ : :-1 ] : ( one size per state ) . In this case , the first entry batch = 32 # ` stack.state_size [ 0 ] == output_dim ` . ( None , 6 ) , # States are a flat list of the individual cell state size . 'work with the natural order of states if you ' # Test reverse_state_order = True for stacked cell . def __init__ ( self , num_unit , * * kwargs ) : def build ( self , input_shape ) : def test_inconsistent_output_state_size ( ) : cell = PlusOneRNNCell ( state_size ) output_dim = state_size [ 0 ] for the cell , the value will be inferred by the first element output_shape = layer.compute_output_shape ( ( None , timesteps , embedding_dim ) ) cells , reverse_state_order=True ) This cell is used for testing state_size and output_size . '' '' '' of the ` state_size ` . class PlusOneRNNCell ( keras.layers.Layer ) : 'eg ` RNN ( return_state=True ) ` . ' ) x = keras.Input ( ( None , input_size ) ) model.train_on_batch ( y = layer ( x ) self.state_size = num_unit nested_states = nested_states [ : :-1 ] if getattr ( self.cell , 'output_size ' , None ) is not None : if hasattr ( cell.state_size , '__len__ ' ) : for cell in self.cells [ : :-1 ] : input_size = 6 assert K.int_shape ( init_state [ 0 ] ) == ( None , state_size ) if K.backend ( ) ! = 'theano ' : # In the case of reverse_state_order=True , the state_size will be 'will soon be deprecated . Please update the code to ' assert len ( init_state ) == 1 warnings.warn ( ' ` reverse_state_order=True ` in ` StackedRNNCells ` ' return inputs , states return self.cells [ -1 ] .state_size ( None , 3 ) , new_output = n_s [ -1 ] super ( PlusOneRNNCell , self ) .__init__ ( * * kwargs ) assert model.output_shape == ( None , input_size ) def output_size ( self ) : expected_output_shape = [ ( None , timesteps , 6 ) , the size of the cell output . new_states += cell_states time_step = 4 return inputs + 1 , [ states [ 0 ] + 1 ] # reverse order of the cells ' state . User might want to set this to True return self.cells [ -1 ] .state_size [ 0 ] `` `` '' Add one to the input and state . backward compatible reason , if this attribute is not available # ` [ h2 , c2 , h1 , c1 ] ` for cell_states in new_nested_states : # order of state_size . output_dim = cell.output_size layer = recurrent.RNN ( cell ) return self.cells [ -1 ] .output_size assert output_shape == expected_output_shape init_state = layer.get_initial_state ( x ) self.output_size = self.units assert layer.cell.state_size == ( 32 , 32 , 16 , 16 , 8 , 8 ) layer = recurrent.RNN ( # e.g . states of a 2-layer LSTM would be np.zeros ( ( batch , time_step , input_size ) ) , else : return inputs , new_states if self.reverse_state_order : model = keras.models.Model ( x , y ) state_size = 5 new_states = [ ] # ` [ h2 , c2 , h1 , c1 ] ` . output_dim = self.cell.output_size new_output = n_s [ 0 ] model.compile ( optimizer='rmsprop ' , loss='mse ' ) # reverse_state_order determines whether the state size will be in a TensorShape , which represent the shape of the output . For if hasattr ( self.cells [ -1 ] .state_size , '__len__ ' ) : # This allows to preserve the requirement # ` RNN ( return_state=True ) ` since the state will be returned as the same if getattr ( cell , 'output_size ' , None ) is not None : # to keep the existing behavior . This is only useful when use 'reply on the RNN states , '","['keras/backend/cntk_backend.py', 'keras/layers/recurrent.py', 'tests/keras/layers/recurrent_test.py']",Update the RNN cell API to be explicit about output_size . ( # 11021 )
303,f9210387088fe91b5bc8999cf0cb41a0fe9eacf6,2018-09-05 11:23:51+09:00,"cond_float = cond_float [ ... , None ] for k in WITH_NP : for k in BACKENDS : for k in WITH_NP : for k in BACKENDS : cond_float = condition.astype ( floatx ( ) ) def switch ( condition , then_expression , else_expression ) : while cond_float.ndim < then_expression.ndim : return cond_float * then_expression + ( 1 - cond_float ) * else_expression","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Made the backend tests faster by using the numpy backend in the ( # 11062 )
304,1915f106fe94a65cab3887c5842cf520f8adedf3,2018-09-03 14:46:21-07:00,"return x check_two_tensor_operation ( 'in_train_phase ' , ( 3 , 3 ) , ( 2 , 2 ) , [ KTH , KTF ] , if callable ( x ) : return alt ( ) check_two_tensor_operation ( 'in_train_phase ' , ( 2 , 3 ) , ( 2 , 3 ) , BACKENDS , return x ( ) if callable ( alt ) : def set_learning_phase ( value ) : return alt def test_in_test_phase ( self ) : else : check_two_tensor_operation ( 'in_test_phase ' , ( 3 , 3 ) , ( 2 , 2 ) , [ KTH , KTF ] , check_two_tensor_operation ( 'in_test_phase ' , ( 3 , 3 ) , ( 2 , 2 ) , WITH_NP , def in_test_phase ( x , alt , training=None ) : training = learning_phase ( ) def test_in_test_phase ( self , training ) : if training is None : check_two_tensor_operation ( 'in_test_phase ' , ( 2 , 3 ) , ( 2 , 3 ) , BACKENDS , global _LEARNING_PHASE if training is 1 or training is True : training=training ) for training in [ True , False ] : return _LEARNING_PHASE return in_train_phase ( alt , x , training=training ) check_two_tensor_operation ( 'in_train_phase ' , ( 2 , 3 ) , ( 2 , 3 ) , WITH_NP , def test_in_train_phase ( self ) : else : def in_train_phase ( x , alt , training=None ) : check_two_tensor_operation ( 'in_train_phase ' , ( 3 , 3 ) , ( 2 , 2 ) , WITH_NP , _LEARNING_PHASE = value def test_in_train_phase ( self , training ) : check_two_tensor_operation ( 'in_test_phase ' , ( 2 , 3 ) , ( 2 , 3 ) , WITH_NP , def learning_phase ( ) : _LEARNING_PHASE = True training=training )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Added in_train_phase and in_test_phase in the numpy backend . ( # 11061 )
305,545acc4b66b5fb0ed5457ef7288d836f022c118d,2018-09-01 10:36:37-07:00,"for _ in range ( x.ndim - y.ndim - 1 ) : if isinstance ( axes , int ) : WITH_NP , cntk_dynamicity=True , def transpose ( x ) : check_two_tensor_operation ( 'dot ' , ( 4 , 2 ) , ( 2 , 4 ) , BACKENDS ) check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , [ KTH , KTF ] , axes= ( 1 , 2 ) ) check_single_tensor_operation ( 'eye ' , 3 , WITH_NP , shape_or_val=False ) BACKENDS , cntk_dynamicity=True , def random_normal_variable ( shape , mean , scale , dtype=None , name=None , seed=None ) : def random_uniform_variable ( shape , low , high , dtype=None , name=None , seed=None ) : check_two_tensor_operation ( 'dot ' , ( 4 , 2 ) , ( 5 , 2 , 3 ) , BACKENDS ) def resize_images ( x , height_factor , width_factor , data_format ) : BACKENDS , cntk_dynamicity=True , WITH_NP , cntk_dynamicity=True , def permute_dimensions ( x , pattern ) : return ( high - low ) * np.random.random ( shape ) .astype ( dtype ) + low def bias_add ( x , y , data_format ) : return np.transpose ( x , pattern ) check_single_tensor_operation ( 'transpose ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , WITH_NP , axes=1 ) assert_list_pairwise ( z_list ) return np.transpose ( x ) y = np.reshape ( y , y.shape [ : :-1 ] ) x = np.flip ( x , a ) return x return scale * np.random.randn ( * shape ) .astype ( dtype ) + mean x = repeat_elements ( x , height_factor , axis=2 ) x = repeat_elements ( x , width_factor , axis=3 ) if data_format == 'channels_first ' : check_two_tensor_operation ( 'dot ' , ( 4 , 2 ) , ( 2 , 4 ) , WITH_NP ) elif data_format == 'channels_last ' : check_single_tensor_operation ( 'random_uniform_variable ' , ( 2 , 3 ) , WITH_NP , x = repeat_elements ( x , depth_factor , axis=2 ) def dot ( x , y ) : check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , WITH_NP , axes= ( 1 , 2 ) ) check_single_tensor_operation ( 'reverse ' , ( 4 , 3 , 2 ) , BACKENDS , axes=1 ) BACKENDS , cntk_dynamicity=True , if K.backend ( ) ! = 'cntk ' : x = repeat_elements ( x , width_factor , axis=4 ) check_two_tensor_operation ( 'dot ' , ( 4 , 2 ) , ( 5 , 2 , 3 ) , WITH_NP ) check_single_tensor_operation ( 'transpose ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'random_uniform_variable ' , ( 2 , 3 ) , BACKENDS , if y.ndim > 1 : check_single_tensor_operation ( 'random_normal_variable ' , ( 2 , 3 ) , WITH_NP , return x + y WITH_NP , cntk_dynamicity=True , y = np.expand_dims ( y , 0 ) axes = [ axes ] x = repeat_elements ( x , width_factor , axis=2 ) x = repeat_elements ( x , height_factor , axis=3 ) else : def reverse ( x , axes ) : z_list = [ k.eval ( k.eye ( 3 ) ) for k in WITH_NP ] check_single_tensor_operation ( 'permute_dimensions ' , ( 4 , 2 , 3 ) , WITH_NP , check_single_tensor_operation ( 'permute_dimensions ' , ( 4 , 2 , 3 ) , BACKENDS , y = np.expand_dims ( y , -1 ) x = repeat_elements ( x , height_factor , axis=1 ) def resize_volumes ( x , depth_factor , height_factor , width_factor , data_format ) : for a in axes : x = repeat_elements ( x , depth_factor , axis=1 ) check_single_tensor_operation ( 'random_normal_variable ' , ( 2 , 3 ) , BACKENDS , return np.dot ( x , y )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Speed up backend tests ( # 11051 )
306,6f53d0ab98e0fe0b7d51eb5a14eaa5b04dd91925,2018-09-01 14:13:40+09:00,"greedy=True ) fetches= [ KTF.update ( y , 5 . ) ] ) if hasattr ( y , '_keras_shape ' ) : x_placeholder = KTF.placeholder ( shape= ( ) , dtype= '' string '' ) updates= [ ( x , x_placeholder + 1 . ) ] , 'way , so can\'t test as this . ' ) input_length , log_prob_pred = K.eval ( log_prob_pred_tf ) assert y._keras_shape == ( 3 , None ) outputs= [ x_placeholder + 1 . ] , y = KTF.variable ( 0 . ) x_placeholder = K.placeholder ( shape= ( ) ) input_length = K.variable ( np.array ( [ seq_len_0 , seq_len_1 ] , dtype=np.int32 ) ) assert K.get_session ( ) .run ( fetches= [ x , y ] ) == [ 11. , 5 . ] greedy=True ) y = KTH.flatten ( x ) x = K.placeholder ( ( 3 , None , 4 ) ) decode_pred_tf , log_prob_pred_tf = KTF.ctc_decode ( inputs , y_placeholder = K.placeholder ( shape= ( ) ) updates= [ ( x , x_placeholder + 1 . ) ] , updates= [ ( x , x_placeholder + 10 . ) ] , x_identity = K.identity ( x_placeholder ) # Test adapted from tensorflow f = K.function ( inputs= [ x_placeholder , y_placeholder ] , fetches= [ KTF.update ( y , y_placeholder * 10 . ) ] ) reason='cntk currently not support function in this ' reason='Uses the ` feed_dict ` argument . ' ) x = K.variable ( 0 . ) outputs= [ x_placeholder + y_placeholder ] , x_placeholder = KTF.placeholder ( shape= ( ) ) f = KTF.function ( inputs= [ x_placeholder ] , outputs= [ x_identity ] ) 'theano backend . ' ) assert y._keras_shape == ( None , ) assert y._keras_shape == ( None , ) assert KTF.get_session ( ) .run ( fetches= [ x , y ] ) == [ 30. , 40 . ] assert K.get_session ( ) .run ( fetches= [ x , y ] ) == [ 20. , 30 . ] decode_pred_tf , log_prob_pred_tf = K.ctc_decode ( inputs , fetches= [ K.update ( y , 5 . ) ] ) reason='Uses the ` string ` type for a tensor . ' ) if hasattr ( y , '_keras_shape ' ) : reason='cntk doesn\'t support gradient in this way . ' ) f = KTF.function ( inputs= [ x_placeholder , y_placeholder ] , reason='We only test the shape inference of the ' assert KTF.get_session ( ) .run ( fetches= [ x , y ] ) == [ 20. , 30 . ] y = K.batch_flatten ( x ) updates= [ ( x , x_placeholder + 10 . ) ] , inputs = K.variable ( np.asarray ( inputs ) .transpose ( ( 1 , 0 , 2 ) ) ) x = KTF.variable ( 0 . ) y = K.variable ( 0 . ) reason='Test adapted from tensorflow . ' ) decode_pred = KTF.eval ( decode_pred_tf [ 0 ] ) x_placeholder = K.placeholder ( shape= ( ) , dtype= '' string '' ) assert y._keras_shape == ( 3 , None ) f = K.function ( inputs= [ x_placeholder ] , outputs= [ x_identity ] ) # cntk currently not support function in this way , so ca n't test as this feed_dict=feed_dict , reason='Uses the ` fetches ` argument . ' ) x = KTH.placeholder ( ( 3 , None , 4 ) ) input_length = KTF.variable ( np.array ( [ seq_len_0 , seq_len_1 ] , dtype=np.int32 ) ) fetches= [ K.update ( y , y_placeholder * 10 . ) ] ) x_identity = KTF.identity ( x_placeholder ) f = KTF.function ( inputs= [ x_placeholder ] , y = KTH.batch_flatten ( x ) y = K.flatten ( x ) y_placeholder = KTF.placeholder ( shape= ( ) ) inputs = KTF.variable ( np.asarray ( inputs ) .transpose ( ( 1 , 0 , 2 ) ) ) log_prob_pred = KTF.eval ( log_prob_pred_tf ) feed_dict=feed_dict , if K.backend ( ) == 'theano ' : decode_pred = K.eval ( decode_pred_tf [ 0 ] ) outputs= [ x_placeholder + 1 . ] , input_length , # cntk does n't support gradient in this way f = K.function ( inputs= [ x_placeholder ] , ' '' only tensorflow tested , need special handle '' ' assert KTF.get_session ( ) .run ( fetches= [ x , y ] ) == [ 11. , 5 . ] outputs= [ x_placeholder + y_placeholder ] , assert K.get_session ( ) .run ( fetches= [ x , y ] ) == [ 30. , 40 . ]",['tests/keras/backend/backend_test.py'],Skipped some duplicated tests . ( # 11049 )
307,c7f4ad56532a378833f88648f6849e76f9d769be,2018-08-31 17:01:51-07:00,"for k in WITH_NP : if shape is None : beam_width=beam_width , # cntk has issue with negative number inputs = K.variable ( np.asarray ( inputs ) .transpose ( ( 1 , 0 , 2 ) ) ) reason='Beam search is only implemented with ' ' '' tensorflow only , need special handle '' ' input_length , from keras.backend import floatx for k in [ KTH , KTF ] : log_prob_pred = K.eval ( log_prob_pred_tf ) reason='cntk has issues with negative number . ' ) decode_pred_tf , log_prob_pred_tf = K.ctc_decode ( inputs , def dtype ( x ) : if dtype is None : inputs = KTF.variable ( np.asarray ( inputs ) .transpose ( ( 1 , 0 , 2 ) ) ) log_prob_pred = KTF.eval ( log_prob_pred_tf ) greedy=False , return np.arange ( start , stop , step , dtype ) dtype = floatx ( ) np_value = value * np.ones ( shape ) top_paths=top_paths ) assert np.alltrue ( decode_truth [ i ] == K.eval ( decode_pred_tf [ i ] ) ) input_length , assert np.alltrue ( decode_truth [ i ] == KTF.eval ( decode_pred_tf [ i ] ) ) input_length = K.variable ( np.array ( [ seq_len_0 ] , dtype=np.int32 ) ) def constant ( value , dtype=None , shape=None , name=None ) : shape = ( ) reason='Sparse tensors are not supported in cntk . ' ) greedy=False , top_paths=top_paths ) 'the TensorFlow backend . ' ) for k in WITH_NP : beam_width=beam_width , decode_pred_tf , log_prob_pred_tf = KTF.ctc_decode ( inputs , def arange ( start , stop=None , step=1 , dtype='int32 ' ) : return np_value for k in [ KTH , KTF ] : input_length = KTF.variable ( np.array ( [ seq_len_0 ] , dtype=np.int32 ) ) np_value.astype ( dtype ) return x.dtype.name","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Used decorators and WITH_NP to avoid tests duplication . ( # 11050 )
308,780f5e447799b4ab343164ab8beceabcd4ab15f0,2018-08-31 13:59:29-07:00,"check_single_tensor_operation ( 'squeeze ' , ( 4 , 1 , 1 ) , WITH_NP , axis=1 ) y = np.expand_dims ( x , 1 ) check_single_tensor_operation ( 'expand_dims ' , ( 4 , 3 ) , WITH_NP , axis=-1 ) print ( x , message ) check_single_tensor_operation ( 'squeeze ' , ( 4 , 3 , 1 ) , BACKENDS , axis=2 ) z_list = [ k.eval ( k.eye ( 3 ) ) for k in WITH_NP ] y = np.repeat ( y , n , axis=1 ) def concatenate ( tensors , axis=-1 ) : check_single_tensor_operation ( 'flatten ' , ( 4 , 1 ) , BACKENDS ) check_single_tensor_operation ( 'print_tensor ' , ( 4 , 3 ) , WITH_NP ) check_single_tensor_operation ( 'print_tensor ' , ( 4 , 3 ) , BACKENDS ) return np.eye ( size , dtype=dtype ) check_single_tensor_operation ( 'print_tensor ' , ( ) , WITH_NP ) check_single_tensor_operation ( 'squeeze ' , ( 4 , 3 , 1 ) , WITH_NP , axis=2 ) def eye ( size , dtype=None , name=None ) : ( 4 , 3 , 1 , 1 ) , WITH_NP ) return x check_single_tensor_operation ( 'print_tensor ' , ( 2 , ) , BACKENDS ) check_single_tensor_operation ( 'batch_flatten ' , ( 20 , 2 , 5 ) , WITH_NP , def print_tensor ( x , message= '' ) : check_single_tensor_operation ( 'print_tensor ' , ( ) , BACKENDS ) ( 4 , 3 , 1 , 1 ) , BACKENDS ) check_single_tensor_operation ( 'expand_dims ' , ( 4 , 3 ) , BACKENDS , axis=-1 ) check_single_tensor_operation ( 'repeat ' , ( 4 , 1 ) , BACKENDS , n=3 ) check_two_tensor_operation ( 'concatenate ' , ( 4 , 3 ) , ( 4 , 2 ) , WITH_NP , check_two_tensor_operation ( 'concatenate ' , ( 4 , 3 ) , ( 4 , 2 ) , BACKENDS , return y def batch_flatten ( x ) : def flatten ( x ) : return np.concatenate ( tensors , axis ) check_single_tensor_operation ( 'print_tensor ' , ( 1 , 2 , 3 ) , WITH_NP ) check_single_tensor_operation ( 'flatten ' , ( 4 , 1 ) , WITH_NP ) check_single_tensor_operation ( 'repeat ' , ( 4 , 1 ) , WITH_NP , n=3 ) check_single_tensor_operation ( 'batch_flatten ' , ( 20 , 2 , 5 ) , BACKENDS , squeeze = np.squeeze expand_dims = np.expand_dims z_list = [ k.eval ( k.eye ( 3 ) ) for k in BACKENDS ] check_single_tensor_operation ( 'print_tensor ' , ( 2 , ) , WITH_NP ) check_single_tensor_operation ( 'expand_dims ' , ( 4 , 3 , 2 ) , WITH_NP , axis=1 ) def repeat ( x , n ) : return np.reshape ( x , ( -1 , ) ) return np.reshape ( x , ( x.shape [ 0 ] , -1 ) ) check_single_tensor_operation ( 'expand_dims ' , ( 4 , 3 , 2 ) , BACKENDS , axis=1 ) check_single_tensor_operation ( 'squeeze ' , ( 4 , 1 , 1 ) , BACKENDS , axis=1 ) check_single_tensor_operation ( 'print_tensor ' , ( 1 , 2 , 3 ) , BACKENDS )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Removing duplicated backend tests . ( # 11037 )
309,6dd087ab73b09e449144ff17450cc14f981b9ac2,2018-08-31 11:31:47-07:00,"input_data=input_data , x = tf.nn.conv2d_transpose ( x , kernel , output_shape , strides , output_padding , dilation=1 ) : force_transpose = True expected_output=expected_output ) data_format=self.data_format ) if dilation_rate == ( 1 , 1 ) : x , tf_data_format = _preprocess_conv2d_input ( x , data_format ) kernel_size = kernel_size + ( kernel_size - 1 ) * ( dilation - 1 ) def deconv_length ( dim_size , stride_size , kernel_size , padding , x , kernel , output_shape , dilation_rate [ 0 ] , padding ) if the ` data_format ` is ` `` channels_first '' ` . out_pad_w ) filter_flip=not flip_filters ) self.dilation_rate [ 1 ] ) padding='valid ' , data_format=None ) : x = tf.nn.conv2d_transpose ( x , kernel , output_shape , strides , if not _has_nchw_support ( ) or force_transpose : dilation=dilation_rate ) dilation : dilation rate , integer . 'kernel_size ' : 3 , in inputs/kernels/outputs . in inputs/kernels/outputs . out_pad_h , filter_flip=not flip_filters , dilation_rate=dilation_rate , kwargs= { 'filters ' : 1 , reason='cntk only supports dilated conv on GPU ' ) [ 336 , 372 , 336 , 372 ] ] ) .reshape ( ( 1 , 4 , 4 , 1 ) ) force_transpose = False def deconv_length ( dim_size , stride_size , kernel_size , padding , output_padding ) : if data_format == 'channels_first ' and dilation_rate ! = ( 1 , 1 ) : x , tf_data_format = _preprocess_conv2d_input ( x , data_format , force_transpose ) [ 192 , 228 , 192 , 228 ] , x = tf.nn.atrous_conv2d_transpose ( dilation_rate=self.dilation_rate ) else : padding='valid ' , data_format=None , dilation_rate= ( 1 , 1 ) ) : # tf.nn.atrous_conv2d_transpose input only supports NHWC format out_pad_w , self.dilation_rate [ 1 ] ) input_data = np.arange ( 48 ) .reshape ( ( 1 , 4 , 4 , 3 ) ) .astype ( np.float32 ) 'data_format ' : 'channels_last ' , if not _has_nchw_support ( ) : kwargs= { 'filters ' : 2 , # Check dilated conv transpose returns expected output assert dilation_rate [ 0 ] == dilation_rate [ 1 ] out_pad_h , out_pad_w ) dilation_rate= ( 1 , 1 ) , self.dilation_rate [ 0 ] ) expected_output = np.float32 ( [ [ 192 , 228 , 192 , 228 ] , 'dilation_rate ' : ( 2 , 2 ) } , data_format=tf_data_format ) filter_dilation=dilation_rate ) 'dilation_rate ' : ( 2 , 2 ) , def _preprocess_conv2d_input ( x , data_format ) : 'padding ' : 'same ' , def _preprocess_conv2d_input ( x , data_format , force_transpose=False ) : data_format=self.data_format , dilation_rate : tuple of 2 integers . self.dilation_rate [ 0 ] ) # Get the dilated kernel size layer_test ( convolutional.Conv2DTranspose , out_pad_w , 'kernel_initializer ' : 'ones ' } , out_pad_h ) padding=padding , out_pad_h ) output_shape=output_shape , output_shape=output_shape ) force_transpose : boolean , whether force to transpose input from NCHW to NHWC padding=padding , [ 336 , 372 , 336 , 372 ] , reason='cntk only supports dilated conv transpose on GPU ' ) def test_conv2d_transpose_dilation ( ) : data_format=tf_data_format ) config.pop ( 'dilation_rate ' ) reason= '' cntk only supports dilated conv on GPU '' ) input_shape= ( 2 , 5 , 6 , 3 ) )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/layers/convolutional.py', 'keras/utils/conv_utils.py', 'tests/keras/layers/convolutional_test.py']","[ P , RELNOTES ] Conv2DTranspose supports dilation ( # 11029 )"
310,76da5f0a21ca98e4bf6706e182fb825243e76204,2018-08-31 09:06:27-07:00,"Tuple of Numpy arrays : ` ( x_train , y_train ) , ( x_test , y_test ) ` . __x_train , x_test__ : uint8 array of RGB image data with shape ( num_samples , 3 , 32 , 32 ) . Tuple of Numpy arrays : ` ( x_train , y_train ) , ( x_test , y_test ) ` . __x_train , x_test__ : uint8 array of RGB image data with shape ( num_samples , 3 , 32 , 32 ) or ( num_samples , 32 , 32 , 3 ) based on the ` image_data_format ` backend setting of either ` channels_first ` or ` channels_last ` respectively .",['docs/templates/datasets.md'],Doc Change : Change in shape for CIFAR Datasets ( # 11043 )
311,5027630fa41f499a9226a8f9d952ceabf2c247aa,2018-08-30 13:23:13-07:00,yield item # Arguments val_enqueuer_gen = iter_sequence_infinite ( generator ) val_enqueuer_gen = iter ( val_data ) `` `` '' for item in seq : def iter_sequence_infinite ( seq ) : output_generator = iter_sequence_infinite ( generator ) yield item seq : Sequence object `` `` '' Create an infinite generator that iterate over the Sequence . '' '' '' `` `` '' Create a generator that iterate over the Sequence . '' '' '' while True : output_generator = iter ( generator ) from .training_utils import iter_sequence_infinite # Returns for item in ( self [ i ] for i in range ( len ( self ) ) ) : while True : Generator yielding batches . for item in ( self [ i ] for i in range ( len ( self ) ) ) : `` `` '' Iterate indefinitely over a Sequence .,"['keras/engine/training_generator.py', 'keras/engine/training_utils.py', 'keras/utils/data_utils.py']",Better UX ( # 11039 )
312,e8f8e62add2a7073e50cd31f4f6571bf1ebe21b3,2018-08-30 12:59:58-07:00,"def on_epoch_end ( epoch , _ ) : from keras.layers import Dense , Activation model.add ( Activation ( 'softmax ' ) ) model.add ( Dense ( len ( chars ) , activation='softmax ' ) ) from keras.layers import Dense def on_epoch_end ( epoch , logs ) : model.add ( Dense ( len ( chars ) ) )",['examples/lstm_text_generation.py'],Update lstm text generation example ( # 11038 )
313,3b7b071bb7b4537576306f00703c5c409bedbb43,2018-08-30 12:04:03-04:00,"r = K.random_binomial ( ( 10 , 10 ) , p ) r = K.random_binomial ( ( 1 , ) , p ) samples = np.array ( [ K.eval ( r ) for _ in range ( 200 ) ] ) r = K.random_uniform ( ( 1 , ) , minval=min_val , maxval=max_val ) samples = [ K.eval ( r ) for _ in range ( 20000 ) ] r = K.random_uniform ( ( 10 , 10 ) , minval=min_val , maxval=max_val ) samples = [ K.eval ( r ) for _ in range ( 200 ) ] samples = np.array ( [ K.eval ( r ) for _ in range ( 200 ) ] )",['tests/keras/backend/backend_test.py'],Speeding up the tests by reducing the number of K.eval ( ) . ( # 11036 )
314,8e8f989b850d37a4cbec7a0409343262bd963d0d,2018-08-29 19:51:43-07:00,"warnings.warn ( monitor_value = logs.get ( self.monitor ) ( self.monitor , ' , '.join ( list ( logs.keys ( ) ) ) ) , RuntimeWarning 'Early stopping conditioned on metric ` % s ` ' 'Early stopping conditioned on metric ` % s ` ' ) current = self.get_monitor_value ( logs ) 'which is not available . Available metrics are : % s ' % return monitor_value current = logs.get ( self.monitor ) ) 'which is not available . Available metrics are : % s ' % ( self.monitor , ' , '.join ( list ( logs.keys ( ) ) ) ) , RuntimeWarning if monitor_value is None : warnings.warn ( def get_monitor_value ( self , logs ) :",['keras/callbacks.py'],[ P ] Expose monitor value getter for easier subclass ( # 11002 )
315,cc3521ea93b0470e9ea11151a44c30b54d4d726a,2018-08-29 19:50:22-07:00,"[ 'channels_first ' , 'channels_last ' ] ) and that with Theano , only ` size= ( 2 , 2 ) ` is possible . stack_size = 2 if not ( height_factor == width_factor == 2 ) : self.interpolation = interpolation interpolation='bilinear ' ) new_height = original_shape [ rows ] * height_factor input_num_col ) new_shape = tf.shape ( x ) [ 1:3 ] output = repeat_elements ( output , width_factor , axis=3 ) width_factor=width_factor , output = repeat_elements ( output , width_factor , axis=2 ) original_shape [ 3 ] * width_factor if original_shape [ 3 ] is not None else None ) ) return output raise ValueError ( 'interpolation should be one ' original_shape = int_shape ( x ) output = repeat_elements ( x , height_factor , axis=axis_1 ) self._helper_bilinear ( data_format , 4 , 4 ) elif data_format == 'channels_last ' : output = repeat_elements ( output , width_factor , axis=axis_2 ) # basic test ratio=height_factor ) assert np_output.shape [ 2 ] == length_col * input_num_col outputs = layer ( K.variable ( inputs ) ) Note that CNTK does not support yet the ` bilinear ` upscaling rows , cols = 2 , 3 new_shape * = tf.constant ( np.array ( [ height_factor , width_factor ] ) .astype ( 'int32 ' ) ) raise ValueError ( 'interpolation should be one of `` nearest '' or `` bilinear '' . ' ) def __init__ ( self , size= ( 2 , 2 ) , data_format=None , * * kwargs ) : 'of `` nearest '' or `` bilinear '' . ' ) if data_format == 'channels_last ' : layer_test ( convolutional.UpSampling2D , x.set_shape ( transpose_shape ( output_shape , data_format , spatial_axes= ( 1 , 2 ) ) ) for length_row in [ 2 ] : for length_col in [ 2 , 3 ] : x.set_shape ( ( None , None , original_shape [ 2 ] * height_factor if original_shape [ 2 ] is not None else None , 'of `` nearest '' or `` bilinear '' . ' ) x = tf.image.resize_bilinear ( x , new_shape ) return x def resize_images ( x , output = x with pytest.raises ( NotImplementedError ) : if hasattr ( x , '_keras_shape ' ) : input_num_col = 12 new_shape = tf.shape ( x ) [ rows : cols + 1 ] output = repeat_elements ( x , height_factor , axis=2 ) else : # tf 'is not available when using the Theano backend . ' ) raise ValueError ( 'Unknown data_format : ' + str ( data_format ) ) self.data_format , self.interpolation ) original_shape [ 2 ] * width_factor if original_shape [ 2 ] is not None else None , None ) ) output = repeat_elements ( output , width_factor , axis=2 ) if interpolation not in [ 'nearest ' , 'bilinear ' ] : new_height = None raise NotImplementedError ( raise ValueError ( 'CNTK Backend : Invalid data_format : % s ' % data_format ) input_num_row = 11 x = tf.image.resize_nearest_neighbor ( x , new_shape ) 'interpolation ' : 'bilinear ' } , output = T.nnet.abstract_conv.bilinear_upsampling ( output , width_factor , output._keras_shape = list ( x._keras_shape ) layer = convolutional.UpSampling2D ( return output interpolation='nearest ' ) : else : axis_2 = 3 stack_size ) def resize_images ( x , height_factor , width_factor , data_format ) : inputs = np.random.rand ( num_samples , stack_size , input_num_row , height_factor=height_factor , new_width = original_shape [ cols ] * width_factor 'data_format ' : data_format , def test_resize_images_bilinear ( self , data_format ) : elif data_format == 'channels_last ' : output = repeat_elements ( x , height_factor , axis=1 ) assert np_output.shape [ 3 ] == length_col * input_num_col if original_shape [ rows ] is None : data_format=data_format , 'Bilinear upscaling with factors other than ( 2 , 2 ) ' height_factor , new_shape * = tf.constant ( np.array ( [ height_factor , width_factor ] , dtype='int32 ' ) ) inputs = np.random.rand ( num_samples , input_num_row , input_num_col , output._keras_shape [ axis_1 ] * = height_factor new_shape = tf.shape ( x ) [ 2 : ] interpolation : A string , one of ` nearest ` or ` bilinear ` . def resize_images ( x , height_factor , width_factor , data_format , interpolation='nearest ' ) : data_format , output._keras_shape [ axis_2 ] * = width_factor if data_format == 'channels_first ' : assert np_output.shape [ 1 ] == length_row * input_num_row input_shape=inputs.shape ) data_format=data_format ) output = repeat_elements ( output , width_factor , axis=3 ) axis_1 = 2 assert np_output.shape [ 2 ] == length_row * input_num_row if data_format == 'channels_first ' : if interpolation == 'nearest ' : output._keras_shape = tuple ( output._keras_shape ) x_shape = ( 2 , 3 , 4 , 5 ) size= ( length_row , length_col ) , else : # tf layer.build ( inputs.shape ) self.data_format ) def _helper_bilinear ( data_format , height_factor , width_factor ) : raise ValueError ( 'interpolation should be one ' np_output = K.eval ( outputs ) def test_upsampling_2d_bilinear ( data_format ) : return x else : output = permute_dimensions ( output , [ 0 , 2 , 3 , 1 ] ) if data_format == 'channels_first ' : [ KTF , KTH ] , if original_shape [ cols ] is None : output = repeat_elements ( x , height_factor , axis=1 ) check_single_tensor_operation ( 'resize_images ' , x_shape , output = repeat_elements ( x , height_factor , axis=2 ) reason='cntk does not support it yet ' ) new_width = None elif interpolation == 'bilinear ' : axis_2 = 2 axis_1 = 1 x.set_shape ( ( None , original_shape [ 1 ] * height_factor if original_shape [ 1 ] is not None else None , return output self._helper_bilinear ( data_format , 2 , 2 ) num_samples = 2 original_shape = int_shape ( x ) if data_format == 'channels_first ' : raise ValueError ( 'CNTK Backend : Invalid data_format : ' , data_format ) raise ValueError ( 'CNTK Backend : Invalid data_format : % s ' % data_format ) rows , cols = 1 , 2 kwargs= { 'size ' : ( 2 , 2 ) , raise NotImplementedError ( 'CNTK only supports ` nearest ` interpolation . ' ) output = permute_dimensions ( x , [ 0 , 3 , 1 , 2 ] ) def __init__ ( self , size= ( 2 , 2 ) , data_format=None , interpolation='nearest ' , * * kwargs ) : output_shape = ( None , new_height , new_width , None )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/layers/convolutional.py', 'tests/keras/backend/backend_test.py', 'tests/keras/layers/convolutional_test.py']",[ RELNOTES ] Added the mode `` bilinear '' in the upscaling2D layer . ( # 10994 )
316,4c015964da4e7e2dfa504461f020408d0692a57b,2018-08-29 09:42:29-07:00,"[ def test_globalpooling_2d ( data_format , pooling_class ) : # Test invalid output padding for given stride . Output padding equal 'data_format ' : data_format } , kwargs= { 'padding ' : ( ( 1 , 2 ) , ( 3 , 4 ) ) , 'data_format ' : data_format } , stack_size = 2 layer_test ( convolutional.Conv2DTranspose , filters = 6 layer_test ( convolutional.MaxPooling2D , 'padding , out_padding , strides ' , if not ( padding == 'same ' and strides ! = 1 ) ] } , expected_output= [ [ [ 1 ] , [ 3 ] , [ 5 ] ] ] def test_conv_1d ( padding , strides ) : padding = _convolution_paddings [ -1 ] reason='cntk only support dilated conv on GPU ' ) filters = 2 inputs = np.ones ( ( num_samples , input_num_row , input_num_col , stack_size ) ) layer_test ( convolutional.MaxPooling3D , kwargs= { 'padding ' : ( 2 , 2 , 2 ) , 'data_format ' : data_format } , ) for out_padding in [ None , ( 0 , 0 , 0 ) , ( 1 , 1 , 1 ) ] batch_size = 2 def test_zero_padding_3d_correctness ( ) : 'output_padding ' : out_padding , layer_test ( stack_size ) ) 'bias_regularizer ' : 'l2 ' , input_shape= ( num_samples , num_row , num_col , stack_size ) , def test_depthwise_conv_2d_invalid ( ) : num_col = 6 filters = 3 kwargs= { 'data_format ' : data_format } , kwargs= { 'kernel_size ' : ( 3 , 3 ) , for strides in [ 1 , 2 ] 'strides ' : strides } , @ keras_test 'padding ' : 'same ' , convolutional.SeparableConv2D , def test_convolution_2d_channels_last ( ) : def test_depthwise_conv_2d_additional_args ( ) : 'bias_constraint ' : 'max_norm ' , for data_format in [ 'channels_first ' , 'channels_last ' ] : 'kernel_initializer ' : 'ones ' , 'use_bias ' : False } , 'pool_size ' : ( 2 , 2 ) , # to stride 'layer_kwargs , input_length , expected_output ' , num_step = 9 def test_convolution_2d ( strides , padding ) : kwargs= { 'filters ' : filters , pooling.GlobalAveragePooling1D ] ] if padding == 'same ' and strides ! = ( 1 , 1 ) : def test_zero_padding_3d ( ) : input_shape= ( 3 , 11 , 12 , 10 , 4 ) ) reason= '' cntk only supports dilated conv on GPU '' ) input_shape= ( 3 , 5 , 6 , 4 ) ) input_shape= ( num_samples , num_row , num_col , stack_size ) , and not ( dilation_rate ! = ( 1 , 1 ) and strides ! = ( 1 , 1 ) ) layer_test ( convolutional.AveragePooling3D , 'strides ' : strides } , else : from keras.utils.test_utils import keras_test , layer_test layer_test ( convolutional.Conv1D , input_data=input_data , layer_test ( convolutional.Convolution3D , 'pool_size ' : pool_size } , for padding in [ ( 2 , 2 , 2 ) , ( ( 1 , 2 ) , ( 3 , 4 ) , ( 0 , 2 ) ) ] ] model = Sequential ( ) 'output_padding ' : out_padding , ( ( 2 , 2 ) , 'valid ' , None , ( 3 , 5 , 6 , 4 ) ) , # Test GlobalAveragePooling1D supports masking if padding == 'same ' and strides ! = 1 : def test_zero_padding_2d ( data_format , padding ) : for stride in [ 1 , 2 ] : def test_convolution_2d_dilation ( ) : for strides in [ ( 1 , 1 , 1 ) , ( 2 , 2 , 2 ) ] [ ( data_format , padding ) from keras.layers import convolutional input_shape= ( 3 , 5 , 6 , 4 ) ) def test_conv2d_transpose ( padding , out_padding , strides ) : def test_globalpooling_1d ( ) : expected_output= [ [ [ 0 ] , [ 1 ] , [ 3 ] , [ 5 ] ] ] def test_convolution_3d ( ) : pytest.main ( [ __file__ ] ) num_row = 5 num_row = 7 input_shape= ( batch_size , steps , input_dim ) ) layer_test ( convolutional.Conv2D , 'data_format ' : data_format } , input_shape=input_shape ) [ ( data_format , pooling_class ) # Test invalid use case kwargs= { 'strides ' : ( 2 , 2 ) , if K.backend ( ) ! = 'cntk ' : layer_test ( convolutional.ZeroPadding3D , def test_separable_conv_2d_invalid ( ) : pool_size = ( 3 , 3 , 3 ) from keras.models import Sequential kwargs= { 'filters ' : filters , if dilation_rate ! = 1 and strides ! = 1 : 'depth_multiplier ' : multiplier , layer_test ( convolutional.ZeroPadding2D , ( { 'filters ' : 1 , 'kernel_size ' : 2 , 'dilation_rate ' : 1 , 'padding ' : 'valid ' , # Test channels_first def test_maxpooling_2d ( ) : if padding == 'same ' and strides ! = ( 1 , 1 ) : input_shape= ( 3 , 4 , 5 , 6 ) ) 'dilation_rate ' : dilation_rate } , 'dilation_rate ' : 2 , 'data_format ' : 'channels_first ' } , layer_test ( convolutional.DepthwiseConv2D , 'kernel_size ' : 3 , input_len_dim1 , input_len_dim2 , input_len_dim3 , 'strides , padding , data_format , input_shape ' , def test_conv3d_transpose_invalid ( ) : 'data_format ' : 'channels_first ' } , def test_zero_padding_2d ( ) : for strides in [ ( 1 , 1 ) , ( 2 , 2 ) ] [ ( 1 , 1 ) , ( 2 , 3 ) ] output = model.predict ( model_input ) model.add ( pooling.GlobalAveragePooling1D ( ) ) and not ( dilation_rate ! = 1 and strides ! = 1 ) # Non-causal if __name__ == '__main__ ' : def test_zero_padding_2d_correctness ( ) : and not ( dilation_rate ! = ( 1 , 1 ) and multiplier == dilation_rate [ 0 ] ) 'depth_multiplier ' : multiplier , pool_size = ( 3 , 3 ) for strides in [ ( 1 , 1 ) , ( 2 , 2 ) ] : input_shape= ( batch_size , steps , input_dim ) ) kwargs= { 'filters ' : filters , [ ( 2 , None , ( 3 , 11 , 12 , 10 , 4 ) ) , 'padding ' : padding , layer_test ( convolutional.AveragePooling1D , stack_size = 3 convolutional.SeparableConv2D , def test_globalpooling_3d ( ) : # Test dilation for padding in [ ( 2 , 2 ) , ( ( 1 , 2 ) , ( 3 , 4 ) ) ] ] kwargs= { 'filters ' : filters , # correctness test 'output_padding ' : out_padding , def test_conv3d_transpose ( ) : num_row , input_dim = 2 ( 1 , input_length , 1 ) ) from keras.layers import pooling def test_conv2d_transpose_invalid ( ) : from keras.layers import pooling def test_averagepooling_1d ( padding , stride , data_format ) : for padding in _convolution_paddings def test_conv2d_transpose_channels_first ( ) : continue # Causal dilated with larger kernel size : 'padding , strides , multiplier , dilation_rate ' , 'kernel_size ' : ( 3 , 3 ) , if strides == ( 1 , 1 , 1 ) and out_padding == ( 1 , 1 , 1 ) : def test_separable_conv_1d_invalid ( ) : def test_globalpooling_1d_supports_masking ( ) : pooling.GlobalAveragePooling2D ] ] kwargs= { 'padding ' : padding , 'data_format ' : data_format } , 'dilation_rate ' : dilation_rate } , layer_test ( convolutional.SeparableConv1D , 'padding , strides , multiplier ' , else : input_len_dim1 , input_len_dim2 , input_len_dim3 , input_shape= ( batch_size , steps , input_dim ) ) model_input [ 0 , 1 : , : ] = 0 kwargs=layer_kwargs , expected_output=expected_output ) 'filters ' : 1 , and not ( strides == ( 1 , 1 , 1 ) and out_padding == ( 1 , 1 , 1 ) ) ) ] def test_depthwise_conv_2d ( ) : input_num_col = 5 layer_test ( convolutional.Conv1D , pool_size = ( 3 , 3 ) 'activity_regularizer ' : 'l2 ' , 'kernel_size ' : 3 , inputs = np.ones ( ( num_samples , 4 , [ [ [ 1 ] , [ 3 ] , [ 5 ] ] ] ) , layer_test ( convolutional.MaxPooling1D , 'strides ' : strides , 'padding ' : padding , kwargs= { 'padding ' : ( ( 1 , 2 ) , ( 3 , 4 ) , ( 0 , 2 ) ) , from keras.layers import Masking num_row , 'padding ' : 'valid ' , for data_format in [ 'channels_first ' , 'channels_last ' ] : for strides in [ ( 1 , 1 , 1 ) , ( 2 , 2 , 2 ) ] : inputs = np.ones ( ( num_samples , stack_size , input_num_row , input_num_col ) ) stack_size ) ) if dilation_rate ! = ( 1 , 1 ) and strides ! = ( 1 , 1 ) : for dilation_rate in [ 1 , 2 ] : for data_format in [ 'channels_first ' , 'channels_last ' ] : convolutional.Conv3DTranspose , input_shape=inputs.shape ) ] 'data_format ' : 'channels_first ' , 'dilation_rate ' : 2 } , model.compile ( loss='mae ' , optimizer='adam ' ) def test_maxpooling_1d ( ) : layer_test ( pooling.GlobalMaxPooling2D , def test_separable_conv_2d ( ) : 'kernel_regularizer ' : 'l2 ' , 'dilation_rate ' : dilation_rate } , 'kernel_size ' : 3 , kwargs= { 'filters ' : filters , def test_separable_conv_2d_additional_args ( ) : kernel_size = ( 3 , 2 ) 'use_bias ' : False , [ ( padding , out_padding , strides ) layer_test ( for pooling_class in [ pooling.GlobalMaxPooling2D , layer_test ( convolutional.Conv2D , layer_test ( convolutional.Conv1D , 'output_padding ' : out_padding , 4 , [ [ [ 0 ] , [ 1 ] , [ 3 ] , [ 5 ] ] ] ) , [ ( padding , strides , multiplier ) layer_test ( convolutional.AveragePooling3D , input_len_dim3 = 3 layer_test ( convolutional.Convolution3D , expected_output=np.float32 ( def test_separable_conv_1d_additional_args ( ) : def test_averagepooling_2d ( strides , padding , data_format , input_shape ) : 'kernel_size ' : ( 3 , 3 ) , # Test invalid output padding for given stride . Output padding equal to stride for padding in [ 'valid ' , 'same ' ] fixed_batch_size=True ) def test_conv_1d_channels_first ( ) : 'depth_multiplier ' : multiplier , ) 'padding ' : padding , layer_test ( pooling.GlobalMaxPooling1D , kwargs= { 'data_format ' : 'channels_first ' } , assert np.array_equal ( output [ 0 ] , model_input [ 0 , 0 , : ] ) 'strides , padding ' , kwargs= { 'strides ' : 3 , if padding == 'same ' and strides ! = ( 1 , 1 ) : layer_test ( convolutional.SeparableConv1D , for strides in [ 1 , 2 ] : 'padding ' : padding , steps = 8 pool_size = ( 3 , 3 , 3 ) 'padding ' : 'causal ' , # basic test 'kernel_initializer ' : 'ones ' , [ ( padding , out_padding , strides , data_format ) def test_separable_conv_2d ( padding , strides , multiplier , dilation_rate ) : def test_averagepooling_3d ( strides , data_format , input_shape ) : def test_averagepooling_1d ( ) : input_shape= ( num_samples , stack_size , num_row , num_col ) ) 'depth_multiplier ' : multiplier , 'depth_multiplier ' : multiplier } , [ ( strides , padding ) for out_padding in [ None , ( 0 , 0 , 0 ) , ( 1 , 1 , 1 ) ] : input_shape= ( num_samples , num_step , stack_size ) ) input_len_dim2 = 5 'depth_multiplier ' : multiplier } , input_shape= ( num_samples , num_row , num_col , stack_size ) ) 'kernel_constraint ' : 'max_norm ' , 'bias_constraint ' : 'max_norm ' , 'kernel_size ' : 3 , def test_zero_padding_3d ( data_format , padding ) : input_shape= ( None , num_depth , num_row , num_col , stack_size ) , input_shape= ( 3 , 4 , 11 , 12 , 10 ) ) kwargs= { 'filters ' : filters , 'kernel_constraint ' : 'max_norm ' , kwargs= { 'strides ' : stride , def test_causal_dilated_conv ( layer_kwargs , input_length , expected_output ) : 'kernel_regularizer ' : 'l2 ' , if ( not ( padding == 'same ' and strides ! = 1 ) def test_conv3d_transpose_additional_args ( ) : 'data_format ' : 'channels_first ' } , layer_test ( pooling.GlobalMaxPooling3D , input_len_dim1 = 4 model_input [ 0 , 1 : , : ] = 0 layer_test ( convolutional.MaxPooling2D , 'strides ' : strides , if ( not ( padding == 'same ' and strides ! = ( 1 , 1 , 1 ) ) ( { 'filters ' : 1 , 'kernel_size ' : 3 , 'dilation_rate ' : 2 , 'padding ' : 'causal ' , # basic test 'padding ' : padding , num_col , def test_separable_conv_1d ( ) : for dilation_rate in [ ( 1 , 1 ) , ( 2 , 2 ) , ( 2 , 1 ) , ( 1 , 2 ) ] import numpy as np continue kernel_size = ( 3 , 2 ) # cntk only support dilated conv on GPU if padding == 'same ' and strides ! = ( 1 , 1 , 1 ) : 'padding ' : padding , 'kernel_size ' : 3 , if padding == 'same ' and strides ! = 1 : def test_convolution_3d_additional_args ( ) : layer_test ( convolutional.ZeroPadding2D , 'padding , stride , data_format ' , 'kernel_size ' : kernel_size , input_shape= ( 3 , 4 , 5 ) ) input_shape= ( num_samples , 'dilation_rate ' : 1 , for strides in [ ( 1 , 1 ) , ( 2 , 2 ) ] : ( 3 , 'channels_first ' , ( 3 , 4 , 11 , 12 , 10 ) ) ] [ ( padding , strides ) fixed_batch_size=True ) and not ( dilation_rate ! = 1 and K.backend ( ) == 'cntk ' ) ) ] 'bias_regularizer ' : 'l2 ' , fixed_batch_size=True ) def test_causal_dilated_conv ( ) : input_shape=inputs.shape ) kwargs= { 'strides ' : strides , 'strides , data_format , input_shape ' , def test_maxpooling_3d ( strides , data_format , input_shape ) : 'strides ' : strides , 'pool_size ' : ( 3 , 3 ) } , def test_maxpooling_1d ( padding , stride , data_format ) : strides = ( 2 , 2 ) layer_test ( pooling.GlobalAveragePooling3D , model_input = np.random.randint ( low=1 , high=5 , size= ( 2 , 3 , 4 ) ) [ ( padding , strides , multiplier , dilation_rate ) input_shape= ( 3 , 5 , 4 ) ) 'padding ' : 'valid ' , if not ( padding == 'same ' and strides ! = ( 1 , 1 , 1 ) ) ] 'data_format , padding ' , 'data_format ' : 'channels_last ' } , 'kernel_size ' : kernel_size , 'strides ' , kwargs= { 'padding ' : ( 2 , 2 ) , 'data_format ' : data_format } , for data_format in [ 'channels_first ' , 'channels_last ' ] ] stack_size ) ) kwargs= { 'filters ' : filters , 'data_format ' : data_format } , ( ( 1 , 1 ) , 'valid ' , 'channels_first ' , ( 3 , 4 , 5 , 6 ) ) ] model.add ( Masking ( mask_value=0. , input_shape= ( 3 , 4 ) ) ) kwargs= { 'strides ' : strides , layer_test ( convolutional.AveragePooling2D , for data_format in [ 'channels_first ' , 'channels_last ' ] : num_samples = 2 input_data=np.reshape ( np.arange ( 4 , dtype='float32 ' ) , ( 1 , 4 , 1 ) ) , input_data=np.reshape ( np.arange ( 10 , dtype='float32 ' ) , ( 1 , 10 , 1 ) ) , layer_test ( convolutional.Conv1D , layer_test ( convolutional.MaxPooling3D , def test_globalpooling_1d ( data_format , pooling_class ) : layer_test ( convolutional.DepthwiseConv2D , and not ( dilation_rate ! = ( 1 , 1 ) and K.backend ( ) == 'cntk ' ) ) ] for pooling_class in [ pooling.GlobalMaxPooling3D , kwargs= { 'data_format ' : 'channels_last ' } , for dilation_rate in [ 1 , 2 ] for pooling_class in [ pooling.GlobalMaxPooling1D , if data_format == 'channels_last ' : input_shape= ( num_samples , model_input = np.random.randint ( low=1 , high=5 , size= ( 2 , 3 , 4 ) ) convolutional.Conv3DTranspose , def test_separable_conv_1d ( padding , strides , multiplier , dilation_rate ) : def test_conv_1d ( ) : for multiplier in [ 1 , 2 ] stack_size ) ) layer_test ( convolutional.AveragePooling1D , input_shape= ( num_samples , num_row , num_col , stack_size ) ) if not ( padding == 'same ' and strides ! = ( 1 , 1 ) ) ] from keras.layers import Masking kwargs= { 'strides ' : stride , for padding in _convolution_paddings : layer_test ( convolutional.MaxPooling1D , input_shape= ( num_samples , num_step , stack_size ) ) assert np.array_equal ( output [ 0 ] , model_input [ 0 , 0 , : ] ) kwargs= { 'strides ' : 2 , 'data_format ' : 'channels_last ' } , def test_convolution_2d_invalid ( ) : model = Sequential ( ) strides = ( 2 , 2 , 2 ) def test_maxpooling_2d ( strides ) : # Non-causal : input_num_row = 4 input_shape= ( num_samples , [ ( ( 2 , 2 ) , 'same ' , None , ( 3 , 5 , 6 , 4 ) ) , inputs = np.ones ( ( num_samples , stack_size , input_num_row , input_num_col ) ) def test_maxpooling_3d ( ) : 'padding ' : 'valid ' , for out_padding in [ None , ( 0 , 0 ) , ( 1 , 1 ) ] # Test GlobalAveragePooling1D supports masking pooling.GlobalAveragePooling3D ] ] def test_globalpooling_1d_supports_masking ( ) : for out_padding in [ None , ( 0 , 0 ) , ( 1 , 1 ) ] : 'dilation_rate ' : dilation_rate } , model.add ( pooling.GlobalAveragePooling1D ( ) ) 'kernel_size ' : 3 , kernel_size = 3 output = model.predict ( model_input ) 'strides ' : strides , input_len_dim3 = 8 'data_format , pooling_class ' , def test_globalpooling_2d ( ) : # Causal : kwargs= { 'strides ' : strides , if strides == ( 1 , 1 ) and out_padding == ( 1 , 1 ) : 'pool_size ' : pool_size } , def test_conv_1d_dilation ( ) : for multiplier in [ 1 , 2 ] : 'pool_size ' : ( 2 , 2 ) , if padding == 'same ' and strides ! = ( 1 , 1 , 1 ) : 10 , np.float32 ( [ [ [ 0 ] , [ 1 ] , [ 2 ] , [ 4 ] , [ 6 ] , [ 9 ] , [ 12 ] , [ 15 ] , [ 18 ] , [ 21 ] ] ] ) ) , layer_test ( pooling.GlobalAveragePooling1D , input_shape= ( 3 , 5 , 6 , 4 ) ) inputs = np.ones ( ( num_samples , stack_size , input_num_row , input_num_col ) ) num_depth = 7 def test_depthwise_conv_2d ( padding , strides , multiplier ) : kwargs= { 'data_format ' : data_format } , for strides in [ ( 1 , 1 ) , ( 2 , 2 ) ] : input_shape= ( num_samples , stack_size , num_row , num_col ) ) for dilation_rate in [ ( 1 , 1 ) , ( 2 , 2 ) , ( 2 , 1 ) , ( 1 , 2 ) ] : input_len_dim1 = 9 [ [ [ 0 ] , [ 1 ] , [ 2 ] , [ 4 ] , [ 6 ] , [ 9 ] , [ 12 ] , [ 15 ] , [ 18 ] , [ 21 ] ] ] ) fixed_batch_size=True ) if dilation_rate ! = ( 1 , 1 ) and multiplier == dilation_rate [ 0 ] : 'kernel_size ' : 2 , continue 'data_format ' : data_format , def test_convolution_2d ( ) : 'activity_regularizer ' : 'l2 ' , inputs = np.ones ( ( num_samples , input_num_row , input_num_col , stack_size ) ) [ ( padding , stride , data_format ) 'strides ' : strides , input_shape= ( 3 , 4 , 3 , 4 , 3 ) ) input_shape= ( None , num_depth , num_row , num_col , stack_size ) , input_shape= ( 3 , 4 , 3 , 4 , 3 ) ) 'data_format ' : data_format } , kwargs= { 'kernel_size ' : ( 3 , 3 ) , 'kernel_size ' : 3 , if ( not ( padding == 'same ' and strides ! = ( 1 , 1 ) ) kwargs= { 'strides ' : ( 1 , 1 ) , 'pool_size ' : pool_size } , def test_averagepooling_2d ( ) : input_len_dim1 , input_len_dim2 , input_len_dim3 , input_shape= ( 3 , 4 , 5 ) ) def test_conv2d_transpose ( ) : if dilation_rate ! = 1 and K.backend ( ) == 'cntk ' : 'padding , strides ' , layer_test ( convolutional.Conv1D , multiplier = 2 for stride in [ 1 , 2 ] num_col , 'padding , out_padding , strides , data_format ' , for data_format in [ 'channels_first ' , 'channels_last ' ] input_shape= ( 3 , 4 , 5 , 6 ) ) # Causal layer_test ( convolutional.ZeroPadding3D , 'pool_size ' : ( 2 , 2 ) } , layer_test ( pooling_class , if dilation_rate ! = ( 1 , 1 ) and K.backend ( ) == 'cntk ' : model.add ( Masking ( mask_value=0. , input_shape= ( 3 , 4 ) ) ) layer_test ( convolutional.Conv2DTranspose , if data_format == 'channels_last ' : 'padding ' : 'valid ' , 'kernel_size ' : kernel_size , def test_conv3d_transpose ( padding , out_padding , strides , data_format ) : 'dilation_rate ' : 2 } , and not ( strides == ( 1 , 1 ) and out_padding == ( 1 , 1 ) ) ) ] input_data = np.reshape ( np.arange ( input_length , dtype='float32 ' ) , model.compile ( loss='mae ' , optimizer='adam ' ) input_shape= ( 3 , 5 , 4 ) ) for padding in [ 'valid ' , 'same ' ] : 'data_format ' : data_format } , padding = 'valid ' 'padding ' : padding , input_len_dim2 = 8 for strides in [ ( 1 , 1 , 1 ) , ( 2 , 2 , 2 ) ] : ( { 'filters ' : 1 , 'kernel_size ' : 2 , 'dilation_rate ' : 1 , 'padding ' : 'causal ' , def test_globalpooling_3d ( data_format , pooling_class ) : def test_averagepooling_3d ( ) : layer_test ( convolutional.AveragePooling2D , import pytest layer_test ( pooling.GlobalAveragePooling2D , def test_convolution_3d ( padding , strides ) : # Causal dilated with larger kernel size","['tests/keras/layers/convolutional_test.py', 'tests/keras/layers/pooling_test.py']",Separate pooling test from convolutional test and parameterize test case ( # 10975 )
317,f06c7e4dcd9d3044580dff155cdeb4978e622188,2018-08-28 12:59:23-07:00,"# cntk only support dilated conv on GPU 'padding ' : padding , layer_test ( convolutional.Conv2D , strides=tuple ( strides ) , @ pytest.mark.skipif ( ( K.backend ( ) == 'cntk ' ) , dilation=dilation_rate ) strides = strides + ( strides [ 0 ] , ) 'kernel_size ' : kernel_size , 'dilation_rate ' : ( 2 , 2 ) } , padding ] ) if K.backend ( ) ! = 'cntk ' : strides=strides , input_shape= ( num_samples , num_row , num_col , stack_size ) ) 'Please set ` dilation_rate ` to 1 . You passed : % s ' % ( dilation_rate , ) ) 'dilation_rate ' : 2 } , 'Please set ` dilation_rate ` to ( 1 , 1 , 1 ) . ' auto_padding= [ False , padding ] , 'dilation_rate ' : 2 } , reason='cntk only support dilated conv on GPU ' ) 'padding ' : padding , padding , input_shape= ( batch_size , steps , input_dim ) ) if dev.type ( ) == 0 and dilation_rate ! = ( 1 , 1 , 1 ) : False , 'kernel_size ' : kernel_size , strides = [ strides ] reason= '' cntk does not support dilated conv '' ) layer_test ( convolutional.Conv1D , if dev.type ( ) == 0 and dilation_rate ! = 1 : 'Please set ` dilation_rate ` to ( 1 , 1 ) . ' layer_test ( convolutional.Conv2D , raise ValueError ( 'Dilated convolution on CPU is not supported by CNTK backend . ' 'dilation_rate ' : ( 2 , 2 ) } , raise ValueError ( 'Dilated convolution on CPU is not supported by CNTK backend . ' kwargs= { 'filters ' : filters , 'You passed : % s ' % ( dilation_rate , ) ) input_shape= ( batch_size , steps , input_dim ) ) auto_padding= [ False , padding , padding , padding ] , kwargs= { 'filters ' : filters , 'Please set dilation_rate with ( 1 , 1 ) . ' ) layer_test ( convolutional.Conv1D , reason= '' cntk only supports dilated conv on GPU '' ) input_shape= ( num_samples , num_row , num_col , stack_size ) ) auto_padding= [","['keras/backend/cntk_backend.py', 'tests/keras/layers/convolutional_test.py']",Conv1D and Conv3D supporting dilated conv for CNTK backend ( # 10997 )
318,72e326dde38cb731424210bfd6e56f301ddc9e2b,2018-08-28 12:55:02-07:00,"def compute_output_shape ( self , input_shape ) : super ( ZeroPadding2D , self ) .__init__ ( normalized_padding , length = None 'data_format ' : self.data_format } normalized_padding = ( height_padding , width_padding ) dim3 ) inputs with shape ` ( batch , channels , ... ) ` . self.input_spec = InputSpec ( ndim=5 ) dim2 = None self.padding = ( dim1_padding , dim2_padding , dim3_padding ) dim1 = input_shape [ 1 ] + self.padding [ 0 ] [ 0 ] + self.padding [ 0 ] [ 1 ] self.padding = ( height_padding , width_padding ) super ( ZeroPadding2D , self ) .__init__ ( * * kwargs ) return K.temporal_padding ( inputs , padding=self.padding ) config = { 'padding ' : self.padding } dim2 = input_shape [ 3 ] + self.padding [ 1 ] [ 0 ] + self.padding [ 1 ] [ 1 ] spatial_axes = list ( range ( 1 , 1 + self.rank ) ) self.data_format = K.normalize_data_format ( data_format ) return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) super ( ZeroPadding1D , self ) .__init__ ( normalized_padding , data_format : A string , dim2 , # self.rank is 1 for ZeroPadding1D , 2 for ZeroPadding2D . one of ` `` channels_last '' ` or ` `` channels_first '' ` . self.input_spec = InputSpec ( ndim=4 ) self.data_format = K.normalize_data_format ( data_format ) config [ 'padding ' ] = config [ 'padding ' ] [ 0 ] dim3 , output_shape [ dim ] += sum ( padding_all_dims [ dim ] ) dim1 = input_shape [ 2 ] + self.padding [ 0 ] [ 0 ] + self.padding [ 0 ] [ 1 ] The ordering of the dimensions in the inputs . config = super ( ZeroPadding1D , self ) .get_config ( ) elif self.data_format == 'channels_last ' : base_config = super ( ZeroPadding3D , self ) .get_config ( ) config = { 'padding ' : self.padding , self.padding = ( ( padding , padding ) , ( padding , padding ) , ( padding , padding ) ) input_shape [ 3 ] ) rows , data_format , length , length = input_shape [ 1 ] + self.padding [ 0 ] + self.padding [ 1 ] self.padding = ( ( padding , padding ) , ( padding , padding ) ) cols = input_shape [ 3 ] + self.padding [ 1 ] [ 0 ] + self.padding [ 1 ] [ 1 ] class _ZeroPadding ( Layer ) : padding_all_dims = transpose_shape ( padding_all_dims , Keras config file at ` ~/.keras/keras.json ` . def get_config ( self ) : return ( input_shape [ 0 ] , input_shape [ 1 ] , normalized_padding = ( dim1_padding , dim2_padding , dim3_padding ) self.input_spec = InputSpec ( ndim=self.rank + 2 ) return tuple ( output_shape ) rows = None ` `` channels_last '' ` corresponds to inputs with shape padding_all_dims = ( ( 0 , 0 ) , ) + self.padding + ( ( 0 , 0 ) , ) def call ( self , inputs ) : output_shape = list ( input_shape ) `` `` '' Abstract nD ZeroPadding layer ( private , used as implementation base ) . 'channels_last ' , else : dim3 = input_shape [ 4 ] + self.padding [ 2 ] [ 0 ] + self.padding [ 2 ] [ 1 ] ` ( batch , ... , channels ) ` while ` `` channels_first '' ` corresponds to normalized_padding = ( ( padding , padding ) , ( padding , padding ) , ( padding , padding ) ) dim1 = None input_shape [ 2 ] ) if output_shape [ dim ] is not None : if input_shape [ 4 ] is not None : if input_shape [ 1 ] is not None : super ( ZeroPadding3D , self ) .__init__ ( * * kwargs ) self.data_format , base_config = super ( _ZeroPadding , self ) .get_config ( ) rows = input_shape [ 1 ] + self.padding [ 0 ] [ 0 ] + self.padding [ 0 ] [ 1 ] cols , class ZeroPadding3D ( _ZeroPadding ) : for dim in range ( len ( output_shape ) ) : `` `` '' config.pop ( 'data_format ' ) self.padding = padding normalized_padding = ( conv_utils.normalize_tuple ( padding , 2 , 'padding ' ) , ) def compute_output_shape ( self , input_shape ) : if input_shape [ 1 ] is not None : dim3 = input_shape [ 3 ] + self.padding [ 2 ] [ 0 ] + self.padding [ 2 ] [ 1 ] def __init__ ( self , padding , data_format=None , * * kwargs ) : normalized_padding = ( ( padding , padding ) , ( padding , padding ) ) if input_shape [ 3 ] is not None : base_config = super ( ZeroPadding2D , self ) .get_config ( ) super ( _ZeroPadding , self ) .__init__ ( * * kwargs ) super ( ZeroPadding3D , self ) .__init__ ( normalized_padding , super ( ZeroPadding1D , self ) .__init__ ( * * kwargs ) class ZeroPadding1D ( Layer ) : config = { 'padding ' : self.padding , if self.data_format == 'channels_first ' : class ZeroPadding1D ( _ZeroPadding ) : If you never set it , then it will be `` channels_last '' . dim1 , if input_shape [ 2 ] is not None : It defaults to the ` image_data_format ` value found in your self.padding = conv_utils.normalize_tuple ( padding , 2 , 'padding ' ) else : return K.temporal_padding ( inputs , padding=self.padding [ 0 ] ) class ZeroPadding2D ( Layer ) : return config cols = None raise NotImplementedError rank is 1 . # Arguments spatial_axes ) rows = input_shape [ 2 ] + self.padding [ 0 ] [ 0 ] + self.padding [ 0 ] [ 1 ] * * kwargs ) base_config = super ( ZeroPadding1D , self ) .get_config ( ) class ZeroPadding2D ( _ZeroPadding ) : dim3 = None dim2 = input_shape [ 2 ] + self.padding [ 1 ] [ 0 ] + self.padding [ 1 ] [ 1 ] padding : Tuple of tuples of two ints . Can be a tuple of ints when cols = input_shape [ 2 ] + self.padding [ 1 ] [ 0 ] + self.padding [ 1 ] [ 1 ] return ( input_shape [ 0 ] , 'data_format ' : self.data_format } class ZeroPadding3D ( Layer ) : self.input_spec = InputSpec ( ndim=3 ) def get_config ( self ) : return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) self.rank = len ( padding ) cols ) input_shape [ 4 ] )",['keras/layers/convolutional.py'],Made a base class for ZeroPadding . ( # 10984 )
319,7a195b64ea02e02698083b0b795287ffa6e72cf8,2018-08-28 11:27:57-07:00,"def test_conv_float64 ( input_shape , conv_class ) : [ ( ( 2 , 4 , 2 ) , convolutional.Conv1D ) , ( ( 2 , 4 , 4 , 2 ) , convolutional.Conv2D ) , kernel_size = 3 layer_test ( conv_class , strides = 1 and StrictVersion ( tf.__version__ ) < StrictVersion ( ' 1.8.0 ' ) ) : # tensorflow does n't support float64 for conv layer before 1.8.0 if ( dtype ( x ) == 'float64 ' K.set_floatx ( 'float64 ' ) kwargs= { 'filters ' : filters , 'padding ' : 'valid ' , 'kernel_size ' : kernel_size , ) reason='CNTK does not support float64 ' ) if dtype ( x ) == 'float64 ' : from distutils.version import StrictVersion input_shape=input_shape ) 'strides ' : strides } , ( ( 2 , 4 , 4 , 4 , 2 ) , convolutional.Conv3D ) ] K.set_floatx ( 'float32 ' ) filters = 3 'input_shape , conv_class ' ,","['keras/backend/tensorflow_backend.py', 'tests/keras/layers/convolutional_test.py']",Convolutional layer supports float64 dtype after tensorflow 1.8.0 ( # 10977 )
320,be6d8293f26d6d577b61874ccf6d68f116734d5a,2018-08-28 10:05:12-07:00,"y = func ( x , w , args [ 2 ] , args [ 3 ] ) check_single_tensor_operation ( 'relu ' , ( 4 , 2 ) , WITH_NP , alpha=0.1 , max_value=0.5 ) x2 = depthwise_conv ( x , w1 , padding=padding , data_format=data_format ) check_single_tensor_operation ( 'round ' , ( 4 , 2 ) , WITH_NP ) assert_list_pairwise ( z_list , allclose=assert_value_equality ) z = f ( [ x_val , y_val ] ) [ 0 ] y_shape , y_val = parse_shape_or_val ( y_shape_or_val ) [ k.variable ( x_val ) , k.variable ( y_val ) ] , * * kwargs ) check_two_tensor_operation ( 'equal ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS ) check_two_tensor_operation ( 'maximum ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) return ( target * -np.log ( sigmoid ( output ) ) def greater_equal ( x , y ) : k.variable ( x_val ) , k.variable ( y_val ) , * * kwargs ) def not_equal ( x , y ) : check_single_tensor_operation ( 'tanh ' , ( 4 , 2 ) , WITH_NP ) assert_value_with_ref = kwargs.pop ( 'assert_value_with_ref ' , None ) k.variable ( x_val ) , k.variable ( convert_kernel ( y_val ) ) , * * kwargs ) check_two_tensor_operation ( 'binary_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP , from_logits=False ) assert_list_pairwise ( z_list , allclose=assert_value_equality ) op , input_shape , WITH_NP , if K.backend ( ) == k.__name__.split ( ' . ' ) [ -1 ] [ : -8 ] ] return_results = kwargs.pop ( 'return_results ' , False ) depthwise_conv2d = depthwise_conv return np.sum ( target * -np.log ( output ) , axis=-1 , keepdims=False ) z = k.eval ( t ) assert_value_with_ref = getattr ( KNP , function_name ) ( x_val , * * kwargs ) check_single_tensor_operation ( 'sign ' , ( 4 , 2 ) , BACKENDS ) if shape_or_val : y1 = KNP.separable_conv ( x , depthwise , pointwise , check_single_tensor_operation ( 'softmax ' , ( 4 , 5 , 3 , 10 ) , BACKENDS , axis=2 ) return conv ( x2 , w2 , padding , data_format ) conv3d = conv conv1d = conv output = np.clip ( output , 1e-7 , 1 - 1e-7 ) z = k.eval ( t ) elif ( k == KTH ) & ( function_name [ :4 ] == 'conv ' ) : y1 = KNP.depthwise_conv ( x , w , padding , data_format ) check_single_tensor_operation ( 'sigmoid ' , ( 4 , 2 ) , WITH_NP ) check_two_tensor_operation ( 'not_equal ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) return x < y return np.minimum ( x , y ) WITH_NP , cntk_two_dynamicity=True , from_logits=True ) rep=reps , axis=rep_axis ) t , f = cntk_func_two_tensor ( function_name , x_shape , y=y_shape , * * kwargs ) check_single_tensor_operation ( 'softplus ' , ( 4 , 10 ) , BACKENDS ) ( 1 - target ) * -np.log ( 1 - sigmoid ( output ) ) ) if K.backend ( ) ! = 'cntk ' : z = f ( [ x_val , y_val ] ) [ 0 ] check_single_tensor_operation ( 'softmax ' , ( 4 , 10 ) , BACKENDS ) output = softmax ( output ) k.variable ( x_val ) , k.variable ( y_val ) , * * kwargs ) return x == y except : x_shape , x_val = parse_shape_or_val ( x_shape_or_val ) check_two_tensor_operation ( 'greater_equal ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) BACKENDS , cntk_two_dynamicity=True , from_logits=True ) if return_results : WITH_NP = [ KTH if K.backend ( ) == 'theano ' else KC if K.backend ( ) == 'cntk ' else KTF , KNP ] def wrapper ( * args ) : check_single_tensor_operation ( 'repeat_elements ' , arr , BACKENDS , if assert_value_with_ref is not None : check_two_tensor_operation ( _ , w = parse_shape_or_val ( kernel_shape ) for z in z_list : if from_logits : check_single_tensor_operation ( 'hard_sigmoid ' , ( 4 , 2 ) , BACKENDS ) assert_value_with_ref = None check_single_tensor_operation ( 'repeat_elements ' , arr , WITH_NP , backend_list = [ k for k in backend_list check_single_tensor_operation ( 'exp ' , ( 4 , 2 ) , WITH_NP ) cntk_dynamicity=True , return_results=True ) output /= output.sum ( axis=-1 , keepdims=True ) z = f ( [ x_val ] ) [ 0 ] def eval ( x ) : z = f ( [ x_val ] ) [ 0 ] check_two_tensor_operation ( 'less ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS ) # If we can take a NumPy output , it is efficient to compare the outputs def less ( x , y ) : assert_allclose ( z , ref , atol=1e-05 ) y1 = KNP.separable_conv ( x , depthwise , pointwise , padding , data_format ) elif ( k == KTH ) & ( function_name [ :4 ] == 'conv ' ) : pool3d = pool else : if ( k == KC ) & ( cntk_dynamicity ) : check_two_tensor_operation ( 'greater_equal ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS ) else : else : if args [ 3 ] == 'channels_last ' : check_two_tensor_operation ( 'greater ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'l2_normalize ' , ( 4 , 3 ) , BACKENDS , axis=-1 ) # from a single backend and NumPy . check_two_tensor_operation ( 'minimum ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) return x > = y assert_value_with_ref=np_rep ) def minimum ( x , y ) : check_single_tensor_operation ( 'sigmoid ' , ( 4 , 2 ) , BACKENDS ) check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP , from_logits=False ) [ k.variable ( x_val ) , k.variable ( y_val ) ] , * * kwargs ) output = np.log ( output / ( 1 - output ) ) if ( k == KC ) & ( cntk_dynamicity ) : check_two_tensor_operation ( 'less_equal ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) return z_list [ 0 ] _ , x = parse_shape_or_val ( input_shape ) y1 = KNP.conv ( x , w , padding , data_format ) check_single_tensor_operation ( 'softmax ' , ( 4 , 5 , 3 , 10 ) , WITH_NP , axis=2 ) check_single_tensor_operation ( 'exp ' , ( 4 , 2 ) , BACKENDS ) separable_conv2d = separable_conv assert_list_with_ref ( z_list , assert_value_with_ref ) check_single_tensor_operation ( 'sign ' , ( 4 , 2 ) , WITH_NP ) def assert_list_with_ref ( z_list , ref ) : if len ( z_list ) > 1 : op , input_shape , kernel_shape , WITH_NP , op , x , w , [ KTH if k == 'theano ' else KC if k == 'cntk ' else KTF ] , y_shape , y_val = parse_shape_or_val ( y_shape_or_val ) WITH_NP = [ K , KNP ] check_single_tensor_operation ( 'hard_sigmoid ' , ( 4 , 2 ) , WITH_NP ) pool2d = pool elif ( k == KC ) & ( cntk_two_dynamicity ) : k.variable ( x_val ) , k.variable ( convert_kernel ( y_val ) ) , * * kwargs ) check_single_tensor_operation ( 'elu ' , ( 4 , 10 ) , BACKENDS , alpha=0.5 ) rep=reps , axis=rep_axis , check_two_tensor_operation ( 'binary_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS , from_logits=False ) check_single_tensor_operation ( 'l2_normalize ' , ( 4 , 3 ) , WITH_NP , axis=-1 ) check_two_tensor_operation ( 'less_equal ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'softplus ' , ( 4 , 10 ) , WITH_NP ) t = getattr ( k , function_name ) ( check_two_tensor_operation ( 'binary_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP , from_logits=True ) def equal ( x , y ) : check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , [ KTH , KTF ] , from_logits=True ) return x > y op , x , [ KTH if k == 'theano ' else KC if k == 'cntk ' else KTF ] , output = np.clip ( output , 1e-7 , 1 - 1e-7 ) t , f = cntk_func_two_tensor ( function_name , x_shape , y=y_val , * * kwargs ) check_single_tensor_operation ( 'tanh ' , ( 4 , 2 ) , BACKENDS ) elif ( k == KC ) & ( cntk_two_dynamicity ) : if args [ 3 ] == 'channels_last ' : check_single_tensor_operation ( 'elu ' , ( 4 , 10 ) , WITH_NP , alpha=0.5 ) k = K.backend ( ) return conv ( x2 , w2 , padding=padding , data_format=data_format ) check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP , from_logits=True ) def less_equal ( x , y ) : check_single_tensor_operation ( 'softmax ' , ( 4 , 10 ) , WITH_NP ) padding=padding , data_format=data_format ) check_single_tensor_operation ( 'relu ' , ( 4 , 2 ) , BACKENDS , alpha=0.1 , max_value=0.5 ) if not from_logits : if shape_or_val : y = func ( x , w , * * kwargs ) t , f = cntk_func_two_tensor ( function_name , x_shape , y=y_shape , * * kwargs ) else : if kwargs [ 'data_format ' ] == 'channels_last ' : check_two_tensor_operation ( 'not_equal ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS ) check_two_tensor_operation ( 'minimum ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS ) t , f = cntk_func_two_tensor ( function_name , x_shape , y=y_val , * * kwargs ) check_two_tensor_operation ( 'greater ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS ) check_two_tensor_operation ( 'less ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) np_rep = np.repeat ( arr , reps , axis=rep_axis ) check_two_tensor_operation ( 'binary_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS , from_logits=True ) separable_conv1d = separable_conv check_single_tensor_operation ( 'l2_normalize ' , ( 4 , 3 ) , WITH_NP , axis=1 ) check_two_tensor_operation ( 'categorical_crossentropy ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS , from_logits=False ) def maximum ( x , y ) : t = getattr ( k , function_name ) ( x_shape_or_val , y_shape_or_val , * * kwargs ) def repeat_elements ( x , rep , axis ) : check_single_tensor_operation ( 'l2_normalize ' , ( 4 , 3 ) , BACKENDS , axis=1 ) elif concat_args : y1 = KNP.pool ( x , pool_size , strides , padding , data_format , pool_mode ) return z_list return x return x < = y y2 = check_single_tensor_operation ( return np.repeat ( x , rep , axis=axis ) check_single_tensor_operation ( if kwargs [ 'data_format ' ] == 'channels_last ' : assert z.shape == ref.shape check_single_tensor_operation ( 'round ' , ( 4 , 2 ) , BACKENDS ) x2 = depthwise_conv ( x , w1 , padding , data_format ) elif concat_args : def wrapper ( * args , * * kwargs ) : x_shape , x_val = parse_shape_or_val ( x_shape_or_val ) try : conv2d = conv t = getattr ( k , function_name ) ( assert_allclose ( y1 , y2 , atol=1e-05 ) shape_or_val = kwargs.pop ( 'shape_or_val ' , True ) check_two_tensor_operation ( 'equal ' , ( 4 , 2 ) , ( 4 , 2 ) , WITH_NP ) return np.maximum ( x , y ) y2 = check_two_tensor_operation ( cntk_dynamicity=True ) def binary_crossentropy ( target , output , from_logits=False ) : def categorical_crossentropy ( target , output , from_logits=False ) : # Leave only the designated backend from the test list of backends . def greater ( x , y ) : check_two_tensor_operation ( 'maximum ' , ( 4 , 2 ) , ( 4 , 2 ) , BACKENDS ) return x ! = y","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Speed up backend tests ( # 10992 )
321,785d6e396ce81575bb155bede412a6182d5f22ed,2018-08-28 09:45:16-07:00,pip install https : //cntk.ai/PythonWheel/GPU/cntk-2.1-cp36-cp36m-linux_x86_64.whl & & \ pip install cntk pip install https : //cntk.ai/PythonWheel/CPU-Only/cntk-2.5.1-cp36-cp36m-linux_x86_64.whl ; elif [ [ `` $ TRAVIS_PYTHON_VERSION '' == `` 3.6 '' ] ] ; then fi cntk-gpu & & \ if [ [ `` $ TRAVIS_PYTHON_VERSION '' == `` 2.7 '' ] ] ; then pip install https : //cntk.ai/PythonWheel/CPU-Only/cntk-2.5.1-cp27-cp27mu-linux_x86_64.whl ;,"['.travis.yml', 'docker/Dockerfile']",Used `` pip install cntk '' to simplify the installation of CNTK . ( # 11011 )
322,028c47dc5ce8de8a5513d984969f1f4231be6cc6,2018-08-27 10:05:44-07:00,[ How can I install HDF5 or h5py to save my models in Keras ? ] ( # how-can-i-install-hdf5-or-h5py-to-save-my-models-in-keras ) [ How can I install HDF5 or h5py to save my models in Keras ? ] ( # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) Please also see [ How can I install HDF5 or h5py to save my models in Keras ? ] ( # how-can-i-install-hdf5-or-h5py-to-save-my-models-in-keras ) for instructions on how to install ` h5py ` . Please also see [ How can I install HDF5 or h5py to save my models in Keras ? ] ( # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) for instructions on how to install ` h5py ` .,['docs/templates/getting-started/faq.md'],Fix broken links in FAQ ( # 11010 )
323,4bcb8c95d8bba3147dd86446cd91be77051cef96,2018-08-25 15:00:07-07:00,conda install pil ; fi keras/preprocessing/ * if [ [ `` $ TEST_MODE '' == `` INTEGRATION_TESTS '' ] ] || [ [ `` $ TEST_MODE '' == `` PEP8 '' ] ] ; then if [ [ `` $ TRAVIS_PYTHON_VERSION '' == `` 2.7 '' ] ] ; then # install PIL for preprocessing tests conda install pil ; if [ [ `` $ TRAVIS_PYTHON_VERSION '' == `` 2.7 '' ] ] ; then else else conda install Pillow ; conda install Pillow ; # install PIL for preprocessing tests ( they are integration tests ) .,"['.coveragerc', '.travis.yml', 'tests/{keras => integration_tests}/preprocessing/image_test.py', 'tests/{keras => integration_tests}/preprocessing/sequence_test.py', 'tests/{keras => integration_tests}/preprocessing/text_test.py']",Moved the preprocessing tests into the integration directory . ( # 10963 )
324,12d5732ad20cece418d47336dbced11d1b76b841,2018-08-25 14:47:35-07:00,"pip install -- only-binary=numpy , scipy numpy nose scipy matplotlib h5py theano conda create -q -n test-environment python= $ TRAVIS_PYTHON_VERSION pytest pandas travis_retry pip install -- only-binary=numpy , scipy numpy nose scipy matplotlib h5py theano travis_retry conda create -q -n test-environment python= $ TRAVIS_PYTHON_VERSION pytest pandas",['.travis.yml'],Added travis_retry for commands which often fail . ( # 10980 )
325,96c3c195ca805758e094780b59077bea9db99f41,2018-08-25 14:46:40-07:00,"stitched_filters [ rotate=False , ud=False , multi_fonts=False ) callbacks= [ viz_cb , img_gen ] , model.fit_generator ( generator=img_gen.next_train ( ) , np.random.choice ( [ cairo.FONT_WEIGHT_BOLD , cairo.FONT_WEIGHT_NORMAL ] ) ) print ( 'Average train sequence length : { } '.format ( np.mean ( list ( map ( len , x_train ) ) , dtype=int ) ) ) val_split=words_per_epoch - val_words os.path.join ( run_name , 'weights % 02d.h5 ' % ( start_epoch - 1 ) ) ) bigram_file=os.path.join ( fdir , 'wordlist_bi_clean.txt ' ) , epochs=stop_epoch , ' % .3f Mean normalized edit distance : % 0.3f ' [ Dmitry Ulyanov 's blog on fast-neural-doodle ] ( http : //dmitryulyanov.github.io/feed-forward-neural-doodle/ ) a = K.square ( self.paint_func = lambda text : paint_text ( text , self.img_w , self.img_h , [ A Hierarchical Neural Autoencoder for Paragraphs and Documents ] self.paint_func = lambda text : paint_text ( text , self.img_w , self.img_h , conv_to_rnn_dims = ( img_w // ( pool_size * * 2 ) , ( img_h // ( pool_size * * 2 ) ) * conv_filters ) ( word_batch [ 'source_str ' ] [ i ] , res [ i ] ) ) context.select_font_face ( np.random.choice ( fonts ) , cairo.FONT_SLANT_NORMAL , path = get_file ( 'nietzsche.txt ' , origin='https : //s3.amazonaws.com/text-datasets/nietzsche.txt ' ) return K.sqrt ( K.maximum ( sum_square , K.epsilon ( ) ) ) np.random.choice ( [ cairo.FONT_WEIGHT_BOLD , cairo.FONT_WEIGHT_NORMAL ] ) ) max_string_len is None or res = decode_batch ( self.test_func , kernel_initializer='he_normal ' , name='gru1 ' ) ( inner ) ( 1 - y_true ) * K.square ( K.maximum ( margin - y_pred , 0 ) ) ) examples/imdb_lstm.py E501 \ epochs=epochs , return K.sqrt ( K.maximum ( K.sum ( K.square ( x - y ) , axis=1 , keepdims=True ) , K.epsilon ( ) ) ) examples/mnist_tfrecord.py E501 \ ( https : //github.com/DmitryUlyanov/fast-neural-doodle ) print ( '\nOut of % d samples : Mean edit distance : % .3f Mean normalized edit distance : % 0.3f ' img/starry_night.jpg results/my_result x [ : , : , : img_nrows - 1 , : img_ncols - 1 ] - x [ : , : , : img_nrows - 1 , 1 : ] ) OUTPUT_DIR , examples/image_ocr.py E501 \ rotate=False , ud=True , multi_fonts=True ) 'nietzsche.txt ' , assert ( layer_name in layer_dict.keys ( ) , 'Truth = \ ' % s\'\nDecoded = \ ' % s\ '' % outputs=loss_out ) conv_to_rnn_dims = ( img_w // ( pool_size * * 2 ) , fonts = [ 'Century Schoolbook ' , 'Courier ' , 'STIX ' , 'URW Chancery L ' , 'FreeMono ' ] 'from supplied monogram and bigram files . ' ) ( img_h // ( pool_size * * 2 ) ) * conv_filters ) rotate=False , ud=True , multi_fonts=False ) pylab.xlabel ( self.minibatch_size , train=True ) img_w=img_w , weight_file = os.path.join ( OUTPUT_DIR , os.path.join ( run_name , 'weights % 02d.h5 ' % ( start_epoch - 1 ) ) ) monogram_file=os.path.join ( fdir , 'wordlist_mono_clean.txt ' ) , cairo.FONT_SLANT_NORMAL , ) get_file ( 'wordlists.tgz ' , margin_square = K.square ( K.maximum ( margin - y_pred , 0 ) ) raise IOError ( 'Could not pull enough words ' def _is_length_of_word_valid ( word ) : gru_2 = GRU ( rnn_size , return_sequences=True , return K.mean ( y_true * sqaure_pred + ( 1 - y_true ) * margin_square ) gru_1 = GRU ( rnn_size , return_sequences=True , kernel_initializer='he_normal ' , name='gru1 ' ) ( inner ) [ Hierarchical recurrent neural network for skeleton based action recognition ] b = K.square ( ( http : //arxiv.org/abs/1603.03417 ) self.paint_func = lambda text : paint_text ( labels = Input ( name='the_labels ' , if _is_length_of_word_valid ( word ) : loss_out = Lambda ( [ Discussion on parameter tuning ] self.model.save_weights ( ( https : //github.com/DmitryUlyanov/online-neural-doodle ) width_margin = ( img_width + margin ) * i b = K.square ( x [ : , : , : img_nrows - 1 , : img_ncols - 1 ] - x [ : , : , : img_nrows - 1 , 1 : ] ) os.path.join ( self.output_dir , 'weights % 02d.h5 ' % ( epoch ) ) ) self.model.save_weights ( os.path.join ( self.output_dir , 'weights % 02d.h5 ' % ( epoch ) ) ) model = Model ( inputs= [ input_data , labels , input_length , label_length ] , outputs=loss_out ) img_gen = TextImageGenerator ( monogram_file=os.path.join ( fdir , 'wordlist_mono_clean.txt ' ) , maxlen = 80 # cut texts after this number of words ( among top max_features most common words ) kernel_initializer='he_normal ' , name='gru2_b ' ) ( gru1_merged ) examples/neural_doodle.py E501 \ gru_2b = GRU ( rnn_size , return_sequences=True , go_backwards=True , rotate=True , ud=True , multi_fonts=True ) examples/neural_style_transfer.py E501 \ np.mean ( list ( map ( len , x_train ) ) , dtype=int ) ) ) if is_valid_str ( word ) and _is_length_of_word_valid ( word ) : X_data [ i , 0 : self.img_w , : , 0 ] = self.paint_func ( self.X_text [ index + i ] ) [ 0 , : , : ] .T generator=img_gen.next_train ( ) , path = get_file ( ( http : //ieeexplore.ieee.org/stamp/stamp.jsp ? tp= & arnumber=7298714 ) X_data [ i , 0 , 0 : self.img_w , : ] = ( model.fit_generator ( context.select_font_face ( validation_data=img_gen.next_val ( ) , initial_epoch=start_epoch ) np.mean ( list ( map ( len , x_train ) ) , dtype=int ) ) ) [ Torch code for online-neural-doodle ] ( https : //github.com/DmitryUlyanov/online-neural-doodle ) decoded_res = decode_batch ( self.test_func , origin='http : //www.mythic-ai.com/datasets/wordlists.tgz ' , untar=True ) ) examples/imdb_fasttext.py E501 \ 0 , 0.1 , val_split=words_per_epoch - val_words ) train_model.fit ( name='gru1_b ' ) ( inner ) print ( 'Average train sequence length : { } '.format ( cairo.FONT_SLANT_NORMAL , size=teacher_w2.shape [ :2 ] + ( n , teacher_w2.shape [ 3 ] ) ) 'Century Schoolbook ' , 'Courier ' , 'STIX ' , go_backwards=True , kernel_initializer='he_normal ' , [ Torch code for fast-neural-doodle ] ( https : //github.com/DmitryUlyanov/fast-neural-doodle ) word_batch [ 'source_str ' ] [ j ] ) X_data [ i , 0 : self.img_w , : , 0 ] = ( text , self.img_w , self.img_h , ( 1 , m , nr , nc ) for 'channels_first ' or ( 1 , nr , nc , m ) for 'channels_last ' [ Hierarchical recurrent neural network for skeleton based action recognition ] ( http : //ieeexplore.ieee.org/stamp/stamp.jsp ? tp= & arnumber=7298714 ) x [ : , : img_nrows - 1 , : img_ncols - 1 , : ] - x [ : , : img_nrows - 1 , 1 : , : ] ) examples/mnist_hierarchical_rnn.py E501 \ maxlen = 80 fonts = [ examples/mnist_siamese.py E501 \ len ( word ) < = max_string_len ) img_gen = TextImageGenerator ( validation_data=img_gen.next_val ( ) , examples/mnist_net2net.py E501 \ validation_steps=val_words // minibatch_size , path_to_your_reference.jpg prefix_for_results ( http : //dmitryulyanov.github.io/feed-forward-neural-doodle/ ) if max_string_len == -1 or max_string_len is None or len ( word ) < = max_string_len : ret = self.get_batch ( self.cur_val_index , # The learned weights are reloaded initial_epoch=start_epoch ) callbacks= [ EvaluateInputTensor ( test_model , steps=100 ) ] ) the style transfer takes place ( Default is 10 ) edit_dist = editdistance.eval ( decoded_res [ j ] , return K.mean ( y_true * K.square ( y_pred ) width_margin : width_margin + img_width , print ( 'Average test sequence length : { } '.format ( np.mean ( list ( map ( len , x_test ) ) , dtype=int ) ) ) steps_per_epoch= ( words_per_epoch - val_words ) // minibatch_size , 'URW Chancery L ' , 'FreeMono ' ] ( max_string_len == -1 or max_string_len is None or len ( word ) < = max_string_len ) : a = K.square ( x [ : , : img_nrows - 1 , : img_ncols - 1 , : ] - x [ : , 1 : , : img_ncols - 1 , : ] ) height_margin : height_margin + img_height , : ] = img name='ctc ' ) ( [ y_pred , labels , input_length , label_length ] ) labels = Input ( name='the_labels ' , shape= [ img_gen.absolute_max_string_len ] , dtype='float32 ' ) model = Model ( inputs= [ input_data , labels , input_length , label_length ] , x [ : , : , : img_nrows - 1 , : img_ncols - 1 ] - x [ : , : , 1 : , : img_ncols - 1 ] ) print ( 'Average train sequence length : { } '.format ( assert layer_name in layer_dict.keys ( ) , 'Layer ' + layer_name + ' not found in model . ' examples/conv_filter_visualization.py E501 \ callbacks= [ EvaluateInputTensor ( test_model , steps=100 ) ] ) if is_valid_str ( word ) and \ gru_1b = GRU ( rnn_size , return_sequences=True , go_backwards=True , kernel_initializer='he_normal ' , name='gru1_b ' ) ( inner ) X_data [ i , 0 , 0 : self.img_w , : ] = self.paint_func ( self.X_text [ index + i ] ) [ 0 , : , : ] .T raise IOError ( ( 'Could not fit string into image . ' gru_1b = GRU ( rnn_size , return_sequences=True , ( https : //arxiv.org/abs/1506.01057 ) examples/lstm_text_generation.py E501 \ raise IOError ( 'Could not pull enough words from supplied monogram and bigram files . ' ) weight_file = os.path.join ( [ Paper Texture Networks : Feed-forward Synthesis of Textures and Stylized Images ] ( http : //arxiv.org/abs/1603.03417 ) python neural_style_transfer.py img/tuebingen.jpg \ bigram_file=os.path.join ( fdir , 'wordlist_bi_clean.txt ' ) , kernel_initializer='he_normal ' , name='gru2 ' ) ( gru1_merged ) b = K.square ( x [ : , : img_nrows - 1 , : img_ncols - 1 , : ] - x [ : , : img_nrows - 1 , 1 : , : ] ) rotate=False , ud=False , multi_fonts=False ) iter , To specify the number of iterations \ context.select_font_face ( 'Courier ' , self.minibatch_size , train=False ) img_w=img_w , epochs=stop_epoch , text , self.img_w , self.img_h , fdir = os.path.dirname ( as a 4D boolean tensor : ( 1 , m , nr , nc ) for 'channels_first ' or ( 1 , nr , nc , m ) for 'channels_last ' python neural_style_transfer.py path_to_your_base_image.jpg path_to_your_reference.jpg prefix_for_results # increase to wider images and start at epoch 20 . self.paint_func = lambda text : paint_text ( [ A Hierarchical Neural Autoencoder for Paragraphs and Documents ] ( https : //arxiv.org/abs/1506.01057 ) stitched_filters [ ( img_width + margin ) * i : ( img_width + margin ) * i + img_width , new_w2 = np.random.normal ( 0 , 0.1 , ( img_height + margin ) * j : ( img_height + margin ) * j + img_height , : ] = img np.random.choice ( fonts ) , self.paint_func ( self.X_text [ index + i ] ) [ 0 , : , : ] .T ) steps_per_epoch=int ( np.ceil ( data.train.num_examples / float ( batch_size ) ) ) , # cut texts after this number of words ( among top max_features most common words ) examples/deep_dream.py E501 \ a = K.square ( x [ : , : , : img_nrows - 1 , : img_ncols - 1 ] - x [ : , : , 1 : , : img_ncols - 1 ] ) raise IOError ( 'Could not fit string into image . Max char count is too large for given image width . ' ) origin='https : //s3.amazonaws.com/text-datasets/nietzsche.txt ' ) python neural_style_transfer.py img/tuebingen.jpg img/starry_night.jpg results/my_result gru_2 = GRU ( rnn_size , return_sequences=True , kernel_initializer='he_normal ' , name='gru2 ' ) ( gru1_merged ) gru_2b = GRU ( rnn_size , return_sequences=True , go_backwards=True , kernel_initializer='he_normal ' , name='gru2_b ' ) ( gru1_merged ) decoded_res = decode_batch ( self.test_func , word_batch [ 'the_input ' ] [ 0 : num_proc ] ) untar=True ) ) validation_steps=val_words // minibatch_size , 'Max char count is too large for given image width . ' ) ) sum_square = K.sum ( K.square ( x - y ) , axis=1 , keepdims=True ) steps_per_epoch=int ( np.ceil ( data.train.num_examples / float ( batch_size ) ) ) , rotate=True , ud=True , multi_fonts=True ) # increase to wider images and start at epoch 20 . The learned weights are reloaded origin='http : //www.mythic-ai.com/datasets/wordlists.tgz ' , ctc_lambda_func , output_shape= ( 1 , ) , gru_1 = GRU ( rnn_size , return_sequences=True , [ Torch code for online-neural-doodle ] minibatch_size=minibatch_size , print ( '\nOut of % d samples : Mean edit distance : ' ret = self.get_batch ( self.cur_train_index , cairo.FONT_WEIGHT_BOLD ) pylab.xlabel ( 'Truth = \ ' % s\'\nDecoded = \ ' % s\ '' % ( word_batch [ 'source_str ' ] [ i ] , res [ i ] ) ) edit_dist = editdistance.eval ( decoded_res [ j ] , word_batch [ 'source_str ' ] [ j ] ) downsample_factor= ( pool_size * * 2 ) , context.select_font_face ( 'Courier ' , cairo.FONT_SLANT_NORMAL , cairo.FONT_WEIGHT_BOLD ) iter , To specify the number of iterations the style transfer takes place ( Default is 10 ) as a 4D boolean tensor : img_h=img_h , new_w2 = np.random.normal ( rotate=False , ud=True , multi_fonts=True ) x [ : , : img_nrows - 1 , : img_ncols - 1 , : ] - x [ : , 1 : , : img_ncols - 1 , : ] ) ( https : //github.com/keras-team/keras/issues/3705 ) img_h=img_h , print ( 'Average test sequence length : { } '.format ( np.mean ( list ( map ( len , x_test ) ) , dtype=int ) ) ) callbacks= [ viz_cb , img_gen ] , rotate=False , ud=True , multi_fonts=False ) size=teacher_w2.shape [ :2 ] + ( n , teacher_w2.shape [ 3 ] ) ) train_model.fit ( epochs=epochs , return ( max_string_len == -1 or ret = self.get_batch ( self.cur_train_index , self.minibatch_size , train=True ) word_batch [ 'the_input ' ] [ 0 : self.num_display_words ] ) downsample_factor= ( pool_size * * 2 ) , loss_out = Lambda ( ctc_lambda_func , output_shape= ( 1 , ) , name='ctc ' ) ( [ y_pred , labels , input_length , label_length ] ) word_batch [ 'the_input ' ] [ 0 : num_proc ] ) res = decode_batch ( self.test_func , word_batch [ 'the_input ' ] [ 0 : self.num_display_words ] ) [ Torch code for fast-neural-doodle ] [ Paper Texture Networks : Feed-forward Synthesis of Textures and Stylized Images ] print ( 'Average test sequence length : { } '.format ( [ Dmitry Ulyanov 's blog on fast-neural-doodle ] fdir = os.path.dirname ( get_file ( 'wordlists.tgz ' , np.mean ( list ( map ( len , x_test ) ) , dtype=int ) ) ) 'Layer ' + layer_name + ' not found in model . ' ) steps_per_epoch= ( words_per_epoch - val_words ) // minibatch_size , print ( 'Average test sequence length : { } '.format ( np.mean ( list ( map ( len , x_test ) ) , dtype=int ) ) ) sqaure_pred = K.square ( y_pred ) minibatch_size=minibatch_size , python neural_style_transfer.py path_to_your_base_image.jpg \ shape= [ img_gen.absolute_max_string_len ] , dtype='float32 ' ) [ Discussion on parameter tuning ] ( https : //github.com/keras-team/keras/issues/3705 ) height_margin = ( img_height + margin ) * j print ( 'Average train sequence length : { } '.format ( np.mean ( list ( map ( len , x_train ) ) , dtype=int ) ) ) ret = self.get_batch ( self.cur_val_index , self.minibatch_size , train=False )","['examples/conv_filter_visualization.py', 'examples/deep_dream.py', 'examples/image_ocr.py', 'examples/imdb_fasttext.py', 'examples/imdb_lstm.py', 'examples/lstm_text_generation.py', 'examples/mnist_hierarchical_rnn.py', 'examples/mnist_net2net.py', 'examples/mnist_siamese.py', 'examples/mnist_tfrecord.py', 'examples/neural_doodle.py', 'examples/neural_style_transfer.py', 'pytest.ini']",Enable examples pep8 ( # 10968 )
326,09a984bb256e0c28ef03eb8128bdcc851b3604ec,2018-08-25 16:08:33+09:00,"inputs = np.ones ( ( num_samples , input_num_row , input_num_col , stack_size ) ) inputs = np.ones ( ( num_samples , input_num_row , input_num_col , stack_size ) ) inputs = np.ones ( ( num_samples , stack_size , input_num_row , input_num_col ) ) else : if data_format == 'channels_last ' : inputs = np.ones ( ( num_samples , stack_size , input_num_row , input_num_col ) )",['tests/keras/layers/convolutional_test.py'],Added a if-else which was forgotten in the tests . ( # 10986 )
327,1a2290ae91a944416ef8eb517e97b21733c9681e,2018-08-25 15:48:27+09:00,"check_single_tensor_operation ( 'max ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'pow ' , ( 4 , 2 ) , BACKENDS , a=3 ) check_single_tensor_operation ( 'cumprod ' , ( 4 , 2 ) , [ KTF , KTH ] , axis=1 ) check_single_tensor_operation ( 'std ' , ( 4 , 2 ) , BACKENDS , axis=1 , keepdims=True ) def test_cumsum_cumprod ( self ) : check_single_tensor_operation ( 'any ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) check_single_tensor_operation ( 'abs ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'sqrt ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'abs ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'argmax ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'cumsum ' , ( 4 , 2 ) , [ KTF , KTH ] ) check_single_tensor_operation ( 'clip ' , ( 4 , 2 ) , BACKENDS , min_value=0.4 , check_single_tensor_operation ( 'mean ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'std ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 , 3 ) , BACKENDS , axis=-1 , keepdims=True ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 ) , BACKENDS , axis=1 , keepdims=True ) check_single_tensor_operation ( 'cumprod ' , ( 4 , 2 ) , WITH_NP , axis=1 ) check_single_tensor_operation ( 'prod ' , ( 4 , 2 ) , BACKENDS , axis=1 , keepdims=True ) check_single_tensor_operation ( 'argmax ' , ( 4 , 2 ) , WITH_NP ) 'negative number , not nan , so can\'t ' check_single_tensor_operation ( 'min ' , ( 4 , 2 , 3 ) , BACKENDS , axis= [ 1 , -1 ] ) check_single_tensor_operation ( 'pow ' , ( 4 , 2 ) , WITH_NP , a=3 ) check_single_tensor_operation ( 'prod ' , ( 4 , 2 ) , BACKENDS ) def test_log ( self ) : check_single_tensor_operation ( 'argmin ' , ( 4 , 2 ) , BACKENDS , axis=1 ) check_single_tensor_operation ( 'clip ' , ( 4 , 2 ) , WITH_NP , min_value=0.4 , check_single_tensor_operation ( 'log ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'reshape ' , ( 4 , 2 ) , BACKENDS , shape= ( 8 , 1 ) ) check_single_tensor_operation ( 'cumsum ' , ( 4 , 2 ) , [ KTF , KTH ] , axis=1 ) `` using the NumPy backend . '' ) check_single_tensor_operation ( 'min ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) WITH_NP = [ K , KNP ] check_single_tensor_operation ( 'prod ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 , 3 ) , WITH_NP , axis=-1 , keepdims=True ) check_single_tensor_operation ( 'max ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'argmin ' , ( 4 , 2 ) , WITH_NP ) raise TypeError ( `` Constraint must be None when `` reason='cntk return -85.1 for zero or ' check_single_tensor_operation ( 'any ' , ( 4 , 2 ) , BACKENDS , axis=1 , keepdims=True ) check_single_tensor_operation ( 'cumsum ' , ( 4 , 2 ) , WITH_NP ) 'cumsum and cumprod yet ' ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 , 3 ) , BACKENDS , axis= [ 1 , -1 ] ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) check_single_tensor_operation ( 'exp ' , ( 4 , 2 ) , BACKENDS ) if constraint is not None : check_single_tensor_operation ( 'all ' , ( 4 , 2 ) , BACKENDS , axis=1 , keepdims=True ) def reshape ( x , shape ) : check_single_tensor_operation ( 'all ' , ( 4 , 2 ) , WITH_NP ) # cntk does not support cumsum and cumprod yet check_single_tensor_operation ( 'exp ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'square ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'prod ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) check_single_tensor_operation ( 'argmax ' , ( 4 , 2 ) , BACKENDS , axis=1 ) check_single_tensor_operation ( 'cumsum ' , ( 4 , 2 ) , WITH_NP , axis=1 ) check_single_tensor_operation ( 'std ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'prod ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'max ' , ( 4 , 2 ) , BACKENDS , axis=1 , keepdims=True ) check_single_tensor_operation ( 'reshape ' , ( 4 , 2 ) , WITH_NP , shape= ( 8 , 1 ) ) check_single_tensor_operation ( 'argmax ' , ( 4 , 2 ) , WITH_NP , axis=1 ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'min ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'max ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) return np.array ( value , dtype ) check_single_tensor_operation ( 'argmin ' , ( 4 , 2 ) , WITH_NP , axis=1 ) check_single_tensor_operation ( 'square ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'min ' , ( 4 , 2 ) , BACKENDS , axis=1 , keepdims=True ) check_single_tensor_operation ( 'any ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'std ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'mean ' , ( 4 , 2 , 3 ) , WITH_NP , axis= [ 1 , -1 ] ) check_single_tensor_operation ( 'any ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'log ' , ( 4 , 2 ) , [ KTH , KTF ] ) check_single_tensor_operation ( 'all ' , ( 4 , 2 ) , BACKENDS ) check_single_tensor_operation ( 'all ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) check_single_tensor_operation ( 'cumprod ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'min ' , ( 4 , 2 ) , WITH_NP , axis=1 , keepdims=True ) def variable ( value , dtype=None , name=None , constraint=None ) : 'compare with other backend . ' ) check_single_tensor_operation ( 'min ' , ( 4 , 2 ) , BACKENDS ) # cntk return -85.1 for zero or negative number , not nan , so ca n't compare with other backend . check_single_tensor_operation ( 'cumprod ' , ( 4 , 2 ) , [ KTF , KTH ] ) return np.reshape ( x , shape ) check_single_tensor_operation ( 'prod ' , ( 4 , 2 , 3 ) , BACKENDS , axis= [ 1 , -1 ] ) check_single_tensor_operation ( 'sqrt ' , ( 4 , 2 ) , WITH_NP ) check_single_tensor_operation ( 'argmin ' , ( 4 , 2 ) , BACKENDS )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Created a dummy 'variable ' function in the numpy backend to replace the ` BACKEND ` variable . ( # 10972 )
328,23bdd4d417f0fda90465e3ab612c682e1d207ee3,2018-08-23 11:38:58-07:00,"with pytest.raises ( ValueError ) : # Ca n't pickle tracebacks . last_ones = [ future.get ( ) for future in last_ones if future.successful ( ) ] with self.lock : def _data_generator_task ( self ) : self.genlock = threading.Lock ( ) self.workers = workers setattr ( e , '__traceback__ ' , None ) `` `` '' import traceback val_preds = model.predict_generator ( iter ( val_gen ) , steps=val_samples , workers=0 ) self.wait_time = wait_time self.uid = _GENERATOR_COUNTER try : self.stop_signal.set ( ) if wait_time is not None : # We use a Value to provide unique id to different processes . except : return g if self._use_multiprocessing : # Rethrow any exceptions found in the queue self.run_thread = threading.Thread ( target=self._run ) self.uid = _GENERATOR_COUNTER.value atol=1e-5 ) serializing call to the ` next ` method of given iterator/generator . warnings.warn ( ' ` wait_time ` is not used anymore . ' , if inputs is not None : global _GENERATOR_COUNTER with _GENERATOR_COUNTER.get_lock ( ) : except StopIteration : get a specific one . A single generator would cause the validation to self.queue.put ( ( False , e ) ) except Exception as e : thread.daemon = True def init_pool_generator ( gens , random_seed=None ) : else : history.history [ 'val_true_positives ' ] [ -1 ] , self.stop_signal = None # multiprocessing . We resort to an int raise StopIteration ( ) self.queue.put ( ( True , generator_output ) ) if _GENERATOR_COUNTER is None : self._manager.shutdown ( ) while not self.queue.empty ( ) : initargs= ( gens , timeout : maximum time to wait on ` thread.join ( ) ` . try : time.sleep ( self.wait_time ) except StopIteration : # On all OSes , avoid * * SYSTEMATIC * * error in multithreading mode : _SHARED_GENERATOR = gens self._manager = mp.Manager ( ) if value is not None : _GENERATOR_COUNTER = mp.Value ( ' i ' , 0 ) # Arguments # On Windows , avoid * * SYSTEMATIC * * error in ` multiprocessing ` : else : def is_running ( self ) : self._stop_event = None ' is not supported on Windows ( no marshalling of ' break success , value = self.queue.get ( ) def threadsafe_generator ( f ) : # ` TypeError : ca n't pickle generator objects ` setattr ( e , '__traceback__ ' , sys.exc_info ( ) [ 2 ] ) # For new processes that may spawn # Arguments except StopIteration : `` Your generator is NOT thread-safe . '' generator_output = next ( self._generator ) self.queue = None np.random.seed ( random_seed + ident ) return self.next ( ) uid : int , generator identifier for thread in self._threads ] ) if self.stop_signal.is_set ( ) : self.queue.qsize ( ) < self.max_queue_size ) : assert 6 < = gen_counters [ 0 ] < = 8 if 'generator already executing ' in str ( e ) : preds = model.predict_generator ( iter ( gen ) , steps=samples , workers=0 ) self._stop_event.set ( ) # ` ValueError : generator already executing ` atol=1e-5 ) # Reset random seed else all children processes assert min_train < = gen_counters [ 0 ] < = max_train ' use single thread/process or multithreading . ' ) ( when full , threads could block on ` put ( ) ` ) if not hasattr ( e , '__traceback__ ' ) : def __next__ ( self ) : # We do not need the init since it 's threads . self.stop ( ) self.stop_signal = threading.Event ( ) except OSError : self.executor_fn = lambda gens : mp.Pool ( workers , `` `` '' Start the handler 's workers . val_outs = model.evaluate_generator ( iter ( val_gen ) , steps=val_samples , workers=0 ) raise ValueError ( 'Using a generator with ` use_multiprocessing=True ` ' self.run_thread.join ( timeout ) self._stop_event.set ( ) self._stop_event = mp.Event ( ) self._threads = [ ] else : self._stop_event = threading.Event ( ) def test_generator_enqueuer_threadsafe ( ) : enqueuer.stop ( ) return # The thread.is_alive ( ) test is subject to a race condition : while self.is_running ( ) : if self.seed is not None : thread = mp.Process ( target=self._data_generator_task ) self.lock = threading.Lock ( ) self.random_seed = random_seed if thread.is_alive ( ) : # infinite iterator/generator 's next ( ) function np.testing.assert_allclose ( val_outs [ 2 ] , ref_true_pos ( val_y , val_preds ) , if self.is_running ( ) : self.queue.put ( else : six.raise_from ( StopIteration ( e ) , e ) # As a compromise , print the traceback and pickle None instead . Should be called by the same thread which called ` start ( ) ` . if self.use_multiprocessing : assert 'thread-safe ' in str ( e.value ) try : _SHARED_GENERATOR [ self.uid ] = self.generator val_preds = model.predict_generator ( iter ( val_gen ) , steps=val_samples ) enqueuer.start ( 3 , 10 ) _GENERATOR_COUNTER.value += 1 self.queue.put ( ( False , e ) ) else : def _send_generator ( self ) : def _run ( self ) : ( when full , workers could block on ` put ( ) ` ) `` `` '' A decorator that takes a generator function and makes it thread-safe . thread = threading.Thread ( target=self._data_generator_task ) self.it = it # share the same seed _SHARED_GENERATOR [ self.uid ] = None self.queue.put ( ( True , generator_output ) ) wait_time=0.05 , `` `` '' Get the next value from the generator ` uid ` . # in multithreading mode : return next ( self.it ) np.testing.assert_allclose ( outs [ 2 ] , ref_true_pos ( y , preds ) , atol=1e-5 ) np.testing.assert_allclose ( val_outs [ 2 ] , Should be called by the same thread which called ` start ( ) ` . global _SHARED_GENERATOR if ( self.queue is not None and try : else : # In this case the OS does not allow us to use wait_time=None , np.testing.assert_allclose ( outs [ 2 ] , ref_true_pos ( y , preds ) , import threading # Global variables to be shared across processes generator_output = next ( self._generator ) # for enqueuer indexing . if self._manager : class threadsafe_iter : executor.apply_async ( next_sample , ( self.uid , ) ) , block=True ) self._send_generator ( ) # Share the initial generator import warnings self._stop_event.set ( ) self.queue = queue.Queue ( max_queue_size ) self.seed = seed # the thread could terminate right after the test and before the thread.start ( ) if ( self.queue is not None and # Arguments time.sleep ( self.wait_time ) last_ones.append ( self.queue.get ( block=True ) ) def next_sample ( uid ) : The next value of generator ` uid ` . raise RuntimeError ( self._threads.append ( thread ) self._generator = generator yield inputs self.queue = self._manager.Queue ( maxsize=max_queue_size ) with closing ( self.executor_fn ( _SHARED_GENERATOR ) ) as executor : `` `` '' Stops running threads and wait for them to exit , if necessary . with pytest.raises ( RuntimeError ) : self.workers = 0 preds = model.predict_generator ( iter ( gen ) , steps=samples ) assert_allclose ( val_outs [ 2 ] , ref_true_pos ( val_y , val_preds ) , atol=1e-5 ) enqueuer = GeneratorEnqueuer ( create_generator_from_sequence_pcs ( self.generator = generator self.run_thread.start ( ) # = > Suggest multithreading instead of multiprocessing on Windows self.queue.qsize ( ) < self.max_queue_size ) : self.queue.queue.clear ( ) all_finished = all ( [ not thread.is_alive ( ) `` `` '' Send current generator to all workers . '' '' '' # Keep the good ones def g ( * a , * * kw ) : def stop ( self , timeout=None ) : if os.name is 'nt ' and use_multiprocessing is True : _GENERATOR_COUNTER = None # Wait for them to complete self.queue.task_done ( ) _GENERATOR_COUNTER += 1 def __init__ ( self , it ) : min_train = 2 * 3 if isinstance ( _GENERATOR_COUNTER , int ) : `` `` '' Kicks off threads which add data from the generator into the queue . while self.queue.qsize ( ) > 0 : if not success : while not self._stop_event.is_set ( ) : def next ( self ) : traceback.print_exc ( ) # Yield regular values seed=None ) : return self.stop_signal is not None and not self.stop_signal.is_set ( ) with self.genlock : # = > Serialize calls to infinite iterator/generator 's next ( ) function # Ca n't pickle tracebacks . return self def __iter__ ( self ) : self.queue.not_full.notify ( ) # always , which is ok no matter what the status of the thread . with self.queue.mutex : for _ in range ( workers ) : timeout : maximum time to wait on ` thread.join ( ) ` self.queue = queue.Queue ( maxsize=max_queue_size ) np.random.seed ( self.seed ) break # Special case for finite generators yield value success , value = self.queue.get ( ) self._manager = None # join , rendering this test meaningless - > Call thread.join ( ) `` `` '' `` `` '' Takes an iterator/generator and makes it thread-safe by inputs = self.queue.get ( block=True ) .get ( ) _GENERATOR_COUNTER = 0 for inputs in last_ones : max_train = 3 * 2 + 2 * 2 while True : return threadsafe_iter ( f ( * a , * * kw ) ) time.sleep ( self.wait_time ) last_ones = [ ] except Exception as e : raise `` ` use_multiprocessing=False , workers > 1 ` . '' except Exception as e : self._use_multiprocessing = use_multiprocessing if all_finished and self.queue.empty ( ) : self.random_seed ) ) six.reraise ( value.__class__ , value , value.__traceback__ ) # = > Serialize calls to six.reraise ( * sys.exc_info ( ) ) `` `` '' Submits request to the executor and queue the ` Future ` objects . '' '' '' # As a compromise , print the traceback and # ` ValueError : generator already executing ` random_seed=None ) : overwrite the training generator . self.run_thread = None DummySequence ( [ 3 , 200 , 200 , 3 ] ) ) , use_multiprocessing=False ) def is_running ( self ) : return self._stop_event is not None and not self._stop_event.is_set ( ) if self._use_multiprocessing : To allow multiple generators to be used at the same time , we use ` uid ` to with pytest.raises ( RuntimeError ) as e : try : self.queue.unfinished_tasks = 0 while self.is_running ( ) : # pickle None instead . outs = model.evaluate_generator ( iter ( gen ) , steps=samples ) self.max_queue_size = max_queue_size gen_output = enqueuer.get ( ) with pytest.raises ( StopIteration ) : if random_seed is not None : _SHARED_GENERATOR = { } self.use_multiprocessing = use_multiprocessing ident = mp.current_process ( ) .ident thread.terminate ( ) `` `` '' return six.next ( _SHARED_GENERATOR [ uid ] ) self.stop ( ) if not self.queue.empty ( ) : # Doing Multiprocessing.Value += x is not process-safe . # On all OSes , avoid * * SYSTEMATIC * * error def stop ( self , timeout=None ) : with pytest.raises ( IndexError ) : if not success : assert_allclose ( val_outs [ 2 ] , history.history [ 'val_true_positives ' ] [ -1 ] , # Returns DeprecationWarning ) if self._use_multiprocessing is False : list ( map ( lambda f : f.wait ( ) , last_ones ) ) initializer=init_pool_generator , for thread in self._threads : self.run_thread.daemon = True # Make sure to rethrow the first exception in the queue , if any with pytest.raises ( StopIteration ) : thread.join ( timeout ) `` `` '' Stops running threads and wait for them to exit , if necessary . [ next ( gen_output ) for _ in range ( 10 ) ] self.executor_fn = lambda _ : ThreadPool ( workers ) `` Keras requires a thread-safe generator when '' val_outs = model.evaluate_generator ( iter ( val_gen ) , steps=val_samples ) self.seed += 1 six.reraise ( value.__class__ , value , value.__traceback__ ) self.executor_fn = None `` For more information see issue # 1638 . '' ) outs = model.evaluate_generator ( iter ( gen ) , steps=samples , workers=0 ) ' generators across process boundaries ) . Instead , '","['keras/utils/data_utils.py', 'tests/keras/engine/test_training.py', 'tests/keras/metrics_test.py', 'tests/keras/utils/data_utils_test.py', 'tests/test_multiprocessing.py']",Generators now use same logic as Sequence ( # 10925 )
329,d88f2006af35179b986479ac6ad5a20dac8ac9d1,2018-08-23 11:29:53-07:00,"raise ValueError ( 'Dilated convolution on CPU is not supported by CNTK backend . ' assert strides == ( 1 , 1 ) , 'Invalid strides for dilated convolution ' padding , padding ] ) False , x = C.convolution ( 'Please set dilation_rate with ( 1 , 1 ) . ' ) strides , x = C.convolution ( kernel , auto_padding= [ False , padding , padding ] , if dilation_rate == ( 1 , 1 ) : strides=dilation_rate [ 0 ] , x , reason= '' cntk does not support dilated conv '' ) kernel , auto_padding= [ assert dilation_rate [ 0 ] == dilation_rate [ 1 ] strides = ( 1 , ) + strides dilation=dilation_rate ) reason= '' cntk only supports dilated conv on GPU '' ) if dev.type ( ) == 0 and dilation_rate ! = ( 1 , 1 ) : x , else : strides ,","['keras/backend/cntk_backend.py', 'tests/keras/layers/convolutional_test.py']",Fix dilated convolution for CNTK backend . ( # 10967 )
330,eb8b70addf6ae58ef17739b4c35cc22b1c4c56d6,2018-08-23 06:51:35-07:00,"y = np.max ( np.sum ( x * * 2 , axis , keepdims=True ) , axis , keepdims=True ) y1 = reference_operations.separable_conv ( x , depthwise , pointwise , padding , data_format ) y1 = KNP.pool ( x , pool_size , strides , padding , data_format , pool_mode ) return np.cumprod ( x , axis=axis ) return np.cumsum ( x , axis=axis ) def hard_sigmoid ( x ) : def max ( x , axis=None , keepdims=False ) : y1 = reference_operations.depthwise_conv ( x , w , padding , data_format ) return x / np.sqrt ( y ) x = np.mean ( x , axis=a , keepdims=keepdims ) assert_value_with_ref = None return np.log ( 1 . + np.exp ( x ) ) return np.sum ( x , axis=axis , keepdims=keepdims ) if K.backend ( ) == k.__name__.split ( ' . ' ) [ -1 ] [ : -8 ] ] log = np.log last_y1 , y1 , h1 = KNP.rnn ( x , [ wi , wh , None ] , h0 , * * kwargs ) last_y1 , y1 , h1 = KNP.rnn ( x , [ wi , None , None ] , None , return np.max ( x , axis=axis , keepdims=keepdims ) def clip ( x , min_value , max_value ) : def softplus ( x ) : y = 0.2 * x + 0.5 go_backwards=False , mask=None ) x = np.max ( x , axis=a , keepdims=keepdims ) last_y1 , y1 , h1 = reference_operations.rnn ( x , [ wi , None , None ] , None , def prod ( x , axis=None , keepdims=False ) : last_y1 , y1 , h1 = reference_operations.rnn ( x , [ wi , wh , None ] , h0 , * * kwargs ) import reference_operations backend_list = [ k for k in backend_list return y / np.sum ( y , axis , keepdims=True ) y = np.minimum ( y , max_value ) x = np.sum ( x , axis=a , keepdims=keepdims ) for a in axis : # from a single backend and NumPy . return np.power ( x , a ) y1 = reference_operations.pool ( x , pool_size , strides , padding , data_format , pool_mode ) # Leave only the designated backend from the test list of backends . return np.clip ( x , min_value , max_value ) x = np.prod ( x , axis=a , keepdims=keepdims ) def softmax ( x , axis=-1 ) : def std ( x , axis=None , keepdims=False ) : def cumsum ( x , axis=0 ) : return np.any ( x , axis=axis , keepdims=keepdims ) y = np.minimum ( y , 1 . ) check_single_tensor_operation ( 'std ' , ( 4 , 2 , 3 ) , BACKENDS , axis= [ 1 , -1 ] ) y1 = KNP.depthwise_conv ( x , w , padding , data_format ) except : return np.argmin ( x , axis=axis ) return y def min ( x , axis=None , keepdims=False ) : def mean ( x , axis=None , keepdims=False ) : return np.all ( x , axis=axis , keepdims=keepdims ) def sum ( x , axis=None , keepdims=False ) : y1 = KNP.separable_conv ( x , depthwise , pointwise , padding , data_format ) y = x * ( x > 0 ) + alpha * x * ( x < 0 ) def cumprod ( x , axis=0 ) : else : def sqrt ( x ) : return x y1 = KNP.conv ( x , w , padding , data_format ) # check_single_tensor_operation ( 'std ' , ( 4 , 2 , 3 ) , BACKENDS , axis= [ 1 , -1 ] ) def argmin ( x , axis=-1 ) : y = np.maximum ( y , 0 . ) y = np.exp ( x - np.max ( x , axis , keepdims=True ) ) def sigmoid ( x ) : y [ np.isnan ( y ) ] = 0 . def any ( x , axis=None , keepdims=False ) : # If we can take a NumPy output , it is efficient to compare the outputs return 1 . / ( 1 . + np.exp ( -x ) ) square = np.square def elu ( x , alpha=1 . ) : return np.mean ( x , axis=axis , keepdims=keepdims ) return np.std ( x , axis=axis , keepdims=keepdims ) go_backwards=False , mask=None ) try : y1 = reference_operations.conv ( x , w , padding , data_format ) def tanh ( x ) : def l2_normalize ( x , axis=-1 ) : y = np.sqrt ( x ) round = np.round return np.tanh ( x ) if isinstance ( axis , list ) : x = np.std ( x , axis=a , keepdims=keepdims ) return np.argmax ( x , axis=axis ) assert_value_with_ref = getattr ( KNP , function_name ) ( x_val , * * kwargs ) return np.prod ( x , axis=axis , keepdims=keepdims ) if max_value is not None : exp = np.exp abs = np.abs import reference_operations as KNP def relu ( x , alpha=0. , max_value=None ) : def argmax ( x , axis=-1 ) : x = np.min ( x , axis=a , keepdims=keepdims ) return np.min ( x , axis=axis , keepdims=keepdims ) def all ( x , axis=None , keepdims=False ) : return x * ( x > 0 ) + alpha * ( np.exp ( x ) - 1 . ) * ( x < 0 ) def pow ( x , a=1 . ) : sign = np.sign","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Speed up backend tests ( # 10956 )
331,d8796e04f8f8a814eb8ae3206624b5c4b47362f3,2018-08-22 20:25:51-07:00,"c , d = K.stop_gradient ( [ a , b ] ) e = K.stop_gradient ( b ) for k in BACKENDS : b = k.square ( a ) a = k.variable ( val ) a = K.variable ( val ) b = K.square ( a ) e = k.stop_gradient ( b ) c , d = k.stop_gradient ( [ a , b ] )",['tests/keras/backend/backend_test.py'],Removed duplicated backend test . ( # 10959 )
332,57fba26d8cb8434a14140c0ea65c2c71a14636bf,2018-08-22 20:25:01-07:00,"'data_format ' : self.data_format } return ( input_shape [ 0 ] , length , features ) length = conv_utils.conv_output_length ( steps , self.data_format = K.normalize_data_format ( data_format ) data_format=self.data_format ) return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) padding , data_format , * * kwargs ) dummy_axis = 2 if self.data_format == 'channels_last ' else 3 kwargs= { 'strides ' : stride , data_format : A string , steps = input_shape [ 1 ] corresponds to inputs with shape padding , * * kwargs ) def __init__ ( self , * * kwargs ) : broadcast_shape = [ -1 , input_shape [ 1 ] , 1 ] return K.mean ( inputs , axis=steps_axis ) 'padding ' : padding , If ` data_format='channels_first ' ` : The ordering of the dimensions in the inputs . kwargs= { 'data_format ' : data_format } , * * kwargs ) for data_format in [ 'channels_first ' , 'channels_last ' ] : layer_test ( pooling.GlobalMaxPooling1D , return K.mean ( inputs , axis=1 ) Keras config file at ` ~/.keras/keras.json ` . steps_axis = 1 if self.data_format == 'channels_last ' else 2 data_format='channels_last ' ) 3D tensor with shape : ` ( batch_size , steps , features ) ` . return K.max ( inputs , axis=1 ) inputs = K.expand_dims ( inputs , dummy_axis ) # add dummy last dimension ` channels_last ` corresponds to inputs with shape return ( input_shape [ 0 ] , input_shape [ 1 ] ) return ( input_shape [ 0 ] , features , length ) one of ` channels_last ` ( default ) or ` channels_first ` . layer_test ( convolutional.MaxPooling1D , ` ( batch , features , steps ) ` . 'data_format ' : data_format } , inputs = K.expand_dims ( inputs , 2 ) # add dummy last dimension ` ( batch_size , downsampled_steps , features ) ` kwargs= { 'strides ' : stride , input_shape= ( 3 , 5 , 4 ) ) for data_format in [ 'channels_first ' , 'channels_last ' ] : config = { 'data_format ' : self.data_format } return ( input_shape [ 0 ] , length , input_shape [ 2 ] ) return K.sum ( inputs , axis=1 ) / K.sum ( mask , axis=1 ) super ( GlobalAveragePooling1D , self ) .__init__ ( data_format , layer_test ( convolutional.MaxPooling1D , layer_test ( convolutional.AveragePooling1D , features = input_shape [ 1 ] input_shape= ( 3 , 4 , 5 ) ) ` ( batch_size , features , downsampled_steps ) ` 3D tensor with shape : return ( input_shape [ 0 ] , input_shape [ 2 ] ) return K.squeeze ( output , dummy_axis ) # remove dummy last dimension ` ( batch_size , features , steps ) ` broadcast_shape = [ -1 , input_shape [ steps_axis ] , 1 ] features = input_shape [ 2 ] input_shape= ( 3 , 4 , 5 ) ) padding , * * kwargs ) If you never set it , then it will be `` channels_last '' . return K.sum ( inputs , axis=steps_axis ) / K.sum ( mask , axis=steps_axis ) padding , data_format , It defaults to the ` image_data_format ` value found in your 'padding ' : self.padding , return K.squeeze ( output , 2 ) # remove dummy last dimension steps = input_shape [ 2 ] length = conv_utils.conv_output_length ( input_shape [ 1 ] , * * kwargs ) input_shape= ( 3 , 5 , 4 ) ) # Arguments padding='valid ' , data_format=None , * * kwargs ) : 'padding ' : self.padding } def __init__ ( self , data_format=None , * * kwargs ) : layer_test ( pooling.GlobalAveragePooling1D , else : base_config = super ( _GlobalPooling1D , self ) .get_config ( ) 3D tensor with shape : ` ( batch_size , downsampled_steps , features ) ` . 'padding ' : padding } , return ( input_shape [ 0 ] , input_shape [ 2 ] ) if self.data_format == 'channels_first ' : layer_test ( pooling.GlobalAveragePooling1D , return K.max ( inputs , axis=steps_axis ) ` ( batch_size , steps , features ) ` layer_test ( pooling.GlobalMaxPooling1D , def get_config ( self ) : layer_test ( convolutional.AveragePooling1D , padding='valid ' , * * kwargs ) : ` ( batch , steps , features ) ` while ` channels_first ` If ` data_format='channels_last ' ` : super ( GlobalAveragePooling1D , self ) .__init__ ( * * kwargs )","['keras/layers/pooling.py', 'tests/keras/layers/convolutional_test.py']",[ P ] add data_format support for Pooling1D layers ( # 10966 )
333,1788a0a901efccfffcead3d64a5b8e5876998d32,2018-08-22 10:15:05-07:00,"# TODO : enable Nasnet for Theano and CNTK if K.backend ( ) == 'tensorflow ' : ( applications.NASNetLarge , 4032 ) , ( applications.DenseNet201 , 1920 ) , ( applications.NASNetMobile , 1056 ) , # Note that NASNetLarge is too heavy to test on Travis . ( applications.NASNetMobile , 1056 ) ( applications.DenseNet201 , 1920 ) MODEL_LIST.extend ( [ ] )",['tests/integration_tests/applications_test.py'],Enable NASNetMobile in integration_tests for Theano and CNTK ( # 10962 )
334,79a48bd9ac947dd6bfde19f24c4bf88b0002a5a5,2018-08-22 10:06:19-07:00,"verbose=0 ) # as the loss has not improved after patience=2 epochs . 'GRU ( reset_after=False ) is not compatible with ' cleanup_callback = callbacks.LambdaCallback ( on_train_end=lambda logs : p.terminate ( ) ) preprocess_weights_for_loading ( dest , pred_loss = K.eval ( K.mean ( losses.get ( model.loss ) ( K.variable ( y_test ) , # generate char sequences of length 'sequence_length ' out of alphabet and store the next char as label ( e.g . 'ab'- > ' c ' ) assert_almost_equal ( h [ 'val_loss ' ] , h [ 'val_weighted_ ' + loss_full_name ] , decimal=decimal ) sentences = [ alphabet [ i : i + sequence_length ] for i in range ( len ( alphabet ) - sequence_length ) ] cbks = [ callbacks.ReduceLROnPlateau ( monitor='val_loss ' , factor=0.1 , min_delta=10 , patience=1 , cooldown=5 ) ] `` { } { } are not style properly 'argument ' : documentation '' .format ( np.testing.assert_allclose ( val_outs [ 2 ] , history.history [ 'val_true_positives ' ] [ -1 ] , atol=1e-5 ) ids= [ 'model_func ' , 'model_seq ' ] ) layers.LSTM ( 16 , return_sequences=True , assert_array_almost_equal ( h [ 'val_loss ' ] , h [ 'val_ ' + loss_full_name ] , decimal=decimal ) prediction = model.predict_generator ( data_generator ( x_test , y_test ) , 1 , max_queue_size=2 , verbose=1 ) model_type ) : validation_data=iter ( val_gen ) , from keras.layers.pooling import MaxPooling2D # generate char sequences of length 'sequence_length ' out of alphabet and n_weight_names_arrays = len ( [ attr for attr in h5file [ 'model_weights ' ] [ 'nested_model ' ] .attrs cudnn_model = _make_nested_model ( input_shape , cudnn_layer , assert np.allclose ( float ( K.get_value ( model.optimizer.lr ) ) , 0.1 , atol=K.epsilon ( ) ) metrics= [ 'acc ' ] , weighted_metrics= [ 'acc ' ] ) yield ( x_train [ i * batch_size : ( i + 1 ) * batch_size ] , y_train [ i * batch_size : ( i + 1 ) * batch_size ] ) for i in range ( len ( alphabet ) - sequence_length ) ] @ pytest.mark.parametrize ( 'bidirectional ' , [ False , True ] , ids= [ 'single ' , 'bidirectional ' ] ) if attr.startswith ( 'weight_names ' ) ] ) `` { } the documentation should be on the first line . `` .format ( name ) , # Should stop after epoch 3 , return ( ( x_train , y_train ) , ( x_test , y_test ) , # http : //stackoverflow.com/questions/16060899/alphabet-range-python shuffle=False ) layers.LSTM ( 16 , return_sequences=True , input_shape= ( sequence_length , number_of_chars ) ) , sentences = [ alphabet [ i : i + sequence_length ] yield ( x_test [ i * batch_size : ( i + 1 ) * batch_size ] , y_test [ i * batch_size : ( i + 1 ) * batch_size ] ) decimal=decimal ) [ ( 100 , 100 ) , ( 10 , 20 ) , ( 30 , 80 ) , ( 1 , 2 , 3 , 4 ) ] , assert_array_almost_equal ( h [ 'loss ' ] , h [ 'weighted_ ' + loss_full_name ] , from keras.layers import Bidirectional , GRU , LSTM , CuDNNGRU , CuDNNLSTM score = model.evaluate ( temporal_x_test [ test_ids ] , temporal_y_test [ test_ids ] , verbose=0 ) member.__module__ ) # store the next char as label ( e.g . 'ab'- > ' c ' ) kernel_constraint=constraints.MaxNorm ( 1 ) ) ) model.fit ( x_train , y_train , batch_size=batch_size , epochs=epochs , verbose=1 , tests/test_model_saving.py E501 \ n_weight_names_arrays = len ( attrs ) attrs = [ attr for attr in h5file [ 'model_weights ' ] [ 'nested_model ' ] .attrs ( K.backend ( ) ! = 'tensorflow ' ) or ( not K.tensorflow_backend._get_available_gpus ( ) ) , yield ( x_test [ i * batch_size : ( i + 1 ) * batch_size ] , mod.fit ( np.zeros ( [ 10 , 3 ] ) , np.zeros ( [ 10 , 1 ] , np.float32 ) , model.add ( Dense ( num_classes , input_shape= ( 3 , ) , prediction = model.predict_generator ( data_generator ( x_test , y_test ) , 1 , metrics= [ loss ] , weighted_metrics= [ loss ] ) not K.tensorflow_backend._get_available_gpus ( ) ) , assert_allclose ( val_outs [ 2 ] , history.history [ 'val_true_positives ' ] [ -1 ] , model.fit ( x_train , y_train , batch_size=batch_size , epochs=epochs , verbose=1 , validation_data= ( x_test , y_test ) ) model.compile ( loss=loss , optimizer='rmsprop ' , metrics= [ loss ] , weighted_metrics= [ loss ] ) # generate alphabet : http : //stackoverflow.com/questions/16060899/alphabet-range-python from keras.layers import Dense , Lambda , RepeatVector , TimeDistributed from keras.layers import Dense , Lambda , RepeatVector , TimeDistributed , Bidirectional , GRU , LSTM , CuDNNGRU , CuDNNLSTM @ pytest.mark.parametrize ( 'model_nest_level ' , [ 1 , 2 ] , ids= [ 'model_plain ' , 'model_nested ' ] ) from keras.layers.pooling import MaxPooling2D , GlobalAveragePooling1D , GlobalAveragePooling2D max_queue_size=2 , verbose=1 ) cleanup_callback = callbacks.LambdaCallback ( # generate alphabet : min_delta=10 , patience=1 , cooldown=5 ) ] tests/keras/optimizers_test.py E501 \ input_shape= ( sequence_length , number_of_chars ) ) , [ False , True ] , atol=1e-5 ) gen_loss = model.evaluate_generator ( data_generator ( x_test , y_test , 50 ) , 1 , max_queue_size=2 ) pred_loss = K.eval ( K.mean ( losses.get ( model.loss ) ( K.variable ( y_test ) , K.variable ( prediction ) ) ) ) `` { } { } are not style properly 'argument ' : documentation '' .format ( name , list ( ( sample_weight , class_weight , test_ids ) ) model.compile ( loss=loss , optimizer='rmsprop ' , K.variable ( prediction ) ) ) ) model.add ( Dense ( 2 , input_shape= ( 3 , ) , kernel_initializer=Constant ( np.ones ( ( 3 , 2 ) ) ) ) ) # Should stop after epoch 3 , as the loss has not improved after patience=2 epochs . inp_pair = Lambda ( lambda x : x ) ( [ inp_3d , inp_2d ] ) # test a layer with a list of output tensors from numpy.testing import assert_allclose assert_allclose ( val_outs [ 2 ] , ref_true_pos ( val_y , val_preds ) , atol=1e-5 ) # Start an arbitrary process that should run during model training and model_nest_level , model_type ) for i in range ( len ( alphabet ) - sequence_length ) ] 'GRU ( reset_after=True ) is not compatible with GRU ( reset_after=False ) ' ) history = model.fit ( x , y , validation_data= ( val_x , val_y ) , epochs=1 , batch_size=10 ) model.fit ( x_train , y_train , batch_size=batch_size , epochs=epochs , verbose=1 , shuffle=False ) yield ( x_train [ i * batch_size : ( i + 1 ) * batch_size ] , assert_almost_equal ( h [ 'val_loss ' ] , h [ 'val_weighted_ ' + loss_full_name ] , model.compile ( loss=loss_map , optimizer='sgd ' , assert_array_almost_equal ( h [ 'val_loss ' ] , h [ 'val_ ' + loss_full_name ] , assert np.allclose ( float ( K.get_value ( model.optimizer.lr ) ) , 0.01 , atol=K.epsilon ( ) ) yield ( np.ones ( [ batch_size , input_dim ] ) * np.nan , from keras.layers.pooling import GlobalAveragePooling1D model_nest_level , model_type ) : early_stop = callbacks.EarlyStopping ( monitor='val_acc ' , baseline=0.75 , patience=2 ) assert_allclose ( float ( K.get_value ( model.optimizer.lr ) ) , 0.1 , atol=K.epsilon ( ) ) scores = model.evaluate ( x_test , y_test , verbose=0 , kernel_initializer=Constant ( np.ones ( ( 3 , 2 ) ) ) ) ) sample_weight=test_sample_weight ) return ( x_train , y_train ) , ( x_test , y_test ) , ( sample_weight , class_weight , test_ids ) tests/keras/test_sequential_model.py E501 \ raise ValueError ( `` { } the documentation should be on the first line . `` .format ( name ) , cbks = [ callbacks.ReduceLROnPlateau ( monitor='val_loss ' , factor=0.1 , 'GRU ( reset_after=True ) ' ) 'GRU ( reset_after=False ) is not compatible with GRU ( reset_after=True ) ' ) @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , reason='Requires TensorFlow backend ' ) # Start an arbitrary process that should run during model training and be terminated after training has completed . assert_array_almost_equal ( h [ 'loss ' ] , h [ 'weighted_ ' + loss_full_name ] , decimal=decimal ) [ 'func ' , 'seq ' ] , ids= [ 'single ' , 'bidirectional ' ] ) decimal=decimal ) assert_array_almost_equal ( h [ 'val_loss ' ] , h [ 'val_weighted_ ' + loss_full_name ] , next_chars = [ alphabet [ i + sequence_length ] if attr.startswith ( 'weight_names ' ) ] # test a layer with a list of output tensors cbks = [ callbacks.ReduceLROnPlateau ( monitor='val_loss ' , factor=0.1 , min_delta=0 , patience=1 , cooldown=5 ) ] batch_size=10 , epochs=10 ) mod.fit ( np.zeros ( [ 10 , 3 ] ) , np.zeros ( [ 10 , 1 ] , np.float32 ) , batch_size=10 , epochs=10 ) def test_load_weights_between_noncudnn_rnn ( rnn_type , to_cudnn , bidirectional , tests/test_documentation.py E501 \ cudnn_model = _make_nested_model ( input_shape , cudnn_layer , model_nest_level , model_type ) assert_array_almost_equal ( h [ 'val_loss ' ] , h [ 'val_weighted_ ' + loss_full_name ] , decimal=decimal ) ( sample_weight , class_weight , test_ids ) ) = _get_test_data ( ) preprocess_weights_for_loading ( dest , initialize_weights ( src ) .get_weights ( ) ) def test_load_weights_between_noncudnn_rnn ( rnn_type , to_cudnn , bidirectional , implementation , tests/test_loss_weighting.py E501 \ min_delta=0 , patience=1 , cooldown=5 ) ] from keras.layers.pooling import GlobalAveragePooling2D member.__module__ ) inp_pair = Lambda ( lambda x : x ) ( [ inp_3d , inp_2d ] ) raise ValueError ( score = model.evaluate ( temporal_x_test [ test_ids ] , temporal_y_test [ test_ids ] , # be terminated after training has completed . 'GRU ( reset_after=True ) is not compatible with ' validation_data=iter ( val_gen ) , validation_steps=val_samples ) next_chars = [ alphabet [ i + sequence_length ] for i in range ( len ( alphabet ) - sequence_length ) ] model.compile ( loss=loss_map , optimizer='sgd ' , metrics=metrics , weighted_metrics=weighted_metrics ) 'GRU ( reset_after=False ) ' ) implementation , model_nest_level , tests/keras/initializers_test.py E501 \ ( ( x_train , y_train ) , ( x_test , y_test ) , early_stop = callbacks.EarlyStopping ( monitor='val_acc ' , baseline=0.75 , scores = model.evaluate ( x_test , y_test , verbose=0 , sample_weight=test_sample_weight ) validation_data= ( x_test , y_test ) ) assert_allclose ( float ( K.get_value ( model.optimizer.lr ) ) , 0.01 , atol=K.epsilon ( ) ) metrics=metrics , weighted_metrics=weighted_metrics ) tests/keras/metrics_test.py E501 \ patience=2 ) np.testing.assert_allclose ( val_outs [ 2 ] , ref_true_pos ( val_y , val_preds ) , atol=1e-5 ) y_train [ i * batch_size : ( i + 1 ) * batch_size ] ) name , model.add ( Dense ( 2 , input_shape= ( 3 , ) , layers.Dense ( y_train.shape [ -1 ] ) , input_shape= ( x_train.shape [ 1 ] , x_train.shape [ 2 ] ) ) ) validation_steps=val_samples ) initialize_weights ( src ) .get_weights ( ) ) ( K.backend ( ) ! = 'tensorflow ' or max_queue_size=2 ) @ pytest.mark.parametrize ( 'model_type ' , [ 'func ' , 'seq ' ] , ids= [ 'model_func ' , 'model_seq ' ] ) on_train_end=lambda logs : p.terminate ( ) ) y_test [ i * batch_size : ( i + 1 ) * batch_size ] ) model.fit ( x_train , y_train , batch_size=batch_size , epochs=epochs , verbose=2 , validation_split=0.1 ) validation_split=0.1 ) ( x_train , y_train ) , ( x_test , y_test ) , ( sample_weight , class_weight , test_ids ) = _get_test_data ( ) @ pytest.mark.parametrize ( 'tensor_shape ' , [ ( 100 , 100 ) , ( 10 , 20 ) , ( 30 , 80 ) , ( 1 , 2 , 3 , 4 ) ] , ids= [ 'model_plain ' , 'model_nested ' ] ) np.ones ( [ batch_size , num_classes ] ) * np.nan ) epochs=1 , batch_size=10 ) tests/integration_tests/test_temporal_data_tasks.py E501 \ gen_loss = model.evaluate_generator ( data_generator ( x_test , y_test , 50 ) , 1 , layers.Dense ( y_train.shape [ -1 ] ) , input_shape=x_train.shape [ 1:3 ] ) ) compress ( args , styles ) ) ) , member.__module__ ) [ 1 , 2 ] , `` `` '' Test saving and loading model of constant initializer with numpy ndarray as input . model.add ( Dense ( num_classes , input_shape= ( 3 , ) , kernel_constraint=constraints.MaxNorm ( 1 ) ) ) list ( compress ( args , styles ) ) ) , yield np.ones ( [ batch_size , input_dim ] ) * np.nan , np.ones ( [ batch_size , num_classes ] ) * np.nan model.fit ( x_train , y_train , batch_size=batch_size , epochs=epochs , verbose=2 , `` `` '' Test saving and loading model of constant initializer with numpy inputs . history = model.fit ( x , y , validation_data= ( val_x , val_y ) , model.compile ( loss=loss , optimizer='rmsprop ' , metrics= [ 'acc ' ] , weighted_metrics= [ 'acc ' ] ) tests/keras/test_callbacks.py E501 \","['pytest.ini', 'tests/integration_tests/test_temporal_data_tasks.py', 'tests/keras/initializers_test.py', 'tests/keras/metrics_test.py', 'tests/keras/optimizers_test.py', 'tests/keras/test_callbacks.py', 'tests/keras/test_sequential_model.py', 'tests/test_documentation.py', 'tests/test_loss_weighting.py', 'tests/test_model_saving.py']",Style fixes for enabling PEP8 501 ( # 10957 )
335,38ce4657bad26cb359c2852b792478264d92ee9f,2018-08-22 10:02:44-07:00,"shape_a , shape_b = input_shape return [ K.dot ( a , self.kernel ) + b , K.mean ( b , axis=-1 ) ] # Create a trainable weight variable for this layer . import numpy as np super ( MyLayer , self ) .__init__ ( * * kwargs ) trainable=True ) from keras.engine.topology import Layer assert isinstance ( x , list ) initializer='uniform ' , def compute_output_shape ( self , input_shape ) : `` ` `` ` python super ( MyLayer , self ) .build ( input_shape ) # Be sure to call this at the end class MyLayer ( Layer ) : def call ( self , x ) : shape= ( input_shape [ 0 ] [ 1 ] , self.output_dim ) , def __init__ ( self , output_dim , * * kwargs ) : self.output_dim = output_dim a , b = x return [ ( shape_a [ 0 ] , self.output_dim ) , shape_b [ : -1 ] ] from keras import backend as K self.kernel = self.add_weight ( name='kernel ' , It is also possible to define Keras layers which have multiple input tensors and multiple ouput tensors . To do this , you should assume that the inputs and outputs of the methods ` build ( input_shape ) ` , ` call ( x ) ` and ` compute_output_shape ( input_shape ) ` are lists . Here is an example , similar to the one above : assert isinstance ( input_shape , list ) def build ( self , input_shape ) :",['docs/templates/layers/writing-your-own-keras-layers.md'],Added in the documentation an example of custom layer with multiple inputs and outputs . ( # 10939 )
336,c60f1e19dceacf77749a6215b1fc77f758f46327,2018-08-22 13:33:55+09:00,"assert K.is_keras_tensor ( keras_var ) is False np_var = np.array ( [ 1 , 2 ] ) keras_var = K.variable ( np_var ) with pytest.raises ( ValueError ) : K.is_keras_tensor ( np_var ) for k in BACKENDS : with pytest.raises ( ValueError ) : np_var = np.array ( [ 1 , 2 ] ) assert k.is_keras_tensor ( keras_placeholder ) is False K.set_learning_phase ( 2 ) k.set_learning_phase ( 2 ) assert K.is_keras_tensor ( keras_placeholder ) is False assert k.is_keras_tensor ( keras_var ) is False keras_placeholder = k.placeholder ( shape= ( 2 , 4 , 5 ) ) keras_placeholder = K.placeholder ( shape= ( 2 , 4 , 5 ) ) keras_var = k.variable ( np_var ) k.is_keras_tensor ( np_var )",['tests/keras/backend/backend_test.py'],Removed redundant tests in the backends . ( # 10953 )
337,6d30ab70a4f116554aa610ec1c00a6bc5f1247f3,2018-08-21 19:32:39-07:00,"from __future__ import division ` `` binary '' ` : binary targets ( if there are only two classes ) , return base_fun ( * args , * * kwargs ) x , being yielded , in a viewable format . This is useful # ( applications.NASNetMobile , 1056 ) , # Examples def decode_predictions ( * args , * * kwargs ) : save_prefix=save_prefix , are integers ` [ -1 , 0 , +1 ] ` , def MobileNetV2 ( * args , * * kwargs ) : return mobilenet_v2.preprocess_input ( * args , * * kwargs ) ImageDataGenerator = image.ImageDataGenerator width_shift_range=0.1 , Example of using ` .flow ( x , y ) ` : datagen = ImageDataGenerator ( same as with ` width_shift_range= [ -1 , 0 , +1 ] ` , assert np.array_equal ( x , target size is different from that of the loaded image . from .. import engine 'reflect ' : abcddcba|abcd|dcbaabcd return densenet.DenseNet201 ( * args , * * kwargs ) horizontal_flip=True ) interpolation='nearest ' , zca_epsilon=zca_epsilon , def array_to_img ( x , data_format=None , scale=True , dtype=None ) : return mobilenet.preprocess_input ( * args , * * kwargs ) ( x_train , y_train ) , ( x_test , y_test ) = cifar10.load_data ( ) This class takes in a sequence of data-points gathered at array_to_img = image.array_to_img in the output sequences . This is useful to reserve part of the zca_whitening=False , targets = np.array ( [ [ i ] for i in range ( 50 ) ] ) 'wrap ' : abcdabcd|abcd|abcdabcd shear_range=shear_range , utils=utils ) return xception.Xception ( * args , * * kwargs ) return image.array_to_img ( x , x , y = batch_0 def MobileNet ( * args , * * kwargs ) : from .mobilenet_v2 import * return inception_v3.preprocess_input ( * args , * * kwargs ) return inception_resnet_v2.preprocess_input ( * args , * * kwargs ) preprocess_input = vgg16.preprocess_input shuffle : Whether to shuffle output samples , ] ) from . import keras_modules_injection train_datagen = ImageDataGenerator ( validation_split=validation_split , A [ Sequence ] ( /utils/ # sequence ) instance . n : Integer , total number of samples in the dataset to loop over . steps_per_epoch=2000 , fill_mode='nearest ' , zoom_range=0.2 ) zoom_range : Float or [ lower , upper ] . Range for random zoom . via the ` classes ` argument . scale=scale , * * kwargs ) to use for random transformations and normalization . if K.backend ( ) == 'tensorflow ' : assert len ( data_gen ) == 20 preprocess_input = mobilenet_v2.preprocess_input dtype=dtype ) super ( DirectoryIterator , self ) .__init__ ( preprocess_input = inception_resnet_v2.preprocess_input `` hamming '' are also supported . By default , `` nearest '' is used . dtype : Dtype to use for generated arrays . `` ` preprocessing_function : function that will be implied on each input . batch_size=32 , data = np.array ( [ [ i ] for i in range ( 50 ) ] ) kwargs [ 'dtype ' ] = dtype MODEL_LIST.extend ( [ NASNetMobile = nasnet.NASNetMobile x : Numpy array of input data or tuple . mask_datagen.fit ( masks , augment=True , seed=seed ) from keras.utils.test_utils import layer_test super ( ImageDataGenerator , self ) .__init__ ( data_format=data_format , target_size : tuple of integers , dimensions to resize input images to . interpolation : Interpolation method used to resample the image if the zoom_range=0. , return inception_v3.InceptionV3 ( * args , * * kwargs ) def __init__ ( self , x , y , image_data_generator , # combine generators into one which yields image and masks return image.img_to_array ( img , data_format=data_format , dtype=dtype ) keras_applications.set_keras_submodules ( if not hasattr ( keras_applications , 'get_submodules_from_kwargs ' ) : samplewise_center=samplewise_center , featurewise_center=False , from .. import backend return inception_resnet_v2.InceptionResNetV2 ( * args , * * kwargs ) Supported methods are `` nearest '' , `` bilinear '' , and `` bicubic '' . preprocessing_function=preprocessing_function , shuffle=shuffle , ( except maybe the last one ) . file_format=file_format , zca_whitening=zca_whitening , return vgg19.preprocess_input ( * args , * * kwargs ) if 'dtype ' in inspect.getargspec ( image.img_to_array ) .args : `` `` '' Utility class for generating batches of temporal data . end_index : Data points later than ` end_index ` will not be used MobileNetV2 = mobilenet_v2.MobileNetV2 follow_links=False , ` ( -width_shift_range , +width_shift_range ) ` rotation_range=90 , `` ` python * * kwargs ) width_shift_range=0. , class ImageDataGenerator ( image.ImageDataGenerator ) : rescale=1./255 , rotation_range=20 , cval=cval , # Arguments decode_predictions = mobilenet_v2.decode_predictions import numpy as np layers=layers , np.array ( [ [ [ 0 ] , [ 2 ] , [ 4 ] , [ 6 ] , [ 8 ] ] , mask_generator = mask_datagen.flow_from_directory ( kwargs [ 'layers ' ] = layers backend=backend , rescale : rescaling factor . Defaults to None . def wrapper ( * args , * * kwargs ) : batch_size=batch_size , y : Numpy array of targets data . from __future__ import absolute_import directory , image_data_generator , zca_whitening : Boolean . Apply ZCA whitening . return image.save_img ( path , return mobilenet.MobileNet ( * args , * * kwargs ) mask_datagen = ImageDataGenerator ( * * data_gen_args ) image_generator = image_datagen.flow_from_directory ( targets : Targets corresponding to timesteps in ` data ` . return imagenet_utils.preprocess_input ( * args , * * kwargs ) preprocess_input = vgg19.preprocess_input zoom_range=0.2 , If None or 0 , no rescaling is applied , 'constant ' : kkkkkkkk|abcd|kkkkkkkk ( cval=k ) scale=True , * * kwargs ) : subset : Subset of data ( ` `` training '' ` or ` `` validation '' ` ) if return inception_v3.decode_predictions ( * args , * * kwargs ) seed = 1 for visualizing the random transformations being Every ` Iterator ` must implement the ` _get_batches_of_transformed_samples ` height_shift_range=0.1 , seed : Random seeding for data shuffling . stride , length of history , etc. , to produce batches for Divide inputs by std of the dataset , feature-wise . length=10 , sampling_rate=2 , `` `` '' Base class for image data iterators . featurewise_std_normalization=False , def preprocess_input ( * args , * * kwargs ) : return image.img_to_array ( img , data_format=data_format ) 1-D array-like : random elements from the array . batch_size=32 , decode_predictions = densenet.decode_predictions def VGG19 ( * args , * * kwargs ) : def DenseNet169 ( * args , * * kwargs ) : horizontal_flip=horizontal_flip , if batches > = len ( x_train ) / 32 : while with ` width_shift_range=1.0 ` possible values are floats validation_split=0.0 , It should have same length as ` data ` . y_test = np_utils.to_categorical ( y_test , num_classes ) ` `` input '' ` : targets are images identical to input images ( mainly ( Shear angle in counter-clockwise direction in degrees ) save_format : Format to use for saving sample images `` `` '' Generate batches of tensor image data with real-time data augmentation . For stride ` s ` , consecutive output samples would vertical_flip=False , preprocess_input = imagenet_utils.preprocess_input data_format=data_format , Points outside the boundaries of the input are filled brightness_range=brightness_range , Xception = xception.Xception from __future__ import print_function def InceptionV3 ( * args , * * kwargs ) : def keras_modules_injection ( base_fun ) : ( after applying all other transformations ) . int : integer number of pixels from interval The function should take one argument : from keras.applications import mobilenet_v2 samplewise_std_normalization : Boolean . Divide each input by its std . np.array ( [ [ 10 ] , [ 11 ] ] ) ) rescale=rescale , from keras.applications import mobilenetv2 image_datagen.fit ( images , augment=True , seed=seed ) supported . If PIL version 3.4.0 or newer is installed , `` box '' and vertical_flip : Boolean . Randomly flip inputs vertically . channel_shift_range=channel_shift_range , dtype : Dtype to use for the generated arrays . InceptionResNetV2 = inception_resnet_v2.InceptionResNetV2 method . ` None ` : no targets get yielded ( only input images are yielded ) . images ( if ` save_to_dir ` is set ) . if hasattr ( keras_applications , 'get_submodules_from_kwargs ' ) : save_to_dir : Optional directory where to save the pictures batch_size=2 ) data : Indexable generator ( such as list or Numpy array ) def save_img ( path , # Only for backwards compatibility . class_mode : Mode for yielding the targets : ` `` categorical '' ` : categorical targets , channel_shift_range=0. , shear_range=0. , float : fraction of total height , if < 1 , or pixels if > = 1 . def DenseNet201 ( * args , * * kwargs ) : a brightness shift value from . preprocess_input = xception.preprocess_input validation_split : Float . Fraction of images reserved for validation interpolation=interpolation , `` `` '' Iterator capable of reading images from a directory on disk . return vgg19.VGG19 ( * args , * * kwargs ) def NASNetMobile ( * args , * * kwargs ) : color_mode=color_mode , class TimeseriesGenerator ( sequence.TimeseriesGenerator , utils.Sequence ) : return mobilenet_v2.decode_predictions ( * args , * * kwargs ) another numpy array or a list of numpy arrays , either `` channels_first '' or `` channels_last '' . 'data/validation ' , zca_epsilon : epsilon for ZCA whitening . Default is 1e-6 . model.fit ( x_batch , y_batch ) The data should be at 2D , and axis 0 is expected It defaults to the ` image_data_format ` value found in your ` ( -height_shift_range , +height_shift_range ) ` otherwise we multiply the data by the value provided dtype=None ) : class_mode='categorical ' , img_to_array = image.img_to_array ( if ` save_to_dir ` is set ) . epochs=50 , InceptionV3 = inception_v3.InceptionV3 layers=layers , ` data [ i ] ` , ` data [ i-r ] ` , ... ` data [ i - length ] ` class DirectoryIterator ( image.DirectoryIterator , Iterator ) : kwargs [ 'utils ' ] = utils class_mode=class_mode , # TODO : enable nasnet tests if they support Theano and CNTK Default is 'nearest ' . print ( 'Epoch ' , e ) return densenet.preprocess_input ( * args , * * kwargs ) import inspect decode_predictions = nasnet.decode_predictions featurewise_std_normalization=True , data_format = backend.image_data_format ( ) save_prefix : String prefix to use for saving sample validation_steps=800 ) def VGG16 ( * args , * * kwargs ) : shear_range : Float . Shear Intensity return resnet50.decode_predictions ( * args , * * kwargs ) Set input mean to 0 over the dataset , feature-wise . return xception.decode_predictions ( * args , * * kwargs ) cval=0. , return image.array_to_img ( x , batches = 0 data_gen_args = dict ( featurewise_center=True , scale=scale ) featurewise_std_normalization : Boolean . models=models , 'data/masks ' , * args , * * kwargs ) from .mobilenet_v2 import MobileNetV2 TimeseriesGenerator = sequence.TimeseriesGenerator return wrapper model.fit_generator ( datagen.flow ( x_train , y_train , batch_size=32 ) , super ( NumpyArrayIterator , self ) .__init__ ( break decode_predictions = imagenet_utils.decode_predictions from .mobilenetv2 import MobileNetV2 preprocess_input = inception_v3.preprocess_input Example of transforming images and masks together . reverse : Boolean : if ` true ` , timesteps in each output sample will be kwargs [ 'models ' ] = models dtype = backend.floatx ( ) color_mode : One of ` `` rgb '' ` , ` `` rgba '' ` , ` `` grayscale '' ` . y_train = np_utils.to_categorical ( y_train , num_classes ) from __future__ import print_function considered to contain images from one class , float : fraction of total width , if < 1 , or pixels if > = 1 . return vgg16.decode_predictions ( * args , * * kwargs ) kwargs = { } assert hasattr ( mobilenet_v2 , 'MobileNetV2 ' ) `` channels_last '' mode means that the images should have shape def InceptionResNetV2 ( * args , * * kwargs ) : featurewise_std_normalization=True , class Iterator ( image.Iterator , utils.Sequence ) : Each subdirectory in this directory will be return densenet.decode_predictions ( * args , * * kwargs ) be centered around ` data [ i ] ` , ` data [ i+s ] ` , ` data [ i+2 * s ] ` , etc . array_to_img.__doc__ = image.array_to_img.__doc__ `` `` '' Iterator yielding data from a Numpy array . return vgg19.decode_predictions ( * args , * * kwargs ) def __init__ ( self , directory , image_data_generator , sample_weight=sample_weight , def DenseNet121 ( * args , * * kwargs ) : in reverse chronological order . With ` width_shift_range=2 ` possible values rotation_range=rotation_range , if 'dtype ' in inspect.getargspec ( image.array_to_img ) .args : The function will run after the image is resized and augmented . seed=seed , # the generator loops indefinitely Example of using ` .flow_from_directory ( directory ) ` : for x_batch , y_batch in datagen.flow ( x_train , y_train , batch_size=32 ) : data for test or validation . decode_predictions = resnet50.decode_predictions scale=scale , DenseNet201 = densenet.DenseNet201 pass decode_predictions = vgg16.decode_predictions x , model.fit_generator ( if data_format is None : Keras config file at ` ~/.keras/keras.json ` . horizontal_flip : Boolean . Randomly flip inputs horizontally . if dtype is None : save_img.__doc__ = image.save_img.__doc__ dtype = backend.floatx ( ) from __future__ import division 'data/images ' , directory : Path to the directory to read images from . when ` fill_mode = `` constant '' ` . return nasnet.NASNetMobile ( * args , * * kwargs ) image_data_generator : Instance of ` ImageDataGenerator ` sampling_rate : Period between successive individual timesteps rotation_range : Int . Degree range for random rotations . data_format=data_format , validation_data=validation_generator , for e in range ( epochs ) : color_mode='rgb ' , classes=classes , horizontal_flip=True ) ( applications.NASNetLarge , 4032 ) , epochs=50 ) return mobilenet_v2.MobileNetV2 ( * args , * * kwargs ) data_format=None , target_size= ( 150 , 150 ) , decode_predictions = inception_v3.decode_predictions subset=subset , train_generator = train_datagen.flow_from_directory ( def ResNet50 ( * args , * * kwargs ) : steps_per_epoch=2000 , width_shift_range=0.2 , from __future__ import absolute_import width_shift_range : Float , 1-D array-like or int sample_weight : Numpy array of sample weights . featurewise_center=featurewise_center , return resnet50.ResNet50 ( * args , * * kwargs ) decode_predictions = mobilenet.decode_predictions It will be computed automatically if not set . width_shift_range=width_shift_range , height_shift_range=0. , 'data/train ' , return xception.preprocess_input ( * args , * * kwargs ) return resnet50.preprocess_input ( * args , * * kwargs ) decode_predictions = inception_resnet_v2.decode_predictions while with ` height_shift_range=1.0 ` possible values are floats decode_predictions = vgg19.decode_predictions save_to_dir=save_to_dir , sample_weight=None , preprocessing_function=None , save_format=save_format , If you never set it , then it will be `` channels_last '' . data_gen = TimeseriesGenerator ( data , targets , class NumpyArrayIterator ( image.NumpyArrayIterator , Iterator ) : Value used for points outside the boundaries train_generator , in the interval [ -1.0 , +1.0 ) . Iterator = image.Iterator image_datagen = ImageDataGenerator ( * * data_gen_args ) target_size=target_size , samplewise_std_normalization=False , [ [ 1 ] , [ 3 ] , [ 5 ] , [ 7 ] , [ 9 ] ] ] ) ) if data_format is None : ` ( samples , height , width , channels ) ` , If tuple , the second elements is either `` channels_first '' mode means that the images should have shape image.ImageDataGenerator.__init__ ) .args : train_generator , img_to_array.__doc__ = image.img_to_array.__doc__ used to work with autoencoders ) , assert np.array_equal ( y , start_index : Data points earlier than ` start_index ` will not be used file_format=None , shear_range=0.2 , containing consecutive data points ( timesteps ) . each of which gets passed DirectoryIterator = image.DirectoryIterator batch_0 = data_gen [ 0 ] zca_epsilon=1e-6 , shuffle=True , return nasnet.preprocess_input ( * args , * * kwargs ) class_mode=None , if dtype is None : x , y , image_data_generator , def img_to_array ( img , data_format=None , dtype=None ) : featurewise_std_normalization=featurewise_std_normalization , training/validation . NASNetLarge = nasnet.NASNetLarge preprocess_input = resnet50.preprocess_input assert hasattr ( mobilenetv2 , 'MobileNetV2 ' ) samplewise_std_normalization=samplewise_std_normalization , 'nearest ' : aaaaaaaa|abcd|dddddddd seed=seed ) DenseNet169 = densenet.DenseNet169 return nasnet.NASNetLarge ( * args , * * kwargs ) ` ( samples , channels , height , width ) ` . equal intervals , along with time series parameters such as within sequences . For rate ` r ` , timesteps steps_per_epoch=len ( x_train ) / 32 , epochs=epochs ) # fits the model on batches with real-time data augmentation : horizontal_flip=False , height_shift_range=height_shift_range , return vgg16.preprocess_input ( * args , * * kwargs ) batches += 1 subset=None , data_format = backend.image_data_format ( ) preprocess_input = densenet.preprocess_input batch_size : Integer , size of a batch . return densenet.DenseNet169 ( * args , * * kwargs ) The data will be looped over ( in batches ) . If a float , ` [ lower , upper ] = [ 1-zoom_range , 1+zoom_range ] ` . stride : Period between successive output sequences . same as with ` height_shift_range= [ -1 , 0 , +1 ] ` , # we create two instances with the same arguments fill_mode : One of { `` constant '' , `` nearest '' , `` reflect '' or `` wrap '' } . and should output a Numpy tensor with the same shape . data_format : String , one of ` channels_first ` , ` channels_last ` . from keras_applications import mobilenet_v2 fill_mode=fill_mode , utils=utils ) # TODO : enable Nasnet for Theano and CNTK classes=None , channel_shift_range : Float . Range for random channel shifts . containing images from each class ( e.g . ` [ `` dogs '' , `` cats '' ] ` ) . # ( std , mean , and principal components if ZCA whitening is applied ) brightness_range=None , # ( applications.NASNetLarge , 4032 ) NumpyArrayIterator = image.NumpyArrayIterator batch_size : Number of timeseries samples in each batch return mobilenet.decode_predictions ( * args , * * kwargs ) If PIL version 1.1.3 or newer is installed , `` lanczos '' is also ( applications.NASNetMobile , 1056 ) , data_format=data_format , def __init__ ( self , return imagenet_utils.decode_predictions ( brightness_range : Tuple or list of two floats . Range for picking backend=backend , # here 's a more `` manual '' example follow_links=follow_links , preprocess_input = nasnet.preprocess_input applied , for debugging purposes . height_shift_range : Float , 1-D array-like or int data_format=None , to be the time dimension . # compute quantities required for featurewise normalization VGG19 = vgg19.VGG19 return densenet.DenseNet121 ( * args , * * kwargs ) validation_split is set in ImageDataGenerator . save_prefix= '' , return vgg16.VGG16 ( * args , * * kwargs ) save_to_dir=None , according to the given mode : `` `` '' height_shift_range=0.2 , rescale=None , vertical_flip=vertical_flip , ResNet50 = resnet50.ResNet50 one image ( Numpy tensor with rank 3 ) , validation_generator = test_datagen.flow_from_directory ( cval : Float or Int . shuffle : Boolean , whether to shuffle the data between epochs . featurewise_center=True , train_generator = zip ( image_generator , mask_generator ) seed : Random seed for data shuffling . keras_applications.set_keras_submodules ( zoom_range=zoom_range , from keras.models import Sequential through as an output without any modifications . # Returns preprocess_input = mobilenet.preprocess_input def Xception ( * args , * * kwargs ) : class_mode='binary ' ) test_datagen = ImageDataGenerator ( rescale=1./255 ) if 'dtype ' in inspect.getargspec ( data_format : Image data format , shuffle=False , models=models , featurewise_center : Boolean . are used for create a sample sequence . or instead draw them in chronological order . decode_predictions = xception.decode_predictions classes : Optional list of strings , names of subdirectories return inception_resnet_v2.decode_predictions ( * args , * * kwargs ) MobileNet = mobilenet.MobileNet # Provide the same seed and keyword arguments to the fit and flow methods datagen.fit ( x_train ) ( strictly between 0 and 1 ) . rotation_range=0 , Color mode to read images . def test_mobilenet_v2_legacy_import ( ) : from .. import utils VGG16 = vgg16.VGG16 save_img = image.save_img image.NumpyArrayIterator.__init__ ) .args : With ` height_shift_range=2 ` possible values kwargs [ 'backend ' ] = backend length : Length of the output sequences ( in number of timesteps ) . # we need to break the loop by hand because samplewise_center : Boolean . Set each sample mean to 0 . samplewise_center=False , DenseNet121 = densenet.DenseNet121 target_size= ( 256 , 256 ) , from keras_applications import mobilenet_v2 ` `` sparse '' ` : integer targets , return nasnet.decode_predictions ( * args , * * kwargs ) save_format='png ' , def NASNetLarge ( * args , * * kwargs ) : from keras.preprocessing.sequence import TimeseriesGenerator or alternatively you could specify class subdirectories seed=None ,","['keras/applications/__init__.py', 'keras/applications/densenet.py', 'keras/applications/imagenet_utils.py', 'keras/applications/inception_resnet_v2.py', 'keras/applications/inception_v3.py', 'keras/applications/mobilenet.py', 'keras/applications/mobilenet_v2.py', 'keras/applications/mobilenetv2.py', 'keras/applications/nasnet.py', 'keras/applications/resnet50.py', 'keras/applications/vgg16.py', 'keras/applications/vgg19.py', 'keras/applications/xception.py', 'keras/preprocessing/image.py', 'keras/preprocessing/sequence.py', 'tests/integration_tests/applications_test.py']",Integration with redesigned preprocessing & applications modules . ( # 10952 )
338,23c40e68de2cffa77c92ae48cca1f81913756d2a,2018-08-21 16:41:37-07:00,"base_config = super ( Cropping2D , self ) .get_config ( ) def compute_output_shape ( self , input_shape ) : 'channels_last ' , 'data_format ' : self.data_format } end = -end inputs with shape ` ( batch , channels , ... ) ` . self.input_spec = InputSpec ( ndim=5 ) self.cropping = cropping super ( Cropping2D , self ) .__init__ ( normalized_cropping , ( cropping , cropping ) ) config = { 'cropping ' : self.cropping , spatial_axes = list ( range ( 1 , 1 + len ( cropping ) ) ) spatial_axes = list ( range ( 1 , 1 + self.rank ) ) config = { 'cropping ' : self.cropping , self.data_format = K.normalize_data_format ( data_format ) cropping_all_dims = ( ( 0 , 0 ) , ) + cropping + ( ( 0 , 0 ) , ) return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) else : if output_shape [ dim ] is not None : data_format : A string , # self.rank is 1 for Cropping1D , 2 for Cropping2D ... ( self.cropping , ) ) normalized_cropping = ( height_cropping , width_cropping ) class Cropping3D ( _Cropping ) : super ( Cropping3D , self ) .__init__ ( normalized_cropping , one of ` `` channels_last '' ` or ` `` channels_first '' ` . class Cropping2D ( Layer ) : self.input_spec = InputSpec ( ndim=4 ) self.data_format = K.normalize_data_format ( data_format ) slices_dims = [ ] * * kwargs ) The ordering of the dimensions in the inputs . self.cropping = ( ( cropping , cropping ) , ( cropping , cropping ) ) for start , end in self.cropping : def call ( self , inputs ) : return tuple ( output_shape ) cropping : A tuple of tuples of 2 ints . end = None self.cropping ) * * kwargs ) : output_shape = list ( input_shape ) for dim in range ( len ( output_shape ) ) : super ( Cropping1D , self ) .__init__ ( normalized_cropping , Keras config file at ` ~/.keras/keras.json ` . base_config = super ( _Cropping , self ) .get_config ( ) def get_config ( self ) : cropping_all_dims = ( ( 0 , 0 ) , ) + self.cropping + ( ( 0 , 0 ) , ) self.cropping = conv_utils.normalize_tuple ( cropping , 2 , 'cropping ' ) end = None self.data_format , return tuple ( output_shape ) self.data_format , return base_config ` `` channels_last '' ` corresponds to inputs with shape def call ( self , inputs ) : output_shape = list ( input_shape ) slices_dims.append ( slice ( start , end ) ) self.cropping = ( ( cropping , cropping ) , else : ` ( batch , ... , channels ) ` while ` `` channels_first '' ` corresponds to data_format , slices = transpose_shape ( slices , self.data_format , spatial_axes ) ( cropping , cropping ) , super ( _Cropping , self ) .__init__ ( * * kwargs ) slices = tuple ( slices ) slices = tuple ( slices ) if output_shape [ dim ] is not None : if end == 0 : slices = transpose_shape ( slices , data_format , spatial_axes ) end = -end data_format=None , normalized_cropping = ( dim1_cropping , dim2_cropping , dim3_cropping ) output_shape [ dim ] -= sum ( cropping_all_dims [ dim ] ) slices = [ slice ( None ) ] + slices_dims + [ slice ( None ) ] `` `` '' Abstract nD copping layer ( private , used as implementation base ) . if end == 0 : slices = [ slice ( None ) ] + slices_dims + [ slice ( None ) ] return _compute_output_shape_cropping ( input_shape , for dim in range ( len ( output_shape ) ) : `` `` '' def compute_output_shape ( self , input_shape ) : class Cropping1D ( _Cropping ) : class Cropping3D ( Layer ) : slices_dims = [ ] 'channels_last ' , slices_dims.append ( slice ( start , end ) ) config = { 'cropping ' : self.cropping } def __init__ ( self , cropping , super ( Cropping2D , self ) .__init__ ( * * kwargs ) If you never set it , then it will be `` channels_last '' . return _call_cropping ( inputs , 'channels_last ' , ( self.cropping , ) ) It defaults to the ` image_data_format ` value found in your cropping_all_dims = transpose_shape ( cropping_all_dims , output_shape [ dim ] -= sum ( cropping_all_dims [ dim ] ) def _compute_output_shape_cropping ( input_shape , data_format , cropping ) : base_config = super ( Cropping3D , self ) .get_config ( ) normalized_cropping = ( ( cropping , cropping ) , ( cropping , cropping ) ) return inputs [ slices ] # Arguments class Cropping1D ( Layer ) : normalized_cropping = ( ( cropping , cropping ) , return inputs [ slices ] self.cropping = ( height_cropping , width_cropping ) super ( Cropping3D , self ) .__init__ ( * * kwargs ) normalized_cropping = ( conv_utils.normalize_tuple ( cropping , 2 , 'cropping ' ) , ) class _Cropping ( Layer ) : self.rank = len ( cropping ) class Cropping2D ( _Cropping ) : 'data_format ' : self.data_format } self.input_spec = InputSpec ( ndim=3 ) def _call_cropping ( inputs , data_format , cropping ) : for start , end in cropping : For Cropping1D , the data format is always ` `` channels_last '' ` . def get_config ( self ) : return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) super ( Cropping1D , self ) .__init__ ( * * kwargs ) base_config.pop ( 'data_format ' ) base_config [ 'cropping ' ] = base_config [ 'cropping ' ] [ 0 ] self.input_spec = InputSpec ( ndim=2 + self.rank ) cropping_all_dims = transpose_shape ( cropping_all_dims , data_format , spatial_axes ) self.cropping = ( dim1_cropping , dim2_cropping , dim3_cropping ) ( cropping , cropping ) , return _call_cropping ( inputs , self.data_format , self.cropping ) ( cropping , cropping ) ) spatial_axes )",['keras/layers/convolutional.py'],Refactoring : Made an abstract class for all the cropping layers . ( # 10888 )
339,6746bda3dcda273580fef2d911c6cc333c8a626c,2018-08-21 15:51:01-07:00,"assert len ( val_seq.logs ) == 12 * 5 max_queue_size=1 , validation_steps = len ( val_data ) validation_data=val_seq , self.logs = [ ] # It will work for use_multiprocessing=False validation_steps = validation_steps or len ( val_data ) assert len ( val_seq.logs ) < = 4 * 5 self.logs.append ( idx ) validation_data=RandomSequence ( 4 ) , val_seq = RandomSequence ( 4 )","['keras/engine/training_generator.py', 'tests/keras/engine/test_training.py']",Fix val_step in fit_generator with Sequence ( # 10946 )
340,8c6f8d8c797c1b8576063cc973e6b9f729ec7612,2018-08-21 15:22:20-07:00,"layer = layer_class ( units , recurrent_initializer='identity ' ) np.concatenate ( [ np.identity ( units ) ] * num_kernels , axis=1 ) ) def test_rnn_cell_identity_initializer ( layer_class ) : @ pytest.mark.parametrize ( 'tensor_shape ' , [ ( 100 , 100 ) , ( 1 , 2 , 3 , 4 ) ] , ids= [ 'FC ' , 'CONV ' ] ) raise ValueError ( 'Identity matrix initializer can only be used ' Only use for square 2D matrices . if len ( tensor_shape ) > 2 or max ( tensor_shape ) % min ( tensor_shape ) ! = 0 : raise ValueError ( Only use for 2D matrices . [ np.identity ( shape [ 0 ] ) ] * ( shape [ 1 ] // shape [ 0 ] ) , axis=1 ) layer ( inputs ) assert np.array_equal ( recurrent_kernel , If the long side of the matrix is a multiple of the short side , if shape [ 0 ] == shape [ 1 ] : [ np.identity ( shape [ 1 ] ) ] * ( shape [ 0 ] // shape [ 1 ] ) , axis=0 ) 'for 2D square matrices . ' ) num_kernels = recurrent_kernel.shape [ 1 ] // recurrent_kernel.shape [ 0 ] if len ( shape ) ! = 2 : if max ( shape ) % min ( shape ) ! = 0 : elif shape [ 0 ] > shape [ 1 ] : return self.gain * np.identity ( shape [ 0 ] ) raise ValueError ( 'Long side should be multiple of short side . ' ) inputs = Input ( shape= ( timesteps , embedding_dim ) ) multiple identity matrices are concatenated along the long side . if len ( shape ) ! = 2 or shape [ 0 ] ! = shape [ 1 ] : return self.gain * np.identity ( shape [ 0 ] ) 'Identity matrix initializer can only be used for 2D matrices . ' ) ids= [ 'FC ' , 'RNN ' , 'RNN_INVALID ' , 'CONV ' ] ) if len ( tensor_shape ) > 2 : return self.gain * np.concatenate ( recurrent_kernel = layer.get_weights ( ) [ 1 ]","['keras/initializers.py', 'tests/keras/initializers_test.py', 'tests/keras/layers/recurrent_test.py']",Support recurrent kernel using identity initializer ( # 10934 )
341,bca4cc4df563a1d72b91460bc29cb18e91d14057,2018-08-21 10:59:39-07:00,"' a dict of tensors . ' ) : assert [ l.name for l in model.layers ] [ 2 : ] == expected_names 'Warning raised even when .compile ( ) is called after modifying .trainable ' ) assert len ( w ) == 0 , ( out = model.train_on_batch ( # the final output . We need the computed depth of the input layers to be super ( TestMultiInputLayer , self ) .__init__ ( * * kwargs ) assert ( len ( symbolic_weights ) == len ( weights_bi_conv_new ) ) with pytest.raises ( ValueError , # depth of the input layers to be the same , because they both pass through the embedding layer z = TestMultiInputLayer ( ) ( [ x , y ] ) expected_shapes = [ ( None , 64 ) , ( None , 5 ) ] # the same , because they both pass through the embedding layer before anything 'tensor as ` target_tensors ` . Expected a list or ' # different depths of computation in the graph before the final output . We need the computed : , args ) : # before anything else happens . That 's what we 're testing . weight_tensor_bi_convlstm_old.append ( np.zeros ( ( 3 , 3 , 10 , 10 ) ) ) # recurrent kernel ' A warning was raised for Sequence . ' ) out_shape = super ( ArbitraryMultiOutputLayer , self ) .compute_output_shape ( input_shape ) assert ( len ( symbolic_weights ) == len ( weight_tensor_bi_convlstm_new ) ) class ArbitraryMultiOutputLayer ( Layer ) : super ( ArbitraryMultiInputLayer , self ) .__init__ ( * * kwargs ) # Basic outline here : we have a shared embedding layer , and two inputs that go through weight_tensor_bi_convlstm_old.append ( np.zeros ( ( 3 , 3 , 15 , 10 ) ) ) # kernel assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 3 ] ) == weights_bi_conv_new [ 3 ] ) ' ` target_tensors ` . Expected a list or a dict of tensors . ' ) : [ output_a_np , output_b_np ] , assert warning_raised , ( tests/keras/engine/test_topology.py E501 \ weight_tensor_bi_convlstm_old.append ( np.zeros ( ( 10 , ) ) ) # bias # old ConvLSTM2D took a list of 12 weight tensors , assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 4 ] ) == weights_bi_conv_new [ 4 ] ) weights_bi_conv_old , assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 3 ] ) == weight_tensor_bi_convlstm_new [ 3 ] ) match='When passing a list as ` target_tensors ` , it should ' assert model.compute_output_shape ( [ ( None , 32 ) , ( None , 32 ) ] ) == [ ( None , 64 ) , ( None , 5 ) ] bi_conv = Bidirectional ( ConvLSTM2D ( 10 , ( 3 , 3 ) ) , merge_mode='concat ' ) ( td_conv ) weights_bi_conv_old.append ( np.zeros ( ( 3 , 3 , 15 , 10 ) ) ) # kernel reason='sparse operations supported only by TensorFlow ' ) assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 5 ] ) == weights_bi_conv_new [ 5 ] ) def test_preprocess_weights_for_loading_cudnn_rnn_should_be_idempotent ( layer_class , layer_args ) : assert model.compute_output_shape ( [ ( None , 32 ) , ( None , 32 ) ] ) == expected_shapes # the function is being called correctly for Conv2D assert [ l.name for l in model.layers ] [ 2 : ] == [ 'dense_1 ' , 'merge ' , 'dense_2 ' , 'dense_3 ' ] input_shape ) assert [ l.name for l in recreated_model.layers ] [ 2 : ] == [ 'dense_1 ' , 'merge ' , 'dense_2 ' , 'dense_3 ' ] assert final_model.compute_output_shape ( [ ( 10 , 32 ) , ( 10 , 32 ) ] ) == expected_shapes # use 'channels_first ' data format to check that the function is being called correctly for Conv2D # old ConvLSTM2D took a list of 12 weight tensors , returns a list of 3 concatenated larger tensors . weight_tensor_bi_convlstm_old , 'No warning raised when trainable is modified without .compile . ' ) # else happens . That 's what we 're testing . ( K.backend ( ) ! = 'tensorflow ' ) or ( not K.tensorflow_backend._get_available_gpus ( ) ) , out_shape = super ( TestMultiOutputLayer , self ) .compute_output_shape ( @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , reason='sparse operations supported only by TensorFlow ' ) not K.tensorflow_backend._get_available_gpus ( ) ) , assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 0 ] ) == weight_tensor_bi_convlstm_new [ 0 ] ) y.append ( x [ : , assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 5 ] ) == weight_tensor_bi_convlstm_new [ 5 ] ) assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 2 ] ) == weights_bi_conv_new [ 2 ] ) assert all ( [ 'Sequence ' not in str ( w_.message ) for w_ in w ] ) , ( tests/keras/engine/test_training.py E501 m : m1 : strides [ 2 ] ] ) model.add ( Dense ( 2 , input_shape= ( 3 , ) , kernel_initializer=Constant ( np.ones ( ( 3 , 2 ) ) ) ) ) expected_names = [ 'dense_1 ' , 'merge ' , 'dense_2 ' , 'dense_3 ' ] 'output . The model has \d outputs , but you passed target_tensors= ' ) : layer = layer_class ( * * layer_args ) with pytest.raises ( ValueError , match='The model has \d outputs , but you passed a single tensor as ' from keras.layers import Bidirectional , Conv2D , Input 'have one entry per model output . The model has \d ' @ pytest.mark.parametrize ( 'layer_class , layer_args ' , [ tests/keras/backend/backend_test.py E501 test_preprocess_weights_for_loading_rnn_should_be_idempotent ( layer_class , args ) # use 'channels_first ' data format to check that assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 1 ] ) == weights_bi_conv_new [ 1 ] ) assert len ( w ) == 0 , `` Warning raised even when .compile ( ) is called after modifying .trainable '' expected_shapes = [ ( 10 , 7 ) , ( 10 , 64 ) ] weight_tensor_bi_convlstm_old = [ ] bi_convlstm2d = Bidirectional ( ConvLSTM2D ( 10 , ( 3 , 3 ) ) , merge_mode='concat ' ) ( td_conv ) z = ArbitraryMultiInputLayer ( ) ( [ x , y ] ) kernel_initializer=Constant ( np.ones ( ( 3 , 2 ) ) ) ) ) @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , reason='Requires TensorFlow backend ' ) ( not K.tensorflow_backend._get_available_gpus ( ) ) ) , sample_weight= [ None , np.random.random ( ( 10 , 20 , 30 ) ) ] ) # Basic outline here : we have a shared embedding layer , and two inputs that class TestMultiOutputLayer ( Layer ) : with pytest.raises ( ValueError , match='When passing a list as ` target_tensors ` , it should have one entry per model ' [ input_a_np , input_b_np ] , assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 2 ] ) == weight_tensor_bi_convlstm_new [ 2 ] ) assert all ( [ 'Sequence ' not in str ( w_.message ) for w_ in w ] ) , ' A warning was raised for Sequence . ' assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 1 ] ) == weight_tensor_bi_convlstm_new [ 1 ] ) def test_preprocess_weights_for_loading_rnn_should_be_idempotent ( layer_class , layer_args ) : assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 0 ] ) == weights_bi_conv_new [ 0 ] ) match='The model has \d outputs , but you passed a single ' x , y = TestMultiOutputLayer ( ) ( input_layer ) sample_weight= [ None , np.random.random ( ( 10 , 20 , 30 ) ) ] ) test_preprocess_weights_for_loading_rnn_should_be_idempotent ( layer_class , layer_args ) [ output_a_np , output_b_np ] , assert [ l.name for l in recreated_model.layers ] [ 2 : ] == expected_names reason='Requires TensorFlow backend ' ) super ( TestMultiOutputLayer , self ) .__init__ ( * * kwargs ) from keras.layers import ConvLSTM2D , TimeDistributed super ( ArbitraryMultiOutputLayer , self ) .__init__ ( * * kwargs ) weight_value_tuples += zip ( symbolic_weights , weight_tensor_bi_convlstm_new ) # go through different depths of computation in the graph before tests/keras/backend/backend_test.py E501 \ k : k1 : strides [ 0 ] , model.add ( Dense ( 2 , input_shape= ( 3 , ) , weights_bi_conv_old.append ( np.zeros ( ( 3 , 3 , 10 , 10 ) ) ) # recurrent kernel ( K.backend ( ) ! = 'tensorflow ' or def test_preprocess_weights_for_loading_rnn_should_be_idempotent ( layer_class , args ) : y.append ( x [ : , : , k : k1 : strides [ 0 ] , l : l1 : strides [ 1 ] , m : m1 : strides [ 2 ] ] ) 'outputs , but you passed target_tensors= ' ) : def test_preprocess_weights_for_loading_cudnn_rnn_should_be_idempotent ( layer_class , tests/keras/backend/reference_operations.py E501 \ l : l1 : strides [ 1 ] , reason=' Can not catch warnings in python 2 ' ) x , y = ArbitraryMultiOutputLayer ( ) ( input_layer ) @ pytest.mark.skipif ( sys.version_info < ( 3 , ) , reason=' Can not catch warnings in python 2 ' ) from keras.layers import ConvLSTM2D , TimeDistributed , Bidirectional , Conv2D , Input class ArbitraryMultiInputLayer ( Layer ) : weights_bi_conv_old = [ ] weight_value_tuples += zip ( symbolic_weights , weights_bi_conv_new ) model = Model ( inputs=inputs , outputs=bi_convlstm2d ) # returns a list of 3 concatenated larger tensors . layer = layer_class ( * * args ) assert warning_raised , 'No warning raised when trainable is modified without .compile . ' assert final_model.compute_output_shape ( [ ( 10 , 32 ) , ( 10 , 32 ) ] ) == [ ( 10 , 7 ) , ( 10 , 64 ) ] out = model.train_on_batch ( [ input_a_np , input_b_np ] , weights_bi_conv_new = saving.preprocess_weights_for_loading ( weight_tensor_bi_convlstm_new = saving.preprocess_weights_for_loading ( model = Model ( inputs=inputs , outputs=bi_conv ) class TestMultiInputLayer ( Layer ) : ( ( K.backend ( ) ! = 'tensorflow ' ) or weights_bi_conv_old.append ( np.zeros ( ( 10 , ) ) ) # bias assert np.all ( K.eval ( model.layers [ 2 ] .weights [ 4 ] ) == weight_tensor_bi_convlstm_new [ 4 ] )","['pytest.ini', 'tests/keras/backend/reference_operations.py', 'tests/keras/engine/test_topology.py', 'tests/keras/engine/test_training.py', 'tests/keras/layers/cudnn_recurrent_test.py']",Style fixes for enabling PEP8 501 ( # 10931 )
342,907e8035656b0597a7a2f3cc1bf237a8cbf48821,2018-08-21 10:58:50-07:00,"r = k.random_uniform ( ( 1 , ) , minval=min_val , maxval=max_val ) rand = k.eval ( k.truncated_normal ( ( 300 , 200 ) , mean=mean , stddev=std , seed=1337 ) ) assert max_val - 0.015 < np.max ( rand ) < = max_val assert np.min ( samples ) == 0 x = k.variable ( np.random.random ( x_shape ) ) assert np.abs ( np.mean ( samples ) - mean ) < std * 0.015 np.log ( np.sum ( np.exp ( x_np ) , axis=axis , keepdims=keepdims ) ) , assert np.abs ( np.mean ( samples ) - p ) < 0.015 assert np.abs ( np.std ( rand ) - std ) < std * 0.015 K.bias_add ( x , b , data_format='channels_middle ' ) k.resize_volumes ( k.variable ( xval ) , 2 , 2 , 2 , assert np.max ( rand ) < = max_val data_format='channels_middle ' ) x = K.variable ( x_np ) assert np.abs ( np.std ( samples ) - std ) < std * 0.015 assert rand.shape == ( 300 , 200 ) data_format='channels_middle ' ) samples = [ k.eval ( r ) for _ in range ( 60000 ) ] with pytest.raises ( ValueError ) : assert min_val + 0.015 > np.min ( samples ) > = min_val assert np.abs ( np.mean ( rand ) - mean ) < std * 0.015 b = k.variable ( np.random.random ( bias_shape ) ) assert np.max ( rand ) == 1 assert rand.shape == ( 200 , 100 ) assert np.all ( koh == oh ) assert np.max ( samples ) == 1 assert max_val - 0.015 < np.max ( samples ) < = max_val assert np.max ( samples ) == 1 assert np.max ( rand ) == 1 data_format='channels_middle ' ) rand = K.eval ( K.random_normal ( ( 300 , 200 ) , mean=mean , stddev=std , seed=1337 ) ) assert np.abs ( np.mean ( rand ) ) < 0.015 b = K.variable ( np.random.random ( bias_shape ) ) assert_allclose ( K.eval ( K.logsumexp ( x , axis=axis , keepdims=keepdims ) ) , assert min_val + 0.015 > np.min ( samples ) > = min_val x = k.variable ( x_np ) assert np.all ( koh == oh ) assert rand.shape == ( 300 , 200 ) k.resize_images ( k.variable ( xval ) , 2 , 2 , assert min_val + 0.015 > np.min ( rand ) > = min_val samples = [ K.eval ( r ) for _ in range ( 60000 ) ] K.resize_images ( K.variable ( xval ) , 2 , 2 , assert np.abs ( np.mean ( rand ) - mean ) < 0.015 k.spatial_2d_padding ( k.variable ( xval ) , padding=padding , rand = K.eval ( K.truncated_normal ( ( 300 , 200 ) , mean=mean , stddev=std , seed=1337 ) ) assert max_val - 0.015 < np.max ( rand ) < = max_val assert np.max ( rand ) < = max_val with pytest.raises ( ValueError ) : assert np.min ( rand ) == 0 assert rand.shape == ( 200 , 100 ) samples = [ K.eval ( r ) for _ in range ( 20000 ) ] r = k.random_binomial ( ( 1 , ) , p ) rand = K.eval ( K.random_uniform ( ( 200 , 100 ) , min_val , max_val ) ) r = K.random_normal ( ( 1 , ) , mean=mean , stddev=std , seed=1337 ) assert np.abs ( np.mean ( rand ) - p ) < 0.015 assert np.abs ( np.std ( rand ) - std * 0.87962 ) < 0.015 for k in BACKENDS : x = K.variable ( np.random.random ( x_shape ) ) rand = k.eval ( k.random_normal ( ( 300 , 200 ) , mean=mean , stddev=std , seed=1337 ) ) koh = k.eval ( k.one_hot ( k.variable ( indices , dtype='int32 ' ) , num_classes ) ) assert np.abs ( np.std ( rand ) - std ) < std * 0.015 r = K.random_binomial ( ( 1 , ) , p ) koh = K.eval ( K.one_hot ( K.variable ( indices , dtype='int32 ' ) , num_classes ) ) assert rand.shape == ( 300 , 200 ) assert np.abs ( np.mean ( samples ) - mean ) < std * 0.015 rand = K.eval ( K.random_binomial ( ( 200 , 100 ) , p ) ) k.spatial_3d_padding ( k.variable ( xval ) , padding=padding , assert np.abs ( np.mean ( rand ) ) < 0.015 assert np.abs ( np.std ( rand ) - std * 0.87962 ) < 0.015 rand = k.eval ( k.random_binomial ( ( 200 , 100 ) , p ) ) assert np.abs ( np.mean ( samples ) - p ) < 0.015 for mean , std in [ ( 0. , 1 . ) , ( -10. , 5 . ) ] : rtol=1e-5 ) # assumption in initializers.VarianceScaling K.spatial_3d_padding ( K.variable ( xval ) , padding=padding , K.spatial_2d_padding ( K.variable ( xval ) , padding=padding , assert min_val + 0.015 > np.min ( rand ) > = min_val r = K.random_uniform ( ( 1 , ) , minval=min_val , maxval=max_val ) data_format='channels_middle ' ) k.bias_add ( x , b , data_format='channels_middle ' ) assert np.abs ( np.mean ( samples ) ) < 0.015 assert np.min ( rand ) > = min_val z = K.dropout ( K.variable ( val ) , level=-0.5 ) assert np.min ( rand ) > = min_val r = k.random_normal ( ( 1 , ) , mean=mean , stddev=std , seed=1337 ) assert max_val - 0.015 < np.max ( samples ) < = max_val z = k.dropout ( k.variable ( val ) , level=-0.5 ) rtol=1e-5 ) assert_allclose ( k.eval ( k.logsumexp ( x , axis=axis , keepdims=keepdims ) ) , assert np.abs ( np.mean ( samples ) ) < 0.015 # test that random_normal also generates different values when used within a function assert rand.shape == ( 300 , 200 ) assert np.abs ( np.mean ( rand ) - p ) < 0.015 np.log ( np.sum ( np.exp ( x_np ) , axis=axis , keepdims=keepdims ) ) , assert np.min ( rand ) == 0 for mean , std in [ ( 0. , 1 . ) , ( -10. , 5 . ) ] : assert np.abs ( np.mean ( rand ) - mean ) < 0.015 K.resize_volumes ( K.variable ( xval ) , 2 , 2 , 2 , assert np.abs ( np.std ( samples ) - std ) < std * 0.015 # test that random_normal also generates different values when used within a function rand = k.eval ( k.random_uniform ( ( 200 , 100 ) , min_val , max_val ) ) assert np.abs ( np.mean ( rand ) - mean ) < std * 0.015 data_format='channels_middle ' ) data_format='channels_middle ' ) assert np.min ( samples ) == 0 samples = [ k.eval ( r ) for _ in range ( 20000 ) ] # assumption in initializers.VarianceScaling",['tests/keras/backend/backend_test.py'],Speed up backend tests ( # 10930 )
343,c8e3b601cd8bf3062d416a5ef74c4a97be611923,2018-08-21 20:02:11+09:00,"In my test , highest validation accuracy is 83.79 % after 50 epochs . and 79 % after 15 epochs , and 83 % after 30 epochs . In my test , highest validation accuracy is 83.79 % after 50 epcohs . This is a fast Implement , just 20s/epoch with a gtx 1070 gpu . This is a fast Implement , just 20s/epcoh with a gtx 1070 gpu . and 79 % after 15 epochs , and 83 % after 30 epcohs .",['examples/cifar10_cnn_capsule.py'],Update cifar10_cnn_capsule.py ( # 10927 )
344,213170d20241c918ff42078e2de3e9a908244093,2018-08-20 21:56:54-07:00,"conflict_counter = { n : 0 for n , count in occurrences.items ( ) if count > 1 } if n in conflict_counter : assert parallel_siamese.output_names == [ 'add ' , 'nested_1 ' , 'nested_2 ' ] def test_multi_gpu_with_siamese ( ) : for name , outputs in zip ( output_names , all_outputs ) : score_sum = keras.layers.Add ( name='add ' ) ( [ score1 , score2 ] ) nested_model = keras.models.Sequential ( [ input_shape = ( 3 , ) input1 = keras.Input ( input_shape ) occurrences = { } else : siamese = keras.models.Model ( inputs= [ input1 , input2 ] , output_names = [ ] n += ' _ % d ' % conflict_counter [ n ] # Deduplicate output names to handle Siamese networks . parallel_siamese = multi_gpu_model ( siamese , 2 ) ] , name='nested ' ) for name , outputs in zip ( model.output_names , all_outputs ) : keras.layers.Dense ( 1 ) score2 = nested_model ( input2 ) name='siamese ' ) for n in model.output_names : conflict_counter [ n ] += 1 keras.layers.Dense ( 32 , input_shape=input_shape ) , input2 = keras.Input ( input_shape ) outputs= [ score_sum , score1 , score2 ] , score1 = nested_model ( input1 ) occurrences [ n ] = 1 output_names.append ( n ) occurrences [ n ] += 1 if n not in occurrences :","['keras/utils/multi_gpu_utils.py', 'tests/keras/utils/multi_gpu_test.py']",Fix multi-output Siamese networks in multi_gpu_model ( # 10856 )
345,7386841194d139ee8d1de2749fa64f86150f9c2f,2018-08-20 21:56:19-07:00,"reason='Temporarily disabled until the use_multiprocessing problem is solved ' ) K.backend ( ) == 'tensorflow ' , from keras import backend as K pytestmark = pytest.mark.skipif (","['tests/keras/utils/data_utils_test.py', 'tests/test_multiprocessing.py']",Fix CI on TensorFlow ( # 10929 )
346,1fc585adb57f20a2acf69f0cd08b731259b8d2f8,2018-08-18 23:32:27+09:00,"early_stop.on_train_begin ( ) # The best configuration is in epoch 2 ( loss = 0.1000 ) , If False , the model weights obtained at the last step of baseline=None , epochs_trained = 0 epochs_trained += 1 def test_EarlyStopping_final_weights_when_restoring_model_weights ( ) : def __init__ ( self ) : def test_EarlyStopping_final_weights ( ) : assert early_stop.model.get_weights ( ) == 2 self.weights = -1 self.best_weights = self.model.get_weights ( ) self.stop_training = False early_stop.on_epoch_end ( epoch , logs= { 'val_loss ' : losses [ epoch ] } ) self.model.set_weights ( self.best_weights ) print ( `` Restoring model weights from the end of the best epoch '' ) assert early_stop.model.get_weights ( ) == 4 def set_weight_to_epoch ( self , epoch ) : early_stop.model = DummyModel ( ) break losses = [ 0.2 , 0.15 , 0.1 , 0.11 , 0.12 ] training are used . self.restore_best_weights = restore_best_weights early_stop = callbacks.EarlyStopping ( monitor='val_loss ' , patience=2 ) if early_stop.model.stop_training : self.best_weights = None def set_weights ( self , weights ) : # so with patience=2 we need to end up at epoch 4 early_stop.model.set_weight_to_epoch ( epoch=epoch ) restore_best_weights=True ) for epoch in range ( len ( losses ) ) : baseline=None ) : self.weights = epoch self.weights = weights pass if self.verbose > 0 : # and while patience = 2 , we 're restoring the best weights , restore_best_weights=False ) : # so we end up at the epoch with the best weights , i.e . epoch 2 return [ ] def get_weights ( self ) : if self.restore_best_weights : the epoch with the best value of the monitored quantity . early_stop = callbacks.EarlyStopping ( monitor='val_loss ' , patience=2 , # The best configuration is in the epoch 2 ( loss = 0.1000 ) . class DummyModel ( object ) : restore_best_weights : whether to restore model weights from # The best configuration is in the epoch 2 ( loss = 0.1000 ) , if self.restore_best_weights : return self.weights","['keras/callbacks.py', 'tests/keras/test_callbacks.py']",EarlyStopping : restore model weights corresponding to best value of monitored quantity ( # 10765 )
347,c7b7328cc99fd5d7c298e57c6020043451d89a61,2018-08-16 13:27:00-07:00,"y = T.all ( x , axis=axis , keepdims=keepdims ) output = th_sparse_module.basic.vstack ( tensors , format='csr ' ) keras_shape_list.pop ( a ) if axis is None : break input2 = layers.Input ( shape= ( 6 , ) ) output_shape [ axis ] += shape [ axis ] x = layers.wrappers.TimeDistributed ( layers.Dense ( 3 , activation='softmax ' ) ) ( x ) input_shapes = [ tensor._keras_shape for tensor in tensors ] input1 = layers.Input ( shape= ( 6 , ) ) if keepdims : output_shape = list ( input_shapes [ 0 ] ) output_shape [ axis ] = None for a in axis_list [ : :-1 ] : axis_list = list ( set ( int ( a ) for a in axis ) ) x2 = layers.Embedding ( 10 , 5 , input_length=6 , mask_zero=True ) ( input2 ) y._keras_shape = tuple ( keras_shape_list ) else : return th_sparse_module.basic.hstack ( tensors , format='csr ' ) if isinstance ( axis , int ) : models.Model ( inputs= [ input1 , input2 ] , outputs= [ x ] ) if not keras_shape_list : return output output = T.concatenate ( [ to_dense ( x ) for x in tensors ] , axis=axis ) else : axis_list = [ axis ] return y x1 = layers.Embedding ( 10 , 5 , input_length=6 , mask_zero=True ) ( input1 ) for shape in input_shapes [ 1 : ] : for a in axis_list : x = layers.concatenate ( [ x1 , x2 ] ) keras_shape_list [ a ] = 1 if hasattr ( x , '_keras_shape ' ) : keras_shape_list = ( 1 , ) return th_sparse_module.basic.vstack ( tensors , format='csr ' ) output = th_sparse_module.basic.hstack ( tensors , format='csr ' ) if py_all ( [ hasattr ( tensor , '_keras_shape ' ) for tensor in tensors ] ) : return T.all ( x , axis=axis , keepdims=keepdims ) output._keras_shape = tuple ( output_shape ) return T.concatenate ( [ to_dense ( x ) for x in tensors ] , axis=axis ) def test_masking_concatenate ( ) : y._keras_shape = ( 1 , ) * len ( x._keras_shape ) if keepdims else ( 1 , ) keras_shape_list = list ( x._keras_shape ) if output_shape [ axis ] is None or shape [ axis ] is None :","['keras/backend/theano_backend.py', 'tests/keras/layers/merge_test.py']",Add static shape inference for theano concatenate and all ( # 10873 )
348,695a928224bb8d5efc6483fce1c40dae65dc7bbd,2018-08-16 13:26:22-07:00,"model.compile ( loss='mae ' , optimizer='adam ' ) return K.mean ( inputs , axis=1 ) def call ( self , inputs ) : # Test GlobalAveragePooling1D supports masking if mask is not None : model_input = np.random.randint ( low=1 , high=5 , size= ( 2 , 3 , 4 ) ) def test_globalpooling_1d_supports_masking ( ) : def call ( self , inputs , mask=None ) : output = model.predict ( model_input ) def compute_mask ( self , inputs , mask=None ) : broadcast_shape = [ -1 , input_shape [ 1 ] , 1 ] return None input_shape = K.int_shape ( inputs ) else : return K.mean ( inputs , axis=1 ) from keras.layers import Masking def __init__ ( self , * * kwargs ) : model.add ( Masking ( mask_value=0. , input_shape= ( 3 , 4 ) ) ) assert np.array_equal ( output [ 0 ] , model_input [ 0 , 0 , : ] ) mask = K.cast ( mask , K.floatx ( ) ) inputs * = mask self.supports_masking = True super ( GlobalAveragePooling1D , self ) .__init__ ( * * kwargs ) return K.sum ( inputs , axis=1 ) / K.sum ( mask , axis=1 ) model_input [ 0 , 1 : , : ] = 0 model.add ( pooling.GlobalAveragePooling1D ( ) ) mask = K.reshape ( mask , broadcast_shape ) model = Sequential ( )","['keras/layers/pooling.py', 'tests/keras/layers/convolutional_test.py']",GlobalAveragePooling1D supports masking ( # 10913 )
349,0151028441d30525a0a553eb7fab4b08269e8266,2018-08-16 12:00:19-07:00,' ` class_weight ` argument will be ignored . ' ) if sample_weight is not None and class_weight is not None : import warnings warnings.warn ( 'Found both ` sample_weight ` and ` class_weight ` : ',['keras/engine/training_utils.py'],Raise warning if both class_weight and sample_weight given ( # 10921 )
350,b2f75b7a3d1da174a4859b1c26e2faab99041d29,2018-08-16 11:21:24-07:00,"assert_allclose ( out , out2 , atol=1e-05 ) loaded_model = load_model ( g ) opened_new_file = not isinstance ( filepath , h5py.File ) if not isinstance ( filepath , h5py.File ) : save_model ( model , g ) f.file.flush ( ) h5py.File or h5py.Group object from which to load the model h5py.File object where to save the model f.flush ( ) out2 = loaded_model.predict ( x ) if not isinstance ( filepath , h5py.Group ) : opened_new_file = not isinstance ( filepath , h5py.Group ) h5py.File object from which to load the model g = h5file.create_group ( 'model ' ) with h5py.File ( fname , mode= ' r+ ' ) as h5file : h5py.File or h5py.Group object where to save the model","['keras/engine/saving.py', 'tests/test_model_saving.py']",[ P ] all save_model/load_model to accept h5py.Group ( # 10912 )
351,67fc091c6daf70bd91717e59799c3e014509ca0c,2018-08-15 11:44:54-07:00,pip install tensorflow==1.9 WORKERS = 4 if K.backend ( ) ! = 'tensorflow ' else 2 WORKERS = 4 from keras import backend as K pip install tensorflow==1.7,"['.travis.yml', 'tests/test_multiprocessing.py']",Update TensorFlow as 1.9 on Travis ( # 10674 )
352,e6817ea2cacff41f2b11b5c148a8f220288fcf82,2018-08-15 11:40:37-07:00,"old_layer = keras.layers.GlobalAveragePooling3D ( dim_ordering='th ' , name='global_avgpool3d ' ) new_layer = keras.layers.LSTM ( 2 , input_shape= [ None , 5 ] , name= 'd ' , implementation=1 ) new_layer = keras.layers.MaxPool2D ( pool_size=2 , padding='valid ' , data_format='channels_last ' , name='maxpool2d ' ) input_shape= ( None , None ) ) ) # N by t_1 by t_2 by 6 layer = convolutional_recurrent.ConvLSTM2D ( filters=filters , pool_size= ( 2 , 2 , 2 ) , strides= ( 2 , 2 , 2 ) , padding='valid ' , name='maxpool3d ' ) 'depth_multiplier ' : multiplier , layer = wrappers.Bidirectional ( rnn ( units , return_state=True ) , implementation=1 ) layer = wrappers.Bidirectional ( rnn ( units , return_sequences=True ) , merge_mode=merge_mode ) old_layer = keras.layers.AveragePooling2D ( pool_size= ( 2 , 2 ) , border_mode='valid ' , name='avgpooling2d ' ) new_layer = keras.layers.AvgPool2D ( pool_size= ( 2 , 2 ) , strides= ( 2 , 2 ) , padding='valid ' , name='avgpooling2d ' ) old_layer = keras.layers.MaxPooling3D ( pool_size= ( 2 , 2 , 2 ) , border_mode='valid ' , name='maxpool3d ' ) pool_size= ( 2 , 2 , 2 ) , strides= ( 2 , 2 , 2 ) , padding='valid ' , name='global_maxpool3d ' ) new_layer = keras.layers.GlobalAvgPool3D ( data_format='channels_first ' , new_layer = keras.layers.AvgPool3D ( pool_size= ( 2 , 2 , 2 ) , padding='valid ' , data_format='channels_last ' , name='avgpooling3d ' ) 'depth_multiplier ' : multiplier , assert json.dumps ( old_layer.get_config ( ) ) == json.dumps ( new_layer_2.get_config ( ) ) new_layer = keras.layers.AvgPool3D ( pool_size= ( 2 , 2 , 2 ) , strides= ( 2 , 2 , 2 ) , padding='valid ' , name='avgpooling3d ' ) old_layer = keras.layers.Deconvolution2D ( 5 , 3 , 3 , output_shape= ( 6 , 7 , 5 ) , name='deconv ' ) filters=filters , kernel_size=3 , padding=padding , old_layer = keras.layers.MaxPooling2D ( new_layer = keras.layers.AvgPool2D ( pool_size=2 , padding='valid ' , data_format='channels_first ' , name='avgpooling2d ' ) model = Sequential ( [ convolutional.Conv2D ( filters=filters , new_layer_2 = keras.layers.Dropout ( 3 , name='drop ' ) consume_less='mem ' ) old_layer = keras.layers.LSTM ( input_shape= [ 3 , 5 ] , output_dim=2 , name= 'd ' , consume_less='mem ' ) new_layer = keras.layers.GlobalMaxPool2D ( data_format='channels_first ' , padding=padding , 'padding ' : padding , new_layer = keras.layers.MaxPool3D ( pool_size= ( 2 , 2 , 2 ) , padding='valid ' , data_format='channels_last ' , name='maxpool3d ' ) old_layer = keras.layers.GlobalAveragePooling2D ( dim_ordering='th ' , tests/keras/layers/embeddings_test.py E501 \ batch_input_shape= ( None , 5 , None ) ) ] ) new_layer = keras.layers.GlobalMaxPool2D ( data_format='channels_last ' , model = Sequential ( [ convolutional.SeparableConv1D ( filters=filters , else : new_layer = keras.layers.MaxPool3D ( pool_size= ( 2 , 2 , 2 ) , strides= ( 2 , 2 , 2 ) , padding='valid ' , name='maxpool3d ' ) old_layer = keras.layers.GlobalAveragePooling3D ( dim_ordering='default ' , old_layer = keras.layers.GlobalMaxPooling3D ( dim_ordering='default ' , 'output_padding ' : out_padding , kernel_size= ( num_row , num_col ) , new_layer = keras.layers.MaxPool3D ( pool_size= ( 2 , 2 , 2 ) , padding='valid ' , name='maxpool3d ' ) momentum=0.8 ) old_layer = keras.layers.MaxPooling2D ( ( 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='maxpool2d ' ) new_layer = keras.layers.MaxPool2D ( pool_size=2 , padding='valid ' , data_format='channels_first ' , name='maxpool2d ' ) batch_input_shape= ( None , None , 5 , None ) ) ] ) ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='maxpool3d ' ) old_layer = keras.layers.GlobalAveragePooling3D ( dim_ordering='tf ' , name='global_avgpool3d ' ) 'input_length ' : ( None , 5 ) } , layer_test ( layers.SpatialDropout2D if len ( shape ) == 2 else layers.SpatialDropout3D , pool_size=2 , padding='valid ' , name='avgpooling2d ' ) pool_size=2 , padding='valid ' , name='maxpool2d ' ) 'data_format ' : data_format } , ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='avgpooling3d ' ) return_sequences=True ) ) norm = normalization.BatchNormalization ( axis=1 , input_shape= ( 3 , 4 , 4 ) , pool_size= ( 2 , 2 , 2 ) , padding='valid ' , data_format='channels_first ' , kwargs= { 'output_dim ' : 4 , 'input_dim ' : 10 , 'mask_zero ' : True , 'input_length ' : ( None , 5 ) } , old_layer = keras.layers.AveragePooling3D ( ( 2 , 2 , 2 ) , ( 2 , 2 , 2 ) , 'valid ' , name='avgpooling3d ' ) [ np.zeros ( ( 6 , 5 , 5 ) ) , np.zeros ( ( 6 , 32 ) ) , old_layer = keras.layers.GlobalMaxPooling2D ( dim_ordering='th ' , name='global_maxpool2d ' ) new_layer = keras.layers.GlobalMaxPool2D ( data_format='channels_first ' , name='global_maxpool2d ' ) new_layer = keras.layers.GlobalAvgPool2D ( data_format='channels_last ' , new_layer = keras.layers.GlobalAvgPool3D ( data_format='channels_first ' , name='global_avgpool3d ' ) new_layer = keras.layers.MaxPool2D ( pool_size=2 , padding='valid ' , name='maxpool2d ' ) new_layer = keras.layers.GlobalMaxPool3D ( data_format='channels_first ' , name='global_maxpool3d ' ) fixed_batch_size=True ) name='maxpool2d ' ) old_layer = keras.layers.MaxPooling3D ( ( 2 , 2 , 2 ) , ( 2 , 2 , 2 ) , 'valid ' , name='maxpool3d ' ) old_layer = keras.layers.AveragePooling3D ( pool_size= ( 2 , 2 , 2 ) , border_mode='valid ' , name='avgpooling3d ' ) pool_size= ( 2 , 2 ) , strides= ( 2 , 2 ) , padding='valid ' , name='avgpooling2d ' ) kernel_size=kernel_size , layer_test ( kwargs= { 'output_dim ' : 4 , 'input_dim ' : 10 , 'mask_zero ' : True , pool_size=2 , padding='valid ' , data_format='channels_last ' , merge_mode=merge_mode ) 'dilation_rate ' : dilation_rate } , tests/keras/layers/wrappers_test.py E501 \ tests/keras/layers/normalization_test.py E501 \ ( 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='avgpooling2d ' ) ( K.backend ( ) ! = 'tensorflow ' ) or ( not K.tensorflow_backend._get_available_gpus ( ) ) , new_layer = keras.layers.AvgPool3D ( [ np.zeros ( ( 6 , 5 , 5 ) ) , np.zeros ( ( 6 , 32 ) ) , np.zeros ( ( 6 , 32 ) ) , np.zeros ( ( 6 , 3 ) ) ] , kwargs= { 'padding ' : ( ( 1 , 2 ) , ( 3 , 4 ) , ( 0 , 2 ) ) , norm = normalization.BatchNormalization ( axis=1 , input_shape= ( 3 , 4 , 4 ) , momentum=0.8 ) name='global_avgpool2d ' ) 'data_format ' : data_format } , layer = wrappers.Bidirectional ( rnn ( units , return_state=True , return_sequences=True ) ) ( 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='avgpooling2d ' ) layer = layers.SpatialDropout2D new_layer1 = keras.layers.GaussianDropout ( rate=0.6 , name='drop ' ) convolutional.Conv3DTranspose , mask_outputs += [ model.layers [ 2 ] .compute_mask ( model.layers [ 2 ] .input , new_layer = keras.layers.GlobalMaxPool3D ( data_format='channels_first ' , layer = wrappers.Bidirectional ( rnn ( units , return_sequences=True ) , new_layer_1 = keras.layers.GaussianDropout ( rate=0.6 , name='drop ' ) 'kernel_size ' : 3 , old_layer = keras.layers.GlobalAveragePooling3D ( dim_ordering='th ' , 'kernel_size ' : 3 , old_layer = keras.layers.AveragePooling3D ( ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='avgpooling3d ' ) model = Sequential ( [ convolutional.Conv2D ( input_shape= ( num_samples , num_row , num_col , stack_size ) ) old_layer = keras.layers.LSTM ( input_shape= [ 3 , 5 ] , output_dim=2 , name= 'd ' , old_layer = keras.layers.Deconvolution2D ( 5 , 3 , nb_col=3 , output_shape= ( 6 , 7 , 5 ) , old_layer = keras.layers.AveragePooling3D ( ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='avgpooling3d ' ) name='avgpooling3d ' ) ( 2 , 2 ) , ( 2 , 2 ) , 'valid ' , name='avgpooling2d ' ) kwargs= { 'filters ' : filters , ( 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='maxpool2d ' ) consume_less='gpu ' ) old_layer = keras.layers.Deconvolution2D ( 5 , nb_row=3 , nb_col=3 , np.zeros ( ( 6 , 32 ) ) , np.zeros ( ( 6 , 3 ) ) ] , old_layer = keras.layers.Deconvolution2D ( 5 , nb_row=3 , nb_col=3 , output_shape= ( 6 , 7 , 5 ) , name='deconv ' ) old_layer = keras.layers.AveragePooling2D ( ( 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='avgpooling2d ' ) old_layer = keras.layers.AveragePooling3D ( ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='avgpooling3d ' ) old_layer = keras.layers.GlobalMaxPooling2D ( dim_ordering='default ' , filters=filters , kernel_size= ( num_row , num_col ) , tests/keras/layers/cudnn_recurrent_test.py E501 \ old_layer = keras.layers.GlobalMaxPooling3D ( dim_ordering='th ' , name='global_maxpool3d ' ) ( 2 , 2 , 2 ) , ( 2 , 2 , 2 ) , 'valid ' , name='maxpool3d ' ) 'padding ' : padding , tests/keras/engine/test_training.py E501 input_shape= ( None , num_depth , num_row , num_col , stack_size ) , pool_size= ( 2 , 2 , 2 ) , border_mode='valid ' , name='avgpooling3d ' ) old_layer = keras.layers.AveragePooling2D ( ( 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='avgpooling2d ' ) name='global_maxpool2d ' ) old_layer = keras.layers.GlobalAveragePooling2D ( dim_ordering='tf ' , name='global_avgpool2d ' ) new_layer = keras.layers.GlobalMaxPool3D ( data_format='channels_last ' , norm = normalization.BatchNormalization ( axis=1 , input_shape= ( 10 , 6 ) , model = Sequential ( [ convolutional.SeparableConv1D ( [ [ [ 0 ] , [ 1 ] , [ 2 ] , [ 4 ] , [ 6 ] , [ 9 ] , [ 12 ] , [ 15 ] , [ 18 ] , [ 21 ] ] ] ) model = Sequential ( [ convolutional.SeparableConv2D ( filters=filters , layer = convolutional_recurrent.ConvLSTM2D ( ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='avgpooling3d ' ) old_layer = keras.layers.GlobalAveragePooling2D ( dim_ordering='th ' , name='global_avgpool2d ' ) new_layer = keras.layers.GlobalAvgPool2D ( data_format='channels_first ' , output_shape= ( 6 , 7 , 5 ) , name='deconv ' ) old_layer = keras.layers.LSTM ( input_shape= [ 3 , 5 ] , output_dim=2 , name= 'd ' , consume_less='gpu ' ) assert json.dumps ( old_layer.get_config ( ) ) == json.dumps ( new_layer1.get_config ( ) ) ( 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='maxpool2d ' ) ( 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='avgpooling2d ' ) new_layer = keras.layers.MaxPool2D ( pool_size=2 , strides=2 , padding='valid ' , name='maxpool2d ' ) old_layer = keras.layers.MaxPooling2D ( ( 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='maxpool2d ' ) pool_size=2 , padding='valid ' , data_format='channels_last ' , name='maxpool2d ' ) tests/keras/layers/core_test.py E501 \ 'strides ' : strides , pool_size= ( 2 , 2 , 2 ) , border_mode='valid ' , name='maxpool3d ' ) new_layer = keras.layers.LSTM ( 2 , input_shape= [ None , 5 ] , name= 'd ' , old_layer = keras.layers.AveragePooling3D ( old_layer = keras.layers.GlobalMaxPooling3D ( dim_ordering='default ' , name='global_maxpool3d ' ) mask_outputs += [ model.layers [ 1 ] .compute_mask ( model.layers [ 1 ] .input , assert model.predict ( inputs ) .shape == layer.compute_output_shape ( inputs.shape ) old_layer = keras.layers.GlobalMaxPooling3D ( dim_ordering='tf ' , name='global_maxpool3d ' ) [ K.reverse ( layer.backward_layer.call ( inputs ) , 1 ) ] ) old_layer = keras.layers.MaxPooling3D ( kernel_size=3 , padding=padding , pool_size= ( 2 , 2 , 2 ) , padding='valid ' , name='avgpooling3d ' ) old_layer = keras.layers.GlobalAveragePooling2D ( dim_ordering='default ' , pool_size= ( 2 , 2 ) , border_mode='valid ' , name='avgpooling2d ' ) new_layer1 = keras.layers.Dropout ( rate=3 , name='drop ' ) old_layer = keras.layers.AveragePooling2D ( ( 2 , 2 ) , ( 2 , 2 ) , 'valid ' , name='avgpooling2d ' ) if len ( shape ) == 2 : old_layer = keras.layers.GlobalMaxPooling2D ( dim_ordering='tf ' , name='global_maxpool2d ' ) old_layer = keras.layers.GlobalMaxPooling3D ( dim_ordering='tf ' , return_sequences=return_sequences ) 'dilation_rate ' : dilation_rate } , ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='maxpool3d ' ) old_layer = keras.layers.GlobalMaxPooling2D ( dim_ordering='th ' , 'kernel_size ' : ( 3 , 3 ) , layer_test ( layer , mask_outputs += [ model.layers [ 1 ] .compute_mask ( model.layers [ 1 ] .input , mask_outputs [ -1 ] ) ] pool_size= ( 2 , 2 ) , border_mode='valid ' , name='maxpool2d ' ) tests/keras/layers/convolutional_test.py E501 \ new_layer = keras.layers.AvgPool3D ( pool_size= ( 2 , 2 , 2 ) , padding='valid ' , data_format='channels_first ' , name='avgpooling3d ' ) new_layer = keras.layers.MaxPool3D ( pool_size= ( 2 , 2 , 2 ) , padding='valid ' , data_format='channels_first ' , name='maxpool3d ' ) layer_test ( layer , 'kernel_size ' : ( 3 , 3 ) , model = Sequential ( [ convolutional.SeparableConv2D ( old_layer = keras.layers.GlobalMaxPooling2D ( dim_ordering='tf ' , name='global_maxpool3d ' ) kwargs= { 'filters ' : filters , new_layer = keras.layers.AvgPool2D ( pool_size=2 , padding='valid ' , data_format='channels_last ' , name='avgpooling2d ' ) batch_input_shape= ( None , None , 5 , None ) ) ] ) name='global_avgpool3d ' ) kwargs= { 'padding ' : ( ( 1 , 2 ) , ( 3 , 4 ) , ( 0 , 2 ) ) , 'data_format ' : data_format } , old_layer = keras.layers.MaxPooling3D ( ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='maxpool3d ' ) # the shape so far : ( N , t_1 , t_2 , 6 ) data_format=data_format , new_layer = keras.layers.GlobalAvgPool3D ( data_format='channels_last ' , name='global_avgpool3d ' ) new_layer = keras.layers.UpSampling2D ( ( 2 , 2 ) , data_format='channels_last ' , name='us2d ' ) 'strides ' : strides , name='global_maxpool2d ' ) new_layer = keras.layers.UpSampling2D ( ( 2 , 2 ) , data_format='channels_last ' , layer_test ( convolutional.SeparableConv2D , new_layer = keras.layers.GlobalMaxPool2D ( data_format='channels_last ' , name='global_maxpool2d ' ) pool_size= ( 2 , 2 , 2 ) , padding='valid ' , name='maxpool3d ' ) pool_size= ( 2 , 2 , 2 ) , padding='valid ' , data_format='channels_last ' , old_layer = keras.layers.GlobalAveragePooling3D ( dim_ordering='default ' , name='global_avgpool3d ' ) old_layer = keras.layers.MaxPooling3D ( ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='maxpool3d ' ) new_layer = keras.layers.AvgPool2D ( pool_size= ( 2 , 2 ) , padding='valid ' , name='avgpooling2d ' ) name='avgpooling2d ' ) name='global_avgpool3d ' ) new_layer = keras.layers.MaxPool3D ( new_layer = keras.layers.AvgPool3D ( pool_size= ( 2 , 2 , 2 ) , padding='valid ' , name='avgpooling3d ' ) batch_input_shape= ( None , None , 5 , None ) ) ] ) input_shape= ( None , None ) ) ) new_layer2 = keras.layers.GaussianDropout ( 0.6 , name='drop ' ) layer.compute_output_shape ( inputs.shape ) ) input_shape= ( None , num_depth , num_row , num_col , stack_size ) , old_layer = keras.layers.GlobalAveragePooling2D ( dim_ordering='default ' , name='global_avgpool2d ' ) new_layer = keras.layers.MaxPool2D ( pool_size=2 , strides=2 , padding='valid ' , name='maxpool2d ' ) old_layer = keras.layers.MaxPooling2D ( ( 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='maxpool2d ' ) ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='th ' , name='maxpool3d ' ) old_layer = keras.layers.GlobalMaxPooling3D ( dim_ordering='th ' , new_layer = keras.layers.GlobalAvgPool3D ( data_format='channels_last ' , layer_test ( layers.SpatialDropout2D if len ( shape ) == 2 else layers.SpatialDropout3D , assert ( model.predict ( inputs ) .shape == new_layer = keras.layers.AvgPool2D ( layer = wrappers.Bidirectional ( rnn ( units , return_state=True ) , merge_mode=merge_mode ) ( 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='maxpool2d ' ) norm = normalization.BatchNormalization ( axis=1 , input_shape= ( 10 , 6 ) , momentum=0.8 ) ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='avgpooling3d ' ) old_layer = keras.layers.Deconvolution2D ( 5 , 3 , nb_col=3 , output_shape= ( 6 , 7 , 5 ) , name='deconv ' ) tests/keras/legacy/interface_test.py E501 filters=filters , kernel_size=kernel_size , padding=padding , ( ( K.backend ( ) ! = 'tensorflow ' ) or tests/keras/engine/test_training.py E501 \ f_backward = K.function ( [ inputs ] , f_backward = K.function ( [ inputs ] , [ K.reverse ( layer.backward_layer.call ( inputs ) , 1 ) ] ) new_layer_1 = keras.layers.Dropout ( rate=3 , name='drop ' ) expected_output=np.float32 ( ( not K.tensorflow_backend._get_available_gpus ( ) ) ) , data_format=data_format , return_sequences=return_sequences ) name='global_avgpool2d ' ) ( 2 , 2 , 2 ) , ( 2 , 2 , 2 ) , 'valid ' , name='avgpooling3d ' ) layer = wrappers.Bidirectional ( rnn ( units , return_state=True , tests/keras/layers/convolutional_recurrent_test.py E501 \ new_layer = keras.layers.GlobalAvgPool2D ( data_format='channels_first ' , name='global_avgpool2d ' ) layer_test ( convolutional.Conv3DTranspose , old_layer = keras.layers.GlobalAveragePooling2D ( dim_ordering='tf ' , pool_size=2 , padding='valid ' , data_format='channels_first ' , new_layer = keras.layers.AvgPool2D ( pool_size=2 , padding='valid ' , name='avgpooling2d ' ) mask_outputs += [ model.layers [ 2 ] .compute_mask ( model.layers [ 2 ] .input , mask_outputs [ -1 ] ) ] 'output_padding ' : out_padding , convolutional.SeparableConv2D , new_layer2 = keras.layers.Dropout ( 3 , name='drop ' ) old_layer = keras.layers.AveragePooling2D ( pool_size= ( 2 , 2 ) , padding='valid ' , name='avgpooling2d ' ) old_layer = keras.layers.AveragePooling2D ( ( 2 , 2 ) , padding='valid ' , dim_ordering='default ' , name='avgpooling2d ' ) new_layer = keras.layers.GlobalMaxPool3D ( data_format='channels_last ' , name='global_maxpool3d ' ) name='us2d ' ) old_layer = keras.layers.Deconvolution2D ( 5 , 3 , 3 , output_shape= ( 6 , 7 , 5 ) , name='deconv ' ) 'data_format ' : data_format } , assert json.dumps ( old_layer.get_config ( ) ) == json.dumps ( new_layer2.get_config ( ) ) input_shape= ( num_samples , num_row , num_col , stack_size ) ) mask_outputs [ -1 ] ) ] name='maxpool3d ' ) layer = layers.SpatialDropout3D old_layer = keras.layers.GlobalMaxPooling2D ( dim_ordering='default ' , name='global_maxpool2d ' ) fixed_batch_size=True ) old_layer = keras.layers.GlobalAveragePooling3D ( dim_ordering='tf ' , batch_input_shape= ( None , 5 , None ) ) ] ) old_layer = keras.layers.MaxPooling2D ( pool_size= ( 2 , 2 ) , border_mode='valid ' , name='maxpool2d ' ) expected_output=np.float32 ( [ [ [ 0 ] , [ 1 ] , [ 2 ] , [ 4 ] , [ 6 ] , [ 9 ] , [ 12 ] , [ 15 ] , [ 18 ] , [ 21 ] ] ] ) new_layer = keras.layers.GlobalAvgPool2D ( data_format='channels_last ' , name='global_avgpool2d ' ) old_layer = keras.layers.MaxPooling3D ( ( 2 , 2 , 2 ) , padding='valid ' , dim_ordering='tf ' , name='maxpool3d ' ) new_layer_2 = keras.layers.GaussianDropout ( 0.6 , name='drop ' ) assert json.dumps ( old_layer.get_config ( ) ) == json.dumps ( new_layer_1.get_config ( ) )","['pytest.ini', 'tests/keras/layers/convolutional_recurrent_test.py', 'tests/keras/layers/convolutional_test.py', 'tests/keras/layers/core_test.py', 'tests/keras/layers/cudnn_recurrent_test.py', 'tests/keras/layers/embeddings_test.py', 'tests/keras/layers/normalization_test.py', 'tests/keras/layers/wrappers_test.py', 'tests/keras/legacy/interface_test.py']",Style fixes for enabling PEP8 501 ( # 10916 )
353,cf7d3d259f93b1ddbc601208280a460ee3379591,2018-08-14 14:49:32-07:00,"def on_epoch_end ( self , epoch , logs=None ) : logs [ 'lr ' ] = K.get_value ( self.model.optimizer.lr ) logs = logs or { }",['keras/callbacks.py'],[ P ] Add LR for Tensorboard in LearningRateScheduler ( # 10908 )
354,bb9b800ebba8914b69db362cfac4e7c8a9b17a9e,2018-08-14 10:22:54-07:00,"' ` target_tensors ` . Expected a list or a dict of tensors . ' ) : model.compile ( optimizer='rmsprop ' , loss='mse ' , with pytest.raises ( ValueError , match='When passing a list as ` target_tensors ` , it should have one entry per model ' ' a list of tensors , or dict of tensors , but got : ' , target_tensors ) with pytest.raises ( ValueError , match='The model has \d outputs , but you passed a single tensor as ' # single-output , as tensor target_tensors=target ) elif K.is_tensor ( target_tensors ) : target_tensors= [ target_a ] ) ' a list or dict , but got : ' , target_tensors ) raise TypeError ( 'Expected ` target_tensors ` to be a tensor , ' ' outputs , but you passed a single tensor as ' raise TypeError ( 'Expected ` target_tensors ` to be ' 'output . The model has \d outputs , but you passed target_tensors= ' ) : if len ( self.outputs ) ! = 1 : # multi-output , not enough target tensors when ` target_tensors ` is not a dict model.compile ( optimizer='rmsprop ' , loss='mse ' , raise ValueError ( 'The model has ' + str ( len ( self.outputs ) ) model.train_on_batch ( input_val , None ) 'of tensors . ' ) target_tensors = [ target_tensors ] ' ` target_tensors ` . Expected a list or a dict ' target_tensors=target_a )","['keras/engine/training.py', 'tests/keras/engine/test_training.py']",Argument target_tensors from Model.compile : accept single tensor ( # 10616 )
355,f99f633725815635a1cf24aa2dd70bdb094ed7aa,2018-08-13 18:18:17-07:00,"y._keras_shape = output_shape x._keras_shape [ 4 ] + padding [ 2 ] [ 0 ] + padding [ 2 ] [ 1 ] ) z = k.eval ( getattr ( k , function_name ) ( x_shape_or_val , * * kwargs ) ) x._keras_shape [ 1 ] , assert_list_keras_shape ( t_list , z_list ) if s : x._keras_shape [ 3 ] + padding [ 2 ] [ 0 ] + padding [ 2 ] [ 1 ] , x._keras_shape [ 3 ] ) z = f ( [ x_val , y_val ] ) [ 0 ] z = k.eval ( getattr ( k , function_name ) ( k.variable ( x_val ) , * * kwargs ) ) [ k.variable ( x_val ) , k.variable ( y_val ) ] , * * kwargs ) k.variable ( x_val ) , k.variable ( y_val ) , * * kwargs ) ) t = getattr ( k , function_name ) ( x_shape_or_val , * * kwargs ) for t , z in zip ( t_list , z_list ) : t_list = [ ] x._keras_shape [ 3 ] + padding [ 1 ] [ 0 ] + padding [ 1 ] [ 1 ] , return output_cntk , KC.function ( [ xc ] , [ output_cntk ] ) z = k.eval ( getattr ( k , function_name ) ( t = getattr ( k , function_name ) ( t_list = [ k.gather ( k.variable ( ref ) , k.variable ( inds , dtype='int32 ' ) ) z = k.eval ( getattr ( k , function_name ) ( for k in BACKENDS ] x._keras_shape [ 1 ] + top_pad + bottom_pad , t , f = cntk_func_two_tensor ( function_name , x_shape , y=y_shape , * * kwargs ) for z in z_list : k.variable ( x_val ) , k.variable ( y_val ) , * * kwargs ) t , f = cntk_func_single_tensor ( function_name , x_shape , * * kwargs ) return KC.function ( [ xc ] , [ output_cntk ] ) else : t , f = cntk_func_two_tensor ( function_name , x_shape , y=y_val , * * kwargs ) return output_cntk , KC.function ( [ xc , yc ] , [ output_cntk ] ) z = k.eval ( t ) x._keras_shape [ 3 ] + left_pad + right_pad ) assert t._keras_shape [ i ] == z.shape [ i ] output_keras_shape = ( x._keras_shape [ 0 ] , x._keras_shape [ 1 ] + padding [ 0 ] [ 0 ] + padding [ 0 ] [ 1 ] , x._keras_shape [ 2 ] + padding [ 0 ] [ 0 ] + padding [ 0 ] [ 1 ] , for i , s in enumerate ( t._keras_shape ) : return T.set_subtensor ( output [ indices ] , x ) k.variable ( x_val ) , k.variable ( convert_kernel ( y_val ) ) , * * kwargs ) ) def assert_list_keras_shape ( z_list ) : return y assert_list_keras_shape ( z_list ) y=y_shape , * * kwargs ) ( [ x_val , y_val ] ) [ 0 ] if hasattr ( t , '_keras_shape ' ) and len ( t._keras_shape ) > 1 : return output_cntk , KC.function ( [ xc ] , [ output_cntk ] ) return KC.function ( [ xc ] , [ output_cntk ] ) if hasattr ( z , '_keras_shape ' ) : t = getattr ( k , function_name ) ( k.variable ( x_val ) , * * kwargs ) z = k.eval ( t ) if hasattr ( x , '_keras_shape ' ) : [ k.variable ( x_val ) , k.variable ( y_val ) ] , * * kwargs ) ) y._keras_shape = output_keras_shape z = cntk_func_two_tensor ( function_name , x_shape , if data_format == 'channels_first ' : z = f ( [ x_val ] ) [ 0 ] x._keras_shape [ 2 ] + padding [ 1 ] [ 0 ] + padding [ 1 ] [ 1 ] , assert_list_keras_shape ( z_list ) assert_list_keras_shape ( t_list , z_list ) x._keras_shape [ 2 ] + left_pad + right_pad , x._keras_shape [ 4 ] ) return KC.function ( [ xc , yc ] , [ output_cntk ] ) y=y_val , * * kwargs ) ( [ x_val ] ) [ 0 ] k.variable ( x_val ) , k.variable ( convert_kernel ( y_val ) ) , * * kwargs ) y = T.set_subtensor ( output [ indices ] , x ) * * kwargs ) ( [ x_val ] ) [ 0 ] t_list += [ t ] t = getattr ( k , function_name ) ( x_shape_or_val , y_shape_or_val , * * kwargs ) x._keras_shape [ 2 ] + top_pad + bottom_pad , def assert_list_keras_shape ( t_list , z_list ) : x_shape_or_val , y_shape_or_val , * * kwargs ) ) assert z._keras_shape == z.shape z = cntk_func_single_tensor ( function_name , x_shape ,","['keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py']",Fix check equality between static shape and dynamic shape in test ( # 10879 )
356,aa8d7f76a047b24a97614524d29fb5070cf549c7,2018-08-13 18:16:16-07:00,"raise Exception ( error_msg.format ( origin , e.code , e.msg ) ) except HTTPError as e : except HTTPError as e : raise Exception ( error_msg.format ( origin , e.code , e.msg ) )",['keras/utils/data_utils.py'],avoid url error exception ( # 10887 )
357,812c9fa61a7cf5a85cc360ad2d85b0a7a47d33d6,2018-08-09 13:28:38-07:00,"# For further details , see : https : //stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res # have reproducible behavior for certain hash-based operations . # https : //github.com/keras-team/keras/issues/2280 # issuecomment-306959926 import os inter_op_parallelism_threads=1 ) 3204480642156461591 # For further details , see : print ( hash ( `` keras '' ) ) 4883664951434749476 # For further details , see : https : //www.tensorflow.org/api_docs/python/tf/set_random_seed Moreover , when using the TensorFlow backend and running on a GPU , some operations have non-deterministic outputs , in particular ` tf.reduce_sum ( ) ` . This is due to the fact that GPUs run many operations in parallel , so the order of execution is not always guaranteed . Due to the limited precision of floats , even adding several numbers together may give slightly different results depending on the order in which you add them . You can try to avoid the non-deterministic operations , but some may be created automatically by TensorFlow to compute the gradients , so it is much simpler to just run the code on the CPU . For this , you can set the ` CUDA_VISIBLE_DEVICES ` environment variable to an empty string , for example : os.environ [ 'PYTHONHASHSEED ' ] = ' 0 ' # https : //docs.python.org/3.4/using/cmdline.html # envvar-PYTHONHASHSEED # For further details , see : https : //stackoverflow.com/questions/42022950/ During development of a model , sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification , or merely a result of a new random sample . $ cat test_hash.py 8127205062320133199 `` ` $ python3 test_hash.py # non-reproducible hash ( Python 3.2.3+ ) $ PYTHONHASHSEED=0 python3 test_hash.py # reproducible hash During development of a model , sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification , or merely a result of a new random sample . The below snippet of code provides an example of how to obtain reproducible results - this is geared towards a TensorFlow backend for a Python 3 environment . # Multiple threads are a potential source of First , you need to set the ` PYTHONHASHSEED ` environment variable to ` 0 ` before the program starts ( not within the program itself ) . This is necessary in Python 3.2.3 onwards to have reproducible behavior for certain hash-based operations ( e.g. , the item order in a set or a dict , see [ Python 's documentation ] ( https : //docs.python.org/3.7/using/cmdline.html # envvar-PYTHONHASHSEED ) or [ issue # 2280 ] ( https : //github.com/keras-team/keras/issues/2280 # issuecomment-306959926 ) for further details ) . One way to set the environment variable is when starting python like this : # non-reproducible results . session_conf = tf.ConfigProto ( intra_op_parallelism_threads=1 , session_conf = tf.ConfigProto ( intra_op_parallelism_threads=1 , inter_op_parallelism_threads=1 ) The below snippet of code provides an example of how to obtain reproducible results - this is geared towards a TensorFlow backend for a Python 3 environment : # Multiple threads are a potential source of non-reproducible results . # See these references for further details : # The below is necessary in Python 3.2.3 onwards to $ CUDA_VISIBLE_DEVICES= '' '' PYTHONHASHSEED=0 python your_program.py # https : //www.tensorflow.org/api_docs/python/tf/set_random_seed",['docs/templates/getting-started/faq.md'],Set PYTHONHASHSEED=0 before startup and mention reproducibility issues with TF+GPU ( # 10871 )
358,672a873ffb344dfa030103cad69bdbc948184e8e,2018-08-08 10:08:33-04:00,"elif data_format == 'channels_last ' : x += reshape ( bias , ( 1 , bias_shape [ 1 ] , bias_shape [ 0 ] ) ) x += reshape ( bias , ( 1 , bias_shape [ 0 ] , 1 ) ) else : if data_format == 'channels_first ' : if len ( bias_shape ) == 1 : x += reshape ( bias , ( 1 , 1 , bias_shape [ 0 ] ) ) new_shape = ( 1 , ) + bias_shape new_shape = ( 1 , 1 , bias_shape [ 0 ] ) else : x += reshape ( bias , new_shape ) new_shape = transpose_shape ( new_shape , data_format , spatial_axes= ( 1 , ) ) x += reshape ( bias , ( 1 , ) + bias_shape ) if len ( bias_shape ) == 1 :",['keras/backend/tensorflow_backend.py'],Some code refactoring using ` transpose_shape ` in tensorflow_backend.py . Part 3 ( # 10860 )
359,359e7627b46e850688c95cf6d1de48b77f9df901,2018-08-08 10:07:41-04:00,"elif data_format == 'channels_last ' : x += reshape ( bias , ( 1 , bias_shape [ 0 ] , 1 , 1 , 1 ) ) else : if data_format == 'channels_first ' : x += reshape ( bias , ( 1 , bias_shape [ 3 ] ) + bias_shape [ :3 ] ) new_shape = transpose_shape ( new_shape , data_format , spatial_axes= ( 1 , 2 , 3 ) ) new_shape = ( 1 , ) + bias_shape else : new_shape = ( 1 , 1 , 1 , 1 , bias_shape [ 0 ] ) x += reshape ( bias , new_shape ) x += reshape ( bias , ( 1 , ) + bias_shape ) x += reshape ( bias , ( 1 , 1 , 1 , bias_shape [ 0 ] ) ) if len ( bias_shape ) == 1 : if len ( bias_shape ) == 1 :",['keras/backend/tensorflow_backend.py'],Some code refactoring using ` transpose_shape ` in tensorflow_backend.py . Part 2 ( # 10859 )
360,9aed521e6650384533f69d7b46c0210b45ae9391,2018-08-08 10:04:46-04:00,"[ padding [ 2 ] [ 0 ] , padding [ 2 ] [ 1 ] ] , ] pattern = [ pattern = transpose_shape ( pattern , data_format , spatial_axes= ( 1 , 2 , 3 ) ) [ padding [ 2 ] [ 0 ] , padding [ 2 ] [ 1 ] ] [ padding [ 1 ] [ 0 ] , padding [ 1 ] [ 1 ] ] , [ 0 , 0 ] [ padding [ 0 ] [ 0 ] , padding [ 0 ] [ 1 ] ] , pattern = [ [ 0 , 0 ] , [ 0 , 0 ] , [ padding [ 1 ] [ 0 ] , padding [ 1 ] [ 1 ] ] , [ padding [ 2 ] [ 0 ] , padding [ 2 ] [ 1 ] ] , if data_format == 'channels_first ' : else : ] [ padding [ 0 ] [ 0 ] , padding [ 0 ] [ 1 ] ] , [ 0 , 0 ]",['keras/backend/tensorflow_backend.py'],Some code refactoring using ` transpose_shape ` in tensorflow_backend.py . ( # 10858 )
361,8f4e4f2a0b543f3d74327cb04113f65cd5d11c62,2018-08-08 10:02:40-04:00,"input_shape [ 2 ] ) ( self.cropping , ) ) output_shape [ dim ] -= sum ( cropping_all_dims [ dim ] ) length = None input_shape [ 1 ] - self.cropping [ 0 ] [ 0 ] - self.cropping [ 0 ] [ 1 ] if input_shape [ 1 ] else None , else : dim3 ) cropping_all_dims = transpose_shape ( cropping_all_dims , data_format , spatial_axes ) def _compute_output_shape_cropping ( input_shape , data_format , cropping ) : input_shape [ 3 ] ) if input_shape [ 4 ] is not None : if input_shape [ 1 ] is not None : for dim in range ( len ( output_shape ) ) : dim2 = None dim3 = input_shape [ 3 ] - self.cropping [ 2 ] [ 0 ] - self.cropping [ 2 ] [ 1 ] 'channels_last ' , length , if output_shape [ dim ] is not None : dim3 = None input_shape [ 3 ] - self.cropping [ 1 ] [ 0 ] - self.cropping [ 1 ] [ 1 ] if input_shape [ 3 ] else None ) dim2 = input_shape [ 3 ] - self.cropping [ 1 ] [ 0 ] - self.cropping [ 1 ] [ 1 ] dim2 = input_shape [ 2 ] - self.cropping [ 1 ] [ 0 ] - self.cropping [ 1 ] [ 1 ] cropping_all_dims = ( ( 0 , 0 ) , ) + cropping + ( ( 0 , 0 ) , ) return ( input_shape [ 0 ] , dim3 = input_shape [ 4 ] - self.cropping [ 2 ] [ 0 ] - self.cropping [ 2 ] [ 1 ] spatial_axes = list ( range ( 1 , 1 + len ( cropping ) ) ) return ( input_shape [ 0 ] , if input_shape [ 1 ] is not None : input_shape [ 1 ] , dim1 = input_shape [ 2 ] - self.cropping [ 0 ] [ 0 ] - self.cropping [ 0 ] [ 1 ] if input_shape [ 3 ] is not None : return tuple ( output_shape ) output_shape = list ( input_shape ) input_shape [ 2 ] - self.cropping [ 1 ] [ 0 ] - self.cropping [ 1 ] [ 1 ] if input_shape [ 2 ] else None , input_shape [ 2 ] - self.cropping [ 0 ] [ 0 ] - self.cropping [ 0 ] [ 1 ] if input_shape [ 2 ] else None , dim2 , dim1 = input_shape [ 1 ] - self.cropping [ 0 ] [ 0 ] - self.cropping [ 0 ] [ 1 ] if self.data_format == 'channels_first ' : else : self.data_format , return _compute_output_shape_cropping ( input_shape , dim3 , dim1 , if input_shape [ 2 ] is not None : length = input_shape [ 1 ] - self.cropping [ 0 ] - self.cropping [ 1 ] self.cropping ) elif self.data_format == 'channels_last ' : input_shape [ 4 ] ) dim1 = None",['keras/layers/convolutional.py'],Refactoring of the cropping layers . ( # 10865 )
362,e94e2cc5761e1a516f70e1706498b85cf9287297,2018-08-07 11:04:21-07:00,"slices_dims.append ( slice ( start , end ) ) if self.cropping [ 1 ] == 0 : return inputs [ : , self.cropping [ 0 ] : -self.cropping [ 1 ] , : ] elif self.cropping [ 0 ] [ 1 ] == self.cropping [ 2 ] [ 1 ] == 0 : : , return inputs [ : , elif self.cropping [ 0 ] [ 1 ] == 0 : : ] end = None return inputs [ : , self.cropping [ 0 ] : , : ] self.cropping [ 2 ] [ 0 ] : -self.cropping [ 2 ] [ 1 ] , self.cropping [ 1 ] [ 0 ] : ] from .. utils.generic_utils import transpose_shape self.cropping [ 1 ] [ 0 ] : -self.cropping [ 1 ] [ 1 ] ] return _call_cropping ( inputs , 'channels_last ' , ( self.cropping , ) ) elif self.cropping [ 2 ] [ 1 ] == 0 : self.cropping [ 2 ] [ 0 ] : ] self.cropping [ 2 ] [ 0 ] : -self.cropping [ 2 ] [ 1 ] ] self.cropping [ 2 ] [ 0 ] : -self.cropping [ 2 ] [ 1 ] , def _call_cropping ( inputs , data_format , cropping ) : self.cropping [ 0 ] [ 0 ] : -self.cropping [ 0 ] [ 1 ] , : ] slices = tuple ( slices ) else : return inputs [ : , self.cropping [ 2 ] [ 0 ] : , if end == 0 : spatial_axes = list ( range ( 1 , 1 + len ( cropping ) ) ) slices = transpose_shape ( slices , data_format , spatial_axes ) self.cropping [ 1 ] [ 0 ] : -self.cropping [ 1 ] [ 1 ] , if self.cropping [ 0 ] [ 1 ] == self.cropping [ 1 ] [ 1 ] == 0 : elif self.cropping [ 1 ] [ 1 ] == self.cropping [ 2 ] [ 1 ] == 0 : return inputs [ slices ] end = -end slices = [ slice ( None ) ] + slices_dims + [ slice ( None ) ] elif self.cropping [ 1 ] [ 1 ] == 0 : : , self.cropping [ 1 ] [ 0 ] : -self.cropping [ 1 ] [ 1 ] , self.cropping [ 1 ] [ 0 ] : -self.cropping [ 1 ] [ 1 ] ] self.cropping [ 1 ] [ 0 ] : , for start , end in cropping : if self.data_format == 'channels_first ' : return _call_cropping ( inputs , self.data_format , self.cropping ) else : self.cropping [ 0 ] [ 0 ] : , self.cropping [ 2 ] [ 0 ] : -self.cropping [ 2 ] [ 1 ] ] self.cropping [ 0 ] [ 0 ] : -self.cropping [ 0 ] [ 1 ] , self.cropping [ 1 ] [ 0 ] : -self.cropping [ 1 ] [ 1 ] , slices_dims = [ ] elif self.data_format == 'channels_last ' : if self.cropping [ 0 ] [ 1 ] == self.cropping [ 1 ] [ 1 ] == self.cropping [ 2 ] [ 1 ] == 0 : elif self.cropping [ 0 ] [ 1 ] == self.cropping [ 1 ] [ 1 ] == 0 :",['keras/layers/convolutional.py'],Removed some code in the cropping layer by making use of slices . ( # 10864 )
363,63e204693b782b5f068240568853cdd637c29030,2018-08-07 10:31:55-07:00,"( ` float32 ` , ` float64 ` , ` int32 ` ... ) categorical = np.zeros ( ( n , num_classes ) , dtype=dtype ) def to_categorical ( y , num_classes=None , dtype='float32 ' ) : categorical = np.zeros ( ( n , num_classes ) , dtype=np.float32 ) def to_categorical ( y , num_classes=None ) : dtype : The data type expected by the input , as a string",['keras/utils/np_utils.py'],Let utils.to_categorical support different dtypes . ( # 10846 )
364,7205d903fbc079bb99fbae0e3c02e6d2b4d227f0,2018-08-05 16:53:56-07:00,stateful_metric_indices = [ ] # build the model lazily on ` fit ` /etc . # We will have to batch_shape = None else : # The layer does n't know about its expected shape . # computed_masks might be used in the future .,"['keras/engine/network.py', 'keras/engine/sequential.py', 'keras/engine/training_generator.py']",Removed some unused variables . ( # 10686 )
365,1068e173eafd15e5aa9f40b9f73590a7f5be4a84,2018-08-05 16:48:16-07:00,"if target_format == 'channels_first ' : Correspond to the indexes of the spatial axes . > > > transpose_shape ( ( 16 , 128 , 128 , 32 ) , 'channels_last ' , spatial_axes= ( 1 , 2 ) ) output_shape = transpose_shape ( output_shape , 'channels_first ' , A tuple or list , with the elements permuted according [ 0 , 0 ] , shape [ 1 ] = output_shape [ 0 ] output_shape = input_shape [ :2 ] + ( rows , cols , cell.filters ) shape [ 2 ] = output_shape [ 1 ] output_shape += [ ( input_shape [ 0 ] , rows , cols , cell.filters ) target_format : A string , either ` 'channels_first ' ` or ` 'channels_last ' ` . output_shape = input_shape [ :2 ] + ( cell.filters , rows , cols ) shape [ 0 ] = output_shape [ 2 ] to ` target_format ` . [ 0 , 0 ] ] output_shape = input_shape [ :2 ] + ( rows , cols , cell.filters ) pattern = transpose_shape ( pattern , data_format , spatial_axes= ( 1 , 2 ) ) ' '' channels_first '' , `` channels_last '' . Received : ' > > > transpose_shape ( ( 16 , 128 , 128 , 32 ) , 'channels_first ' , spatial_axes= ( 1 , 2 ) ) shape [ 3 ] = output_shape [ 2 ] For example , if you pass a shape for _ in range ( 2 ) ] elif cell.data_format == 'channels_last ' : list ( padding [ 0 ] ) , pattern = [ [ 0 , 0 ] , image_shape = ( image_shape [ 0 ] , image_shape [ 3 ] , base = transpose_shape ( base , cell.data_format , spatial_axes= ( 1 , 2 ) ) elif cell.data_format == 'channels_last ' : output_shape += [ base [ : ] for _ in range ( 2 ) ] `` `` '' Converts a tuple or a list to the correct ` data_format ` . [ 0 , 0 ] ] def transpose_shape ( shape , target_format , spatial_axes ) : image_shape [ 1 ] , image_shape [ 2 ] ) return shape ( 32 , 128 , 128 ) `` ` else : image_shape = transpose_shape ( image_shape , 'channels_first ' , else : pattern = [ [ 0 , 0 ] , > > > transpose_shape ( ( 128 , 128 , 32 ) , 'channels_first ' , spatial_axes= ( 0 , 1 ) ) spatial_axes= ( 0 , 1 , 2 ) ) shape = list ( output_shape ) It does so by switching the positions of its elements . then ` spatial_axes= ( 2 , 3 ) ` . from .. utils.generic_utils import transpose_shape spatial_axes= ( 1 , 2 ) ) ValueError : if ` value ` or the global ` data_format ` invalid . spatial_axes= ( 0 , 1 ) ) `` `` '' # Example base = ( input_shape [ 0 ] , rows , cols , cell.filters ) raise ValueError ( 'The ` data_format ` argument must be one of ' spatial_axes= ( 2 , 3 ) ) new_values += ( shape [ -1 ] , ) list ( padding [ 0 ] ) , if cell.data_format == 'channels_first ' : # Raises new_values += tuple ( shape [ x ] for x in spatial_axes ) > > > from keras.utils.generic_utils import transpose_shape ( 16 , 128 , 128 , 32 ) shape [ 0 ] = output_shape [ 3 ] list ( padding [ 1 ] ) , if data_format == 'channels_first ' : # Returns return list ( new_values ) elif target_format == 'channels_last ' : new_values = shape [ : spatial_axes [ 0 ] ] if cell.data_format == 'channels_first ' : output_shape = tuple ( shape ) `` ` python list ( padding [ 0 ] ) , list ( padding [ 1 ] ) , # Arguments return new_values output_shape = transpose_shape ( output_shape , cell.data_format , if isinstance ( shape , list ) : shape : Tuple or list , often representing shape , list ( padding [ 1 ] ) ] spatial_axes : A tuple of integers . ( 16 , 32 , 128 , 128 ) representing ( batch_size , timesteps , rows , cols , channels ) , corresponding to ` 'channels_last ' ` . output_shape += [ ( input_shape [ 0 ] , cell.filters , rows , cols ) str ( target_format ) )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/layers/convolutional_recurrent.py', 'keras/utils/generic_utils.py']",Created a function to_data_format to abstract the shape and data_format handling . ( # 10781 )
366,f6fad22671db9506acee34b9fd33361de78ab148,2018-08-05 16:33:29-07:00,"backward_inputs += inputs [ -self._num_constants : ] x = [ np.random.randn ( 2 , 4 , 3 ) , np.random.randn ( 2 , 3 ) ] forward_inputs = [ inputs [ 0 ] ] 3 , return_sequences=True ) ( inputs , initial_state=init_state ) backward_state = initial_state [ len ( initial_state ) // 2 : ] initial_state = inputs [ 1 : -self._num_constants ] if len ( initial_state ) == 0 : backward_inputs = [ inputs [ 0 ] ] initial_state = None y_rev = self.backward_layer.call ( inputs , initial_state=backward_state , * * kwargs ) parallel_model = multi_gpu_model ( model , 2 ) backward_state = inputs [ pivot : ] backward_state = inputs [ pivot : -self._num_constants ] pivot = len ( initial_state ) // 2 + 1 # add backward initial state else : y = self.forward_layer.call ( forward_inputs , def test_multi_gpu_with_multi_input_layers ( ) : # as they could be copied to multiple GPU . initial_state=forward_state , * * kwargs ) y = self.forward_layer.call ( inputs , initial_state=forward_state , * * kwargs ) initial_state=backward_state , * * kwargs ) inputs = keras.Input ( ( 4 , 3 ) ) y_rev = self.backward_layer.call ( backward_inputs , # add constants for forward and backward layers outputs = keras.layers.SimpleRNN ( forward_inputs += forward_state forward_state = initial_state [ : len ( initial_state ) // 2 ] parallel_model.compile ( loss='mean_squared_error ' , optimizer='adam ' ) forward_state = inputs [ 1 : pivot ] # get initial_state from full input spec model = keras.models.Model ( [ inputs , init_state ] , outputs ) initial_state = inputs [ 1 : ] # add forward initial state y = np.random.randn ( 2 , 4 , 3 ) parallel_model.train_on_batch ( x , y ) backward_inputs += backward_state init_state = keras.Input ( ( 3 , ) ) forward_inputs += inputs [ -self._num_constants : ] if self._num_constants is None :","['keras/layers/recurrent.py', 'keras/layers/wrappers.py', 'tests/keras/utils/multi_gpu_test.py']",[ MP ] RNN.call should get initial state from full input spec ( # 10845 )
367,e77a4cf3f85b45ad4275dfd661073ace1cc06e5a,2018-08-01 12:10:50-04:00,"on each sample per epoch which is not the case with generators . If you want to modify your dataset between epochs you may implement initializer=init_pool , The method ` __getitem__ ` should return a complete batch . If you want to modify your dataset between epochs you may implement ` on_epoch_end ` . self._manager = multiprocessing.Manager ( ) 'including element ' + str ( single_value ) + ' of type ' the case with generators . thread = multiprocessing.Process ( target=self._data_generator_task ) import multiprocessing self._stop_event = mp.Event ( ) raise ValueError ( 'The ` padding ` argument must be one of ` `` valid '' ` , ` `` same '' ` ( or ` `` causal '' ` for Conv1D ) . ' self._stop_event = multiprocessing.Event ( ) initargs= ( seqs , ) ) initargs= ( seqs , ) ) keras/utils/data_utils.py E501 \ for thread in self._threads ] ) # As a compromise , print the traceback and self.executor_fn = lambda seqs : multiprocessing.Pool ( workers , # pickle None instead . _SEQUENCE_COUNTER = mp.Value ( ' i ' , 0 ) all_finished = all ( [ not thread.is_alive ( ) ` Sequence ` are a safer way to do multiprocessing . This structure guarantees self.executor_fn = lambda seqs : mp.Pool ( workers , # As a compromise , print the traceback and pickle None instead . 'including element ' + str ( single_value ) + ' of ' `` `` '' import multiprocessing as mp thread = mp.Process ( target=self._data_generator_task ) ' ( or `` causal '' for Conv1D ) . Received : ' + str ( padding ) ) 'type ' + str ( type ( single_value ) ) ) all_finished = all ( [ not thread.is_alive ( ) for thread in self._threads ] ) ` on_epoch_end ` . The method ` __getitem__ ` should return a complete batch . _SEQUENCE_COUNTER = multiprocessing.Value ( ' i ' , 0 ) keras/utils/conv_utils.py E501 \ 'Received : ' + str ( padding ) ) raise ValueError ( 'The ` padding ` argument must be one of `` valid '' , `` same '' ' that the network will only train once on each sample per epoch which is not ' ' + str ( type ( single_value ) ) ) self._manager = mp.Manager ( ) ` Sequence ` are a safer way to do multiprocessing . This structure guarantees that the network will only train once initializer=init_pool , `` `` '' # noqa","['keras/utils/conv_utils.py', 'keras/utils/data_utils.py', 'pytest.ini']",Improve keras/utils by enabling PEP8 501 ( # 10817 )
368,4ba641806b4afcf95bece96c3b15e646fc092161,2018-07-31 08:26:24-07:00,"padding : one of `` same '' , `` valid '' , `` full '' . ` channels_last ` corresponds to inputs with shape one of ` channels_last ` ( default ) or ` channels_first ` . name : The name of the argument being validated , e.g . ` strides ` or If you never set it , then it will be `` channels_last '' . if data_format ='channels_first ' If you never set it , then it will be ` `` channels_last '' ` . `` kernel_size '' . This is only used to format error messages . while ` `` channels_first '' ` corresponds to padding : one of ` `` valid '' ` or ` `` same '' ` ( case-insensitive ) . value : The value to validate and convert . Could be an int , or any iterable one of ` `` channels_last '' ` ( default ) or ` `` channels_first '' ` . while ` channels_first ` corresponds to padding : one of ` `` same '' ` , ` `` valid '' ` , ` `` full '' ` . raise ValueError ( 'The ` padding ` argument must be one of ` `` valid '' ` , ` `` same '' ` ( or ` `` causal '' ` for Conv1D ) . ' if data_format='channels_first ' ` `` channels_last '' ` corresponds to inputs with shape value : The value to validate and convert . Could an int , or any iterable name : The name of the argument being validated , e.g . `` strides '' or ` kernel_size ` . This is only used to format error messages . raise ValueError ( 'The ` padding ` argument must be one of `` valid '' , `` same '' ( or `` causal '' for Conv1D ) . ' padding : one of ` 'valid ' ` or ` 'same ' ` ( case-insensitive ) .","['keras/layers/convolutional.py', 'keras/layers/convolutional_recurrent.py', 'keras/utils/conv_utils.py']",Fix doc ( # 10812 )
369,4db04d8701a64ee249dfe6ce09ebd89063bfb99c,2018-07-30 12:56:05-07:00,"with pytest.raises ( ValueError ) : texts = [ u'ali veli kırk dokuz elli ' , valid_iterator = generator.flow_from_directory ( tmp_folder , x = np.random.random ( ( height , width , 5 ) ) assert np.allclose ( image.apply_affine_transform ( x , theta=45 , channel_axis=2 , assert ( text_to_word_sequence ( text ) == normalizer=normalizer_dtype ) img = image.img_to_array ( x , data_format='channels ' ) # reg = grid_search.GridSearchCV ( regressor , parameters , assert text_to_word_sequence ( text ) == [ u'ali ' , u'veli ' , u'kırk ' , u'dokuz ' , u'elli ' ] It may get ordered , but not a lot , one thread can take the GIL before he was supposed to . model.add ( Conv2D ( filters=2 , kernel_size= ( 2 , 3 ) , input_shape= ( 3 , 5 , 5 ) , name='conv ' ) ) assert acc ! = list ( range ( 100 ) ) , ( 'Order was keep in GeneratorEnqueuer ' assert ' ` x ` ( images tensor ) and ` y ` ( labels ) ' in str ( e_info.value ) generator = image.ImageDataGenerator ( num_training ) : class_mode='input ' ) use_multiprocessing=True ) # Sometimes exec adds builtins to the context assert text_to_word_sequence ( text , split='stop ' ) == [ u'ali ' , u'veli ' , u'kırk ' , u'dokuz ' , u'elli ' ] tests/keras/preprocessing/image_test.py E501 \ subset='training ' ) 'with threads ' ) model.add ( Conv2D ( 2 , kernel_size= ( 2 , 3 ) , input_shape= ( 3 , 5 , 5 ) , name='conv ' ) ) clf.fit ( X_train , y_train , sample_weight=np.ones ( X_train.shape [ 0 ] ) , batch_size=batch_size , epochs=epochs ) sample_weight=np.arange ( images.shape [ 0 ] ) + 1 , [ u'ali ' , u'veli ' , u'kırk ' , u'dokuz ' , u'elli ' ] ) tests/keras/utils/generic_utils_test.py E501 \ from keras.preprocessing.text import text_to_word_sequence x = np.random.random ( ( height , width , 5 ) ) # neither RGB nor gray-scale valid_iterator = generator.flow_from_directory ( tmp_folder , subset='validation ' ) filename = os.path.join ( classpaths [ count % len ( classpaths ) ] , 'image- { } .jpg'.format ( count ) ) dir_iterator = generator.flow_from_directory ( str ( tmpdir ) , class_mode='input ' ) train_iterator = generator.flow_from_directory ( tmp_folder , layer_utils.convert_dense_weights_data_format ( for x , y , w in generator.flow ( images , np.arange ( images.shape [ 0 ] ) , subset='validation ' ) It may get ordered , but not a lot , one thread can take context.pop ( '__builtins__ ' , None ) # Sometimes exec adds builtins to the context # n_jobs=1 , cv=2 , verbose=2 ) assert str ( e_info.value ) .find ( 'All of the arrays in ' ) ! = -1 enqueuer = OrderedEnqueuer ( DummySequence ( [ 3 , 200 , 200 , 3 ] ) , use_multiprocessing=True ) with pytest.raises ( ValueError ) : # neither RGB nor gray-scale assert acc ! = list ( range ( 100 ) ) , `` Order was not keep in GeneratorEnqueuer with threads '' enqueuer2 = OrderedEnqueuer ( DummySequence ( [ 3 , 200 , 200 , 3 ] , value=15 ) , use_multiprocessing=True ) assert y_train.dtype == np.dtype ( ' i ' ) , ( model1.layers [ 2 ] , prev_shape , target_data_format ) assert acc == list ( range ( 100 ) ) , `` Order was not keep in GeneratorEnqueuer with processes '' clf.fit ( X_train , y_train , sample_weight=np.ones ( X_train.shape [ 0 ] ) , assert ( text_to_word_sequence ( text , split='stop ' ) == texts = [ u'ali veli kırk dokuz elli ' , u'ali veli kırk dokuz elli veli kırk dokuz ' ] # parameters = dict ( hidden_dims = [ 20 , 30 ] , batch_size= [ 64 , 128 ] , epochs= [ 2 ] , verbose= [ 0 ] ) normalizer=normalizer_rs ) dir_iterator = generator.flow_from_directory ( str ( tmpdir ) , with pytest.raises ( ValueError ) : # unknown data_format 'Evaluation value does not meet criteria : { } '.format ( out_eval ) ) transform_dict2 [ 'channel_shift_intensity ' ] ) generator.flow ( ( images , x_misc1 ) , np.arange ( dsize + 1 ) , batch_size=3 ) generator = image.ImageDataGenerator ( preprocessing_function=preprocessing_function ) # reg = grid_search.GridSearchCV ( regressor , parameters , scoring='mean_squared_error ' , n_jobs=1 , cv=2 , verbose=2 ) img = image.array_to_img ( x , data_format='channels ' ) # unknown data_format def test_directory_iterator_with_validation_split ( self , validation_split , # Instantiating HDF5Matrix for the training set , which is a slice of the first 150 elements 'Order was not keep in GeneratorEnqueuer with processes ' ) assert acc == list ( [ k * 5 for k in range ( 100 ) ] ) , `` Order was not keep in GeneratorEnqueuer with processes '' assert out_eval > 0 , 'Evaluation value does not meet criteria : { } '.format ( out_eval ) normalized_rs_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , batch_size=batch_size , epochs=epochs ) tests/keras/legacy/interface_test.py E501 enqueuer = OrderedEnqueuer ( DummySequence ( [ 3 , 200 , 200 , 3 ] ) , use_multiprocessing=False ) # scoring='mean_squared_error ' , u'ali veli kırk dokuz elli veli kırk dokuz ' ] assert str ( e_info.value ) .find ( ' ` x ` ( images tensor ) and ` y ` ( labels ) ' ) ! = -1 for x , y in generator.flow ( images , np.arange ( images.shape [ 0 ] ) , tests/keras/legacy/interface_test.py E501 \ the GIL before he was supposed to . # see : batch_size=3 ) # test that evalutation and prediction do n't crash and return reasonable results tests/keras/utils/vis_utils_test.py E501 \ # test that evalutation and prediction do n't crash and enqueuer2 = OrderedEnqueuer ( DummySequence ( [ 3 , 200 , 200 , 3 ] , value=15 ) , use_multiprocessing=True ) # Instantiating HDF5Matrix for the training set , tests/keras/utils/data_utils_test.py E501 \ enqueuer = OrderedEnqueuer ( DummySequence ( [ 3 , 200 , 200 , 3 ] ) , # https : //github.com/evhub/keras/blob/2.1.1/keras/utils/generic_utils.py # L166 num_samples = images.shape [ 0 ] 'image- { } .jpg'.format ( count ) ) context.pop ( '__builtins__ ' , None ) assert acc [ 100 : ] == list ( [ k * 5 for k in range ( 100 ) ] ) , ( assert acc ! = list ( range ( 100 ) ) , ( 'Order was not keep in GeneratorEnqueuer ' normalized_dtype_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , normalizer=normalizer_dtype ) tests/keras/utils/layer_utils_test.py E501 \ normalized_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , np.arange ( dsize + 1 ) , x = np.random.random ( ( height , width , 5 , 3 ) ) # neither RGB nor gray-scale img = image.img_to_array ( x , data_format='channels ' ) # unknown data_format train_iterator = generator.flow_from_directory ( tmp_folder , subset='training ' ) assert y_train.dtype == np.dtype ( ' i ' ) , 'HDF5Matrix dtype should match input array ' assert acc == list ( range ( 100 ) ) , `` Order was not keep in GeneratorEnqueuer with threads '' for x , y in generator.flow ( images , np.arange ( num_samples ) , fill_mode='constant ' ) , x_rotated ) img = image.array_to_img ( x , data_format='channels ' ) from keras.preprocessing.text import hashing_trick assert acc [ 100 : ] == list ( [ k * 5 for k in range ( 100 ) ] ) , `` Order was not keep in GeneratorEnqueuer with processes '' normalized_dtype_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , x , theta=45 , channel_axis=2 , fill_mode='constant ' ) , x_rotated ) assert acc ! = list ( range ( 100 ) ) , `` Order was keep in GeneratorEnqueuer with processes '' filename = os.path.join ( classpaths [ count % len ( classpaths ) ] , generator.flow ( ( images , x_misc1 ) , preprocessing_function=preprocessing_function ) from keras.preprocessing.text import Tokenizer , one_hot , hashing_trick , text_to_word_sequence # which is a slice of the first 150 elements # epochs= [ 2 ] , verbose= [ 0 ] ) 'HDF5Matrix dtype should match input array ' ) from keras.preprocessing.text import one_hot # return reasonable results x = np.random.random ( ( height , width , 5 , 3 ) ) normalizer=normalizer ) def test_directory_iterator_with_validation_split ( self , validation_split , num_training ) : sample_weight=np.arange ( num_samples ) + 1 , assert ( transform_dict [ 'channel_shift_intensity ' ] ! = # parameters = dict ( hidden_dims = [ 20 , 30 ] , batch_size= [ 64 , 128 ] , from keras.preprocessing.text import Tokenizer assert acc == list ( [ k * 5 for k in range ( 100 ) ] ) , ( # see https : //github.com/evhub/keras/blob/2.1.1/keras/utils/generic_utils.py # L166 normalized_rs_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , normalizer=normalizer_rs ) for x , y , w in generator.flow ( images , np.arange ( num_samples ) , use_multiprocessing=False ) tests/keras/preprocessing/text_test.py E501 \ assert acc == list ( range ( 100 ) ) , ( 'Order was not keep in GeneratorEnqueuer ' normalized_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , normalizer=normalizer ) assert out_eval > 0 , ( layer_utils.convert_dense_weights_data_format ( model1.layers [ 2 ] , prev_shape , target_data_format ) 'with processes ' ) assert 'All of the arrays in ' in str ( e_info.value ) assert transform_dict [ 'channel_shift_intensity ' ] ! = transform_dict2 [ 'channel_shift_intensity ' ] tests/keras/wrappers/scikit_learn_test.py E501 assert np.allclose ( image.apply_affine_transform ( tests/keras/utils/io_utils_test.py E501 \","['pytest.ini', 'tests/keras/preprocessing/image_test.py', 'tests/keras/preprocessing/text_test.py', 'tests/keras/utils/data_utils_test.py', 'tests/keras/utils/generic_utils_test.py', 'tests/keras/utils/io_utils_test.py', 'tests/keras/utils/layer_utils_test.py', 'tests/keras/utils/vis_utils_test.py', 'tests/keras/wrappers/scikit_learn_test.py']",Improve docstrings to enable PEP8 501 ( # 10797 )
370,93cd2400ef005cca2048e1c78a84f24cb429e0f5,2018-07-30 12:53:40-07:00,"def _old_batch_normalization ( x , mean , var , beta , gamma , def _old_normalize_batch_in_training ( x , gamma , beta , reduction_axes , reduction_axes , epsilon=1e-3 ) : epsilon=1e-3 ) : # pragma : no cover def _old_batch_normalization ( x , mean , var , beta , gamma , epsilon=1e-3 ) : epsilon=1e-3 ) : # pragma : no cover def _old_normalize_batch_in_training ( x , gamma , beta ,",['keras/backend/theano_backend.py'],Exclude Theano legacy functions when reporting coverages ( # 10801 )
371,20384818e1c95ce8b575c32a6800d3adf7f25a92,2018-07-30 12:53:10-07:00,"else : # pragma : no cover else : pragma : no cover def _padding ( x , pattern , axis ) : def _padding ( x , pattern , axis ) : # pragma : no cover","['.coveragerc', 'keras/backend/cntk_backend.py']",Exclude CNTK legacy codes when reporting coverages ( # 10802 )
372,ee02d256611b17d11e37b86bd4f618d7f2a37d84,2018-07-30 10:27:46-07:00,"inputs = layers.Input ( shape= ( 3 , 4 ) ) from keras.models import Sequential s.add ( layers.Dense ( 5 , input_shape= ( 4 , ) ) ) s = Sequential ( ) np.random.random ( ( 10 , 3 , 5 ) ) , epochs=1 , batch_size=6 ) model = Model ( inputs=inputs , outputs=x ) x = layers.Masking ( mask_value=0. , input_shape= ( 3 , 4 ) ) ( inputs ) model_input = np.random.randint ( low=1 , high=5 , size= ( 10 , 3 , 4 ) ) for i in range ( 4 ) : mask_outputs = [ model.layers [ 1 ] .compute_mask ( model.layers [ 1 ] .input ) ] assert np.array_equal ( mask_outputs_val [ 0 ] , np.any ( model_input , axis=-1 ) ) mask_outputs += [ model.layers [ 2 ] .compute_mask ( model.layers [ 2 ] .input , mask_outputs [ -1 ] ) ] model.fit ( model_input , x = layers.wrappers.TimeDistributed ( s ) ( x ) func = K.function ( [ model.input ] , mask_outputs ) model_input [ i , i : , : ] = 0 . s.add ( layers.Activation ( 'relu ' ) ) self.supports_masking = True model.compile ( optimizer='rmsprop ' , loss='mse ' ) assert np.array_equal ( mask_outputs_val [ 1 ] , np.any ( model_input , axis=-1 ) ) mask_outputs_val = func ( [ model_input ] ) def test_sequential_as_downstream_of_masking_layer ( ) :","['keras/engine/input_layer.py', 'tests/keras/layers/core_test.py']",Make InputLayer support masking ( # 10794 )
373,dd07a4c41efed22afb5780cd584555cbff141c74,2018-07-26 09:02:36-04:00,"outs = to_list ( outs ) if not isinstance ( outputs , list ) : from .. utils.generic_utils import to_list input_spec = to_list ( input_spec ) val_outs = [ val_outs ] if not isinstance ( val_outs , list ) : output_metrics = to_list ( output_metrics ) outputs = [ outputs ] batch_outs = [ batch_outs ] if not isinstance ( outputs , list ) : outs = to_list ( outs ) masks = to_list ( masks ) if not isinstance ( output_metrics , list ) : if not isinstance ( batch_outs , list ) : if not isinstance ( state_spec , list ) : val_outs = [ val_outs ] if not isinstance ( val_outs , list ) : input_spec = [ input_spec ] outputs = to_list ( outputs ) if not isinstance ( outs , list ) : outs = [ outs ] if not isinstance ( outs , list ) : outs = [ outs ] if not isinstance ( val_outs , list ) : state_spec = to_list ( state_spec ) val_outs = to_list ( val_outs ) outputs = to_list ( outputs ) state_spec = [ state_spec ] val_outs = to_list ( val_outs ) outputs = [ outputs ] output_metrics = [ output_metrics ] val_outs = to_list ( val_outs ) if not isinstance ( input_spec , list ) : masks = [ masks ] if not isinstance ( masks , list ) : val_outs = [ val_outs ] batch_outs = to_list ( batch_outs )","['keras/engine/training.py', 'keras/engine/training_arrays.py', 'keras/engine/training_generator.py', 'keras/engine/training_utils.py', 'keras/legacy/layers.py', 'keras/utils/multi_gpu_utils.py', 'keras/wrappers/scikit_learn.py']",Refactoring : Simplified some code by using the ` to_list ` function . ( # 10678 )
374,b2979c25d3d7d55a6579de0b2906c66e8fd2e172,2018-07-23 15:21:01-07:00,"model_from_json ( json_str ) .summary ( ) model.add ( Dense ( 3 ) ) model.compile ( loss='mse ' , optimizer='sgd ' , metrics= [ 'acc ' ] ) return obj.item ( ) from keras.initializers import Constant else : return obj.item ( ) if isinstance ( obj , np.ndarray ) : return obj.tolist ( ) model.add ( Dense ( 2 , input_shape= ( 3 , ) , kernel_initializer=Constant ( np.ones ( ( 3 , 2 ) ) ) ) ) model_from_yaml ( yaml_str ) .summary ( ) json_str = model.to_json ( ) model = Sequential ( ) def test_constant_initializer_with_numpy ( ) : yaml_str = model.to_yaml ( )","['keras/engine/network.py', 'tests/keras/engine/test_topology.py']",Network.to_json should handle numpy.ndarray correctly . ( # 10754 )
375,da9ce7d8f2e4212cebdacc80f6252b4ae1e68773,2018-07-23 13:38:03-07:00,"pass self.stateful = True self.state = K.variable ( value=0 , dtype='int32 ' ) from keras.layers import Input , Dense , Dropout , add , dot , Lambda def __init__ ( self , name='dummy_stateful_metric ' , * * kwargs ) : super ( DummyStatefulMetric , self ) .__init__ ( name=name , * * kwargs ) def reset_states ( self ) : from keras.layers import Input , Dense , Dropout , add , dot , Lambda , Layer averages.append ( np.float64 ( outs_per_batch [ -1 ] [ i ] ) ) class DummyStatefulMetric ( Layer ) : return self.state def __call__ ( self , y_true , y_pred ) : metrics= [ 'accuracy ' ] ) averages.append ( float ( outs_per_batch [ -1 ] [ i ] ) ) metrics= [ 'accuracy ' , DummyStatefulMetric ( ) ] )","['keras/engine/training_generator.py', 'tests/keras/test_callbacks.py']",Fix Stateful Metrics in fit_generator with TensorBoard ( # 10673 )
376,1c4bb1ed8286b723d99b419a6a5e63286353b149,2018-07-23 12:38:38-07:00,"return self._base_dtype normalized_rs_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , normalizer=normalizer_rs ) normalizer_dtype = lambda x : x.astype ( np.uint8 ) normalized_dtype_X_train = HDF5Matrix ( h5_path , 'my_data ' , start=0 , end=150 , normalizer=normalizer_dtype ) self._base_dtype = first_val.dtype normalizer_rs = lambda x : x [ : , : :2 ] if self.normalizer is not None : assert ( normalized_dtype_X_train.dtype == np.uint8 ) else : self._base_shape = first_val.shape [ 1 : ] return ( self.end - self.start , ) + self.data.shape [ 1 : ] return ( self.end - self.start , ) + self._base_shape # test resizing normalizer # test dtype changing normalizer assert ( normalized_rs_X_train.shape [ 1 ] == 5 ) first_val = self.normalizer ( self.data [ 0:1 ] ) return self.data.dtype first_val = self.data [ 0:1 ]","['keras/utils/io_utils.py', 'tests/keras/utils/io_utils_test.py']",Fixing HDF5Matrix ` .dtype ` and ` .shape ` properties ( # 10749 )
377,d4c1f69693ec476b33add0d540631021eea36a0c,2018-07-23 11:54:32-07:00,elif axis == 3 : elif axis == 3 or axis == -1 : if axis == 1 or axis == -3 : if axis == 1 :,['keras/backend/tensorflow_backend.py'],TF Batch Normalization should use FusedBatchNorm when axis = -1 ( # 10742 )
378,749413a5d5b94f4335fe5704a107d8b2dd84c7f1,2018-07-23 11:48:35-07:00,"` ( batch , channels , rows , cols ) ` ` ( batch , height , width , channels ) ` while ` `` channels_first '' ` ` ( samples , channels , rows , cols ) ` ` ( samples , new_rows , new_cols , filters ) ` 3D tensor with shape : ` ( batch , steps , channels ) ` ` ( batch , new_rows , new_cols , filters ) ` ` ( batch , rows , cols , channels ) ` ` ( batch , channels , height , width ) ` . ` ( samples , filters , new_rows , new_cols ) ` ` ( batch , new_conv_dim1 , new_conv_dim2 , new_conv_dim3 , filters ) ` ` ( batch , channels , steps ) ` . 3D tensor with shape : ` ( batch , new_steps , filters ) ` 3D tensor with shape : ` ( batch_size , new_steps , filters ) ` ` ( batch , filters , new_rows , new_cols ) ` ` ( batch , steps , channels ) ` while ` `` channels_first '' ` with shape ` ( batch , channels , steps ) ` . ` ( samples , new_conv_dim1 , new_conv_dim2 , new_conv_dim3 , filters ) ` ` ( batch , channels , conv_dim1 , conv_dim2 , conv_dim3 ) ` 3D tensor with shape : ` ( batch_size , steps , input_dim ) ` ` ( batch , steps , channels ) ` ` ( batch , filters , new_conv_dim1 , new_conv_dim2 , new_conv_dim3 ) ` ` [ batch , channels , rows , cols ] ` ` [ batch , new_rows , new_cols , filters ] ` ` ( samples , rows , cols , channels ) ` ` ( samples , conv_dim1 , conv_dim2 , conv_dim3 , channels ) ` ` ( samples , filters , new_conv_dim1 , new_conv_dim2 , new_conv_dim3 ) ` ` ( samples , channels , conv_dim1 , conv_dim2 , conv_dim3 ) ` ` [ batch , rows , cols , channels ] ` ` [ batch , filters , new_rows , new_cols ] ` ` ( batch , length , channels ) ` ` ( batch , conv_dim1 , conv_dim2 , conv_dim3 , channels ) ` with shape ` ( batch , channels , length ) ` .",['keras/layers/convolutional.py'],Fix # 10737 ( # 10745 )
379,aab55e649c34f8a24f00ee63922d049d3417c979,2018-07-22 17:36:35-04:00,"'functions ' : [ if not method : clean_module_name ( function.__module__ ) + ' . ' , `` , 1 ) signature = signature.replace ( # Prepend the module name . methods = read_page_data ( page_data , 'methods ' ) for method in methods : else : 'methods ' : [ assert type in [ 'classes ' , 'functions ' , 'methods ' ] signature = clean_module_name ( function.__module__ ) + ' . ' + signature assert type in [ 'classes ' , 'functions ' ] blocks.append ( render_function ( method , method=True ) )",['docs/autogen.py'],Fix signature of functions and methods in generated docs ( # 10743 )
380,b88bbbab2695dd0e24905417006d2240c6595481,2018-07-20 11:30:48-07:00,"model.add ( Dense ( 3 ) ) model.compile ( loss='mse ' , optimizer='sgd ' , metrics= [ 'acc ' ] ) save_model ( model , fname ) from keras.initializers import Constant model = load_model ( fname ) os.remove ( fname ) _ , fname = tempfile.mkstemp ( '.h5 ' ) return { 'type ' : type ( obj ) , `` `` '' Test saving and loading model of constant initializer with numpy ndarray as input . 'value ' : obj.tolist ( ) } `` `` '' model.add ( Dense ( 2 , input_shape= ( 3 , ) , kernel_initializer=Constant ( np.ones ( ( 3 , 2 ) ) ) ) ) def test_saving_constant_initializer_with_numpy ( ) : return obj.tolist ( ) model = Sequential ( )","['keras/engine/saving.py', 'tests/test_model_saving.py']",Numpy ndarray should be serialized as Python list . ( # 10727 )
381,75114feeac5ee6aa7679802ce7e5172c63565e2c,2018-07-19 10:32:06-07:00,"% ( len ( cntk_shape ) , dynamic_axis_num ) ) except ( Exception , KeyboardInterrupt ) as e : % ( len ( cntk_shape , dynamic_axis_num ) ) ) layer_id = str ( id ( layer ) ) except ( Exception , KeyboardInterrupt ) :","['keras/backend/cntk_backend.py', 'keras/utils/data_utils.py', 'keras/utils/vis_utils.py']",Removed some unused variables and fixed a formatting error . ( # 10688 )
382,d40656e5799dfd22e96b8bab217638b2934a6894,2018-07-19 08:48:43-07:00,"> > > K.normalize_data_format ( None ) return data_format from .common import floatx , epsilon , image_data_format `` `` '' Checks that the value correspond to a valid data format . raise ValueError ( 'Unknown data_format ' + str ( data_format ) ) `` ` python raise ValueError ( 'The ` data_format ` argument must be one of ' if data_format not in { 'channels_first ' , 'channels_last ' } : if value is None : str ( value ) ) def normalize_data_format ( value ) : K.normalize_data_format ( 'channels_middle ' ) from .common import normalize_data_format raise ValueError ( 'Unknown data_format : ' , data_format ) from .common import floatx , epsilon from .common import image_data_format data_format = value.lower ( ) 'channels_last ' # Arguments value : String or None . ` 'channels_first ' ` or ` 'channels_last ' ` . data_format = value.lower ( ) if data_format not in { 'channels_last ' , 'channels_first ' } : self.data_format = data_format A string , either ` 'channels_first ' ` or ` 'channels_last ' ` 'channels_first ' ValueError : if ` value ` or the global ` data_format ` invalid . if data_format not in { 'channels_first ' , 'channels_last ' } : from .common import image_data_format raise ValueError ( 'Unknown data_format ' + data_format ) > > > from keras import backend as K `` `` '' raise ValueError ( 'Unknown data_format : ' + str ( data_format ) ) self.data_format = K.normalize_data_format ( data_format ) value = K.image_data_format ( ) if data_format is None : from .common import epsilon # Example str ( value ) ) raise ValueError ( ' ` data_format ` must be in ' def normalize_data_format ( value ) : raise ValueError ( 'The ` data_format ` argument must be one of ' value = image_data_format ( ) from .common import floatx # Raises self.data_format = conv_utils.normalize_data_format ( data_format ) ' '' channels_first '' , `` channels_last '' . Received : ' data_format = K.image_data_format ( ) conv_utils.normalize_data_format ( 'channels_middle ' ) data_format = normalize_data_format ( data_format ) ' '' channels_first '' , `` channels_last '' . Received : ' ' { ` `` channels_last '' ` , ` `` channels_first '' ` } ' ) > > > K.normalize_data_format ( 'channels_last ' ) 'normalize_data_format ' , `` ` raise ValueError ( 'Unknown data_format ' , data_format ) # Returns from keras import backend as K data_format = image_data_format ( ) return data_format if value is None : if data_format is None : from .common import floatx , epsilon , image_dim_ordering , image_data_format","['docs/autogen.py', 'keras/backend/__init__.py', 'keras/backend/cntk_backend.py', 'keras/backend/common.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/layers/convolutional.py', 'keras/layers/convolutional_recurrent.py', 'keras/layers/core.py', 'keras/layers/local.py', 'keras/layers/pooling.py', 'keras/legacy/layers.py', 'keras/utils/conv_utils.py', 'tests/keras/utils/conv_utils_test.py']",Refactoring : Used the function ` normalize_data_format ` to remove some code . ( # 10690 )
383,d2803c0fb7d0ba9361dcba8eb9bcebbf2f774958,2018-07-18 16:08:38-07:00,"examples/babi_rnn.py E501 \ 'single_supporting_fact_10k ' : 'tasks_1-20_v1-2/en-10k/qa1_ ' return pad_sequences ( xs , maxlen=story_maxlen ) , pad_sequences ( xqs , maxlen=query_maxlen ) , np.array ( ys ) validation_split=0.0 ) # fraction of images reserved for validation ( strictly between 0 and 1 ) ' '' # noqa 'single-supporting-fact_ { } .txt ' , # image data format , either `` channels_first '' or `` channels_last '' fill_mode='nearest ' , # set mode for filling points outside the input boundaries 'two-supporting-facts_ { } .txt ' , ' '' data = [ ( flatten ( story ) , q , answer ) for story , q , answer in data if not max_length or len ( flatten ( story ) ) < max_length ] path = get_file ( 'babi-tasks-v1-2.tar.gz ' , data_format=None , height_shift_range=0.1 , # randomly shift images vertically ( fraction of total height ) height_shift_range=0.1 , examples/babi_memnn.py E501 \ path = get_file ( 'babi-tasks-v1-2.tar.gz ' , origin='https : //s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz ' ) examples/addition_rnn.py E501 \ width_shift_range=0.1 , # set function that will be applied on each input '.tar.gz\n ' pad_sequences ( xqs , maxlen=query_maxlen ) , np.array ( ys ) ) # fraction of images reserved for validation ( strictly between 0 and 1 ) data_format=None , # image data format , either `` channels_first '' or `` channels_last '' 'two_supporting_facts_10k ' : 'tasks_1-20_v1-2/en-10k/qa2_ ' rescale=None , # set rescaling factor ( applied before any other transformation ) if not max_length or len ( flatten ( story ) ) < max_length ] rescale=None , preprocessing_function=None , from keras.layers import Input , Activation , Dense , Permute , Dropout , add , dot , concatenate ' $ wget http : //www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\n ' data = [ ( flatten ( story ) , q , answer ) for story , q , answer in data preprocessing_function=None , # set function that will be applied on each input examples/cifar10_cnn_capsule.py E501 \ return ( pad_sequences ( xs , maxlen=story_maxlen ) , 'single_supporting_fact_10k ' : 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_ { } .txt ' , 'two_supporting_facts_10k ' : 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_ { } .txt ' , from keras.layers import add , dot , concatenate from keras.layers import Input , Activation , Dense , Permute , Dropout # set mode for filling points outside the input boundaries fill_mode='nearest ' , # randomly shift images vertically ( fraction of total height ) width_shift_range=0.1 , # randomly shift images horizontally ( fraction of total width ) 'babi_tasks_1-20_v1-2.tar.gz ' ) # set rescaling factor ( applied before any other transformation ) ' $ wget http : //www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2 ' examples/cifar10_cnn.py E501 \ # randomly shift images horizontally ( fraction of total width ) validation_split=0.0 ) origin='https : //s3.amazonaws.com/text-datasets/ '","['examples/addition_rnn.py', 'examples/babi_memnn.py', 'examples/babi_rnn.py', 'examples/cifar10_cnn.py', 'examples/cifar10_cnn_capsule.py', 'pytest.ini']",PEP8 fixed line length examples/ ( # 10724 )
384,d18c564548104c862892a1f73423e333f11f7ce2,2018-07-18 10:59:14-07:00,"return averages if len ( masks ) == 1 : output_shape = layer.compute_output_shape ( unpack_singleton ( input_shapes ) ) return outs output_shape = layer.compute_output_shape ( input_shapes [ 0 ] ) return unpack_singleton ( averages ) return unpack_singleton ( values ) return values [ 0 ] return shapes [ 0 ] outputs = self.call ( unpack_singleton ( self.inputs ) ) return unpack_singleton ( outputs ) shapes = to_list ( layer.compute_output_shape ( input_shapes ) ) if isinstance ( output_shapes , list ) : return input_shapes return averages [ 0 ] return output_shapes [ 0 ] if len ( outs ) == 1 : outputs = self.call ( self.inputs [ 0 ] ) if len ( outs ) == 1 : layer ( input_tensors [ 0 ] , * * kwargs ) output_masks = output_masks [ 0 ] self._output_mask_cache [ cache_key ] = output_masks def unpack_singleton ( x ) : output_tensors = output_tensors [ 0 ] mask = masks [ 0 ] if len ( input_shapes ) == 1 : if len ( outputs ) == 1 : return unpack_singleton ( shapes ) shapes = to_list ( layer.compute_output_shape ( computed_tensors [ 0 ] ._keras_shape ) ) return masks [ 0 ] self._output_shape_cache [ cache_key ] = output_shapes output_shapes = unpack_singleton ( output_shapes ) Otherwise return the iterable . if len ( shapes ) == 1 : if len ( outputs ) == 1 : return shapes self.build ( unpack_singleton ( input_shapes ) ) if len ( specs ) == 1 : return masks self._output_tensor_cache [ cache_key ] = output_tensors self._output_mask_cache [ cache_key ] = output_masks if len ( input_tensors ) == 1 : self._output_tensor_cache [ cache_key ] = output_tensors output = output_ls_copy return unpack_singleton ( output_shapes ) return unpack_singleton ( output_shapes ) if len ( output_shapes ) == 1 : return input_shapes [ 0 ] else : if len ( masks ) == 1 : return unpack_singleton ( input_shapes ) output_shape = layer.compute_output_shape ( input_shapes ) else : else : output = unpack_singleton ( output_ls_copy ) output_masks = unpack_singleton ( output_masks ) `` `` '' Gets the first element if the iterable has only one value . mask = masks self._output_shape_cache [ cache_key ] = output_shapes if len ( x ) == 1 : uses_learning_phase = any ( [ x._uses_learning_phase for x in computed_tensors ] ) return output_shapes outputs = self.call ( unpack_singleton ( self.inputs ) , training=training ) if len ( input_shapes ) == 1 : return unpack_singleton ( outputs ) return x [ 0 ] if isinstance ( output_shapes , list ) and len ( output_shapes ) == 1 : if len ( averages ) == 1 : uses_learning_phase = computed_tensors [ 0 ] ._uses_learning_phase if len ( output_masks ) == 1 : return unpack_singleton ( outs ) `` `` '' output_shapes = output_shapes [ 0 ] if len ( output_ls_copy ) == 1 : if len ( output_tensors ) == 1 : layer ( unpack_singleton ( input_tensors ) , * * kwargs ) return outputs [ 0 ] self.build ( input_shapes ) layer ( input_tensors , * * kwargs ) if self._expects_training_arg : return outputs [ 0 ] return outs if self._expects_training_arg : shapes = to_list ( layer.compute_output_shape ( [ x._keras_shape for x in computed_tensors ] ) ) output = output_ls_copy [ 0 ] outputs = self.call ( self.inputs [ 0 ] , training=training ) # Returns : else : return outs [ 0 ] output_tensors = unpack_singleton ( output_tensors ) if len ( self.inputs ) == 1 : return values uses_learning_phase = any ( [ x._uses_learning_phase for x in computed_tensors ] ) return specs [ 0 ] if len ( computed_tensors ) == 1 : return x if len ( values ) == 1 : return unpack_singleton ( specs ) mask = unpack_singleton ( masks ) return specs input_shapes = unpack_singleton ( [ x._keras_shape for x in computed_tensors ] ) outputs = self.call ( self.inputs , training=training ) outputs = self.call ( self.inputs ) return unpack_singleton ( masks ) from .. utils.generic_utils import unpack_singleton x : A list or tuple . # Argument : return outputs self.build ( input_shapes [ 0 ] ) else : The same iterable or the first element . return unpack_singleton ( outs ) if len ( input_shapes ) == 1 : return outs [ 0 ]","['keras/engine/base_layer.py', 'keras/engine/input_layer.py', 'keras/engine/network.py', 'keras/engine/training.py', 'keras/engine/training_arrays.py', 'keras/engine/training_generator.py', 'keras/utils/generic_utils.py']",Refactoring : Simplified some if-else by creating a function similar to ` to_list ` . ( # 10679 )
385,066aa6ab7e5aaea495f2f0e1783737ba4567bd66,2018-07-18 10:36:14-07:00,"# so it may have extra axes with 1 , it is not needed and should be removed if ndim ( var ) > 1 : elif ndim ( gamma ) > 1 : elif ndim ( beta ) > 1 : beta = tf.squeeze ( beta ) if ndim ( mean ) > 1 : var = tf.squeeze ( var ) # The mean / var / beta / gamma may be processed by broadcast gamma = tf.squeeze ( gamma ) mean = tf.squeeze ( mean )",['keras/backend/tensorflow_backend.py'],Batch Normalization should squeeze mean/var/beta/gama tensors when calling tf.nn.fused_batch_norm ( # 10684 )
386,583f92a6f9332574a4a2abee5ae454e906389377,2018-07-18 11:07:38-04:00,examples/conv_lstm.py E501 \ keras/regularizers.py E501 \ keras/utils/np_utils.py E501 \ examples/pretrained_word_embeddings.py E501 \ keras/utils/multi_gpu_utils.py E501 \ examples/mnist_transfer_cnn.py E501 \ examples/reuters_mlp.py E501 \ keras/utils/generic_utils.py E501 \ keras/applications/inception_resnet_v2.py E501 \ keras/utils/io_utils.py E501 \ keras/engine/topology.py E501 \ keras/engine/__init__.py E501 \ keras/objectives.py E501 \ tests/keras/utils/np_utils_test.py E501 \ keras/applications/mobilenet.py E501 \ keras/applications/inception_v3.py E501 \ keras/applications/mobilenetv2.py E501 \ examples/cifar10_resnet.py E501 \ keras/utils/test_utils.py E501 \ keras/activations.py E501 \ keras/engine/input_layer.py E501 \ tests/test_multiprocessing.py E501 \ examples/mnist_irnn.py E501 \ keras/utils/__init__.py E501 \ keras/__init__.py E501 \ keras/applications/__init__.py E501 \ examples/mnist_denoising_autoencoder.py E501 \ keras/applications/vgg19.py E501 \ tests/keras/regularizers_test.py E501 \ examples/variational_autoencoder.py E501 \ tests/integration_tests/test_image_data_tasks.py E501 \ tests/keras/constraints_test.py E501 \ examples/imdb_cnn.py E501 \ keras/datasets/mnist.py E501 \ keras/engine/sequential.py E501 \ keras/losses.py E501 \ keras/datasets/cifar10.py E501 \ keras/preprocessing/__init__.py E501 \ tests/test_dynamic_trainability.py E501 \ examples/imdb_bidirectional_lstm.py E501 \ examples/mnist_swwae.py E501 \ tests/keras/layers/merge_test.py E501 \ tests/keras/losses_test.py E501 \ tests/keras/layers/recurrent_test.py E501 \ tests/keras/layers/local_test.py E501 \ keras/layers/__init__.py E501 \ tests/test_loss_masking.py E501 \ keras/applications/imagenet_utils.py E501 \ examples/tensorboard_embeddings_mnist.py E501 \ keras/utils/vis_utils.py E501 \ examples/mnist_acgan.py E501 \ keras/wrappers/scikit_learn.py E501 \ tests/keras/utils/conv_utils_test.py E501 \ tests/keras/layers/advanced_activations_test.py E501 \ tests/integration_tests/imagenet_utils_test.py E501 \ keras/layers/pooling.py E501 \ keras/wrappers/__init__.py E501 \ examples/mnist_sklearn_wrapper.py E501 \ keras/preprocessing/text.py E501 \ keras/applications/nasnet.py E501 \ examples/mnist_mlp.py E501 \ keras/datasets/cifar100.py E501 \ examples/lstm_seq2seq.py E501 \ tests/test_doc_auto_generation.py E501 \ examples/lstm_stateful.py E501 \ tests/integration_tests/test_datasets.py E501 \ examples/variational_autoencoder_deconv.py E501 \ keras/preprocessing/image.py E501 \ tests/keras/legacy/layers_test.py E501 \ keras/preprocessing/sequence.py E501 \ examples/antirectifier.py E501 \ keras/datasets/cifar.py E501 \ keras/initializers.py E501 \ keras/datasets/fashion_mnist.py E501 \ keras/utils/layer_utils.py E501 \ examples/cifar10_cnn_tfaugment2d.py E501 \ keras/datasets/__init__.py E501 \ tests/keras/utils/multi_gpu_test.py E501 \ keras/applications/xception.py E501 \ tests/integration_tests/applications_test.py E501 \ keras/engine/base_layer.py E501 \ keras/applications/vgg16.py E501 \ tests/keras/layers/noise_test.py E501 \ keras/applications/resnet50.py E501 \ keras/legacy/__init__.py E501 \ tests/keras/activations_test.py E501 \ examples/mnist_cnn.py E501 \ keras/backend/__init__.py E501 \ examples/mnist_dataset_api.py E501 \ examples/lstm_seq2seq_restore.py E501 \ keras/engine/training_utils.py E501 \ tests/integration_tests/test_vector_data_tasks.py E501 \ examples/reuters_mlp_relu_vs_selu.py E501 \ tests/keras/preprocessing/sequence_test.py E501 \ examples/imdb_cnn_lstm.py E501 \ keras/applications/densenet.py E501 \ keras/engine/training_arrays.py E501 \,['pytest.ini'],Removed the exceptions concerning files which were already complient with the 85 chars rule . ( # 10714 )
387,f86bc57d621fb0325e928271b246d197611b0e65,2018-07-17 11:24:37-04:00,"clean_module_name ( function.__module__ ) + ' . ' , `` , 1 ) clean_module_name=clean_module_name ( cls.__module__ ) , ending_point - 1 ) ] signature = signature.replace ( clean_module_name ( function.__module__ ) + ' . ' , `` ) block = docstring [ starting_point : ( None if ending_point == -1 else ) clean_module_name ( function.__module__ ) + ' . ' , `` ) signature = signature.replace ( docs/__init__.py E501 \ ' tag . ' ) docs/autogen.py E501 \ ' but missing { { autogenerated } } tag . ' ) class_signature = clean_module_name ( cls.__module__ ) + ' . ' + cls.__name__ + ' ( ) ' block = docstring [ starting_point : None if ending_point == -1 else ending_point - 1 ] ' but missing { { autogenerated } } ' signature = signature.replace ( clean_module_name ( function.__module__ ) + ' . ' , `` , 1 ) class_signature = `` { clean_module_name } . { cls_name } ( ) '' .format ( cls_name=cls.__name__","['docs/autogen.py', 'pytest.ini']",PEP8 styled files in docs/ ( # 10713 )
388,2526c85ae17d83cdcd826ceeceb7643ae5cbcb2b,2018-07-16 19:54:44+02:00,tests/keras/layers/noise_test.py E501 \ tests/keras/layers/core_test.py E501 \ examples/variational_autoencoder_deconv.py E501 \ keras/utils/data_utils.py E501 \ examples/variational_autoencoder.py E501 \ tests/keras/preprocessing/text_test.py E501 \ tests/keras/utils/conv_utils_test.py E501 \ keras/callbacks.py E501 \ tests/keras/test_sequential_model.py E501 \ keras/backend/theano_backend.py E501 \ tests/integration_tests/imagenet_utils_test.py E501 \ keras/layers/convolutional_recurrent.py E501 \ keras/datasets/reuters.py E501 \ examples/conv_lstm.py E501 \ keras/preprocessing/sequence.py E501 \ keras/preprocessing/text.py E501 \ keras/datasets/boston_housing.py E501 \ keras/applications/vgg19.py E501 \ examples/reuters_mlp_relu_vs_selu.py E501 \ keras/engine/training_generator.py E501 \ keras/applications/inception_v3.py E501 \ keras/utils/io_utils.py E501 \ keras/models.py E501 \ keras/datasets/imdb.py E501 \ keras/layers/__init__.py E501 \ tests/test_loss_masking.py E501 \ tests/keras/utils/generic_utils_test.py E501 \ keras/datasets/cifar100.py E501 \ examples/addition_rnn.py E501 \ examples/mnist_transfer_cnn.py E501 \ keras/applications/densenet.py E501 \ keras/applications/__init__.py E501 \ keras/engine/__init__.py E501 \ keras/layers/noise.py E501 \ tests/keras/engine/test_topology.py E501 \ tests/keras/layers/advanced_activations_test.py E501 \ examples/mnist_acgan.py E501 \ tests/integration_tests/test_image_data_tasks.py E501 \ keras/regularizers.py E501 \ keras/engine/network.py E501 \ tests/keras/initializers_test.py E501 \ keras/wrappers/__init__.py E501 \ keras/legacy/interfaces.py E501 \ tests/keras/engine/test_training.py E501 \ tests/keras/utils/layer_utils_test.py E501 \ examples/mnist_irnn.py E501 \ examples/mnist_cnn.py E501 \ keras/utils/np_utils.py E501 \ keras/datasets/cifar10.py E501 \ keras/engine/training_arrays.py E501 \ tests/keras/metrics_test.py E501 \ keras/datasets/__init__.py E501 \ tests/keras/optimizers_test.py E501 \ tests/keras/wrappers/scikit_learn_test.py E501 tests/test_doc_auto_generation.py E501 \ tests/keras/layers/normalization_test.py E501 \ examples/mnist_mlp.py E501 \ keras/applications/xception.py E501 \ examples/neural_doodle.py E501 \ tests/test_multiprocessing.py E501 \ examples/pretrained_word_embeddings.py E501 \ tests/keras/activations_test.py E501 \ tests/integration_tests/test_datasets.py E501 \ examples/imdb_lstm.py E501 \ tests/keras/layers/convolutional_recurrent_test.py E501 \ keras/__init__.py E501 \ tests/keras/layers/recurrent_test.py E501 \ examples/imdb_cnn_lstm.py E501 \ examples/mnist_denoising_autoencoder.py E501 \ keras/activations.py E501 \ tests/keras/preprocessing/image_test.py E501 \ tests/keras/utils/np_utils_test.py E501 \ pep8maxlinelength = 85 examples/lstm_stateful.py E501 \ examples/cifar10_cnn_tfaugment2d.py E501 \ examples/mnist_siamese.py E501 \ tests/keras/utils/data_utils_test.py E501 \ keras/layers/wrappers.py E501 \ keras/legacy/__init__.py E501 \ examples/cifar10_cnn.py E501 \ tests/keras/utils/multi_gpu_test.py E501 \ examples/mnist_tfrecord.py E501 \ keras/applications/vgg16.py E501 \ setup.py E501 \ keras/engine/input_layer.py E501 \ tests/keras/test_callbacks.py E501 \ keras/layers/cudnn_recurrent.py E501 \ tests/keras/regularizers_test.py E501 \ pep8ignore= * E402 \ examples/cifar10_resnet.py E501 \ keras/datasets/mnist.py E501 \ tests/keras/backend/backend_test.py E501 \ examples/babi_rnn.py E501 \ keras/applications/mobilenet.py E501 \ examples/neural_style_transfer.py E501 \ keras/applications/imagenet_utils.py E501 \ tests/integration_tests/applications_test.py E501 \ tests/integration_tests/test_vector_data_tasks.py E501 \ keras/layers/pooling.py E501 \ keras/datasets/fashion_mnist.py E501 \ keras/applications/resnet50.py E501 \ examples/antirectifier.py E501 \ examples/babi_memnn.py E501 \ examples/lstm_seq2seq_restore.py E501 \ keras/backend/cntk_backend.py E501 \ examples/deep_dream.py E501 \ tests/test_model_saving.py E501 \ examples/lstm_seq2seq.py E501 \ keras/losses.py E501 \ examples/mnist_dataset_api.py E501 \ keras/layers/merge.py E501 \ keras/layers/recurrent.py E501 \ tests/keras/constraints_test.py E501 \ tests/keras/legacy/layers_test.py E501 \ keras/applications/nasnet.py E501 \ keras/backend/common.py E501 \ keras/datasets/cifar.py E501 \ keras/engine/training.py E501 \ keras/layers/convolutional.py E501 \ keras/utils/test_utils.py E501 \ keras/engine/saving.py E501 \ keras/utils/conv_utils.py E501 \ docs/__init__.py E501 \ keras/utils/multi_gpu_utils.py E501 \ keras/layers/core.py E501 \ tests/integration_tests/test_temporal_data_tasks.py E501 \ tests/keras/layers/convolutional_test.py E501 \ * E402 \ examples/imdb_fasttext.py E501 \ tests/keras/backend/reference_operations.py E501 \ keras/layers/normalization.py E501 \ tests/keras/layers/wrappers_test.py E501 \ tests/test_documentation.py E501 \ keras/constraints.py E501 \ tests/keras/utils/io_utils_test.py E501 \ examples/imdb_cnn.py E501 \ keras/metrics.py E501 \ examples/conv_filter_visualization.py E501 \ pep8ignore= * E501 \ tests/keras/preprocessing/sequence_test.py E501 \ examples/tensorboard_embeddings_mnist.py E501 \ examples/mnist_net2net.py E501 \ keras/applications/mobilenetv2.py E501 \ keras/objectives.py E501 \ tests/keras/legacy/interface_test.py E501 \ keras/layers/embeddings.py E501 \ keras/preprocessing/image.py E501 \ tests/keras/layers/cudnn_recurrent_test.py E501 \ tests/keras/layers/local_test.py E501 \ keras/engine/training_utils.py E501 \ keras/layers/advanced_activations.py E501 \ keras/backend/tensorflow_backend.py E501 \ tests/keras/layers/embeddings_test.py E501 \ keras/utils/generic_utils.py E501 \ examples/mnist_sklearn_wrapper.py E501 \ keras/applications/inception_resnet_v2.py E501 \ examples/cifar10_cnn_capsule.py E501 \ keras/layers/local.py E501 \ examples/imdb_bidirectional_lstm.py E501 \ keras/wrappers/scikit_learn.py E501 \ docs/autogen.py E501 \ keras/legacy/layers.py E501 \ keras/utils/layer_utils.py E501 \ keras/engine/base_layer.py E501 \ examples/mnist_hierarchical_rnn.py E501 \ keras/engine/sequential.py E501 \ tests/test_loss_weighting.py E501 \ tests/keras/layers/merge_test.py E501 \ examples/reuters_mlp.py E501 \ keras/initializers.py E501 \ tests/keras/utils/vis_utils_test.py E501 \ # Enable line length testing with maximum line length of 85 keras/utils/vis_utils.py E501 \ examples/image_ocr.py E501 \ tests/keras/losses_test.py E501 \ examples/lstm_text_generation.py E501 \ keras/optimizers.py E501 \ keras/engine/topology.py E501 \ keras/utils/__init__.py E501 \ keras/backend/__init__.py E501 \ keras/preprocessing/__init__.py E501 \ examples/mnist_swwae.py E501 \ tests/test_dynamic_trainability.py E501 \,['pytest.ini'],Enable line length checking on a per file basis . ( # 10698 )
389,1fcc791ee1c94c2a3efa5072aa2f10f1aab51a2a,2018-07-16 18:29:24+02:00,"input_shape= ( 3 , 32 , 32 ) , padding='same ' , ) ) padding='same ' , model.add ( Conv2D ( 64 , ( 3 , 3 ) , model.add ( Conv2D ( 64 , 3 , 3 , input_shape= ( 3 , 32 , 32 ) ) )",['keras/layers/core.py'],( docs ) Update ` Conv2D ` call to the Keras 2 API ( # 10692 )
390,5103fd8134ccf2ef6ed114d525d8b07d148716d3,2018-07-14 20:18:11+02:00,"model.train_on_batch ( input_data , actual_output ) # check shape inference model.train_on_batch ( input_data , actual_output ) actual_output = model.predict ( input_data ) if model.weights : recovered_model.set_weights ( weights ) actual_output_shape = actual_output.shape # test serialization , weight setting at model level # test training mode ( e.g . useful for dropout tests ) _output = recovered_model.predict ( input_data ) assert_allclose ( actual_output , expected_output , rtol=1e-3 ) # test serialization , weight setting at model level assert expected_dim == actual_dim expected_output_shape = layer.compute_output_shape ( input_shape ) actual_output = model.predict ( input_data ) expected_output_shape = layer.compute_output_shape ( input_shape ) # check with the sequential API if expected_output is not None : actual_output_shape = actual_output.shape def _layer_in_model_test ( model ) : assert expected_dim == actual_dim if model.weights : weights = model.get_weights ( ) _output = recovered_model.predict ( input_data ) if expected_dim is not None : assert_allclose ( actual_output , expected_output , rtol=1e-3 ) for expected_dim , actual_dim in zip ( expected_output_shape , model_config = model.get_config ( ) model_config = model.get_config ( ) assert_allclose ( _output , actual_output , rtol=1e-3 ) recovered_model = model.__class__.from_config ( model_config ) if expected_dim is not None : if expected_output is not None : recovered_model.set_weights ( weights ) actual_output_shape ) : for expected_dim , actual_dim in zip ( expected_output_shape , recovered_model = Sequential.from_config ( model_config ) actual_output = _layer_in_model_test ( model ) weights = model.get_weights ( ) # check with the functional API return actual_output # test training mode ( e.g . useful for dropout tests ) assert_allclose ( _output , actual_output , rtol=1e-3 ) recovered_model = Model.from_config ( model_config ) model.compile ( 'rmsprop ' , 'mse ' ) model.compile ( 'rmsprop ' , 'mse ' ) _layer_in_model_test ( model ) actual_output_shape ) :",['keras/utils/test_utils.py'],Refactoring the ` layer_test ` function . ( # 10660 )
391,8fe86302ff1cce23c019fdaf24fc6448c3b30090,2018-07-12 16:53:12-04:00,"else : signature = signature.replace ( function.__module__ + ' . ' , `` ) signature = signature.replace ( clean_module_name ( function.__module__ ) + ' . ' , `` , 1 ) signature = signature.replace ( clean_module_name ( function.__module__ ) + ' . ' , `` ) if method :",['docs/autogen.py'],Fix redundant functions ' module names in auto-gen docs ( # 10664 )
392,69c30a150f0b2caee7961ca1c0080960ef5ad6f6,2018-07-12 17:18:26+02:00,"new_model = keras.models.Sequential.from_config ( config ) assert len ( inner_model.layers ) == 2 assert new_model.built is True assert model.built is True assert len ( model.weights ) == 0 assert len ( new_model.weights ) == 6 model.add ( inner_model ) inner_model.add ( keras.layers.Dense ( 3 ) ) new_inner_model = new_model.layers [ 0 ] model = keras.models.Sequential ( ) model.compile ( 'sgd ' , 'mse ' ) assert len ( inner_model.weights ) == 0 assert len ( new_inner_model.weights ) == 4 assert inner_model.built is False assert len ( new_inner_model.layers ) == 2 config = model.get_config ( ) model.add ( keras.layers.Dense ( 5 ) ) dtype = first_layer.dtype assert model.built is False inner_model = keras.models.Sequential ( ) def test_nested_sequential_deferred_build ( ) : assert new_inner_model.built is True assert len ( inner_model.weights ) == 4 model.train_on_batch ( batch_shape = first_layer.batch_input_shape assert inner_model.built is True np.random.random ( ( 2 , 4 ) ) , np.random.random ( ( 2 , 5 ) ) ) assert len ( new_model.layers ) == 2 assert len ( model.layers ) == 2 assert len ( model.weights ) == 6","['keras/engine/sequential.py', 'tests/keras/test_sequential_model.py']",Fix nested sequential deferred build ( # 10655 )
393,3dcd9c767ce6875fc8b69c74971ac8a552e23131,2018-07-12 10:58:03+02:00,self.input_spec = InputSpec ( ndim=5 ) self.input_spec = InputSpec ( ndim=4 ) self.input_spec = InputSpec ( ndim=3 ),['keras/layers/convolutional.py'],Remove duplicate calls to InputSpec ( # 10654 )
394,da5a6ac579dfc80cbb220bf092cd151eca7ffb2d,2018-07-12 10:56:20+02:00,"`` the output shape automatically . '' ) reason= '' Only Tensorflow raises a `` layer_test ( layers.Lambda , def test_no_grad ( ) : output_shape=lambda x : [ x [ 0 ] , 1 ] ) ( x ) reason= '' theano can not compute `` ( or auto-inferred when using TensorFlow or CNTK ) . if K.backend ( ) in ( 'tensorflow ' , 'cntk ' ) : ( or auto-inferred when using TensorFlow ) . kwargs= { 'function ' : lambda x : K.mean ( x , axis=-1 ) } , mod.compile ( 'sgd ' , 'mse ' ) input_shape= ( 3 , 2 , 4 ) ) mod.compile ( optimizer , 'mse ' ) def test_lambda_output_shape ( ) : if K.backend ( ) == 'tensorflow ' : _test_no_grad ( sgd ) # With TensorFlow or CNTK , we can infer the output shape directly : `` ValueError if the gradient is null . '' ) x = Lambda ( lambda l : 1.0 * K.reshape ( K.cast ( K.argmax ( l ) , 'float32 ' ) , [ -1 , 1 ] ) , # With TensorFlow , we can infer the output shape directly : def _test_no_grad ( optimizer ) : x = Lambda ( lambda l : 1.0 * K.reshape ( K.cast ( K.argmax ( l ) , 'float32 ' ) , [ -1 , 1 ] ) ) ( x )","['keras/layers/core.py', 'tests/keras/layers/core_test.py', 'tests/keras/optimizers_test.py']",Enable automatic shape inference when using Lambda layers and CNTK . Also fixed the test_no_grad that was catching ValueErrors that had nothing to do with not having a gradient . ( # 10647 )
395,e2531cb8302b2842c0d3b505f6f619e685c3f4b4,2018-07-10 10:06:30-04:00,[ cifar10_cnn_capsule.py ] ( cifar10_cnn_capsule.py ) Trains a denoising autoencoder on the MNIST dataset . MNIST dataset with TensorFlow 's Dataset API . [ lstm_seq2seq_restore.py ] ( lstm_seq2seq_restore.py ) Restores a character-level sequence to sequence model from disk ( saved by [ lstm_seq2seq.py ] ( lstm_seq2seq.py ) ) and uses it to generate predictions . Trains a simple convnet on the MNIST dataset and embeds test data which can be later visualized using TensorBoard 's Embedding Projector . Transfer learning toy example . [ cifar10_cnn_tfaugment2d.py ] ( cifar10_cnn_tfaugment2d.py ) Transfer learning toy example on the MNIST dataset . Trains a basic character-level sequence-to-sequence model . Trains a simple deep CNN on the CIFAR10 small images dataset using Tensorflow internal augmentation APIs . MNIST dataset with TensorFlow 's Dataset API . [ lstm_seq2seq.py ] ( lstm_seq2seq.py ) [ mnist_denoising_autoencoder.py ] ( mnist_denoising_autoencoder.py ) Trains a simple CNN-Capsule Network on the CIFAR10 small images dataset . [ tensorboard_embeddings_mnist.py ] ( tensorboard_embeddings_mnist.py ),['examples/README.md'],Add missing examples to examples/README.md ( # 10637 )
396,83cd9b78a241a04d3762ba7c71c1a0c47f9aba58,2018-07-07 17:33:25+03:00,"keras.regularizers.l1_l2 ( 0 . ) see the [ keras/regularizers.py ] ( https : //github.com/keras-team/keras/blob/master/keras/regularizers.py ) module for examples . keras.regularizers.l1_l2 ( l1=0.01 , l2=0.01 ) see the [ keras/regularizers.py ] ( https : //github.com/keras-team/keras/blob/master/keras/regularizers.py ) module for examples .",['docs/templates/regularizers.md'],Update regularizers.md ( # 10549 )
397,6eb0c7bdbb68d7de0ef507102c757c7ccbf582a0,2018-07-07 10:25:04+03:00,"assert max_val - 0.015 < np.max ( rand ) < = max_val seed = np.random.randint ( 10e3 ) np.random.seed ( seed ) return C.random.bernoulli ( shape=shape , dtype=dtype , mean=p , seed=seed ) assert rand.shape == ( 300 , 200 ) assert np.abs ( np.std ( rand ) - std ) < 0.015 assert np.min ( samples ) == 0 assert np.abs ( np.mean ( samples ) - mean ) < std * 0.015 assert rand.shape == ( 300 , 200 ) assert np.abs ( np.mean ( samples ) ) < 0.015 size * = _ return C.random.normal ( shape=shape , mean=mean , scale=stddev , seed=seed , dtype=dtype ) return random_normal_variable ( shape=shape , mean=mean , scale=1.0 , seed=seed ) for mean , std in [ ( 0. , 1 . ) , ( -10. , 5 . ) ] : # test that random_normal also generates different values when used within a function rand = k.eval ( k.random_normal ( ( 300 , 200 ) , mean=mean , stddev=std , seed=1337 ) ) rand = k.eval ( k.random_normal ( ( 300 , 200 ) , mean=mean , stddev=std , seed=1337 ) ) assert min_val + 0.015 > np.min ( rand ) > = min_val return random_uniform_variable ( shape , minval , maxval , dtype , seed ) assert np.abs ( np.mean ( rand ) - mean ) < 0.015 assert min_val + 0.015 > np.min ( samples ) > = min_val # how to apply mean and stddev std = 1 . binomial = np.random.binomial ( 1 , p , size ) .astype ( dtype ) .reshape ( shape ) p = C.parameter ( size = 1 # ensure that randomness is conditioned by the Numpy RNG return variable ( value=binomial , dtype=dtype ) return C.random.uniform ( shape=shape , dtype=dtype , low=minval , high=maxval , seed=seed ) return C.parameter ( assert np.max ( rand ) < = max_val assert np.abs ( np.mean ( rand ) - mean ) < std * 0.015 r = k.random_binomial ( ( 1 , ) , p ) mean = 0 . r = k.random_normal ( ( 1 , ) , mean=mean , stddev=std , seed=1337 ) assert np.max ( samples ) == 1 assert np.abs ( np.mean ( samples ) - p ) < 0.015 assert np.abs ( np.std ( rand ) - std ) < std * 0.015 assert max_val - 0.015 < np.max ( samples ) < = max_val samples = [ k.eval ( r ) for _ in range ( 60000 ) ] assert np.abs ( np.std ( samples ) - std ) < std * 0.015 return variable ( value=p.value + mean ) samples = [ k.eval ( r ) for _ in range ( 20000 ) ] # test standard normal as well as a normal with a different set of parameters if seed is None : r = k.random_uniform ( ( 1 , ) , minval=min_val , maxval=max_val ) # use numpy workaround now assert np.min ( rand ) > = min_val","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",Fix most of the issues with the CNTK backend 's random functions mentioned in # 10594 ( # 10595 )
398,a9a68bc9deac5d72170ac48c15c41b0defc9753e,2018-07-06 10:36:44+03:00,"for cbk in callbacks : elif len ( validation_data ) == 3 : val_data += [ 0 . ] val_data += [ 0 . ] ' ` ( val_x , val_y , val_sample_weight ) ` ' val_sample_weight = None val_enqueuer_gen = iter ( val_data ) val_x , val_y , val_sample_weight ) workers=workers , 'or ` ( val_x , val_y ) ` . Found : ' val_x , val_y = validation_data val_x , val_y , val_sample_weight = validation_data str ( validation_data ) ) # Create an Enqueuer that can be reused ' ` ( val_x , val_y , val_sample_weight ) ` ' str ( validation_data ) ) val_data = val_x + val_y + val_sample_weights cbk.validation_data = val_data use_multiprocessing=use_multiprocessing , # Prepare data for validation if isinstance ( val_data , Sequence ) : raise ValueError ( ' ` validation_data ` should be a tuple ' cbk.validation_data = val_data elif val_gen : val_enqueuer = OrderedEnqueuer ( val_data , val_x , val_y , val_sample_weights = model._standardize_user_data ( if len ( validation_data ) == 2 : if model.uses_learning_phase and not isinstance ( K.learning_phase ( ) , val_x , val_y , val_sample_weights = model._standardize_user_data ( else : val_enqueuer_gen , use_multiprocessing=use_multiprocessing ) max_queue_size=max_queue_size ) validation_data , val_x , val_y , val_sample_weight = validation_data if do_validation : # Prepare data for validation val_x , val_y = validation_data val_sample_weight = None for cbk in callbacks : val_enqueuer_gen = val_data use_multiprocessing=use_multiprocessing ) val_enqueuer_gen = val_enqueuer.get ( ) int ) : max_queue_size=max_queue_size ) val_x , val_y , val_sample_weight ) val_data = validation_data val_data = val_x + val_y + val_sample_weights val_enqueuer = GeneratorEnqueuer ( val_data , val_enqueuer.start ( workers=workers , if len ( validation_data ) == 2 : validation_steps = len ( val_data ) 'or ` ( val_x , val_y ) ` . Found : ' if model.uses_learning_phase and not isinstance ( K.learning_phase ( ) , raise ValueError ( ' ` validation_data ` should be a tuple ' if do_validation and not val_gen : elif len ( validation_data ) == 3 : workers=0 ) int ) : if val_gen and workers > 0 :",['keras/engine/training_generator.py'],Reuse validation enqueuer ( # 10476 )
399,8a8a19ba1eb74feb5da22399a44e81b734c2b2d8,2018-07-06 10:36:07+03:00,"' ` keras.layers.Input ` ' 'must come from ` keras.layers.Input ` . ' ' ` tf.layers.Input ` ' 'must come from ` tf.layers.Input ` . ' ' ` tensor = keras.layers.Input ( shape ) ` .\n ' ` layer.get_input_shape_for ( input_shape ) ` , or ' ` tensor = tf.layers.Input ( shape ) ` .\n '","['keras/engine/base_layer.py', 'keras/engine/network.py']",docstring fixes ( # 10603 )
400,b6bb0be10ad84d456bb1cea535a4a922d7f2e2c6,2018-07-04 17:10:25+01:00,# # # Related Issues [ ] This PR changes the current API [ y/n ] ( all API changes need to be approved by fchollet ) [ ] This PR requires new unit tests [ y/n ] ( make sure tests are included ) # # # PR Overview # # # Summary [ ] This PR requires to update the documentation [ y/n ] ( make sure the docs are up-to-date ) [ ] This PR is backwards compatible [ y/n ],['PULL_REQUEST_TEMPLATE.md'],Add pr template ( # 10590 )
401,468f080c98f06780c950e5a78c9eeeaf9fff002e,2018-07-04 13:21:51+09:00,"`` ` To load a MobileNetV2 model via ` load_model ` , import the custom object ` relu6 ` and pass it to the ` custom_objects ` parameter . relu6 = mobilenet_v2.relu6 model = load_model ( 'mobilenet_v2.h5 ' , custom_objects= { `` ` python 'relu6 ' : mobilenet.relu6 } ) To load a MobileNet model via ` load_model ` , import the custom object ` relu6 ` and pass it to the ` custom_objects ` parameter . model = load_model ( 'mobilenet.h5 ' , custom_objects= { relu6 = mobilenet.relu6 'relu6 ' : mobilenetv2.relu6 } ) E.g .","['docs/templates/applications.md', 'keras/applications/mobilenet.py', 'keras/applications/mobilenetv2.py']",Replace relu6 with ReLU layers ( # 10597 )
402,5a7a789ee9766b6a594bd4be8b9edb34e71d6500,2018-07-01 19:08:38+01:00,"The loss function or None if ` identifier ` is None . 'keras.wrappers ' , 'keras.utils ' , g : Tensor , the gradient tensor `` `` '' Get the ` identifier ` activation function . # Arguments c : float > = 0 . Gradients will be clipped 'keras.preprocessing.text ' ] ValueError if unknown identifier identifier : None or str , name of the function . modules = [ 'keras.layers ' , 'keras.models ' , 'keras ' , `` `` '' 'keras.callbacks ' , 'keras.activations ' , Tensor , the gradient clipped if required . The activation function , ` linear ` if ` identifier ` is None . `` `` '' Get the ` identifier ` loss function . `` `` '' Clip the gradient ` g ` if the L2 norm ` n ` exceeds ` c ` . 'keras.backend.tensorflow_backend ' , 'keras.engine ' , # Raises n : Tensor , actual norm of ` g ` . # Returns ValueError if unknown identifier . when their L2 norm exceeds this value . modules = [ 'keras.layers ' , 'keras.models ' , 'keras ' , 'keras.backend.tensorflow_backend ' , 'keras.preprocessing.image ' , 'keras.losses ' , 'keras.models ' , 'keras.optimizers ' ]","['keras/activations.py', 'keras/losses.py', 'keras/optimizers.py', 'tests/test_documentation.py']",Minor improvement documentation ( # 10552 )
403,d1b40b169f3d70afe4db539d140d6ed374dab21e,2018-06-30 10:05:42-07:00,"model.load_weights ( fname , by_name=True , skip_mismatch=True ) with pytest.raises ( ValueError , # delete and recreate model with ` use_bias=False ` with pytest.warns ( UserWarning , model.add ( Conv2D ( 10 , ( 1 , 1 ) , input_shape= ( 1 , 1 , 1 ) , name='rick ' ) ) # delete and recreate model with ` filters=10 ` model.load_weights ( fname , by_name=True ) model.add ( Conv2D ( 2 , ( 1 , 1 ) , input_shape= ( 1 , 1 , 1 ) , use_bias=False , name='rick ' ) ) match=r ' . * load . * [ 0-9 ] + layers into . * [ 0-9 ] + layers . ' ) : os.remove ( fname ) model.add ( Dense ( 3 , name='morty ' ) ) match=r ' . * has shape . * but the saved . * shape . * ' ) : match=r ' . * expects [ 0-9 ] + . * but the saved . * [ 0-9 ] + . * ' ) : match=r'Skipping loading . * due to mismatch . * ' ) : model.load_weights ( fname ) os.remove ( fname ) model.add ( Flatten ( ) ) model = Sequential ( ) del ( model )",['tests/test_model_saving.py'],Add saving tests ( # 10523 )
404,df03bb5b1cc9fd297b0f19e08d916a4faedea267,2018-06-20 10:54:47-07:00,"return filtered_ancestors module_member = getattr ( module , name ) if member in dir ( ancestor ) : # Take care of index page . for name in dir ( module ) : if cls not in module_classes : functions += module_functions if not result : for cls in classes : filtered_ancestors = [ ] subblocks.append ( ' # ' * level + ' ' + function.__name__ + '\n ' ) def read_page_data ( page_data , type ) : module_data.append ( instance ) return cls if name [ 0 ] == ' _ ' or name in EXCLUDE : classes = read_page_data ( page_data , 'classes ' ) module_functions.sort ( key=lambda x : id ( x ) ) def get_classes_ancestors ( classes ) : module_functions.append ( function ) if module.__name__ in function.__module__ : module_member = getattr ( module , name ) continue functions = read_page_data ( page_data , 'functions ' ) classes = page_data.get ( 'classes ' , [ ] ) continue if ( inspect.isclass ( module_member ) and type == 'classes ' or continue classes += module_classes function = module_member result = ancestor instance = module_member if function not in module_functions : parts = re.split ( r'\. ( ? ! \d ) ' , signature ) for module in page_data.get ( 'all_module_ { } '.format ( type ) , [ ] ) : if instance not in module_data : module_classes = [ ] for ancestor in ancestors : module_classes.append ( cls ) inspect.isfunction ( module_member ) and type == 'functions ' ) : result = None functions = page_data.get ( 'functions ' , [ ] ) return data if inspect.isclass ( module_member ) : module_functions = [ ] ancestors = get_classes_ancestors ( [ cls ] ) for name in dir ( module ) : if module.__name__ in instance.__module__ : ws = re.search ( r'\S ' , s ) if cls.__module__ == module.__name__ : data = page_data.get ( type , [ ] ) def get_earliest_class_that_defined_member ( member , cls ) : if ancestor.__name__ in [ 'object ' ] : filtered_ancestors.append ( ancestor ) if name [ 0 ] == ' _ ' or name in EXCLUDE : data += module_data module_classes.sort ( key=lambda x : id ( x ) ) if filtered_ancestors : cls = module_member for module in page_data.get ( 'all_module_classes ' , [ ] ) : level = 3 subblocks.append ( ' # # # ' + function.__name__ + '\n ' ) else : ancestors = [ ] parts = re.split ( '\. ( ? ! \d ) ' , signature ) ws = re.search ( '\S ' , s ) assert type in [ 'classes ' , 'functions ' ] ancestors += cls.__bases__ module_data = [ ] module_data.sort ( key=lambda x : id ( x ) ) if inspect.isfunction ( module_member ) : return result for module in page_data.get ( 'all_module_functions ' , [ ] ) : return filtered_ancestors + get_classes_ancestors ( filtered_ancestors )",['docs/autogen.py'],autogen helper for classes and functions ( remove code duplication ) ( # 10462 )
405,ebdc1c8759f65768212b7e7113b5cae82e9df3d4,2018-06-19 15:47:06-07:00,"output = f ( [ b'test ' ] ) dtype=tensor.dtype.base_dtype.name ) ) x_placeholder = KTF.placeholder ( shape= ( ) , dtype= '' string '' ) # Test functions with string inputs . dtype=tf.as_dtype ( tensor.dtype ) .as_numpy_dtype ) ) dtype=tf.as_dtype ( key.dtype ) .as_numpy_dtype ) ) assert output == [ b'test ' ] def test_function_tf_string_input ( self ) : x_identity = KTF.identity ( x_placeholder ) f = KTF.function ( inputs= [ x_placeholder ] , outputs= [ x_identity ] ) dtype=key.dtype.base_dtype.name ) )","['keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']","Support functions with string inputs in 2.2.0 , fixes # 10459 ( # 10460 )"
406,a2d11d4724d3cf4a0b18a7fe8448723d92e1c716,2018-06-18 18:43:24-07:00,"new_states : List of tensors , same length and shapes or vice versa . However , there 's no conversion required between TF and CNTK . # TODO : support it in subclassd networks after inputs are set . and correspding target sequences from another domain new_states : Tist of tensors , same length and shapes # Used only in conjonction with graph-networks and corresponding target sequences from another domain # TODO : support it in subclassed networks after inputs are set . # Used only in conjunction with graph-networks or vice verca . However , there 's no conversion required between TF and CNTK .","['examples/lstm_seq2seq.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/engine/network.py', 'keras/engine/saving.py']",Fix several typos ( # 10468 )
407,e6cc185af4dd0b5f56fd4a3f30c0933bbca6e284,2018-06-18 11:35:55-07:00,"input_dim : int > 0 . Size of the vocabulary , ( without it , the shape of the dense outputs can not be computed ) . This is useful when using [ recurrent layers ] ( recurrent.md ) used in the vocabulary ( input_dim should equal size of mask_zero : Whether or not the input value 0 is a special `` padding '' vocabulary + 1 ) . value that should be masked out . mask_zero : Whether or not the input value 0 is a special `` padding '' If mask_zero is set to True , as a consequence , index 0 can not be If mask_zero is set to True , as a consequence , index 0 can not be ( see [ regularizer ] ( .. /regularizers.md ) ) . in the model need to support masking or an exception will be raised . in the model need to support masking or an exception will be raised . vocabulary + 1 ) . which may take variable length input . ` Flatten ` then ` Dense ` layers upstream If this is ` True ` then all subsequent layers ( without it , the shape of the dense outputs can not be computed ) . input_dim : int > 0 . Size of the vocabulary , i.e . maximum integer index + 1 . ( see [ initializers ] ( .. /initializers.md ) ) . embeddings_constraint : Constraint function applied to This is useful when using [ recurrent layers ] ( recurrent.md ) This argument is required if you are going to connect embeddings_constraint : Constraint function applied to embeddings_initializer : Initializer for the ` embeddings ` matrix ( see [ constraints ] ( .. /constraints.md ) ) . value that should be masked out . output_dim : int > = 0 . Dimension of the dense embedding . output_dim : int > = 0 . Dimension of the dense embedding . the ` embeddings ` matrix ` Flatten ` then ` Dense ` layers upstream which may take variable length input . ( see [ regularizer ] ( .. /regularizers.md ) ) . used in the vocabulary ( input_dim should equal size of the ` embeddings ` matrix i.e . maximum integer index + 1 . embeddings_regularizer : Regularizer function applied to embeddings_regularizer : Regularizer function applied to ( see [ initializers ] ( .. /initializers.md ) ) . input_length : Length of input sequences , when it is constant . This argument is required if you are going to connect input_length : Length of input sequences , when it is constant . ( see [ constraints ] ( .. /constraints.md ) ) . If this is ` True ` then all subsequent layers embeddings_initializer : Initializer for the ` embeddings ` matrix",['keras/layers/embeddings.py'],Fix docstring of layer.Embedding to properly render arguments ( # 10461 )
408,ffa99d69f7bfa42d060c26032793dc650002419b,2018-06-15 13:52:14-07:00,"if self.use_bias : if self.bias : 'use_bias ' : True , use_bias=True ,","['keras/layers/convolutional.py', 'tests/keras/layers/convolutional_test.py']",Fix use_bias in some convolutional layers ( # 10444 )
409,2d183db0372e5ac2a686608cb9da0a9bd4319764,2018-06-14 11:19:45-07:00,return ( x - mean ) / ( C.sqrt ( var ) + epsilon ) * gamma + beta ` output = ( x - mean ) / sqrt ( var + epsilon ) * gamma + beta ` ` output = ( x - mean ) / ( sqrt ( var ) + epsilon ) * gamma + beta ` return ( x - mean ) / C.sqrt ( var + epsilon ) * gamma + beta,"['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py']",corrected batch norm implementation in cntk ( # 10427 )
410,52f608cbe7cf685bd9f92fcedc3ef02ba4292dc6,2018-06-13 13:44:22-07:00,"model4.inputs = None assert preds4.shape == ( 1 , 19 ) preds4 = model4.predict ( [ np.random.random ( ( 1 , 32 ) ) , model3._set_inputs ( model.input ) model3.inputs = None aux_model = Dense ( 3 ) ( aux_input ) model4 = Model ( inputs=model.inputs + [ aux_input ] , from keras.layers import Activation , Dense , Dropout , Conv2D , Concatenate aux_input = Input ( shape= ( 5 , ) , name='aux_input ' ) from keras.layers import Activation , Dense , Dropout , Conv2D model3._set_inputs ( model.inputs ) assert preds3.shape == ( 1 , 16 ) with pytest.raises ( ValueError ) : outputs=Concatenate ( ) ( model.outputs + [ aux_model ] ) ) model4._set_inputs ( model.inputs + [ aux_input ] ) np.random.random ( ( 1 , 5 ) ) ] ) preds3 = model3.predict ( np.random.random ( ( 1 , 32 ) ) )",['tests/keras/engine/test_training.py'],Add tests for inputs set dynamically ( # 10416 )
411,a68c516e731faba8efa884603bb06d17ad494d69,2018-06-13 11:43:43-07:00,"'CNTK only supports float32 and ' elif dtype == np.float16 : pip install https : //cntk.ai/PythonWheel/CPU-Only/cntk-2.5.1-cp27-cp27mu-linux_x86_64.whl ; with pytest.raises ( ValueError ) : elif dtype == 'float16 ' : assert K.dtype ( K.variable ( 1 , dtype='float16 ' ) ) == 'float16 ' if K.backend ( ) == 'cntk ' : return 'float16 ' 'CNTK only supports float32 , float64 , and ' pip install https : //cntk.ai/PythonWheel/CPU-Only/cntk-2.3.1-cp27-cp27mu-linux_x86_64.whl ; else : return np.float16 # cntk only support float32 and float64 K.variable ( 1 , dtype='float16 ' ) 'float16 . ' % dtype ) assert K.dtype ( K.variable ( 1 , dtype='float16 ' ) ) == 'float16 ' pip install https : //cntk.ai/PythonWheel/CPU-Only/cntk-2.3.1-cp36-cp36m-linux_x86_64.whl ; pip install https : //cntk.ai/PythonWheel/CPU-Only/cntk-2.5.1-cp36-cp36m-linux_x86_64.whl ; 'float64 . ' % dtype )","['.travis.yml', 'keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",Update a CNTK version on Travis ( # 10419 )
412,4f90f95fa5fb087445acb40c110770ebb3817b28,2018-06-13 11:39:47-07:00,"installation instructions here : http : //docs.h5py.org/en/latest/build.html Note that the methods ` predict ` , ` fit ` , ` train_on_batch ` , ` predict_classes ` , etc . will * all * update the states of the stateful layers in a model . This allows you to do not only stateful training , but also stateful prediction . Notes that the methods ` predict ` , ` fit ` , ` train_on_batch ` , ` predict_classes ` , etc . will * all * update the states of the stateful layers in a model . This allows you to do not only stateful training , but also stateful prediction . installation instructions here : http : //docs.h5py.org/en/latest/build.html",['docs/templates/getting-started/faq.md'],Update faq.md ( # 10422 )
413,8e5b8533ea819d0252d08f45648782ecd543ac70,2018-06-13 11:39:12-07:00,"node_index = node.node_indices [ i ] layer , node_index , _ = tensor._keras_history for x in previous_sources : return node.input_tensors layer , if x not in source_tensors : if not hasattr ( tensor , '_keras_history ' ) : if not hasattr ( tensor , '_keras_history ' ) : return source_tensors node_index ) x = node.input_tensors [ i ] return [ tensor ] if not layer._inbound_nodes : if not node.inbound_layers : def get_source_inputs ( tensor , layer=None , node_index=None ) : source_tensors = [ ] layer : Origin layer of the tensor . Will be layer = node.inbound_layers [ i ] List of input tensors . if layer is None or node_index : layer = node.inbound_layers [ i ] determined via tensor._keras_history if not provided . `` `` '' Returns the list of input tensors necessary to compute ` tensor ` . determined via tensor._keras_history if not provided . node_index : Origin node index of the tensor . previous_sources = get_source_inputs ( x , for x in previous_sources : tensor : The tensor to start from . node_index = node.node_indices [ i ] layer , node_index , _ = tensor._keras_history if layer is None or node_index : return node.input_tensors else : else : else : node_index : Origin node index of the tensor . return source_tensors return [ tensor ] node_index ) for i in range ( len ( node.inbound_layers ) ) : source_tensors = [ ] from .. utils.layer_utils import get_source_inputs # Returns # Reached an Input layer , stop recursion . `` `` '' ( potentially with 1 element ) . # Arguments `` `` '' ( potentially with 1 element ) . if not node.inbound_layers : def get_source_inputs ( tensor , layer=None , node_index=None ) : # Reached an Input layer , stop recursion . # Returns if not layer._inbound_nodes : # Avoid input redundancy . source_tensors.append ( x ) return tensor List of input tensors . if x not in source_tensors : from .layer_utils import get_source_inputs previous_sources = get_source_inputs ( x , tensor : The tensor to start from . # Arguments source_tensors.append ( x ) `` `` '' Returns the list of input tensors necessary to compute ` tensor ` . else : x = node.input_tensors [ i ] node = layer._inbound_nodes [ node_index ] # Avoid input redundancy . Output will always be a list of tensors layer , return tensor node = layer._inbound_nodes [ node_index ] for i in range ( len ( node.inbound_layers ) ) : layer : Origin layer of the tensor . Will be Output will always be a list of tensors","['keras/engine/network.py', 'keras/utils/__init__.py', 'keras/utils/layer_utils.py']",Move ` get_source_inputs ` ( # 10415 )
414,a40f33556caef1ab0a3f75758f194728c804dcd3,2018-06-13 11:36:50-07:00,"updated during training . The more updates a parameter receives , lr : float > = 0 . Learning rate . rho : float > = 0 . that adapts learning rates based on a moving window of gradient updates , gradient to keep at each time step . original version of Adadelta you do n't have to set an initial learning which are adapted relative to how frequently a parameter gets rho : float > = 0 . Adadelta decay factor , corresponding to fraction of lr : float > = 0 . Initial learning rate . be set , as in most other Keras optimizers . instead of accumulating all past gradients . This way , Adadelta continues Adagrad is an optimizer with parameter-specific learning rates , decay : float > = 0 . Initial learning rate decay . learning even when many updates have been done . Compared to Adagrad , in the rate . In this version , initial learning rate and decay factor can Adadelta is a more robust extension of Adagrad the smaller the updates . decay : float > = 0 . Learning rate decay over each update . lr : float > = 0 . Initial learning rate , defaults to 1 .",['keras/optimizers.py'],"[ docs ] more details for adagrad/delta , clarifying usage of rho ( # 10410 )"
415,29a22a8d59b5e2c4282f1e7f664d82595049eb9d,2018-06-12 10:57:38-07:00,"# value used for fill_mode = `` constant '' preprocessing_function=None , # set function that will be applied on each input fill_mode='nearest ' , # set mode for filling points outside the input boundaries # image data format , either `` channels_first '' or `` channels_last '' vertical_flip=False ) # randomly flip images # epsilon for ZCA whitening data_format=None , channel_shift_range=0. , shear_range=0. , # set function that will be applied on each input vertical_flip=False ) # set range for random shear vertical_flip=False , # randomly flip images # fraction of images reserved for validation ( strictly between 0 and 1 ) zca_epsilon=1e-06 , # epsilon for ZCA whitening zoom_range=0. , # set range for random zoom channel_shift_range=0. , # set range for random channel shifts zoom_range=0. , rescale=None , preprocessing_function=None , rescale=None , # set rescaling factor ( applied before any other transformation ) # set range for random zoom vertical_flip=False , data_format=None , # image data format , either `` channels_first '' or `` channels_last '' validation_split=0.0 ) # fraction of images reserved for validation ( strictly between 0 and 1 ) # set mode for filling points outside the input boundaries zca_epsilon=1e-06 , fill_mode='nearest ' , cval=0. , # value used for fill_mode = `` constant '' # set range for random channel shifts cval=0. , # set rescaling factor ( applied before any other transformation ) validation_split=0.0 ) shear_range=0. , # set range for random shear","['examples/cifar10_cnn.py', 'examples/cifar10_cnn_capsule.py', 'examples/cifar10_resnet.py']",Add missing named arguments in ImageDataGenerator in examples ( # 10389 )
416,67e242d926b577f440217d9d10b890f54bb875c3,2018-06-07 11:54:23-07:00,"layers.SpatialDropout2D , layers.SpatialDropout1D , layers.SpatialDropout3D , layers.GlobalAveragePooling3D , layers.GlobalMaxPooling3D ,",['docs/autogen.py'],Add spatial dropout and 3D global pooling to docs ( # 10373 )
417,bbf4283457e38859d6dff5a2f0990f8347799e6c,2018-06-06 11:52:26-07:00,"model = _make_nested_model ( input_shape , layer ) cudnn_rnn_layer_class = CuDNNGRU weights = convert_nested_time_distributed ( weights ) def convert_model ( source_model , target_model ) : _ , fname = tempfile.mkstemp ( '.h5 ' ) num_samples = 32 units = 2 # Convert layers nested in Bidirectional/Model/Sequential . weights : List of weights values ( Numpy arrays ) . _convert_model_weights ( model , cudnn_model ) os.remove ( fname ) _convert_model_weights ( cudnn_model , model ) source_model.save_weights ( fname ) steps = 6 inputs = np.random.random ( ( num_samples , ) + input_shape ) 'bias_initializer ' : 'random_uniform ' , layer = TimeDistributed ( layer ) def _convert_model_weights ( source_model , target_model ) : input_shape = ( timesteps , steps , input_size ) 'recurrent_activation ' : 'sigmoid ' , # Convert layers nested in Bidirectional/TimeDistributed/Model/Sequential . assert_allclose ( model.predict ( inputs ) , cudnn_model.predict ( inputs ) , atol=1e-4 ) return preprocess_weights_for_loading ( layer.layer , weights , original_keras_version , original_backend ) `` `` '' rank of input due to usage of TimeDistributed . Issue : # 10356 . # Arguments def convert_nested_time_distributed ( weights ) : Similar test as test_load_weights_between_noncudnn_rnn ( ) but has different target_model.load_weights ( fname ) # ensure biases are non-zero and properly converted layer = rnn_layer_class ( units , * * rnn_layer_kwargs ) cudnn_rnn_layer_class = CuDNNLSTM `` `` '' timesteps = 6 rnn_layer_kwargs [ 'reset_after ' ] = True _ , fname = tempfile.mkstemp ( '.h5 ' ) cudnn_model = _make_nested_model ( input_shape , cudnn_layer ) def test_load_weights_between_noncudnn_rnn_time_distributed ( rnn_type , to_cudnn ) : # Returns source_model.save_weights ( fname ) A list of weights values ( Numpy arrays ) . input_size = 10 cudnn_layer = TimeDistributed ( cudnn_layer ) convert_model ( model , cudnn_model ) cudnn_layer = cudnn_rnn_layer_class ( units ) rnn_layer_class = GRU convert_model ( cudnn_model , model ) if layer.__class__.__name__ == 'TimeDistributed ' : if rnn_type == 'LSTM ' : else : os.remove ( fname ) `` `` '' Converts layers nested in ` TimeDistributed ` wrapper by ` preprocess_weights_for_loading ( ) ` . target_model.load_weights ( fname ) } rnn_layer_kwargs = { if to_cudnn : rnn_layer_class = LSTM","['keras/engine/saving.py', 'tests/test_model_saving.py']",CuDNN RNN layers nested in TimeDistributed are not converted when loading ( # 10357 )
418,13548e8b212b504c9f506495b01855de6d9f6117,2018-06-06 11:51:37-07:00,"model3.inputs = None model2.add ( Dense ( 8 ) ) assert preds2.shape == ( 1 , 8 ) preds2 = model2.predict ( [ np.random.random ( ( 1 , 32 ) ) ] ) from keras.layers import Activation , Dense , Dropout , Conv2D model = Sequential ( ) model3 = Model ( inputs=model.inputs , outputs=model.outputs ) model3._set_inputs ( model.inputs ) model2.add ( model.layers [ -1 ] ) model.add ( Dense ( 16 , input_dim=32 ) ) def test_dynamic_set_inputs ( ) : assert preds3.shape == ( 1 , 16 ) from keras.layers import Dense , Dropout , Conv2D model.add ( Activation ( 'relu ' ) ) preds3 = model3.predict ( [ np.random.random ( ( 1 , 32 ) ) ] ) model2 = Sequential ( )",['tests/keras/engine/test_training.py'],Add tests for inputs set dynamically ( # 10367 )
419,25d0193eaeed91ac899bf8b572d98ce0e0c76d3a,2018-06-05 11:23:27-07:00,"corresponds to data format ` channels_last ` , loss = lambda y_true , y_pred : K.sparse_categorical_crossentropy ( # Labels for testing 4-class sparse_categorical_crossentropy , 4-class feed_output_shapes.append ( output_shape [ : -1 ] + ( 1 , ) ) if loss_name == 'sparse_categorical_crossentropy ' : and ` axis=1 ` corresponds to data format permutation += output_dimensions [ axis_without_batch + 1 : ] output_dimensions = list ( range ( len ( output.get_shape ( ) ) ) ) bias_initializer='ones ' ) ( input_tensor ) [ [ 1 , 0 , 1 ] , [ 1 , 0 , 1 ] , [ 1 , 1 , 0 ] ] ] ] ) ] target = C.one_hot ( target , output.shape [ -1 ] ) len ( output.get_shape ( ) ) - 1 , else : np.array ( [ [ [ [ 0 , 1 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] , data = data_channels_first Tests ` sparse_categorical_crossentropy ` , ` categorical_crossentropy ` , output /= tf.reduce_sum ( output , axis , True ) permutation += output_dimensions [ axis + 1 : ] + [ axis ] batch_size=1 , verbose=0 ) return categorical_crossentropy ( target , output , from_logits , axis=-1 ) output = tf.transpose ( output , perm=permutation ) output_dimensions = list ( range ( len ( int_shape ( output ) ) ) ) old_data_format = K.image_data_format ( ) 'Unexpected channels axis { } . '.format ( axis ) , labels = labels_channels_first [ index ] batch_size=1 , verbose=0 ) [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] , [ 0 , 1 , 0 ] ] , A binary matrix representation of the input . activation = 'softmax ' 'channels_first and channels_last . ' ) ) kernel_initializer='ones ' , target = C.transpose ( target , permutation ) return - tf.reduce_sum ( target * tf.log ( output ) , output_shape ) in [ 4 , 5 ] : losses_to_test = [ 'sparse_categorical_crossentropy ' , data_channels_first = np.array ( [ [ [ [ 8. , 7.1 , 0 . ] , [ 4.5 , 2.6 , 0.55 ] , axis = 1 if K.image_data_format ( ) == 'channels_first ' else -1 # Compute one loss for each loss function in the list ` losses_to_test ` : labels = np.moveaxis ( labels_channels_first [ index ] , 1 , -1 ) # Evaluate the same network with channels first , with all three loss inputs = Input ( shape= ( 1 , 3 , 3 ) ) ` channels_first ` or ` channels_last ` image_data_format . elif loss_name == 'categorical_crossentropy ' : activation = 'sigmoid ' axis=axis_without_batch ) def prepare_simple_model ( input_tensor , loss_name , target ) : # categorical_crossentropy , and 2-class binary_crossentropy : def categorical_crossentropy ( target , output , from_logits=False ) : 'which has { } dimensions . '.format ( len ( output.get_shape ( ) ) ) ) ) simple_model.compile ( optimizer='rmsprop ' , loss=loss ) if axis ! = -1 and axis ! = output_dimensions [ -1 ] : axis : Int specifying the channels axis . ` axis=-1 ` if axis ! = -1 and axis not in output_dimensions : 'which has { } dimensions . '.format ( len ( output.shape ) ) ) ) ' { } { } { } '.format ( 'which has { } dimensions . '.format ( len ( int_shape ( output ) ) ) ) ) data = np.moveaxis ( data_channels_first , 1 , -1 ) if axis_without_batch ! = -1 and axis_without_batch ! = output_dimensions [ -1 ] : len ( output.get_shape ( ) ) - 1 ) err_msg= ' { } { } '.format ( 'Computed different losses for ' , target = permute_dimensions ( target , permutation ) num_channels = target.shape [ axis ] `` `` '' Tests use of all crossentropy losses with ` channels_first ` . return categorical_crossentropy ( target , output , from_logits ) loss = lambda y_true , y_pred : K.binary_crossentropy ( y_true , y_pred ) output = C.transpose ( output , permutation ) target = C.one_hot ( target , output.shape [ axis_without_batch ] , output_dimensions = list ( range ( len ( output.shape ) ) ) [ 0.9 , 4.2 , 11.2 ] ] ] ] , dtype=np.float32 ) def sparse_categorical_crossentropy ( target , output , from_logits=False ) : permutation += [ axis_without_batch ] model = prepare_simple_model ( inputs , loss_function , labels ) Tests PR # 9715 . def sparse_categorical_crossentropy ( target , output , from_logits=False , axis=-1 ) : feed_output_shapes.append ( output_shape [ : -1 ] + ( 1 , ) ) [ [ 0 , 0 , 1 ] , [ 0 , 0 , 0 ] , [ 1 , 0 , 0 ] ] ] ] ) , permutation += [ axis ] def categorical_crossentropy ( target , output , from_logits=False , axis=-1 ) : return categorical_crossentropy ( target , output , from_logits , axis=axis ) 'categorical_crossentropy ' , 'binary_crossentropy ' ] predictions = Conv2D ( num_channels , 1 , activation=activation , y_true , y_pred , axis=axis ) feed_output_shapes.append ( num_channels = np.amax ( target ) + 1 loss = lambda y_true , y_pred : K.categorical_crossentropy ( `` `` '' return - tf.reduce_sum ( target * tf.log ( output ) , axis ) ( output_shape [ 0 ] , 1 ) + output_shape [ 2 : ] ) elif loss_name == 'binary_crossentropy ' : `` `` '' # functions : K.set_image_data_format ( 'channels_first ' ) assert_allclose ( loss_channels_first , loss_channels_last , # Raises raise ValueError ( ` channels_first ` . is placed last . Verifies that evaluate gives the same result with either 'Unexpected channels axis { } . '.format ( axis_without_batch ) , return simple_model A binary matrix representation of the input . The classes axis loss_channels_last [ index ] = model.evaluate ( x=data , y=labels , simple_model = Model ( inputs=input_tensor , outputs=predictions ) permutation = output_dimensions [ : axis ] permutation = output_dimensions [ : axis_without_batch ] loss_channels_first [ index ] = model.evaluate ( x=data , y=labels , if axis_without_batch ! = -1 and axis_without_batch not in output_dimensions : 'Expected to be -1 or one of the axes of ` output ` , ' , loss_channels_last = [ 0. , 0. , 0 . ] inputs = Input ( shape= ( 3 , 3 , 1 ) ) def test_model_with_crossentropy_losses_channels_first ( ) : for index , loss_function in enumerate ( losses_to_test ) : permutation = output_dimensions [ : axis ] + output_dimensions [ axis + 1 : ] loss_channels_first = [ 0. , 0. , 0 . ] if K.image_data_format ( ) == 'channels_first ' and len ( np.array ( [ [ [ [ 0 , 1 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , axis_without_batch = -1 if axis == -1 else axis - 1 [ [ 0 , 0 , 0 ] , [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] ] , True ) # Here , unlike other backends , the tensors lack a batch dimension : from keras.layers import Dense , Dropout and ` binary_crossentropy ` . # If the channels are not in the last axis , move them to be there : output /= tf.reduce_sum ( output , K.set_image_data_format ( old_data_format ) ValueError : if ` axis ` is neither -1 nor one of from keras.layers import Dense , Dropout , Conv2D # Evaluate a simple network with channels last , with all three loss the axes of ` output ` . labels_channels_first = [ np.array ( [ [ [ [ 0 , 1 , 3 ] , [ 2 , 1 , 0 ] , [ 2 , 2 , 1 ] ] ] ] ) , output = permute_dimensions ( output , permutation ) K.set_image_data_format ( 'channels_last ' )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/engine/training.py', 'keras/utils/np_utils.py', 'tests/keras/engine/test_training.py']",Supporting channels_first data format with crossentropy losses ( # 9715 )
420,1e80c1a2dbaceba9a8280c37e4e896599eb927f6,2018-06-05 11:10:43-07:00,"raise IndexError # Assume list/iterable else : X_train [ six.moves.range ( 1000 , 1001 ) ] elif isinstance ( key , list ) : with pytest.raises ( IndexError ) : else : with pytest.raises ( TypeError ) : with pytest.raises ( IndexError ) :","['keras/utils/io_utils.py', 'tests/keras/utils/io_utils_test.py']",Fix HDF5Matrix issue when working in conjunction with TimeSeriesGenerator ( # 10334 )
421,b21764824f181fb7a32fce852e5afcee4042192a,2018-06-04 10:26:41-07:00,"def test_relu ( ) : input_shape= ( 2 , 3 , 4 ) ) def __init__ ( self , max_value=None , * * kwargs ) : return input_shape Arbitrary . Use the keyword argument ` input_shape ` # Input shape class ReLU ( Layer ) : # Arguments Same shape as the input . # Output shape `` `` '' config = { 'max_value ' : self.max_value } return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) def compute_output_shape ( self , input_shape ) : `` `` '' Rectified Linear Unit activation function . ( tuple of integers , does not include the samples axis ) when using this layer as the first layer in a model . super ( ReLU , self ) .__init__ ( * * kwargs ) layer_test ( layers.ReLU , kwargs= { 'max_value ' : max_value } , self.supports_masking = True for max_value in [ None , 1. , 6 . ] : def call ( self , inputs ) : def get_config ( self ) : base_config = super ( ReLU , self ) .get_config ( ) max_value : Float , the maximum output value . return activations.relu ( inputs , max_value=self.max_value ) self.max_value = max_value","['keras/layers/advanced_activations.py', 'tests/keras/layers/advanced_activations_test.py']",Add an advanced activation layer for ReLU ( # 10322 )
422,1365ed5d9a872631b0d451f8508cc61d79228dc0,2018-06-04 10:07:23-07:00,"app , last_dim = random.choice ( MOBILENET_LIST ) app , last_dim = random.choice ( MODEL_LIST ) _test_application_notop ( app , last_dim ) include_top=False , app , last_dim = random.choice ( NASNET_LIST ) app = applications.ResNet50 @ keras_test if K.image_data_format ( ) == 'channels_first ' : ( applications.InceptionV3 , 2048 ) , app , last_dim = random.choice ( DENSENET_LIST ) @ pytest.mark.skipif ( ( K.backend ( ) ! = 'tensorflow ' ) , def test_mobilenet ( ) : app = applications.Xception ( applications.MobileNet , 1024 ) , DENSENET_LIST = [ ( applications.DenseNet121 , 1024 ) , ( applications.MobileNetV2 , 1280 ) , MOBILENET_LIST = [ ( applications.MobileNet , 1024 ) , def _test_application_variable_input_channels ( app , last_dim ) : ( applications.NASNetLarge , 4032 ) ] ( applications.DenseNet169 , 1664 ) , _test_app_pooling ( app , last_dim ) app = applications.InceptionResNetV2 _test_application_notop ( app , last_dim ) assert output_shape == ( None , None , None , last_dim ) ( applications.VGG19 , 512 ) , def test_densenet ( ) : pooling=random.choice ( [ 'avg ' , 'max ' ] ) ) ) def test_applications ( ) : ( applications.DenseNet169 , 1664 ) , lambda : app ( weights=None , include_top=False , input_shape=input_shape ) ) def test_inceptionv3 ( ) : def test_inceptionresnetv2 ( ) : ( applications.DenseNet201 , 1920 ) ] last_dim = 2048 last_dim = 512 app = applications.InceptionV3 def test_nasnet ( ) : _test_application_basic ( app ) _test_application_basic ( app ) ( applications.VGG16 , 512 ) , app = random.choice ( [ applications.VGG16 , applications.VGG19 ] ) last_dim = 1536 ( applications.InceptionResNetV2 , 1536 ) , def test_vgg ( ) : ( applications.DenseNet121 , 1024 ) , # TODO : enable nasnet tests if they support Theano and CNTK lambda : app ( weights=None , def _test_app_pooling ( app , last_dim ) : # ( applications.NASNetMobile , 1056 ) , def test_resnet50 ( ) : ( applications.MobileNetV2 , 1280 ) ] # ( applications.NASNetLarge , 4032 ) for _ in range ( 3 ) : _test_application_variable_input_channels ( app , last_dim ) ] MODEL_LIST = [ ( applications.Xception , 2048 ) , NASNET_LIST = [ ( applications.NASNetMobile , 1056 ) , input_shape = ( None , None , 4 ) else : assert output_shape == ( None , last_dim ) ( applications.DenseNet201 , 1920 ) input_shape = ( 4 , None , None ) ( applications.ResNet50 , 2048 ) , input_shape = ( None , None , 1 ) input_shape = ( 1 , None , None ) reason='NASNets are supported only on TensorFlow ' ) def test_xception ( ) : output_shape = _get_output_shape (",['tests/integration_tests/applications_test.py'],Reduce tests for applications ( # 10346 )
423,52e3f9835c1a25a8b895773f64524e27a5ae10c0,2018-06-02 10:22:09-07:00,"along the height and width . along the depth , height and width . depth , height and width of the 3D convolution window . height and width of the 3D convolution window .",['keras/layers/convolutional.py'],Improve the docstring of Conv3DTranspose ( # 10342 )
424,c9cb6087394656247b13dbcb447135dfcb91685c,2018-06-01 23:53:49-07:00,"rows = input_tensor._keras_shape [ 2 ] self.featurewise_std_normalization = False x = np.rollaxis ( x , 0 , channel_axis + 1 ) x2 = add ( [ x2_1 , x2_2 ] , name='reduction_add_2_ % s ' % block_id ) from __future__ import division decode_predictions = densenet.decode_predictions https : //github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md 'data/masks ' , elif len ( zoom_range ) == 2 : 'is not type input_tensor ' ) BASE_WEIGHT_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.6/ ' if K.image_data_format ( ) == 'channels_first ' : _PIL_INTERPOLATION_METHODS [ 'box ' ] = pil_image.BOX def flip_axis ( x , axis ) : A Keras ` Model ` instance . white_list_formats , data_format=None , `` `` '' Text tokenization utility class . In the case where texts contains lists , we assume each entry of the lists strides=strides , if data_format not in { 'channels_last ' , 'channels_first ' } : for vect in self.texts_to_sequences_generator ( texts ) : if np.max ( self.height_shift_range ) < 1 : K.set_image_data_format ( old_data_format ) lengths = [ ] has to be ` ( 299 , 299 , 3 ) ` ( with ` 'channels_last ' ` data format ) x4_1 = AveragePooling2D ( ( 3 , 3 ) , strides= ( 1 , 1 ) , padding='same ' , elif len ( x.shape ) == 2 : length=10 , sampling_rate=2 , transform_matrix = shift_matrix if transform_matrix is None else np.dot ( transform_matrix , shift_matrix ) WEIGHTS_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 ' def NASNetMobile ( input_shape=None , x = layers.add ( [ x , residual ] ) ' a tuple or list of two floats . ' np.random.seed ( self.seed + self.total_batches_seen ) print ( 'Found % d images belonging to % d classes . ' % input_shape=None , [ MobileNets : Efficient Convolutional Neural Networks for function_partial = partial ( _count_valid_files_in_directory , classes=1000 ) : elif default_size == 331 : # large version `` `` '' Applies an affine transformation specified by the parameters given . with K.name_scope ( 'block_1 ' ) : validation_generator = test_datagen.flow_from_directory ( 'For NASNet-A models , the value of ` penultimate_filters ` ' x = dense_block ( x , blocks [ 1 ] , name='conv3 ' ) filters= ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , name=conv_name ) ( x ) x = MaxPooling2D ( ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='same ' , name='block4_pool ' ) ( x ) batch_y = batch_x.copy ( ) featurewise_std_normalization=True , return np.minimum ( 1. , f / np.sqrt ( f ) ) ' ` validation_split ` must be strictly between 0 and 1 . ' where ` x ` is a numpy array of image data name='bn ' ) ( x ) describing the transformation . Currently , the following parameters 'ty ' : ty , self._set_index_array ( ) for x in sequences : if negative_samples > 0 : name='adjust_bn_ % s ' % block_id ) ( p ) self.std = np.std ( x , axis= ( 0 , self.row_axis , self.col_axis ) ) def skipgrams ( sequence , vocabulary_size , DENSENET201_WEIGHT_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5 ' if alpha not in [ 0.25 , 0.50 , 0.75 , 1.0 ] : shear = np.random.uniform ( row_axis=img_row_axis , col_axis=img_col_axis , 'has been renamed ` num_words ` . ' ) import sys from .. applications.imagenet_utils import _obtain_input_shape if end_index is None : file_hash='4912a53fbd2a69346e7f2c0b5ec8c6d3 ' ) 'flip_horizontal ' : flip_horizontal , block_id : String block_id random.randint ( 1 , vocabulary_size - 1 ) ] def one_hot ( text , n , def _flow_index ( self ) : Adapted from code contributed by BigMoyan . used to work with autoencoders ) , has to be ` ( 299 , 299 , 3 ) ` ( with ` channels_last ` data format ) x = Activation ( 'relu ' , name='block4_sepconv1_act ' ) ( x ) img = array_to_img ( x , data_format=data_format , scale=scale ) data_format=self.data_format , ( dirpath , white_list_formats , split , x = _inverted_res_block ( x , filters=160 , alpha=alpha , stride=2 , x = BatchNormalization ( epsilon=1e-3 , momentum=0.999 , TF_WEIGHTS_PATH , name='reduction_left2_ % s ' % block_id ) ( h ) branch_pool = AveragePooling2D ( ( 3 , 3 ) , strides= ( 1 , 1 ) , padding='same ' ) ( x ) block_type='block17 ' , x = np.asarray ( img , dtype=K.floatx ( ) ) elif pooling == 'max ' : num_row : height of the convolution kernel . x = Concatenate ( axis=channel_axis , name='mixed_7a ' ) ( branches ) import warnings raise ValueError ( ' ` zoom_range ` should be a float or ' texts : can be a list of strings , if sys.version_info < ( 3 , ) : You can also omit this option if you would like # mixed 7 : 17 x 17 x 768 x = transition_block ( x , 0.5 , name='pool4 ' ) utils=utils ) branch_pool = AveragePooling2D ( name='Conv_1 ' ) ( x ) to use for random transformations and normalization . subset=subset , DENSENET121_WEIGHT_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels.h5 ' 'as true , ` classes ` should be 1000 ' ) if j ! = i : x = _inverted_res_block ( x , filters=320 , alpha=alpha , stride=1 , params = self.image_data_generator.get_random_transform ( x.shape ) self.batch_index += 1 for block_idx in range ( 1 , 11 ) : The directory name is used as class label directory , white_list_formats , follow_links ) ) [ start : stop ] 'Received : % s ' % brightness_range ) [ np.sin ( theta ) , np.cos ( theta ) , 0 ] , preprocess_input = vgg16.preprocess_input x /= ( self.std + K.epsilon ( ) ) from .. import initializers for the CIFAR-10 dataset , and then extended to ImageNet 2012 dataset , to the shortcut branch . Let ` r ` be the output from the residual branch , ` `` categorical '' ` : categorical targets , elif blocks == [ 6 , 12 , 48 , 32 ] : fname = ' { prefix } _ { index } _ { hash } . { format } '.format ( the arguments ` padding ` and ` truncating ` , respectively . for i in range ( num_blocks ) : The [ probability ] ( https : //en.wikipedia.org/wiki/Birthday_problem # Probability_table ) ` None ` means that the output of the model if interpolation not in _PIL_INTERPOLATION_METHODS : prefix = 'block_ { } _'.format ( block_id ) into tuples of words of the form : x = BatchNormalization ( name=prefix + '_sepconv3_bn ' ) ( x ) def _adjust_block ( p , ip , filters , block_id=None ) : from six.moves import range `` `` '' Iterates on files with extension in ` white_list_formats ` contained in ` directory ` . ` ( batch , filters , new_rows , new_cols ) ` if data_format='channels_first ' # Returns if hasattr ( pil_image , 'BOX ' ) : x = ZeroPadding2D ( padding= ( 3 , 3 ) , name='conv1_pad ' ) ( img_input ) make_sampling_table = sequence.make_sampling_table x.astype ( K.floatx ( ) ) , params ) is a stable hashing function . to be a token . 'or the path to the weights file to be loaded . ' ) # mixed 8 : 8 x 8 x 1280 ` ( samples , channels , height , width ) ` . image_data_generator : Instance of ` ImageDataGenerator ` x2_1 = _separable_conv_block ( p , filters , ( 5 , 5 ) , branch7x7 = conv2d_bn ( x , 160 , 1 , 1 ) if lower : N is the number of blocks A [ Sequence ] ( /utils/ # sequence ) instance . from hashlib import md5 E.g . ` ( 200 , 200 , 3 ) ` would be one valid value . x = Conv2D ( 256 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block3_conv2 ' ) ( x ) along with the images . `` `` '' Generate batches of tensor image data with real-time data augmentation . rows = np.random.randint ( x = identity_block ( x , 3 , [ 128 , 128 , 512 ] , stage=3 , block= ' c ' ) pointwise_filters = _make_divisible ( pointwise_conv_filters , 8 ) p = p0 if not skip_reduction else p batch_y = self.classes [ index_array ] .astype ( K.floatx ( ) ) model = Model ( inputs , x , name='resnet50 ' ) tx = 0 or if the Tokenizer requires to be fit to sample data . self.word_counts [ w ] += 1 weights=None , from .. layers import AveragePooling2D self.split ) return DenseNet ( [ 6 , 12 , 32 , 32 ] , `` `` '' A block that has a conv layer at shortcut . if data_format == 'channels_first ' : strides= ( 2 , 2 ) , padding='same ' , MobileNetV2 = mobilenet_v2.MobileNetV2 'first by calling ` .fit ( numpy_data ) ` . ' ) classes : optional number of classes to classify images or 4D tensor with shape : maxlen : Int , maximum length of all sequences . file_hash='b0042744bf5b25fce3cb969f33bebb97 ' ) https : //github.com/tensorflow/models/tree/master/research/slim/nets/nasnet input_shape , broadcast_shape = [ 1 , 1 , 1 ] import keras_preprocessing p = Activation ( 'relu ' , name='adjust_relu_1_ % s ' % block_id ) ( p ) x = Conv2D ( stem_block_filters , ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='valid ' , seed : Int ( default : None ) . Random seed . `` `` '' ResNet50 model for Keras . WEIGHTS_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5 ' filters= ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , str ( self.channel_axis ) + ' ) , i.e . expected ' self.rescale = rescale `` `` '' Generates a word rank-based probabilistic sampling table . weigh_path , height_shift_range=0.2 , self.width_shift_range ) for root , _ , files in _recursive_list ( directory ) : ` None ` means that the output of the model will be x = Concatenate ( axis=channel_axis , name='mixed_5b ' ) ( branches ) directly on the dataset of interest . if default_size is None : space-separated sequences of words x = BatchNormalization ( axis=bn_axis , name=bn_name_base + '2a ' ) ( x ) sequences : A list of sequence . x , p = _reduction_a_cell ( x , p , filters // ( filter_multiplier * * 2 ) , super ( NumpyArrayIterator , self ) .__init__ ( x.shape [ 0 ] , ' '' Instantiates a NASNet model . preprocess_input = xception.preprocess_input setattr ( DenseNet121 , '__doc__ ' , DenseNet.__doc__ ) # the indexing of each batch . # Yields A Keras tensor self.samplewise_center = samplewise_center use_bias=False , def __len__ ( self ) : and ` `` bicubic '' ` . reduction : float , compression rate at transition layers . preprocess_input = mobilenet.preprocess_input ( len ( batch_x ) , self.num_classes ) , MobileNets support any input size greater than 32 x 32 , with larger image sizes x , p = _normal_a_cell ( x , p , filters * filter_multiplier * * 2 , weights=weights ) x = Conv2D ( classes , ( 1 , 1 ) , penultimate_filters=4032 , from PIL import Image as pil_image zx , zy = np.random.uniform ( zoom_range [ 0 ] , zoom_range [ 1 ] , 2 ) self.samplewise_std_normalization = samplewise_std_normalization batches += 1 absolute_path = os.path.join ( root , fname ) truncating : String , 'pre ' or 'post ' : if isinstance ( text , unicode ) : classes : Optional list of class subdirectories x = _inverted_res_block ( x , filters=16 , alpha=alpha , stride=1 , if include_top : split , def _empty_batch ( self , num_rows ) : block_id='reduction_right3_ % s ' % block_id ) from .. import backend as K for j in seq : targets : Targets corresponding to timesteps in ` data ` . rotation_matrix = np.array ( [ [ np.cos ( theta ) , -np.sin ( theta ) , 0 ] , rotation_range=0. , path : Path to image file . from .. import models branch7x7dbl = conv2d_bn ( branch7x7dbl , 192 , 1 , 7 ) 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 ' , # Reference self.col_axis = 3 The function will run after the image is resized and augmented . self.featurewise_std_normalization = featurewise_std_normalization axis=channel_axis , name='conv_pw_ % d_bn ' % block_id ) ( x ) # any potential predecessors of ` input_tensor ` num_blocks=4 , name='separable_conv_2_bn_ % s ' % ( block_id ) ) ( x ) if sample_weight is not None : name=name ) ( x ) the 4D tensor output of the last convolutional layer . or the path to the weights file to be loaded . ` 'theta ' ` : Float . Rotation angle in degrees . ' '' binary '' , `` sparse '' , `` input '' ' filters : Number of output filters to be matched weights : ` None ` ( random initialization ) or return imagenet_utils.preprocess_input ( x , data_format , mode='torch ' ) split=split ) ' ` samplewise_center ` . ' ) from keras_applications import densenet ty = np.random.choice ( self.width_shift_range ) If a file object was used instead of a filename , this return new_seq , new_label labels += [ [ 1 , 0 ] ] * num_negative_samples block_idx=block_idx ) default_size=299 , MobileNet is a general architecture and can be used for multiple use cases . transform_matrix = np.dot ( np.dot ( offset_matrix , matrix ) , reset_matrix ) save_to_dir=None , if sampling_table is not None : bn_axis = 3 if K.image_data_format ( ) == 'channels_last ' else 1 x = conv2d_bn ( x , 1536 , 1 , name='conv_7b ' ) # Block 1 This function applies the `` Inception '' preprocessing which converts into , only to be specified if ` include_top ` is True , and from .. layers import Input branch_1 = conv2d_bn ( branch_1 , 256 , [ 3 , 1 ] ) col_axis : Index of axis for columns in the input tensor . def apply_brightness_shift ( x , brightness ) : branch_1 = conv2d_bn ( branch_1 , 64 , 5 ) resample = _PIL_INTERPOLATION_METHODS [ interpolation ] Whether the images will be converted to padding='same ' , name='conv_preds ' ) ( x ) x [ i ] [ j ] = c classes=1000 ) : x = conv2d_bn ( x , 192 , 3 , 3 , padding='valid ' ) for i , j in enumerate ( index_array ) : # Stem block : 35 x 35 x 192 branch3x3dbl = layers.concatenate ( | [ mobilenet_v2_1.3_224 ] | 509 | 5.34 | 74.4 | 92.1 | name='reduction_concat_ % s ' % block_id ) | [ mobilenet_v2_0.75_160 ] | 107 | 2.61 | 66.4 | 87.3 | if categorical : branch_2 = conv2d_bn ( branch_2 , 48 , 3 ) follow_links=False , x = Reshape ( shape , name='reshape_1 ' ) ( x ) block_idx=10 ) pool = multiprocessing.pool.ThreadPool ( ) are provided ( 224 , 192 , 160 , 128 , and 96 ) . continue seed=seed ) lengths.append ( len ( x ) ) x = Activation ( 'relu ' , name='block13_sepconv2_act ' ) ( x ) if j > = num_words : `` `` '' Instantiates the VGG19 architecture . x = x.transpose ( 1 , 2 , 0 ) ` 'tx ' ` : Float . Shift in the x direction . if transform_parameters.get ( 'flip_horizontal ' , False ) : x = _depthwise_conv_block ( x , 128 , alpha , depth_multiplier , block_id=3 ) block_type='block8 ' , input_tensor=None , input_shape=None , branch7x7dbl = conv2d_bn ( branch7x7dbl , 128 , 7 , 1 ) s_inv = 1 . / np.sqrt ( s [ np.newaxis ] + self.zca_epsilon ) and a top-5 validation accuracy of 0.945 . ' ` 0.25 ` , ` 0.50 ` , ` 0.75 ` or ` 1.0 ` only . ' ) return NASNet ( input_shape , filenames.append ( relative_path ) models=models , if default_size == 224 : # mobile version import threading 'Received : % s ' % zoom_range ) ' Received : % s ' % validation_split ) ' ` data_format ` should be ` `` channels_last '' ` ' results = [ ] batch_size=batch_size , If ` alpha ` > 1.0 , proportionally increases the number expansion=6 , block_id=8 ) of corresponding labels . If 'sample_weight ' is not None , input_tensor=None , channel_shift_range=0. , x = dense_block ( x , blocks [ 2 ] , name='conv4 ' ) or in case of invalid shape for a ` sequences ` entry . hash=np.random.randint ( 1e7 ) , # 20x block17 ( Inception-ResNet-B block ) : 17 x 17 x 1088 self.n = n sequences : List of lists , where each element is a sequence . ` 'max ' ` means that global max pooling will be applied . x = BatchNormalization ( axis=bn_axis , name=bn_name_base + '2c ' ) ( x ) start , stop = int ( split [ 0 ] * num_files ) , int ( split [ 1 ] * num_files ) def texts_to_sequences_generator ( self , texts ) : ( trunc.shape [ 1 : ] , idx , sample_shape ) ) raise ValueError ( 'input_shape : ' , input_shape , Residual Connections on Learning ] ( https : //arxiv.org/abs/1602.07261 ) new_seq , new_label : shortened lists for ` seq ` and ` label ` . stride=1 , branch_1 = conv2d_bn ( x , 48 , 1 ) dirname , os.path.relpath ( absolute_path , directory ) ) filter_multiplier : Controls the width of the network . | [ mobilenet_v2_0.5_160 ] | 50 | 1.95 | 61.0 | 83.2 | data_format : data format of the image tensor . x = Activation ( 'relu ' ) ( x ) final_affine_matrix = transform_matrix [ :2 , :2 ] A dictionary containing randomly chosen parameters describing the if augment : text = text.replace ( c , split ) | [ mobilenet_v2_0.75_224 ] | 209 | 2.61 | 69.8 | 89.6 | input_tensor , input_shape , cache_subdir='models ' ) strides : An integer or tuple/list of 2 integers , ' '' NASNet-A models for Keras . 'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5 ' , if mode == 'tfidf ' and not self.document_count : shear_range=0. , Example of using ` .flow_from_directory ( directory ) ` : split = None if featurewise_std_normalization : x = BatchNormalization ( name='block13_sepconv2_bn ' ) ( x ) filter_multiplier=2 , name='normal_left3_ % s ' % ( block_id ) ) ( h ) ` [ i - window_size , i + window_size+1 ] ` . ax = np.zeros ( Architecture | Top-1 Acc | Top-5 Acc | Multiply-Adds | Params ( M ) img : PIL Image instance . validation_split=0.0 ) : same as with ` width_shift_range= [ -1 , 0 , +1 ] ` , batch_x = np.zeros ( tuple ( [ len ( index_array ) ] + list ( self.x.shape ) [ 1 : ] ) , if tx ! = 0 or ty ! = 0 : if len ( self.brightness_range ) ! = 2 : batch_size=32 , https : //github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py ' ` featurewise_center ` . ' ) if trunc.shape [ 1 : ] ! = sample_shape : ( np.asarray ( x ) .shape , np.asarray ( sample_weight ) .shape ) ) if hash_function is None : save_prefix= '' , def __init__ ( self , directory , image_data_generator , self.zoom_range = [ zoom_range [ 0 ] , zoom_range [ 1 ] ] this is how many augmentation passes over the data to use . `` `` '' Applies a random transformation to an image . ' ( no None entries ) . Got : ` input_shape= ' x = dense_block ( x , blocks [ 3 ] , name='conv5 ' ) have 1 or 3 color channels . from .. layers import Dense ( 224 , 224 , 3 ) . load_img = image.load_img else : ' ; expected `` rgb '' or `` grayscale '' . ' ) replace out-of-vocabulary words during text_to_sequence calls for i , j in enumerate ( index_array ) : x1 = Activation ( 'relu ' , name=name + '_0_relu ' ) ( x1 ) ValueError : if interpolation method is not supported . sampling_rate=1 , couples , labels : where ` couples ` are int pairs and if pil_image is None : if featurewise_std_normalization : ( only relevant if ` save_to_dir ` is set ) . end_index : Data points later than ` end_index ` will not be used the 4D tensor output of the decode_predictions = mobilenet.decode_predictions # Reference : 'as other backends do not support ' except ValueError : if x.shape [ 2 ] == 3 : filters : Number of output filters per layer activation : activation function to use at the end of the block if self.mean is not None : and width and height should be no smaller than 48 . hashing function ; unicity of word to index mapping non-guaranteed . warnings.warn ( 'MobileNet shape is undefined . ' # Load weights . # combine generators into one which yields image and masks x = flip_axis ( x , img_row_axis ) char_level : if True , every character will be treated as a token . branch7x7dbl = conv2d_bn ( x , 192 , 1 , 1 ) expansion=6 , block_id=13 ) return batch_x , batch_y raise ValueError ( 'Input data in ` NumpyArrayIterator ` ' for text in texts : x , y = batch_0 block_id=None ) : def _reduction_a_cell ( ip , p , filters , block_id=None ) : x : input tensor . x = Activation ( 'relu ' , name='block3_sepconv2_act ' ) ( x ) # References if self.brightness_range is not None : ( 1 + self.index_docs.get ( j , 0 ) ) ) data_gen_args = dict ( featurewise_center=True , branch3x3dbl_1 = conv2d_bn ( branch3x3dbl , 384 , 1 , 3 ) from the dictionary are used : raise ValueError ( 'Unsupported image shape : ' , x.shape ) x1 = add ( [ x1_1 , x1_2 ] , name='reduction_add_1_ % s ' % block_id ) passing ` x ` through an inception module ) before adding them from .. import layers format to use is determined from the filename extension . filtered from the texts . The default is all punctuation , plus save_to_dir=save_to_dir , for r in range ( rounds ) : branch7x7x3 , 192 , 3 , 3 , strides= ( 2 , 2 ) , padding='valid ' ) ( 3 , rows , cols ) ( with ` channels_first ` data format ) . ( x_train , y_train ) , ( x_test , y_test ) = cifar10.load_data ( ) raise ValueError ( 'Padding type `` % s '' not understood ' % padding ) are padded with ` value ` at the end . # but original PIL image has format ( width , height , channel ) relative_path = os.path.join ( Vector Space ] ( http : //arxiv.org/pdf/1301.3781v3.pdf ) | 1.0 MobileNet-224 | 70.6 % | 529 | 4.2 | If you never set it , then it will be `` channels_last '' . image_generator = image_datagen.flow_from_directory ( x = identity_block ( x , 3 , [ 512 , 512 , 2048 ] , stage=5 , block= ' c ' ) from __future__ import print_function DENSENET121_WEIGHT_PATH , x = conv_block ( x , 3 , [ 256 , 256 , 1024 ] , stage=4 , block= ' a ' ) See [ this script ] ( https : //gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d ) raise ImportError ( 'Could not import PIL.Image . ' def DenseNet121 ( include_top=True , num_blocks : Number of repeated blocks of the NASNet model . Optionally loads weights pre-trained on ImageNet . or the length of the longest sequence otherwise . data_format : Image data format . return samples [ : , : :-1 , ... ] , targets x = inception_resnet_block ( x , name='mixed ' + str ( 5 + i ) ) Pre-trained ImageNet weights are also converted from TF-slim , which can be found in : `` input '' , or None . Default : `` categorical '' . x = self.image_data_generator.apply_transform ( x , params ) # Note seq = set ( seq ) warnings.warn ( 'The ` nb_words ` argument in ` Tokenizer ` ' if input_tensor._keras_shape [ 1 ] ! = input_shape [ 1 ] : A PIL Image instance . for idx , s in enumerate ( sequences ) : self.word_docs = { } branch_2 = conv2d_bn ( x , 64 , 1 ) WEIGHTS_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5 ' augment=False , Required before using ` texts_to_sequences ` or ` texts_to_matrix ` . 'channels on axis ' + str ( channels_axis ) + ' . ' except ValueError : # floating point except that it uses inverted residual blocks with E.g . ` ( 150 , 150 , 3 ) ` would be one valid value . pooling=None , [ branch3x3 , branch3x3dbl , branch_pool ] , axis=channel_axis , name='mixed3 ' ) raise ValueError ( _PIL_INTERPOLATION_METHODS [ 'lanczos ' ] = pil_image.LANCZOS u = np.random.uniform ( brightness_range [ 0 ] , brightness_range [ 1 ] ) for i in range ( 8 ) : ( channels , rows , cols ) ( with ` channels_first ` data format ) . Two or more words may be assigned to the same index , due to possible # Based on the following implementations ' ( 128 , 128 ) , ( 160 , 160 ) , ( 192 , 192 ) , or ( 224 , 224 ) ) . ' A batch of transformed samples . An ` Iterator ` yielding tuples of ` ( x , y ) ` for visualizing the random transformations being Note that the default input image size for this model is 299x299 . tx : Width shift . 2 ) that were designed automatically by learning the model architectures ' '' Adds 2 blocks of [ relu-separable conv-batchnorm ] . split_idx = int ( len ( x ) * image_data_generator._validation_split ) file_hash='6d6bbae143d832006294945121d1f1fc ' ) end_index = len ( data ) - 1 negative_samples : Float > = 0 . 0 for no negative ( i.e . random ) samples . branch1x1 = conv2d_bn ( x , 64 , 1 , 1 ) shuffle=False , steps_per_epoch=len ( x_train ) / 32 , epochs=epochs ) # Create model if zx ! = 1 or zy ! = 1 : weights_path = get_file ( 'xception_weights_tf_dim_ordering_tf_kernels_notop.h5 ' , `` `` '' Instantiates the ResNet50 architecture . input_tensor=None , input_shape=None , use_bias : whether to use a bias in ` Conv2D ` . pooling=None , Only the NASNet-A models , and their respective weights , which are suited labels.append ( [ 0 , 1 ] ) ValueError : In case of invalid ` mode ` argument , lower=True , split= '' `` ) : x = Conv2D ( 512 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block4_conv1 ' ) ( x ) self.zoom_range [ 0 ] , data_format = K.image_data_format ( ) Points outside the boundaries of the input are filled featurewise_std_normalization=False , return [ os.path.join ( root , f ) lower : boolean . Whether to convert the input to lowercase . require_flatten=False , self.target_size = tuple ( target_size ) DENSENET201_WEIGHT_PATH_NO_TOP , self.height_shift_range ) file_hash='cbe5617147190e668d6c5d5026f83318 ' ) input_shape = _obtain_input_shape ( input_shape , from .. layers import Add x : a 4D numpy array consists of RGB values within [ 0 , 255 ] . self.seed = seed or ` ( 3 , 299 , 299 ) ` ( with ` 'channels_first ' ` data format ) . x = Conv2D ( 512 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block5_conv4 ' ) ( x ) tx = np.random.uniform ( -hrg , hrg ) * h `` `` '' Removes sequences that exceed the maximum length . x = SeparableConv2D ( 1024 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block13_sepconv2 ' ) ( x ) interpolation='nearest ' ) : # Examples if self.reverse : from keras_applications import vgg19 'Received : % s ' % data_format ) x = ZeroPadding2D ( padding= ( ( 1 , 1 ) , ( 1 , 1 ) ) ) ( x ) if subset is not None : pooling=None , file_hash='0a58e3b7378bc2990ea3b43d5981f1f6 ' ) classes=1000 ) : elif mode == 'binary ' : raise ValueError ( x : Numpy array . cache_subdir='models ' , mode : one of `` binary '' , `` count '' , `` tfidf '' , `` freq '' . if self.color_mode == 'rgb ' : Note that from stage 3 , self.batch_size * self.stride ) // ( self.batch_size * self.stride ) self.split = split use_bias=False , name='stem_conv1 ' , elif block_type == 'block17 ' : vertical_flip=False , shuffle=shuffle , continue into , only to be specified if ` include_top ` is ` True ` , and class DirectoryIterator ( Iterator ) : If using data augmentation ( ` augment=True ` ) , if rows ! = cols or rows not in [ 96 , 128 , 160 , 192 , 224 ] : from six.moves import zip block_id='reduce_ % d ' % ( num_blocks ) ) last_block_filters = _make_divisible ( 1280 * alpha , 8 ) Can be used to feed the model miscellaneous data zoom_matrix = np.array ( [ [ zx , 0 , 0 ] , name='separable_conv_1_bn_ % s ' % ( block_id ) ) ( x ) `` channels_last '' mode means that the images should have shape Note that index 0 is expected to be a non-word and will be skipped . DENSENET121_WEIGHT_PATH_NO_TOP , raise ValueError ( 'If imagenet weights are being loaded , ' will have ` block_type='block35 ' , block_idx=0 ` , ane the layer names will have if self.width_shift_range : A ransformed version of the input ( same shape ) . DENSENET121_WEIGHT_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5 ' cval : Float or Int . ' '' x = Activation ( 'relu ' , name='block14_sepconv2_act ' ) ( x ) `` `` '' x , p0 = _reduction_a_cell ( x , p , filters * filter_multiplier * * 2 , from keras_preprocessing import image default_size=331 ) zy : Zoom in y direction raise ValueError ( 'Expected image array to have rank 3 ( single image ) . ' def _count_valid_files_in_directory ( directory , ' ( 224 , 224 ) will be loaded . ' ) default_size = rows | [ mobilenet_v2_0.35_96 ] | 11 | 1.66 | 45.5 | 70.4 | file_hash='11577c9a518f0070763c2b964a382f17 ' ) each of which gets passed validation_split = self.image_data_generator._validation_split ' ; expected `` training '' or `` validation '' ' ) if os.path.isdir ( os.path.join ( directory , subdir ) ) : `` `` '' The identity block is the block that has no conv layer at shortcut . batch_x [ i ] = x inputs : Input tensor of shape ` ( rows , cols , channels ) ` use_bias=use_bias , with K.name_scope ( 'separable_conv_block_ % s ' % block_id ) : x = Conv2D ( int ( K.int_shape ( x ) [ bn_axis ] * reduction ) , 1 , use_bias=False , # mixed 2 : 35 x 35 x 256 x = BatchNormalization ( name='block1_conv2_bn ' ) ( x ) name='reduction_right5_ % s ' % block_id ) ( h ) alpha_text = '5_0 ' weigh_path = BASE_WEIGHT_PATH + model_name preprocess_input = mobilenet_v2.preprocess_input transform_parameters = { 'theta ' : theta , self.std = np.reshape ( self.std , broadcast_shape ) x = Conv2D ( expansion * in_channels , kernel_size=1 , padding='same ' , split into lists of tokens . They will then be indexed or vectorized . classes=classes , img_shape : Tuple of integers . Shape of the image that is transformed . name='conv_pw_ % d ' % block_id ) ( x ) horizontal_flip=True ) yield self.index_array [ current_index : branch_2 = conv2d_bn ( branch_2 , 96 , 3 ) if K.backend ( ) in [ 'cntk ' , 'theano ' ] : vect = [ ] if w in self.word_docs : if rows == cols and rows in [ 128 , 160 , 192 , 224 ] : ( also called the resolution multiplier ) target_size= ( 150 , 150 ) , ' or None . ' ) data for test or validation . directory , white_list_formats , follow_links ) def on_epoch_end ( self ) : x = np.reshape ( whitex , x.shape ) self.batch_index = 0 if self.rescale : x = x [ : split_idx ] def transform_matrix_offset_center ( matrix , x , y ) : ` ( -width_shift_range , +width_shift_range ) ` x : 3D tensor , single image . if weights == 'imagenet ' : from .. layers import concatenate of filters in each layer . return self.next ( * args , * * kwargs ) WEIGHTS_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5 ' seed=None , data_format=None , x = _conv_block ( img_input , 32 , alpha , strides= ( 2 , 2 ) ) be treated as a different class 'should have the same length . ' self.height_shift_range = height_shift_range if x.ndim ! = 3 : In case of grayscale data , the channels axis of the image array tx = np.random.uniform ( -self.height_shift_range , height_shift_range=0. , x [ idx , : len ( trunc ) ] = trunc theta = 0 x = conv2d_bn ( x , 32 , 3 , padding='valid ' ) x = BatchNormalization ( name='block1_conv1_bn ' ) ( x ) test_datagen = ImageDataGenerator ( rescale=1./255 ) from keras_applications import nasnet via the ` classes ` argument . if K.image_data_format ( ) == 'channels_first ' : x = GlobalMaxPooling2D ( ) ( x ) name=name + '_2_conv ' ) ( x1 ) bn_axis = 3 if samplewise_std_normalization : Please note that in case of class_mode None , index=j , for fname in sorted ( files ) : self.total_batches_seen += 1 default_size = 224 def random_transform ( self , x , seed=None ) : When using TensorFlow , for best performance you should padding : String , 'pre ' or 'post ' : a common prefix ` 'block35_0 ' ` . penultimate_filters : Number of filters in the penultimate layer . if x_max ! = 0 : name=name + '_1_bn ' ) ( x1 ) x = identity_block ( x , 3 , [ 256 , 256 , 1024 ] , stage=4 , block='d ' ) lower : boolean . Whether to set the text to lowercase . NASNet refers to Neural Architecture Search Network , a family of models 'nasnet_large_no_top.h5 ' , 'nearest ' : pil_image.NEAREST , if self.char_level or isinstance ( text , list ) : valid_files = list ( x -= self.mean zoom_range : Float or [ lower , upper ] . Range for random zoom . can be modified by using the ` alpha ` parameter , fname = 'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5 ' backend=backend , x = Dense ( classes , activation='softmax ' , name='predictions ' ) ( x ) The following table describes the performance of branch_0 = conv2d_bn ( x , 192 , 1 ) [ Very Deep Convolutional Networks for Large-Scale Image Recognition ] ( https : //arxiv.org/abs/1409.1556 ) ' ) , i.e . expected either 1 , 3 or 4 ' stem_block_filters=32 , and that the input preprocessing function layers and different number of filters from the original arXiv paper ) : `` `` '' Performs a random channel shift . for more details . the data still needs to reside in a subdirectory account a certain fraction of files in each directory . | [ mobilenet_v2_1.0_128 ] | 99 | 3.47 | 65.3 | 86.9 | seed=seed , if no ` weights ` argument is specified . use_bias=False , activation=None , self.rotation_range , return x self._validation_split = validation_split x = flip_axis ( x , img_col_axis ) [ TensorNets ] weights='imagenet ' , x = ( np.ones ( ( num_samples , maxlen ) + sample_shape ) * value ) .astype ( dtype ) ` 'flip_vertical ' ` : Boolean . Vertical flip . self.row_axis = 2 seed = random.randint ( 0 , 10e6 ) ( https : //github.com/tensorflow/models/blob/master/research/slim/nets/nasnet/nasnet.py ) x = BatchNormalization ( epsilon=1e-3 , momentum=0.999 , self.classes = np.zeros ( ( self.samples , ) , dtype='int32 ' ) text = text.translate ( translate_map ) branch1x1 = conv2d_bn ( x , 320 , 1 , 1 ) ` 'zy ' ` : Float . Zoom in the y direction . if input_shape is None : x5_2 = MaxPooling2D ( ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='same ' , if self.n > current_index + self.batch_size : x = _depthwise_conv_block ( x , 512 , alpha , depth_multiplier , x = Activation ( 'relu ' , name='block3_sepconv1_act ' ) ( x ) np.random.seed ( self.seed + self.total_batches_seen ) _PIL_INTERPOLATION_METHODS [ 'hamming ' ] = pil_image.HAMMING rows = input_tensor._keras_shape [ 1 ] self.featurewise_center = featurewise_center a numerical approximation of frequency ( rank ) : padding=padding , x = _depthwise_conv_block ( x , 256 , alpha , depth_multiplier , block_id=5 ) get_source_inputs ( input_tensor ) ) dropout : dropout rate prefix = 'block ' + str ( i + 5 ) specifying the strides of the convolution along the width and height . file_hash='020fb642bf7360b370c678b08e0adf61 ' ) NASNetMobile = nasnet.NASNetMobile if sampling_table [ wi ] < random.random ( ) : 'input data format `` channels_last '' ' # Legacy support preprocess_input = inception_v3.preprocess_input x = identity_block ( x , 3 , [ 256 , 256 , 1024 ] , stage=4 , block= ' f ' ) yield vect For each of these ` alpha ` values , weights for 4 different input image sizes padding='pre ' , truncating='pre ' , value=0 . ) : branches = [ branch_0 , branch_1 , branch_2 , branch_pool ] color_mode : One of ` `` rgb '' ` , ` `` grayscale '' ` . Color mode to read images . x4_2 = AveragePooling2D ( ( 3 , 3 ) , strides= ( 1 , 1 ) , padding='same ' , x = conv_block ( x , 32 , name=name + '_block ' + str ( i + 1 ) ) import re input_tensor : optional Keras tensor ( i.e . output of classes.append ( subdir ) default_size = 224 residual = Conv2D ( 728 , ( 1 , 1 ) , strides= ( 2 , 2 ) , ` ( -height_shift_range , +height_shift_range ) ` if fname.lower ( ) .endswith ( '.tiff ' ) : couples.append ( [ wi , wj ] ) on size 224 x 224 : residual = Conv2D ( 128 , ( 1 , 1 ) , strides= ( 2 , 2 ) , `` `` '' Converts a 3D Numpy array to a PIL Image instance . raise ValueError ( 'input_tensor : ' , input_tensor , ( in the case of a single image input ) or a list x = Conv2D ( 64 , ( 7 , 7 ) , strides= ( 2 , 2 ) , padding='valid ' , name='conv1 ' ) ( x ) We assume that the word frequencies follow Zipf 's law ( s=1 ) to derive # This function is taken from the original tf repo . continue from .. layers import add def VGG19 ( include_top=True , weights='imagenet ' , E.g . : ` segment= ( 0.6 , 1.0 ) ` would only account for last 40 percent A list of sequences . str ( self.x.shape ) + ' ( ' x = self.preprocessing_function ( x ) MACs stands for Multiply Adds axis=channel_axis , weights='imagenet ' , classes=1000 ) : ` `` binary '' ` : binary targets ( if there are only two classes ) , 'methods are { } '.format ( classes=None , class_mode='categorical ' , branch_2 = conv2d_bn ( branch_2 , 64 , 3 ) x = Conv2D ( 512 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block4_conv3 ' ) ( x ) self.sampling_rate = sampling_rate file_format : Optional file format override . If omitted , the from keras_applications import inception_v3 penultimate_filters=4032 , branch7x7x3 = conv2d_bn ( branch7x7x3 , 192 , 7 , 1 ) if theta ! = 0 : Rotated Numpy image tensor . if data_format is None : x = MaxPooling2D ( ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='same ' , name='block3_pool ' ) ( x ) transform_matrix = transform_matrix_offset_center ( x = Dropout ( dropout , name='dropout ' ) ( x ) self.row_axis = 1 A Numpy matrix . [ branch1x1 , branch7x7 , branch7x7dbl , branch_pool ] , for x_batch , y_batch in datagen.flow ( x_train , y_train , batch_size=32 ) : Depending on the use case , it can use different input layer size and branch7x7 = conv2d_bn ( x , 128 , 1 , 1 ) Used in situations where the output number of filters needs to be changed . def hashing_trick ( text , n , are used for create a sample sequence . ImageDataGenerator = image.ImageDataGenerator target_size : tuple of integers , dimensions to resize input images to . word indices are expected to match the rank save_to_dir=None , save_prefix= '' , save_format='png ' , keras_applications.set_keras_submodules ( 'which overrides setting of ' if self.y is None : couples = [ ] samples [ j ] = self.data [ indices ] `` `` '' Preprocesses a numpy array encoding a batch of images . intensity = np.random.uniform ( -intensity_range , intensity_range ) # 10x block8 ( Inception-ResNet-C block ) : 8 x 8 x 2080 include_top=True , self.document_count += 1 self.lower , # If both input_shape and input_tensor are used , they should match self.brightness_range = brightness_range [ 0 , 0 , 1 ] ] ) split : str . Separator for word splitting . ( more common words should be sampled less frequently , for balance ) . file_hash='1ed92395b5b598bdda52abe5c0dbfd63 ' ) alpha_text = '1_0 ' rows = input_shape [ 1 ] res = [ ] for j in range ( window_start , window_end ) : equal intervals , along with time series parameters such as if not len ( s ) : branch_0 = conv2d_bn ( x , 256 , 1 ) from .. import utils branch_0 = conv2d_bn ( x , 96 , 1 ) x = layers.add ( [ x , residual ] ) 'channel_shift_intensity ' : channel_shift_intensity , name=prefix + 'expand_BN ' ) ( x ) use_bias=False , name='adjust_conv_2_ % s ' % block_id , the first conv layer at main path is with strides= ( 2 , 2 ) 'are not available . ' ) train_datagen = ImageDataGenerator ( rows = input_shape [ 0 ] name='adjust_avg_pool_2_ % s ' % block_id ) ( x = Conv2D ( 64 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block1_conv1 ' ) ( img_input ) 'setting of ` featurewise_center ` . ' ) x = conv2d_bn ( img_input , 32 , 3 , 3 , strides= ( 2 , 2 ) , padding='valid ' ) from TensorFlow checkpoints found at try : For stride ` s ` , consecutive output samples would validation_data=validation_generator , x = concatenate ( [ p , x1 , x2 , x3 , x4 , x5 ] , axis=channel_dim , branch7x7dbl = conv2d_bn ( branch7x7dbl , 160 , 1 , 7 ) ` imagenet ` ( ImageNet weights ) intensity : Transformation intensity in degrees . # Numpy array x has format ( height , width , channel ) pooling=None , seed ) ` sampling_table [ i ] ` is the probability of sampling x = SeparableConv2D ( 728 , ( 3 , 3 ) , padding='same ' , use_bias=False , name=prefix + '_sepconv2 ' ) ( x ) inputs = img_input block_id : Integer , a unique identification designating the block number . x /= ( self.std + K.epsilon ( ) ) data_format=K.image_data_format ( ) , def load_img ( path , grayscale=False , target_size=None , model = Model ( inputs , x , name='NASNet ' ) branch7x7x3 = conv2d_bn ( x , 192 , 1 , 1 ) x = BatchNormalization ( epsilon=1e-3 , momentum=0.999 , name='Conv_1_bn ' ) ( x ) brightness_range=None , interpolation='nearest ' ) : use_bias=False , padding='same ' , 'data format convention `` ' + data_format + ' '' ' name='reduction_bn_1_ % s ' % block_id ) ( h ) shear = 0 'imagenet ' ( pre-training on ImageNet ) , x = Activation ( relu6 , name='Conv1_relu ' ) ( x ) ` ( samples , filters , new_rows , new_cols ) ` if data_format='channels_first ' raise RuntimeError ( 'Only TensorFlow backend is currently supported , ' A ` DirectoryIterator ` yielding tuples of ` ( x , y ) ` ValueError : in case of invalid argument for ` weights ` , 'is different from expected shape % s ' % width and height of the 2D convolution window . from keras_applications import inception_resnet_v2 | 1.0 MobileNet-224 | 70.6 % | 529 | 4.2 | dirname = os.path.basename ( directory ) self.width_shift_range = width_shift_range | [ mobilenet_v2_0.5_192 ] | 71 | 1.95 | 63.9 | 85.4 | When ` activation=None ` , no activation is applied weights=weights , x = _inverted_res_block ( x , filters=160 , alpha=alpha , stride=1 , with K.name_scope ( 'adjust_projection_block_ % s ' % block_id ) : It will be computed automatically if not set . `` `` '' Performs a random spatial shear of a Numpy image tensor . decode_predictions = vgg16.decode_predictions WEIGHTS_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5 ' for dirpath in ( os.path.join ( directory , subdir ) for subdir in classes ) : batch_y = self.classes [ index_array ] model.fit_generator ( datagen.flow ( x_train , y_train , batch_size=32 ) , steps_per_epoch=2000 , min_x , self.class_mode = class_mode use_bias=True , name='Logits ' ) ( x ) ' ` featurewise_std_normalization ` . ' ) x = x.transpose ( 2 , 0 , 1 ) ty * = np.random.choice ( [ -1 , 1 ] ) samples_shape.extend ( self.data.shape [ 1 : ] ) ` 'channel_shift_intencity ' ` : Float . Channel shift intensity . def _iter_valid_files ( directory , white_list_formats , follow_links ) : broadcast_shape [ self.channel_axis - 1 ] = x.shape [ self.channel_axis ] # note that index 0 is reserved , never assigned to an existing word x = apply_channel_shift ( x , batch_size , ( and the order of the classes , which will map to the label NASNetLarge = nasnet.NASNetLarge from .. layers import GlobalAveragePooling2D seq = text.split ( split ) WEIGHTS_PATH , if K.image_data_format ( ) == 'channels_first ' : with self.lock : end of the network . if self.shear_range : output += ( self.sample_weight [ index_array ] , ) seq = text_to_word_sequence ( text , if grayscale : def dense_block ( x , blocks , name ) : if self.channel_shift_range ! = 0 : Example of transforming images and masks together . return model * * kwargs ) : name='reduction_left4_ % s ' % block_id ) ( x1 ) if target_size is not None : # Needed if we want to do something like : x = _depthwise_conv_block ( x , 512 , alpha , depth_multiplier , block_id=8 ) # First , count the number of samples and classes . integers ( eg . ` [ 0 , 1 , 1 .. ] ` ) , expansion=6 , block_id=4 ) activation='relu ' , from .imagenet_utils import decode_predictions if not K.is_keras_tensor ( input_tensor ) : branch3x3 = layers.concatenate ( and width and height should be no smaller than 71 . set ` `` image_data_format '' : `` channels_last '' ` in the config . if self.featurewise_std_normalization : samplewise_center : Boolean . Set each sample mean to 0 . def get_random_transform ( self , img_shape , seed=None ) : old_data_format = 'channels_first ' filters : list ( or concatenation ) of characters to filter out , such as hashing_trick = text.hashing_trick from .. import engine x = Conv2D ( 256 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block3_conv1 ' ) ( x ) x = ax x = transition_block ( x , 0.5 , name='pool3 ' ) setattr ( DenseNet169 , '__doc__ ' , DenseNet.__doc__ ) name=prefix + 'depthwise ' ) ( x ) elif self.class_mode == 'categorical ' : default_size = rows name='separable_conv_1_ % s ' % block_id , new_seq.append ( x ) return output shuffle : Whether to shuffle output samples , zx , zy = 1 , 1 expansion=6 , block_id=3 ) branch3x3dbl , 96 , 3 , 3 , strides= ( 2 , 2 ) , padding='valid ' ) if input_tensor is None : p1 = Conv2D ( filters // 2 , ( 1 , 1 ) , padding='same ' , branch_1 = conv2d_bn ( branch_1 , 224 , [ 1 , 3 ] ) elif self.oov_token is not None : zoom_range : Tuple of floats ; zoom range for width and height . follow_links : boolean . P is the number of penultimate filters keras_preprocessing.set_keras_submodules ( backend=backend , utils=utils ) ` frequency ( rank ) ~ 1/ ( rank * ( log ( rank ) + gamma ) + 1/2 - 1/ ( 12 * rank ) ) ` x = _depthwise_conv_block ( x , 256 , alpha , depth_multiplier , preprocessing_function=None , n : Dimension of the hashing space . x_misc = [ np.asarray ( xx ) for xx in x [ 1 ] ] x = _depthwise_conv_block ( x , 1024 , alpha , depth_multiplier , branch7x7 = conv2d_bn ( branch7x7 , 128 , 1 , 7 ) batch_0 = data_gen [ 0 ] batch_size=batch_size , shuffle=shuffle , seed=seed , The total number of depthwise convolution output kernel_initializer='he_normal ' ) ( p1 ) padding='same ' , use_bias=False , self.x = np.asarray ( x , dtype=K.floatx ( ) ) elif input_shape is None and K.is_keras_tensor ( input_tensor ) : and ` y ` is a numpy array of corresponding labels . x = Dense ( classes , activation='softmax ' , if self.save_to_dir : the interval [ -1.0 , +1.0 ) . Example of using ` .flow ( x , y ) ` : transform_parameters.get ( 'zy ' , 1 ) , ` [ `` class1/file1.jpg '' , `` class1/file2.jpg '' , ... ] ` ) . dtype=K.floatx ( ) ) if not wi : x = Activation ( relu6 , name=prefix + 'expand_relu ' ) ( x ) self.stride , self.end_index + 1 ) , self.stride ) x3 = add ( [ x3_1 , x3_2 ] , name='reduction_add3_ % s ' % block_id ) model_name = 'mobilenet_ % s_ % d_tf.h5 ' % ( alpha_text , rows ) decode_predictions = nasnet.decode_predictions index_array : Array of sample indices to include in batch . 'densenet201_weights_tf_dim_ordering_tf_kernels.h5 ' , and ` name + '_bn ' ` for the batch norm layer . are repeated many times in this network . We use ` block_idx ` to identify pil_image = None classes = [ ] vect.append ( i ) This class takes in a sequence of data-points gathered at x = GlobalAveragePooling2D ( name='avg_pool ' ) ( x ) if not samplewise_center : x [ idx , -len ( trunc ) : ] = trunc DirectoryIterator = image.DirectoryIterator x = MaxPooling2D ( 3 , strides=2 ) ( x ) while with ` height_shift_range=1.0 ` possible values are floats in use_bias=False , kernel_initializer='he_normal ' ) ( h ) branch5x5 = conv2d_bn ( x , 48 , 1 , 1 ) ` avg ` means that global average pooling DenseNet169 = densenet.DenseNet169 Iterator = image.Iterator classes=classes , class_mode=class_mode , if not classes : DENSENET201_WEIGHT_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet201_weights_tf_dim_ordering_tf_kernels.h5 ' ` 'avg ' ` means that global average pooling MobileNet = mobilenet.MobileNet model = Model ( inputs , x , name='inception_v3 ' ) if self.std is not None : elif weights is not None : [ branch3x3 , branch7x7x3 , branch_pool ] , axis=channel_axis , name='mixed8 ' ) ` ( batch , new_rows , new_cols , filters ) ` if data_format='channels_last ' . bn_name_base = 'bn ' + str ( stage ) + block + '_branch ' rescale : rescaling factor . Defaults to None . [ Torch DenseNets ] seed = 1 DENSENET169_WEIGHT_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet169_weights_tf_dim_ordering_tf_kernels.h5 ' follow_links=False , samples , targets = self._empty_batch ( len ( rows ) ) bottlenecking features . It has a drastically lower h , w = x.shape [ row_axis ] , x.shape [ col_axis ] 'in your Keras config located at ~/.keras/keras.json . ' if scale : expansion=6 , block_id=16 ) layers at the top of the network . return batch_x # the generator loops indefinitely x = _inverted_res_block ( x , filters=32 , alpha=alpha , stride=2 , For each of these ` alpha ` values , weights for 5 different input image sizes if p is None : intensity_range : Transformation intensity . directory , self , split = ( validation_split , 1 ) for root , fname in valid_files : name='conv_dw_ % d ' % block_id ) ( x ) return pil_image.fromarray ( x [ : , : , 0 ] .astype ( 'uint8 ' ) , ' L ' ) `` `` '' Updates internal vocabulary based on a list of sequences . branch_0 = conv2d_bn ( x , 32 , 1 ) [ branch1x1 , branch5x5 , branch3x3dbl , branch_pool ] , file_hash='1c2de60ee40562448dbac34a0737e798 ' ) 'ImageNet weights can only be loaded with NASNetLarge ' x = BatchNormalization ( name=prefix + '_sepconv1_bn ' ) ( x ) Do note that the input image format for this model is different than for return Activation ( relu6 , name='conv_pw_ % d_relu ' % block_id ) ( x ) if idx > = len ( self ) : the filenames will be x = imgenhancer_Brightness = ImageEnhance.Brightness ( x ) Can easily be extended to include new transformations , self.filters = filters with K.name_scope ( 'block_4 ' ) : from .. layers import GlobalMaxPooling2D use_bias=True , # https : //github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py blocks : numbers of building blocks for the four dense layers . stem_block_filters=96 , bn_axis = 1 branch7x7dbl = conv2d_bn ( x , 128 , 1 , 1 ) `` , `` .join ( _PIL_INTERPOLATION_METHODS.keys ( ) ) ) ) BASE_WEIGHT_PATH = 'https : //github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/ ' ` directory ` 's parent ( e.g. , if ` directory ` is `` dataset/class1 '' , name=block_name ) ( [ x , up ] ) class_mode='binary ' ) str ( self.x.shape [ channels_axis ] ) + ' channels ) . ' ) for res in results : from .. utils.data_utils import get_file weights_path = get_file ( model_name , new_seq , new_label = [ ] , [ ] MobileNetV2 is a general architecture and can be used for multiple use cases . from .. models import Model You should set ` image_data_format='channels_last ' ` in your Keras config the number of distinct objects . return self._get_batches_of_transformed_samples ( index_array ) target size is different from that of the loaded image . TF_WEIGHTS_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5 ' x = BatchNormalization ( | [ mobilenet_v2_0.35_128 ] | 20 | 1.66 | 50.8 | 75.0 | block_type : ` 'block35 ' ` , ` 'block17 ' ` or ` 'block8 ' ` , determines x , p = _normal_a_cell ( x , p , filters * filter_multiplier , depth_multiplier=1 , split=split ) x = ZeroPadding2D ( padding= ( ( 3 , 3 ) , ( 3 , 3 ) ) ) ( img_input ) save_prefix : Str ( default : ` `` ` ) . if split : 'Found a pair with : len ( x [ 0 ] ) = % s , len ( x [ ? ] ) = % s ' % categorical : bool . if False , labels will be x = Activation ( 'relu ' , name='block2_sepconv2_act ' ) ( x ) shuffle , # or ( channel , height , width ) ` num_samples ` sequences ( lists of integers ) # Block 5 # Input shape return hashing_trick ( text , n , ' '' Adjusts the input ` previous path ` to match the shape of the ` input ` . self.reset ( ) from collections import OrderedDict for extension in white_list_formats : from keras_applications import vgg16 words = [ c [ 0 ] for c in couples ] split=split ) ( Shear angle in counter-clockwise direction in degrees ) elif subset == 'training ' : transform_parameters.get ( 'ty ' , 0 ) , ( except maybe the last one ) . input_shape = _obtain_input_shape ( from .. layers import SeparableConv2D block_id= ' % d ' % ( 2 * num_blocks + i + 1 ) ) With ` width_shift_range=2 ` possible values x = Conv2D ( pointwise_conv_filters , ( 1 , 1 ) , float : fraction of total height , if < 1 , or pixels if > = 1 . rounds=1 , random.shuffle ( labels ) from .. layers import Cropping2D num_files = len ( list ( A `` sequence '' is a list of integer word indices . and width and height should be no smaller than 197 . tx * = np.random.choice ( [ -1 , 1 ] ) fill_mode='nearest ' , will be included in the generator . from .. utils import conv_utils x_misc = [ np.asarray ( xx [ split_idx : ] ) for xx in x_misc ] p2 = Cropping2D ( cropping= ( ( 1 , 0 ) , ( 1 , 0 ) ) ) ( p2 ) raise ValueError ( ' ` sequences ` must be iterable . ' ) scale : scaling factor to scale the residuals ( i.e. , the output of preprocess_input = inception_resnet_v2.preprocess_input targets [ j ] = self.targets [ rows [ j ] ] def VGG16 ( include_top=True , weights='imagenet ' , residual = BatchNormalization ( ) ( residual ) ' and loading ` ImageNet ` weights , ' NASNET_LARGE_WEIGHT_PATH , raise ValueError ( 'Weights for `` channels_first '' format ' alpha : controls the width of the network . or invalid input shape or invalid depth_multiplier , alpha , ' ` featurewise_center ` , but it hasn\'t ' ( self.samples , self.num_classes ) ) all 16 models from the paper can be built , with ImageNet weights provided . axis=channel_axis , name='conv_dw_ % d_bn ' % block_id ) ( x ) x = Activation ( 'relu ' , name=prefix + '_sepconv2_act ' ) ( x ) char_level=False , indices = range ( rows [ j ] - self.length , rows [ j ] , self.sampling_rate ) float : fraction of total width , if < 1 , or pixels if > = 1 . channel_shift_intensity = None images ( if ` save_to_dir ` is set ) . if rows ! = cols or rows not in [ 128 , 160 , 192 , 224 ] : `` `` '' Converts a text to a sequence of words ( or tokens ) . image_datagen.fit ( images , augment=True , seed=seed ) If PIL version 1.1.3 or newer is installed , `` lanczos '' is also md5_hash='a268eb855778b3df3c7506639542a6af ' ) kernel_size=3 , Tests comparing this model to the existing Tensorflow model can be the 100 % MobileNet on various input sizes : VGG19 = vgg19.VGG19 if x.shape [ self.channel_axis ] not in { 1 , 3 , 4 } : # checking for consistency in the main loop below . split = ( 0 , validation_split ) WEIGHTS_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5 ' x = Activation ( relu6 , name='out_relu ' ) ( x ) branch3x3 = conv2d_bn ( branch3x3 , 320 , 3 , 3 , skip_reduction=True , raise ValueError ( ' ` zoom_range ` should be a tuple or list of two ' x = SeparableConv2D ( 2048 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block14_sepconv2 ' ) ( x ) # if the width multiplier is greater than 1 we 'following the data format convention `` ' x = Activation ( relu6 , name='conv_dw_ % d_relu ' % block_id ) ( x ) self.index_docs [ i ] = 1 channel_axis = 1 if K.image_data_format ( ) == 'channels_first ' else -1 'nasnet_large.h5 ' , include_top=True , ' ` brightness_range should be tuple or list of two floats . ' x : 2D numpy array , single image . dtype=K.floatx ( ) ) str ( self.channel_axis ) + ' . ' raise ValueError ( 'Specify a dimension ( num_words argument ) , ' # Mixed 7a ( Reduction-B block ) : 8 x 8 x 2080 def __getitem__ ( self , idx ) : def InceptionResNetV2 ( include_top=True , channel_images = [ ndi.interpolation.affine_transform ( mask_datagen.fit ( masks , augment=True , seed=seed ) new_label.append ( y ) follow_links=follow_links , x_misc = [ ] p2 = AveragePooling2D ( ( 1 , 1 ) , strides= ( 2 , 2 ) , padding='valid ' , channel_axis=img_channel_axis , padding='same ' , use_bias=False ) ( x ) epsilon=1e-3 , name='stem_bn1 ' ) ( x ) self.zoom_range = [ 1 - zoom_range , 1 + zoom_range ] The dictionary containing the mapping from class names to class ty = np.random.uniform ( -self.width_shift_range , height_shift_range : Float , 1-D array-like or int If tuple , the first element transform_parameters.get ( 'tx ' , 0 ) , elif self.class_mode == 'sparse ' : `` `` '' Adds an initial convolution layer ( with batch normalization and relu6 ) . block_idx : an ` int ` used for generating layer names . The Inception-ResNet blocks x = np.rollaxis ( x , 0 , channel_axis + 1 ) `` `` '' Instantiates the DenseNet architecture . x = layers.concatenate ( use_bias=False , name='Conv1 ' ) ( img_input ) one_hot = text.one_hot # Block 3 classes : a list of class indices ` 0 ` is a reserved index that wo n't be assigned to any word . if color_mode not in { 'rgb ' , 'grayscale ' } : stride : Period between successive output sequences . x = identity_block ( x , 3 , [ 256 , 256 , 1024 ] , stage=4 , block= ' b ' ) theta = np.random.uniform ( -rg , rg ) Numpy image tensor . x = identity_block ( x , 3 , [ 64 , 64 , 256 ] , stage=2 , block= ' b ' ) if zoom_range [ 0 ] == 1 and zoom_range [ 1 ] == 1 : texts : A list of texts ( strings ) . new_v = max ( min_value , int ( v + divisor / 2 ) // divisor * divisor ) the count of files with extension in ` white_list_formats ` contained in name=None ) : Any PNG , JPG , BMP , PPM or TIF images x [ i ] [ j ] = 1 weights_path = get_file ( fname , resample the image if the transform_matrix = zoom_matrix if transform_matrix is None else np.dot ( transform_matrix , zoom_matrix ) if not ( weights in { 'imagenet ' , None } or os.path.exists ( weights ) ) : save_format='png ' , inv_fq = rank * ( np.log ( rank ) + gamma ) + 0.5 - 1 . / ( 12 . * rank ) self.index_generator = self._flow_index ( ) channel_dim = 1 if K.image_data_format ( ) == 'channels_first ' else -1 np.random.seed ( seed ) self.batch_size = batch_size if j not in counts : window_size : Int , size of sampling windows ( technically half-window ) . num_row , # mixed 4 : 17 x 17 x 768 If not provided , the list of classes will be automatically strides= ( 2 , 2 ) , block_id=12 ) self.data_format = data_format hash=np.random.randint ( 1e4 ) , The inputs , normalized . weights : one of ` None ` ( random initialization ) , 2D tensor . hrg : Height shift range , as a float fraction of the height . data_format : Image data format , name=prefix + 'depthwise_BN ' ) ( x ) def __next__ ( self , * args , * * kwargs ) : `` `` '' Updates internal vocabulary based on a list of texts . flat_x = np.reshape ( if img.size ! = width_height_tuple : ` `` input '' ` : targets are images identical to input images ( mainly if self.data_format == 'channels_last ' : ( https : //arxiv.org/abs/1608.06993 ) ( CVPR 2017 Best Paper Award ) data : Indexable generator ( such as list or Numpy array ) sampling_table : 1D array of size ` vocabulary_size ` where the entry i from keras_preprocessing import text ( https : //github.com/taehoonlee/tensornets/blob/master/tensornets/densenets.py ) self.interpolation = interpolation format=self.save_format ) x : Numpy array of input data or tuple . oov_token : if given , it will be added to word_index and used to name=conv_name_base + ' 1 ' ) ( input_tensor ) # Make sure that round down does not go down by more than 10 % . from keras_applications import mobilenet elif len ( split ) == 1 : data_format=None , x = SeparableConv2D ( 728 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block4_sepconv2 ' ) ( x ) x = MaxPooling2D ( 3 , strides=2 , name='pool1 ' ) ( x ) if include_top : x2_2 = _separable_conv_block ( p , filters , ( 3 , 3 ) , if shear ! = 0 : augment : Boolean ( default : False ) . self.index_docs = { } | 0.25 MobileNet-224 | 50.6 % | 41 | 0.5 | if in_channels == pointwise_filters and stride == 1 : cache_subdir='models ' , prefix = 'expanded_conv_ ' x3_2 = _separable_conv_block ( p , filters , ( 5 , 5 ) , strides= ( 2 , 2 ) , raise ValueError ( 'The ` weights ` argument should be either ' ` ( samples , new_rows , new_cols , filters ) ` if data_format='channels_last ' . branch_0 = conv2d_bn ( x , 384 , 3 , strides=2 , padding='valid ' ) # Keeps under lock only the mechanism which advances require_flatten=include_top , input_shape : optional shape tuple , only to be specified branch7x7dbl = conv2d_bn ( x , 160 , 1 , 1 ) ( i.e . the number of output filters in the convolution ) . x = inputs expansion=6 , block_id=7 ) self.shear_range = shear_range seed : Random seed for data shuffling . has to be ` ( 299 , 299 , 3 ) ` . follow_links ) : self.shear_range , class_indices : dictionary mapping a class name to its index . def __init__ ( self , data , targets , length , NASNET_LARGE_WEIGHT_PATH_NO_TOP = 'https : //github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-large-no-top.h5 ' `` `` '' Utilities for real-time data augmentation on image data . self.document_count += len ( sequences ) transform_parameters.get ( 'zx ' , 1 ) , shape = ( 1 , 1 , int ( 1024 * alpha ) ) subset=None , return imagenet_utils.preprocess_input ( x , mode='tf ' ) ` None ` : no targets get yielded ( only input images are yielded ) . x = Dense ( 4096 , activation='relu ' , name='fc2 ' ) ( x ) directory : Absolute path to the directory branch_1 = conv2d_bn ( x , 256 , 1 ) i += len ( classes ) row_axis=0 , col_axis=1 , channel_axis=2 , a generator of strings ( for memory-efficiency ) , shape = ( int ( 1024 * alpha ) , 1 , 1 ) self.classes [ i : i + len ( classes ) ] = classes x = Conv2D ( filters , `` `` '' Inception V3 model for Keras . include_top : whether to include the fully-connected self.zca_whitening = zca_whitening raise NotImplementedError self.cval = cval will be applied to the output of the if self.featurewise_center : # Create model . ( word , random word from the vocabulary ) , with label 0 ( negative samples ) . input_tensor=None , rounds : Int ( default : 1 ) . x [ i ] [ j ] = tf * idf if validation_split and not 0 < validation_split < 1 : self.filters , x = Conv2D ( 128 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block2_conv1 ' ) ( x ) x = Activation ( 'relu ' , name='block1_conv2_act ' ) ( x ) min_size=221 , pad_sequences = sequence.pad_sequences ' ( width , height , channels ) . ' # here 's a more `` manual '' example stride , length of history , etc. , to produce batches for def fit_on_sequences ( self , sequences ) : epochs=50 ) preprocess_input = densenet.preprocess_input save_prefix=save_prefix , the output of the model will be a x = Activation ( 'relu ' , name=name + '_relu ' ) ( x ) def fit ( self , x , gamma = 0.577 return output [ 0 ] penultimate_filters ) apply_brightness_shift = image.apply_brightness_shift sampling_factor : The sampling factor in the word2vec formula . alpha : controls the width of the network . This is known as the channel_axis = 1 Determines the type of label arrays that are returned : x = DepthwiseConv2D ( kernel_size=3 , strides=stride , activation=None , batch_size=128 ) : ( a sequence is a list of integer word indices ) . p = Activation ( 'relu ' ) ( p ) ( os.path.join ( directory , subdir ) img_channel_axis = self.channel_axis - 1 ( https : //github.com/taehoonlee/tensornets/blob/master/tensornets/nasnets.py ) branch7x7x3 = conv2d_bn ( kernel_size : Kernel size of separable convolutions ' ` None ` ( random initialization ) , ` imagenet ` ' targets_shape.extend ( self.targets.shape [ 1 : ] ) if transform_matrix is not None : def _list_valid_filenames_in_directory ( directory , white_list_formats , split , shear_range : Float . Shear Intensity self.channel_shift_range = channel_shift_range in the output sequences . This is useful to reserve part of the def fit_on_texts ( self , texts ) : If tuple , the second elements is either # Reference implementation for the convolution and ` name + '_bn ' ` for the input_shape : optional shape tuple , to be specified if you would ip : Input tensor whose shape needs to be matched pool.join ( ) maketrans = string.maketrans return self self.image_shape = ( 3 , ) + self.target_size x5 = add ( [ x5_1 , x5_2 ] , name='reduction_add4_ % s ' % block_id ) while with ` width_shift_range=1.0 ` possible values are floats in def _remove_long_seq ( maxlen , seq , label ) : If ` alpha ` < 1.0 , proportionally decreases the number from . import imagenet_utils x = Conv2D ( 64 , ( 3 , 3 ) , use_bias=False , name='block1_conv2 ' ) ( x ) mask_generator = mask_datagen.flow_from_directory ( bn_axis = 1 if K.image_data_format ( ) == 'channels_first ' else 3 for root , _ , files in os.walk ( directory ) for f in files shuffle : Whether to shuffle the word couples before returning them . `` `` '' Performs a random rotation of a Numpy image tensor . assert np.array_equal ( x , 'zy ' : zy , for i in range ( 2 ) : def _depthwise_conv_block ( inputs , pointwise_conv_filters , alpha , x /= 128 . break if data_format not in { 'channels_first ' , 'channels_last ' } : # build batch of labels Inception-ResNet-C : ` block_type='block8 ' ` of RGB data , it should have value 3 . return transform_parameters x = Conv2D ( 512 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block5_conv2 ' ) ( x ) `` `` '' A building block for a dense block . grayscale=grayscale , branch7x7 = conv2d_bn ( branch7x7 , 192 , 7 , 1 ) for w in set ( seq ) : if mode == 'count ' : the VGG16 and ResNet models ( 299x299 instead of 224x224 ) , | [ mobilenet_v2_0.35_224 ] | 59 | 1.66 | 60.3 | 82.9 | if zca_whitening : if sample_weight is not None and len ( x ) ! = len ( sample_weight ) : expansion=6 , block_id=12 ) 'config located at ~/.keras/keras.json . ' return couples , labels `` `` '' Transforms each text in ` texts ` in a sequence of integers . self.filenames = [ ] which increases/decreases the number of filters in each layer . another numpy array or a list of numpy arrays , Tokenizer = text.Tokenizer all 22 models from the paper can be built , with ImageNet weights provided . x = Activation ( activation , name=ac_name ) ( x ) branch7x7 = conv2d_bn ( branch7x7 , 192 , 1 , 7 ) when ` fill_mode = `` constant '' ` . num_words = self.num_words couples += [ [ words [ i % len ( words ) ] , 'Found : x.shape = % s , y.shape = % s ' % _remove_long_seq = sequence._remove_long_seq # TODO : make it public ? def identity_block ( input_tensor , kernel_size , filters , stage , block ) : num_words = kwargs.pop ( 'nb_words ' ) or ` ( 3 , 299 , 299 ) ` ( with ` channels_first ` data format ) . wrg : Width shift range , as a float fraction of the width . x : Input Numpy array . subset=None , x = conv2d_bn ( x , 80 , 1 , 1 , padding='valid ' ) input_tensor=None , img_input = Input ( shape=input_shape ) from keras_applications import resnet50 `` `` '' Performs a random spatial zoom of a Numpy image tensor . transform_matrix = shear_matrix if transform_matrix is None else np.dot ( transform_matrix , shear_matrix ) block_type='block8 ' , order=1 , x = apply_affine_transform ( x , tx=tx , ty=ty , channel_axis=channel_axis , [ 0 , 0 , 1 ] ] ) start , stop = 0 , num_files file_hash='7bb75edd58cb43163be7e0005fbe95ef ' ) featurewise_center=False , one image ( Numpy tensor with rank 3 ) , sample_weight : Sample weights . # Use weighting scheme 2 in The sampling probabilities are generated according layers=layers , x5_1 = _separable_conv_block ( x1 , filters , ( 3 , 3 ) , [ 0 , np.cos ( shear ) , 0 ] , if self.preprocessing_function : # mixed 1 : 35 x 35 x 256 'The use of ` array_to_img ` requires PIL . ' ) return stop - start assert np.array_equal ( y , width_height_tuple = ( target_size [ 1 ] , target_size [ 0 ] ) ` ( samples , height , width , channels ) ` , `` `` '' Transforms each text in texts in a sequence of integers . preprocess_input = vgg19.preprocess_input return x , ip batch normalization and relu6 activation . if self.word_index : [ [ 1 ] , [ 3 ] , [ 5 ] , [ 7 ] , [ 9 ] ] ] ) ) if input_tensor is not None : block_id='stem_1 ' ) self.zoom_range [ 1 ] , name=conv_name_base + '2b ' ) ( x ) DENSENET169_WEIGHT_PATH_NO_TOP , return [ ( hash_function ( w ) % ( n - 1 ) + 1 ) for w in seq ] int : integer number of pixels from interval branch3x3_2 = conv2d_bn ( branch3x3 , 384 , 3 , 1 ) interpolation=interpolation ) def __init__ ( self , n , batch_size , shuffle , seed ) : oov_token=None , ty * = img_shape [ img_col_axis ] elif x.shape [ 2 ] == 1 : | 0.50 MobileNet-224 | 63.7 % | 149 | 1.3 | decode_predictions = mobilenet_v2.decode_predictions if weights == 'imagenet ' and include_top and classes ! = 1000 : 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 ' , col_axis : Index of axis for columns in the input image . x = Activation ( 'softmax ' , name='act_softmax ' ) ( x ) ' or NASNetMobile ' ) `` input '' will be images identical def Xception ( include_top=True , weights='imagenet ' , # x is a single image , so it does n't have image number at index 0 def _set_index_array ( self ) : 'Found non-iterable : ' + str ( x ) ) data_gen = TimeseriesGenerator ( data , targets , x = BatchNormalization ( name='block2_sepconv1_bn ' ) ( x ) assert len ( data_gen ) == 20 self.mean = None x = Activation ( 'relu ' , name=name ) ( x ) 'data format `` channels_first '' ( channels , width , height ) . ' List of integers in [ 1 , n ] . Each integer encodes a word ( unicity non-guaranteed ) . branch7x7 = conv2d_bn ( branch7x7 , 160 , 1 , 7 ) filter_multiplier=2 , Model naming and structure follows TF-slim implementation ( which has some additional x = Lambda ( lambda inputs , scale : inputs [ 0 ] + inputs [ 1 ] * scale , data = np.array ( [ [ i ] for i in range ( 50 ) ] ) self.rotation_range ) cache_subdir='models ' , Width Multiplier ( alpha ) | ImageNet Acc | Multiply-Adds ( M ) | Params ( M ) self.sample_weight = None kernel_size=1 , to be within ` [ 0 , 255 ] ` . x = _inverted_res_block ( x , filters=96 , alpha=alpha , stride=1 , split= ' ' ) : weights='imagenet ' , self.word_index [ self.oov_token ] = len ( self.word_index ) + 1 filters : list of integers , the filters of 3 conv layer at main path fname = 'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5 ' x = Conv2D ( filters1 , ( 1 , 1 ) , name=conv_name_base + '2a ' ) ( input_tensor ) 'data/images ' , 'shear ' : shear , def NASNetLarge ( input_shape=None , of numpy arrays ( in the case with translate_map = dict ( ( ord ( c ) , unicode ( split ) ) for c in filters ) save_prefix : String prefix to use for saving sample grayscale : Boolean , whether to load the image as grayscale . be applied . model.load_weights ( weights ) from .. applications.inception_v3 import preprocess_input x = Conv2D ( 512 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block4_conv4 ' ) ( x ) interpolation , x = _depthwise_conv_block ( x , 64 , alpha , depth_multiplier , block_id=1 ) 'wrap ' : abcdabcd|abcd|abcdabcd `` `` '' x = conv2d_bn ( x , 80 , 1 , padding='valid ' ) stage : integer , current stage label , used for generating layer names x1_2 = _separable_conv_block ( p , filters , ( 7 , 7 ) , strides= ( 2 , 2 ) , without any modifications . if ` True ` , labels will be categorical , e.g . to use as image input for the model . padding='same ' , include_top : Whether to include the fully-connected value : Float , padding value . x = _depthwise_conv_block ( x , 512 , alpha , depth_multiplier , block_id=9 ) img_input = Input ( tensor=input_tensor , shape=input_shape ) pip install keras_applications keras_preprocessing sequences : list of sequences self.mean = np.mean ( x , axis= ( 0 , self.row_axis , self.col_axis ) ) Every ` Iterator ` must implement the ` _get_batches_of_transformed_samples ` if transform_parameters.get ( 'channel_shift_intensity ' ) is not None : random.seed ( seed ) x5 = add ( [ x5 , h ] , name='normal_add_5_ % s ' % block_id ) prefix=self.save_prefix , def MobileNetV2 ( input_shape=None , strides=strides , else : continue # empty list/array was found ( the generator will only yield batches of image data , K.is_keras_tensor ( input_tensor ) file_hash='253f8cb515780f3b799900260a226db6 ' ) from .. layers import DepthwiseConv2D x = Conv2D ( filters2 , kernel_size , padding='same ' , Classification Checkpoint| MACs ( M ) | Parameters ( M ) | Top 1 Accuracy| Top 5 Accuracy text into either a sequence of integers ( each integer being the index x = Concatenate ( axis=bn_axis , name=name + '_concat ' ) ( [ x , x1 ] ) x = _inverted_res_block ( x , filters=32 , alpha=alpha , stride=1 , # These methods were only introduced in version 3.4.0 ( 2016 ) . import numpy as np x = BatchNormalization ( name=prefix + '_sepconv2_bn ' ) ( x ) data_format : String , one of ` channels_first ` , ` channels_last ` . vocabulary_size : Int , maximum possible word index + 1 weights_path = get_file ( padding : padding mode in ` Conv2D ` . Sheared Numpy image tensor . file_hash='9a0d58056eeedaa3f26cb7ebd46da564 ' ) self.lock = threading.Lock ( ) white_list_formats : Set of strings containing allowed extensions for filenames = [ ] name=prefix + 'expand ' ) ( x ) name='conv1/bn ' ) ( x ) # compute quantities required for featurewise normalization rotation_range : Int . Degree range for random rotations . import multiprocessing.pool self.word_docs [ w ] = 1 for c in filters : `` `` '' Iterator capable of reading images from a directory on disk . if len ( brightness_range ) ! = 2 : raise ValueError ( ' ` start_index+length= % i > end_index= % i ` ' save_to_dir=None , save_prefix= '' , save_format='png ' , subset=None ) : block : ' a ' , ' b ' ... , current block label , used for generating layer names ` ( batch , rows , cols , channels ) ` if data_format='channels_last ' . name=name + '_bn ' ) ( x ) `` `` '' MobileNet v1 models for Keras . lower=True , rows when weights='imagenet ' offset_matrix = np.array ( [ [ 1 , 0 , o_x ] , [ 0 , 1 , o_y ] , [ 0 , 0 , 1 ] ] ) self.y = None are provided ( 224 , 192 , 160 , 128 ) . # Yields branch_0 = conv2d_bn ( branch_0 , 384 , 3 , strides=2 , padding='valid ' ) E.g . require_flatten=False , and width and height should be no smaller than 32 . # - * - coding : utf-8 - * ip : Input tensor ` x ` name='adjust_conv_projection_ % s ' % block_id , input_shape=None , punctuation . Default : ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , includes expansion=6 , block_id=11 ) growth_rate : float , growth rate at dense layers . # The transformation of images is not under thread lock A Keras model instance . class_mode : Mode for yielding the targets : x = Activation ( 'relu ' , name='block4_sepconv2_act ' ) ( x ) dropout=1e-3 , raise ValueError ( 'Unknown data_format : ' , data_format ) ValueError : if invalid ` img ` or ` data_format ` is passed . if x.ndim ! = 4 : x = x [ : :-1 , ... ] expansion=6 , block_id=14 ) through as an output without any modifications . path : Path or file object . # Original Numpy array x has format ( height , width , channel ) validation_split : Float . Fraction of images reserved for validation x1_1 = _separable_conv_block ( h , filters , kernel_size= ( 5 , 5 ) , found at [ mobilenet_v2_keras ] ( https : //github.com/JonathanCMitchell/mobilenet_v2_keras ) # It can be seen here : y : Labels . self.word_counts [ w ] = 1 x = MaxPooling2D ( ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='same ' , name='block2_pool ' ) ( x ) kernel_size=1 , padding='same ' , use_bias=False , activation=None , target_size= ( 256 , 256 ) , color_mode='rgb ' , x = conv_block ( x , 3 , [ 64 , 64 , 256 ] , stage=2 , block= ' a ' , strides= ( 1 , 1 ) ) min_size=139 , 'been fit on any training data . Fit it ' if not seq : str ( input_shape ) + ' ` . ' ) residual = Conv2D ( 256 , ( 1 , 1 ) , strides= ( 2 , 2 ) , ' Input shape provided = % s ' % ( input_shape , ) ) x = np.asarray ( x ) .swapaxes ( axis , 0 ) # Classification block self.channel_shift_range ) pointwise_conv_filters = int ( pointwise_conv_filters * alpha ) elif alpha == 0.50 : class_mode=None , NASNET_MOBILE_WEIGHT_PATH , ` 'ty ' ` : Float . Shift in the y direction . seed : Random seed . x = BatchNormalization ( axis=bn_axis , epsilon=1.001e-5 , if ( isinstance ( input_shape , tuple ) and name='normal_right4_ % s ' % ( block_id ) ) ( p ) x3 = AveragePooling2D ( ( 3 , 3 ) , strides= ( 1 , 1 ) , padding='same ' , 'Received : % s ' % brightness_range ) the 10-th most frequently occurring token ) . 'depth multiplier must be 1 ' ) file_hash='bcbd6486424b2319ff4ef7d526e38f63 ' ) class ImageDataGenerator ( object ) : classes=1000 , model = Model ( inputs , x , name='densenet ' ) tx * = img_shape [ img_row_axis ] from .imagenet_utils import _obtain_input_shape vertical_flip : Boolean . Randomly flip inputs vertically . # Expand x4 = add ( [ x4_1 , x4_2 ] , name='normal_add_4_ % s ' % block_id ) ' ` zca_whitening ` , which overrides ' `` `` '' Generates random parameters for a transformation . self.sample_weight = np.asarray ( sample_weight ) elif blocks == [ 6 , 12 , 48 , 32 ] : else [ batch_x ] + batch_x_miscs , ) TF_WEIGHTS_PATH_NO_TOP , # RGB weights='imagenet ' , raise ValueError ( x = conv_block ( x , 3 , [ 128 , 128 , 512 ] , stage=3 , block= ' a ' ) 'resnet50_weights_tf_dim_ordering_tf_kernels.h5 ' , scale : Whether to rescale image values channel_shift_range : Float . Range for random channel shifts . x = identity_block ( x , 3 , [ 64 , 64 , 256 ] , stage=2 , block= ' c ' ) branch3x3dbl = conv2d_bn ( self.samples = sum ( pool.map ( function_partial , basic punctuation , tabs , and newlines . if data_format == 'channels_last ' : pool.apply_async ( _list_valid_filenames_in_directory , into a 2D Numpy array of shape ` ( num_samples , num_timesteps ) ` . x = SeparableConv2D ( filters , kernel_size , strides=strides , model = Model ( inputs , x , name='xception ' ) By altering the image size and ` alpha ` parameter , for ImageNet 2012 are provided . # Mixed 6a ( Reduction-A block ) : 17 x 17 x 1088 params = self.get_random_transform ( x.shape , seed ) conv_name = None Output tensor of block . img = load_img ( os.path.join ( self.directory , fname ) , ( only relevant if ` save_to_dir ` is set ) . Default : `` png '' . NASNET_LARGE_WEIGHT_PATH = 'https : //github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-large.h5 ' self.featurewise_center = True activation=None , x = np.stack ( channel_images , axis=0 ) row_axis : Index of axis for rows in the input image . filenames : the path of valid files in ` directory ` , relative from any function that takes in input a string and returns a int . self.word_docs [ w ] += 1 `` `` '' Saves an image stored as a Numpy array to a path or file object . x = _inverted_res_block ( x , filters=24 , alpha=alpha , stride=1 , if is_input_t_tensor : ` num_timesteps ` is either the ` maxlen ` argument if provided , epsilon=1e-3 , It should have same length as ` data ` . self.image_shape = self.target_size + ( 3 , ) self.image_data_generator = image_data_generator name=name + '_1_conv ' ) ( x1 ) do not match then we will throw an error . | NASNet-A ( 4 @ 1056 ) | 74.0 % | 91.6 % | 564 M | 5.3 | # Mixed 5b ( Inception-A block ) : 35 x 35 x 320 x = BatchNormalization ( name='block4_sepconv1_bn ' ) ( x ) for w in seq : branch_pool = conv2d_bn ( branch_pool , 192 , 1 , 1 ) zx , zy = np.random.uniform ( split= ' ' ) : x = conv2d_bn ( x , 64 , 3 , 3 ) index_array = self.index_array [ self.batch_size * idx : Weights obtained from the official TensorFlow repository found at labels = [ ] current_index = ( self.batch_index * self.batch_size ) % self.n self._set_index_array ( ) It should have exactly 3 inputs channels , seq = text NASNet models use the notation ` NASNet ( N @ P ) ` , where : length=len ( self ) ) ) include_top=True , ` maxlen ` , either at the beginning or at the end of the sequences . h = Activation ( 'relu ' ) ( ip ) classes , filenames = res.get ( ) transform_matrix , h , w ) y_test = np_utils.to_categorical ( y_test , num_classes ) Note that the input image format for this model is different than for is_input_t_tensor = K.is_keras_tensor ( input_tensor ) expansion=6 , block_id=15 ) def transition_block ( x , reduction , name ) : inputs : Input tensor of shape ` ( rows , cols , 3 ) ` This allows you to optionally specify a directory name='normal_concat_ % s ' % block_id ) # Determine proper input shape x = Conv2D ( 256 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block3_conv4 ' ) ( x ) sample_shape = tuple ( ) x = transition_block ( x , 0.5 , name='pool2 ' ) tx = np.random.choice ( self.height_shift_range ) branch_pool = MaxPooling2D ( 3 , strides=2 , padding='valid ' ) ( x ) name='conv1 ' ) ( x ) num_files = len ( list ( random.shuffle ( words ) `` `` '' Instantiates the Inception v3 architecture . model.fit ( x_batch , y_batch ) if alpha > 1.0 : model = Model ( inputs , x , name='densenet201 ' ) branch3x3 = conv2d_bn ( x , 384 , 3 , 3 , strides= ( 2 , 2 ) , padding='valid ' ) block_type='block35 ' , E.g . ` ( 224 , 224 , 3 ) ` would be one valid value . is_input_t_tensor = K.is_keras_tensor ( raise ValueError ( 'Invalid subset name : ' , subset , for seq in sequences : A 1D Numpy array of length ` size ` where the ith entry raise ValueError ( 'Invalid class_mode : ' , class_mode , def conv2d_bn ( x , kernel_size , target size is different from that of the loaded image . self.lower = lower scale=1. , save_to_dir : Optional directory where to save the pictures x = Conv2D ( pointwise_filters , `` `` '' Converts a list of sequences into a Numpy matrix . the output of this block will be ` x + scale * r ` . a non-word and will be skipped . sigma = np.dot ( flat_x.T , flat_x ) / flat_x.shape [ 0 ] last convolutional layer , and thus the yielded tuples are of the form ` ( x , y , sample_weight ) ` . def next ( self ) : x = _depthwise_conv_block ( x , 128 , alpha , depth_multiplier , train_generator , should contain the images and the second element 'either 1 , 3 or 4 channels on axis ' input_shape : Optional shape tuple , only to be specified return self.sequences_to_matrix ( sequences , mode=mode ) if K.image_data_format ( ) ! = 'channels_last ' : [ MobileNetV2 : Inverted Residuals and Linear Bottlenecks ] ( https : //arxiv.org/abs/1801.04381 ) warnings.warn ( 'NumpyArrayIterator is set to use the ' 'needs to be divisible by 24 . Current value : % d ' % labels.append ( 1 ) 'data/validation ' , hash_function = lambda w : int ( md5 ( w.encode ( ) ) .hexdigest ( ) , 16 ) x * = 255 If ` filter_multiplier ` < 1.0 , proportionally decreases the number strides=strides , decode_predictions = vgg19.decode_predictions kernel_size , x = apply_brightness_shift ( x , transform_parameters [ 'brightness ' ] ) elif p_shape [ channel_dim ] ! = filters : p1 = AveragePooling2D ( ( 1 , 1 ) , strides= ( 2 , 2 ) , padding='valid ' , idf = np.log ( 1 + self.document_count / raise ValueError ( 'Invalid color mode : ' , color_mode , return Activation ( relu6 , name='conv1_relu ' ) ( x ) Zoomed Numpy image tensor . i = self.word_index.get ( self.oov_token ) x = Activation ( 'relu ' ) ( x ) x = BatchNormalization ( axis=channel_dim , momentum=0.9997 , bn_name = None if self.height_shift_range : same as with ` height_shift_range= [ -1 , 0 , +1 ] ` , function is different from ` imagenet_utils.preprocess_input ( ) ` . x = self.image_data_generator.standardize ( x ) or ( 3 , 224 , 224 ) ( with ` channels_first ` data format ) . directory : absolute path to the directory x1 = Activation ( 'relu ' , name=name + '_1_relu ' ) ( x1 ) scale=0.2 , any ` dilation_rate ` value ! = 1 . `` `` '' Performs a brightness shift . if self.oov_token is not None : from functools import partial ' ` zca_whitening ` ' # Second , build an index of the images x = Conv2D ( filters , kernel , ( e.g . ` [ 'dogs ' , 'cats ' ] ` ) . Default : None . default_size : Specifies the default image size of the model save_format=save_format , from .. layers import Reshape `` `` '' Lists paths of files in ` subdir ` with extensions in ` white_list_formats ` . ( if ` save_to_dir ` is set ) . def array_to_img ( x , data_format=None , scale=True ) : model = Model ( inputs , x , name='vgg19 ' ) 'alpha can be one of ' # TODO Change path to v1.1 from .. layers import BatchNormalization A 3D Numpy array . color_mode : One of `` grayscale '' , `` rbg '' . Default : `` rgb '' . ValueError : In case of invalid values for ` truncating ` or ` padding ` , batch_size=2 ) 'vgg19_weights_tf_dim_ordering_tf_kernels.h5 ' , def DenseNet ( blocks , 1 , include_top=include_top , `` `` '' Gets a batch of transformed samples . import h5py ` `` sparse '' ` : integer targets , 'flip_vertical ' : flip_vertical , shear_matrix = np.array ( [ [ 1 , -np.sin ( shear ) , 0 ] , It should contain one subdirectory per class . img_row_axis = self.row_axis - 1 self.word_counts = OrderedDict ( ) ValueError : if invalid ` x ` or ` data_format ` is passed . 'h5py ' ] , A depthwise convolution block consists of a depthwise conv , # build batch of image data elif mode == 'freq ' : if self.index_array is None : x = BatchNormalization ( name='block4_sepconv2_bn ' ) ( x ) InceptionResNetV2 = inception_resnet_v2.InceptionResNetV2 self.num_words = num_words return self.apply_transform ( x , params ) of a collision is in relation to the dimension of the hashing space and if re.match ( r ' ( [ \w ] +\. ( ? : ' + ext + ' ) ) ' , f.lower ( ) ) ] and must be a key of ` class_indices ` . `` sparse '' will be 1D integer labels , white_list_formats : set of strings containing allowed extensions for [ branch3x3dbl_1 , branch3x3dbl_2 ] , axis=channel_axis ) Whether to fit on randomly augmented samples . x = Conv2D ( 512 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block5_conv3 ' ) ( x ) raise TypeError ( 'Unrecognized keyword arguments : ' + str ( kwargs ) ) wcounts.sort ( key=lambda x : x [ 1 ] , reverse=True ) zx , zy = 1 , 1 name='reduction_left3_ % s ' % block_id ) ( h ) reset_matrix = np.array ( [ [ 1 , 0 , -o_x ] , [ 0 , 1 , -o_y ] , [ 0 , 0 , 1 ] ] ) img = pil_image.open ( path ) branches = [ branch_0 , branch_1 ] file_hash='50662582284e4cf834ce40ab4dfa58c6 ' ) cols = input_tensor._keras_shape [ 2 ] h = Conv2D ( filters , ( 1 , 1 ) , strides= ( 1 , 1 ) , padding='same ' , x = layers.concatenate ( channels_axis = 3 if data_format == 'channels_last ' else 1 alpha=1.0 , | [ mobilenet_v2_1.0_192 ] | 221 | 3.47 | 70.7 | 90.1 | random_zoom = image.random_zoom mixed = Concatenate ( axis=channel_axis , name=block_name + '_mixed ' ) ( branches ) save_img = image.save_img x = BatchNormalization ( name='block14_sepconv1_bn ' ) ( x ) img = array_to_img ( batch_x [ i ] , self.data_format , scale=True ) if len ( x.shape ) == 3 : # take the sample shape from the first non empty sequence window_end = min ( len ( sequence ) , i + window_size + 1 ) width_shift_range=0. , # Load weights ` `` box '' ` and ` `` hamming '' ` are also supported . flip_horizontal = ( np.random.random ( ) < 0.5 ) * self.horizontal_flip if kwargs : x = identity_block ( x , 3 , [ 256 , 256 , 1024 ] , stage=4 , block= ' c ' ) categorical=False , sampling_table=None , seed=None ) : weights='imagenet ' , self.principal_components = ( u * s_inv ) .dot ( u.T ) block_id='reduction_right2_ % s ' % block_id ) lower=True , elif truncating == 'post ' : end_index=None , If ` alpha ` = 1 , default number of filters from the paper x = MaxPooling2D ( ( 2 , 2 ) , strides= ( 2 , 2 ) , name='block5_pool ' ) ( x ) if self.zoom_range [ 0 ] == 1 and self.zoom_range [ 1 ] == 1 : name=prefix + 'project ' ) ( x ) train_generator = zip ( image_generator , mask_generator ) theta = np.random.uniform ( random_shift = image.random_shift batch norm layer . self.save_format = save_format warnings.warn ( 'The Xception model is only available for the ' x -= self.mean from .. utils.data_utils import Sequence img.save ( path , format=file_format , * * kwargs ) tabs and line breaks , minus the ` ' ` character . # mixed 9 : 8 x 8 x 2048 different width factors . This allows different width models to reduce results.append ( def texts_to_sequences ( self , texts ) : x : Numpy array with shape ` ( len ( sequences ) , maxlen ) ` By convention , index 0 in the vocabulary is batch_size : Size of the batches of data ( default : 32 ) . self.vertical_flip = vertical_flip branch5x5 = conv2d_bn ( branch5x5 , 64 , 5 , 5 ) This function transforms a list of return x , ip seed : Random seed . random.shuffle ( couples ) `` `` '' Counts files with extension in ` white_list_formats ` contained in ` directory ` . skip_reduction=True , block_id='reduction_left1_ % s ' % block_id ) from .. engine import get_source_inputs def _normal_a_cell ( ip , p , filters , block_id=None ) : 'Got array with shape : ' + str ( x.shape ) ) skipgrams = sequence.skipgrams x , ( x.shape [ 0 ] , x.shape [ 1 ] * x.shape [ 2 ] * x.shape [ 3 ] ) ) 'h5py ' , for x , y in zip ( seq , label ) : for i in seq : ( np.asarray ( x ) .shape , np.asarray ( y ) .shape ) ) self.channel_axis = 1 width_shift_range=0.2 , branch_2 = conv2d_bn ( x , 32 , 1 ) rescale=1./255 , cval=0. , of ` directory ` for it to work correctly . WEIGHTS_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 ' directory : Path to the directory to read images from . rows = np.arange ( i , min ( i + self.batch_size * n : Integer , total number of samples in the dataset to loop over . if num_words and i > = num_words : considered to contain images from one class , def conv_block ( input_tensor , kernel_size , filters , stage , block , strides= ( 2 , 2 ) ) : x = GlobalAveragePooling2D ( ) ( x ) x = BatchNormalization ( name='block13_sepconv1_bn ' ) ( x ) according to the given mode : text : Input text ( string ) . max_x ) `` ` python white_list_formats = { 'png ' , 'jpg ' , 'jpeg ' , 'bmp ' , channel_images = [ x , y , self , reverse : Boolean : if ` true ` , timesteps in each output sample will be return [ i for i in seq if i ] ( before applying any other transformation ) . If ` y ` is None , only the numpy array ` x ` is returned . The data will be looped over ( in batches ) . padding='same ' , name=conv_name_base + '2b ' ) ( x ) name : name of the ops ; will become ` name + '_conv ' ` if self.samplewise_center : reverse=False , expansion=6 , block_id=9 ) x = identity_block ( x , 3 , [ 512 , 512 , 2048 ] , stage=5 , block= ' b ' ) lower=lower , | [ mobilenet_v2_1.4_224 ] | 582 | 6.06 | 75.0 | 92.5 | x5 = _separable_conv_block ( h , filters , apply_affine_transform = image.apply_affine_transform def standardize ( self , x ) : to the sampling distribution used in word2vec : `` `` '' branch_1 = conv2d_bn ( x , 32 , 1 ) 'relu6 ' : mobilenet.relu6 } ) [ Rethinking the Inception Architecture for Computer Vision ] ( http : //arxiv.org/abs/1512.00567 ) raise ValueError ( 'If imagenet weights are being loaded , ' name=name + '_conv ' ) ( x ) parameter count than the original MobileNet . x = BatchNormalization ( axis=bn_axis , name=bn_name_base + '2b ' ) ( x ) located at ~/.keras/keras.json . if not featurewise_center : 'but it hasn\'t ' 'should have rank 4 . You passed an array ' [ Inception-v4 , Inception-ResNet and the Impact of x = DepthwiseConv2D ( ( 3 , 3 ) , row_axis , col_axis = ( 1 , 2 ) kernel_initializer='he_normal ' ) ( img_input ) p = concatenate ( [ p1 , p2 ] , axis=channel_dim ) TF_WEIGHTS_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels.h5 ' 'for the input data format `` channels_last '' ' 'Found : x.shape = % s , sample_weight.shape = % s ' % name=name + '_0_bn ' ) ( x ) def NASNet ( input_shape=None , warnings.warn ( 'The NASNet family of models is only available ' 1-D array-like : random elements from the array . pool.close ( ) or tuple of ints ` ( img_height , img_width ) ` . if self.shuffle : else : from .. applications.imagenet_utils import decode_predictions horizontal_flip : Boolean . Randomly flip inputs horizontally . # If input_shape is not None , assume default size sequences = self.texts_to_sequences ( texts ) elif input_shape is None : sample_weight=sample_weight , seed=None ) : name='mixed0 ' ) channel_shift_intensity = np.random.uniform ( -self.channel_shift_range , indices can be obtained via the attribute ` class_indices ` . or instead draw them in chronological order . def texts_to_matrix ( self , texts , mode='binary ' ) : self.end_index = end_index x /= x_max x = x.reshape ( ( x.shape [ 0 ] , x.shape [ 1 ] , 1 ) ) self.start_index , self.end_index + 1 , size=self.batch_size ) def apply_affine_transform ( x , theta=0 , tx=0 , ty=0 , shear=0 , zx=1 , zy=1 , num_words = self.num_words last_block_filters = 1280 'keras_applications==1.0.1 ' , _PIL_INTERPOLATION_METHODS = { num_words : the maximum number of words to keep , based channel_axis = 1 if K.image_data_format ( ) == 'channels_first ' else 3 file_hash='d19885ff4a710c122648d3b5c3b684e4 ' ) random_channel_shift = image.random_channel_shift weights == 'imagenet ' ) : def save_img ( path , last convolutional layer . class TimeseriesGenerator ( Sequence ) : return x x = conv2d_bn ( img_input , 32 , 3 , strides=2 , padding='valid ' ) branch_pool = conv2d_bn ( branch_pool , 32 , 1 , 1 ) num_blocks=6 , name='adjust_avg_pool_1_ % s ' % block_id ) ( p ) def __init__ ( self , num_words=None , if data_format == 'channels_first ' : x = MaxPooling2D ( ( 2 , 2 ) , strides= ( 2 , 2 ) , name='block1_pool ' ) ( x ) # any potential predecessors of ` input_tensor ` . WEIGHTS_PATH_NO_TOP , None in input_shape and num_words = len ( self.word_index ) + 1 data_format=K.image_data_format ( ) , If PIL version 1.1.3 or newer is installed , ` `` lanczos '' ` is also 'to follow the `` channels_last '' data format . ' ) ` layers.Input ( ) ` ) self.reverse = reverse self.num_classes = len ( classes ) def _separable_conv_block ( ip , filters , `` `` '' ( https : //github.com/liuzhuang13/DenseNet/blob/master/models/densenet.lua ) x = apply_affine_transform ( x , transform_parameters.get ( 'theta ' , 0 ) , The data should be at 2D , and axis 0 is expected if K.image_data_format ( ) == 'channels_last ' : self.filenames += filenames split= ' ' , x = SeparableConv2D ( 728 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block4_sepconv1 ' ) ( x ) warnings.warn ( 'Using \'.tiff\ ' files with multiple bands ' from .. layers import Activation self.directory = directory datagen.fit ( x_train ) subset=None ) : batch_x = np.zeros ( is also different ( same as Inception V3 ) . self.index_docs [ self.word_index [ w ] ] = c branch3x3dbl = conv2d_bn ( branch3x3dbl , 96 , 3 , 3 ) for i , label in enumerate ( self.classes [ index_array ] ) : raise ValueError ( 'When specifying the input shape of a NASNet ' _iter_valid_files ( directory , white_list_formats , follow_links ) ) ) TimeseriesGenerator = sequence.TimeseriesGenerator f = sampling_factor * inv_fq self.zca_epsilon = zca_epsilon class Tokenizer ( object ) : classes.append ( class_indices [ dirname ] ) validation_steps=800 ) Optionally loads weights pre-trained on ImageNet . This model can Adjusted Keras tensor interpolation=self.interpolation ) Read more about Skipgram in this gnomic paper by Mikolov et al . : branch3x3 = conv2d_bn ( x , 384 , 1 , 1 ) for j , row in enumerate ( rows ) : from .. layers import MaxPooling2D of images with shape ` ( batch_size , * target_size , channels ) ` if self.rotation_range : `` `` '' Xception V1 model for Keras . obtaining state of the art performance on CIFAR-10 and ImageNet 2012 . from .. engine.base_layer import InputSpec return new_v p = _adjust_block ( p , ip , filters , block_id ) rows = input_shape [ row_axis ] ' ` featurewise_std_normalization ` , ' classes = [ ] expansion=6 , block_id=1 ) self.classes = classes return pil_image.fromarray ( x.astype ( 'uint8 ' ) , 'RGB ' ) x -= 1 . elif block_type == 'block8 ' : ( i.e . the number of output filters in the pointwise convolution ) . p : Input tensor which needs to be modified np.array ( [ [ 10 ] , [ 11 ] ] ) ) that gets passed to the output ( len ( index_array ) , ) + self.image_shape , save_format : Format to use for saving sample images file_hash='e693bd0210a403b3192acc6073ad2e96 ' ) try : 'and input_tensor : ' , input_tensor , are used at each layer . batch_size=32 , shuffle=False , sample_weight=None , strides= ( 1 , 1 ) , name='mixed2 ' ) `` `` '' Instantiates the VGG16 architecture . # Returns model.load_weights ( weights_path ) ResNet50 = resnet50.ResNet50 ' ; expected one of `` categorical '' , ' num_blocks=6 , 1.0 ( also called 100 % MobileNet ) , 0.35 , 0.5 , 0.75 , 1.0 , 1.3 , and 1.4 from scipy import linalg ip_shape = K.int_shape ( ip ) Sequences that are shorter than ` num_timesteps ` if self.x.ndim ! = 4 : if hasattr ( pil_image , 'LANCZOS ' ) : `` `` '' Applies the normalization configuration to a batch of inputs . # Ensure self.batch_index is 0 . length : Length of the output sequences ( in number of timesteps ) . pooling=pooling , target_size= ( 256 , 256 ) , color_mode='rgb ' , def apply_transform ( self , x , transform_parameters ) : Shifted Numpy image tensor . featurewise_std_normalization=True , from .. layers import Flatten ' ( pre-training on ImageNet ) , ' if penultimate_filters % 24 ! = 0 : the channels axis should have value 1 , and in case o_y = float ( y ) / 2 + 0.5 where ` x ` is a numpy array containing a batch ( 3 , 3 ) , strides= ( 1 , 1 ) , padding='same ' ) ( x ) self.col_axis = 2 name='normal_left4_ % s ' % ( block_id ) ) ( p ) to be the time dimension . if self.start_index > self.end_index : x = Dense ( classes , activation='softmax ' , name='fc1000 ' ) ( x ) min_size=71 , inputs = get_source_inputs ( input_tensor ) index_array = next ( self.index_generator ) directory : Path to the target directory . self.image_shape = self.target_size + ( 1 , ) filters=filters , indices , will be alphanumeric ) . Supported methods are `` nearest '' , `` bilinear '' , and `` bicubic '' . 'but got : ' + str ( block_type ) ) return classes , filenames # so it can be done in parallel return K.relu ( x , max_value=6 ) if rows is None : VGG16 = vgg16.VGG16 'brightness ' : brightness } brightness : Float . The new brightness value . return p channel_axis = 3 x = Conv2D ( first_block_filters , self.save_to_dir = save_to_dir if i is not None : shift_matrix = np.array ( [ [ 1 , 0 , tx ] , if transform_parameters.get ( 'flip_vertical ' , False ) : else : with K.name_scope ( 'block_2 ' ) : strides : strides in ` Conv2D ` . from .. layers import Conv2D default_size=None ) : return NumpyArrayIterator ( self.save_prefix = save_prefix % ( self.start_index , self.end_index ) ) x = SeparableConv2D ( 256 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block3_sepconv1 ' ) ( x ) `` `` '' Computes the internal data stats related to the data-dependent transformations , based on an array of sample data . 'However , it was passed an array with shape ' if len ( s ) > 0 : # check ` trunc ` has expected shape input_shape=None , x1 = add ( [ x1_1 , x1_2 ] , name='normal_add_1_ % s ' % block_id ) seed : Random seeding for data shuffling . x = SeparableConv2D ( 128 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block2_sepconv1 ' ) ( x ) if shuffle : samples_shape = [ num_rows , self.length // self.sampling_rate ] transform_parameters [ 'channel_shift_intensity ' ] , remove values from sequences larger than has to be ` ( 224 , 224 , 3 ) ` ( with ` channels_last ` data format ) scale=0.17 , x = np.zeros ( ( len ( sequences ) , num_words ) ) filters , ( num_row , num_col ) , pointwise_conv_filters = int ( filters * alpha ) # Raises self.index_array = np.random.permutation ( self.n ) 'is disallowed , as no part of the sequence ' sequence : A word sequence ( sentence ) , encoded as a list `` `` '' Instantiates the MobileNet architecture . self.index_array = np.arange ( self.n ) def _conv_block ( inputs , filters , alpha , kernel= ( 3 , 3 ) , strides= ( 1 , 1 ) ) : if subset == 'validation ' : depth_multiplier : The number of depthwise convolution output channels if depth_multiplier ! = 1 : batch_y [ i , label ] = 1 . name='normal_bn_1_ % s ' % block_id ) ( h ) pooling=None , min_size=197 , return apply_brightness_shift ( x , u ) # Output shape `` `` '' Applies a transformation to an image according to given parameters . window_size=4 , negative_samples=1. , shuffle=True , from keras.preprocessing.sequence import TimeseriesGenerator lower=lower , Preprocessed array . raise ValueError ( 'Fit the Tokenizer on some data ' batch_size : Int ( default : 32 ) . kernel_initializer='he_normal ' ) ( x ) 'vgg16_weights_tf_dim_ordering_tf_kernels.h5 ' , x = Conv2D ( filters1 , ( 1 , 1 ) , strides=strides , ty = 0 super ( DirectoryIterator , self ) .__init__ ( self.samples , from .. layers import Lambda weights_path = get_file ( 'xception_weights_tf_dim_ordering_tf_kernels.h5 ' , 'inception_v3_weights_tf_dim_ordering_tf_kernels.h5 ' , kernel_size= ( 3 , 3 ) , try : 'bicubic ' : pil_image.BICUBIC , x : Batch of inputs to be normalized . seed : Optional random seed for shuffling and transformations . ip : Input tensor `` ` hash_function=None , name : name of the ops ; will become ` name + '_ac ' ` for the activation brightness_range : Tuple of floats ; brightness range . epochs=50 , model = Model ( inputs , x , name='mobilenetv2_ % 0.2f_ % s ' % ( alpha , rows ) ) # Arguments if data_format is None : If you choose to include both input_tensor and input_shape then p = BatchNormalization ( axis=channel_dim , momentum=0.9997 , elif blocks == [ 6 , 12 , 32 , 32 ] : h = BatchNormalization ( axis=channel_dim , momentum=0.9997 , def ResNet50 ( include_top=True , weights='imagenet ' , from __future__ import absolute_import for each token could be binary , based on word count , based on tf-idf ... from keras_preprocessing import sequence ` 'block17 ' ` or ` 'block8 ' ` . ` 'shear ' ` : Float . Shear angle in degrees . width_shift_range : Float , 1-D array-like or int _iter_valid_files ( directory , white_list_formats , follow_links ) ) ) x * = self.rescale include_top : whether to include the 3 fully-connected # Project [ branch1x1 , branch3x3 , branch3x3dbl , branch_pool ] , text = text.translate ( translate_map ) ' '' Adds a Reduction cell for NASNet-A ( Fig . 4 in the paper ) . # Block 4 maketrans = str.maketrans # Arguments for subdir in sorted ( os.listdir ( directory ) ) : depth_multiplier=1 , strides= ( 1 , 1 ) , block_id=1 ) : The weights for all 16 models are obtained and translated x = Conv2D ( last_block_filters , with K.name_scope ( 'block_5 ' ) : branch_1 = conv2d_bn ( branch_1 , 288 , 3 , strides=2 , padding='valid ' ) self.fill_mode = fill_mode self.data_format + ' '' ( channels on axis ' for xx in x_misc : Used for generating the ` sampling_table ` argument for ` skipgrams ` . branch3x3dbl = conv2d_bn ( branch3x3dbl , 384 , 3 , 3 ) with K.name_scope ( 'reduction_A_block_ % s ' % block_id ) : self.color_mode = color_mode cols = input_shape [ 1 ] Value used for points outside the boundaries scale=0.1 , filters1 , filters2 , filters3 = filters x = x.reshape ( ( 1 , x.shape [ 0 ] , x.shape [ 1 ] ) ) 'However , it was passed an array with shape ' if block_id : def random_rotation ( x , rg , row_axis=1 , col_axis=2 , channel_axis=0 , raise ValueError ( 'If using ` weights ` as imagenet with ` include_top ` ' padding=padding , 'nearest ' : aaaaaaaa|abcd|dddddddd if type ( x [ 1 ] ) is not list : if old_data_format : output tensor for the block . def apply_channel_shift ( x , intensity , channel_axis=0 ) : x = _inverted_res_block ( x , filters=24 , alpha=alpha , stride=2 , shortcut = BatchNormalization ( axis=bn_axis , name=bn_name_base + ' 1 ' ) ( shortcut ) classes : Optional list of strings , names of subdirectories ` ( samples , channels , rows , cols ) ` if data_format='channels_first ' on word frequency . Only the most common ` num_words ` words will In case of grayscale data , `` `` '' Takes the path to a directory & generates batches of augmented data . `` `` '' Iterator yielding data from a Numpy array . block_id='normal_left5_ % s ' % block_id ) x = array_to_img ( x ) from .. utils import layer_utils grayscale = self.color_mode == 'grayscale ' of the words in a reference dataset ( e.g . 10 would encode min_x , max_x = np.min ( x ) , np.max ( x ) `` `` '' Instantiates the MobileNetV2 architecture . classes=1000 ) : additional inputs ) and ` y ` is a numpy array follow_links : Whether to follow symlinks inside filters=filters , if activation is not None : x = MaxPooling2D ( ( 2 , 2 ) , strides= ( 2 , 2 ) , name='block4_pool ' ) ( x ) If ` filter_multiplier ` > 1.0 , proportionally increases the number self.preprocessing_function = preprocessing_function i = self.word_index.get ( self.oov_token ) def random_channel_shift ( x , intensity_range , channel_axis=0 ) : to input images ( mainly used to work with autoencoders ) . file_format=None , | 1.0 MobileNet-128 | 64.4 % | 529 | 4.2 | if seed is None : Each subdirectory in this directory will be x = SeparableConv2D ( 728 , ( 3 , 3 ) , padding='same ' , use_bias=False , name=prefix + '_sepconv1 ' ) ( x ) 'Got array with shape : ' , x.shape ) sampling_rate : Period between successive individual timesteps 'input must have a static square shape ( one of ' 'However your settings specify the default ' x = Conv2D ( 32 , ( 3 , 3 ) , strides= ( 2 , 2 ) , use_bias=False , name='block1_conv1 ' ) ( img_input ) scale : Whether to rescale image values to be within ` [ 0 , 255 ] ` . If None , no labels are returned def _make_divisible ( v , divisor , min_value=None ) : if p is not None : ValueError : In case of invalid argument for ` weights ` , ` max ` means that global max pooling will targets_shape = [ num_rows ] batch_size=32 , shuffle=True , seed=None , in_channels = inputs._keras_shape [ -1 ] img.save ( os.path.join ( self.save_to_dir , fname ) ) branch_1 = conv2d_bn ( branch_1 , 256 , 3 ) or ` ( 3 , 224 , 224 ) ` ( with ` channels_first ` data format ) . if i not in self.index_docs : input_tensor : Optional Keras tensor ( i.e . output of conv_name = name + '_conv ' min_size=48 , output = ( batch_x if batch_x_miscs == [ ] lower=True , MobileNets support any input size greater branch_pool = conv2d_bn ( branch_pool , 64 , 1 , 1 ) x = BatchNormalization ( axis=bn_axis , scale=False , name=bn_name ) ( x ) 'Please verify your output . ' ) # in the different class subfolders . # we need to break the loop by hand because return ( self.end_index - self.start_index # load weights | 0.75 MobileNet-224 | 68.4 % | 325 | 2.6 | def sequences_to_matrix ( self , sequences , mode='binary ' ) : RuntimeError : If attempting to run this model with a if fname.lower ( ) .endswith ( ' . ' + extension ) : shortcut = Conv2D ( filters3 , ( 1 , 1 ) , strides=strides , if y is not None : x = BatchNormalization ( name='block14_sepconv2_bn ' ) ( x ) # If input_shape is None , infer shape from input_tensor # for x , y in data_gen.flow ( ... ) : channels will be equal to ` filters_in * depth_multiplier ` . def relu6 ( x ) : final_affine_matrix , # fits the model on batches with real-time data augmentation : [ TensorNets implementation ] elif mode == 'tfidf ' : # mixed 3 : 17 x 17 x 768 expansion=6 , block_id=5 ) x = SeparableConv2D ( 728 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block13_sepconv1 ' ) ( x ) ' '' Instantiates a Mobile NASNet model in ImageNet mode . preprocessing_function : function that will be implied on each input . if batches > = len ( x_train ) / 32 : This function transforms a sequence of word indexes ( list of integers ) | [ mobilenet_v2_0.5_224 ] | 97 | 1.95 | 65.4 | 86.4 | if self.zca_whitening : x = _depthwise_conv_block ( x , 512 , alpha , depth_multiplier , block_id=7 ) x = Flatten ( ) ( x ) self._set_index_array ( ) offering better performance . x = Conv2D ( 512 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block5_conv1 ' ) ( x ) if not self.num_words : expansion=6 , block_id=2 ) x = np.copy ( x ) input_tensor=None , filters : filters in ` Conv2D ` . hash_function = hash filters = penultimate_filters // 24 ax [ i + r * x.shape [ 0 ] ] = self.random_transform ( x [ i ] ) or invalid input shape . decode_predictions = resnet50.decode_predictions Prefix to use for filenames of saved pictures new_v += divisor if alpha == 1.0 : * * kwargs : Additional keyword arguments passed to ` PIL.Image.save ( ) ` . branch7x7 = conv2d_bn ( branch7x7 , 192 , 7 , 1 ) `` `` '' Converts a PIL Image instance to a Numpy array . of images in each directory . the output of the model will be a 2D tensor . raise ValueError ( 'Invalid subset name : ' , subset , while 1 : | [ mobilenet_v2_1.0_160 ] | 154 | 3.47 | 68.8 | 89.0 | the files to be counted . ( useful for visualizing what you are doing ) . supported . If PIL version 3.4.0 or newer is installed , another numpy array or a list of numpy arrays being yielded , in a viewable format . This is useful p_shape = K.int_shape ( p ) weights_path = get_file ( model_name , weigh_path , raise ValueError ( ' ` x ` ( images tensor ) and ` y ` ( labels ) ' Only top `` num_words '' most frequent words will be taken into account . branch7x7dbl = conv2d_bn ( branch7x7dbl , 128 , 1 , 7 ) if input_shape is None and input_tensor is not None : x = _depthwise_conv_block ( x , 1024 , alpha , depth_multiplier , block_id=13 ) interpolation : Interpolation method used to resample the image if the This is a wrapper to the ` hashing_trick ` function using ` hash ` as the x = MaxPooling2D ( ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='same ' , name='block13_pool ' ) ( x ) Yields individual sequences . final_offset = transform_matrix [ :2 , 2 ] block name used in the official TF-slim implementation ) : setattr ( DenseNet201 , '__doc__ ' , DenseNet.__doc__ ) each of the repetitions . For example , the first Inception-ResNet-A block if transform_parameters.get ( 'brightness ' ) is not None : zca_epsilon=1e-6 , # It ensures that all layers have a channel number that is divisible by 8 from keras_applications import mobilenet_v2 penultimate_filters=1056 , `` `` '' Performs a random spatial shift of a Numpy image tensor . height_shift_range=0.1 , channel_axis : Index of axis for channels in the input image . fname = self.filenames [ j ] horizontal_flip=True ) NASNET_MOBILE_WEIGHT_PATH = 'https : //github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile.h5 ' for s in sequences : ` [ [ 1,0 ] , [ 0,1 ] , [ 0,1 ] .. ] ` . bn_name = name + '_bn ' 'or fit on some text data first . ' ) file_hash='0962ca643bae20f9b6771cb844dca3b0 ' ) x , p = _reduction_a_cell ( x , p , filters // filter_multiplier , block_id='normal_left2_ % s ' % block_id ) else : name=block_name + '_conv ' ) ( with ` channels_last ` data format ) or branch_1 = conv2d_bn ( x , 192 , 1 ) 'do not meet the same shape requirements ' ) model_name = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_ ' + \ x = BatchNormalization ( name='block3_sepconv2_bn ' ) ( x ) save_format : One of `` png '' , `` jpeg '' x = np.stack ( channel_images , axis=0 ) are filled according to the given mode The weights for all 16 models are obtained and translated from the Tensorflow checkpoints x = np.rollaxis ( x , channel_axis , 0 ) x4 = add ( [ x2 , x4 ] ) ` 'brightness ' ` : Float . Brightness shift intensity . def __iter__ ( self ) : invalid input shape or invalid ` penultimate_filters ` value . raise ValueError ( 'Invalid data_format : ' , data_format ) `` `` '' Adds a Inception-ResNet block . x = apply_affine_transform ( x , shear=shear , channel_axis=channel_axis , return res continue if truncating == 'pre ' : split : tuple of floats ( e.g . ` ( 0.2 , 0.6 ) ` ) to only take into ValueError if ` brightness_range ` is n't a tuple . use_bias=False , name='adjust_conv_1_ % s ' % block_id , if np.isscalar ( zoom_range ) : def random_shift ( x , wrg , hrg , row_axis=1 , col_axis=2 , channel_axis=0 , subset : Subset of data ( ` `` training '' ` or ` `` validation '' ` ) if branch_1 = conv2d_bn ( branch_1 , 384 , 3 , strides=2 , padding='valid ' ) filters= ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , if subset not in { 'training ' , 'validation ' } : x [ i ] [ j ] = c / len ( seq ) cols = input_tensor._keras_shape [ 3 ] https : //github.com/tensorflow/models/tree/master/research/slim # pre-trained-models name=conv_name_base + '2a ' ) ( input_tensor ) def _inverted_res_block ( inputs , expansion , stride , alpha , filters , block_id ) : final_offset , raise ValueError ( 'Asked to retrieve element { idx } , ' default_size=default_size , containing files to be counted and should output a Numpy tensor with the same shape . img = img.convert ( 'RGB ' ) input_tensor=None , self.x_misc = x_misc 'nasnet_mobile_no_top.h5 ' , arguments= { 'scale ' : scale } , otherwise we multiply the data by the value provided img_dim = 2 if K.image_data_format ( ) == 'channels_first ' else -2 paper are used at each layer . Tuple of ( root , filename ) with extension in ` white_list_formats ` . save_to_dir : None or str ( default : None ) . class NumpyArrayIterator ( Iterator ) : mode : one of `` binary '' , `` count '' , `` tfidf '' , `` freq '' if self.class_mode == 'input ' : x : Sample data . Should have rank 4 . x = ZeroPadding2D ( padding= ( 1 , 1 ) , name='conv_pad_ % d ' % block_id ) ( inputs ) model = Model ( inputs , x , name='mobilenet_ % 0.2f_ % s ' % ( alpha , rows ) ) should have value 1 , and in case 'You should set ` image_data_format= '' channels_last '' ` in your Keras ' x = BatchNormalization ( axis=channel_axis , name='conv1_bn ' ) ( x ) x = identity_block ( x , 3 , [ 256 , 256 , 1024 ] , stage=4 , block= ' e ' ) BASE_WEIGHT_URL = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.7/ ' def __init__ ( self , x , y , image_data_generator , ( if ` fit_on_texts ` was never called ) . if not wj : valid_files = _iter_valid_files ( transform_matrix = None wj = sequence [ j ] counts [ j ] = 1 . x = ZeroPadding2D ( padding= ( 1 , 1 ) , name='conv1_pad ' ) ( inputs ) decode_predictions = inception_v3.decode_predictions The transformed version of the input . random_rotation = image.random_rotation shear = np.random.uniform ( -intensity , intensity ) Output tensor for the block . train_generator = train_datagen.flow_from_directory ( use_bias=False , only be used with the data format ` ( width , height , channels ) ` . if len ( x ) ! = len ( xx ) : expansion=1 , block_id=0 ) self.subset = subset | -- -- -- -- -- -- | -- -- -- -- -- -- -- -| -- -- -- -- -| -- -- | shuffle , x = Conv2D ( 512 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block4_conv2 ' ) ( x ) `` `` '' VGG19 model for Keras . branch_1 = conv2d_bn ( branch_1 , 160 , [ 1 , 7 ] ) self.samplewise_center = True Each item in texts can also be a list , in which case we assume each item of that list ` validation_split ` is set in ` ImageDataGenerator ` . branch7x7dbl = conv2d_bn ( branch7x7dbl , 192 , 1 , 7 ) Can be a single integer to specify the same value for DENSENET169_WEIGHT_PATH , p = None hash_function=hash , self.index_docs [ i ] += 1 branch7x7x3 = conv2d_bn ( branch7x7x3 , 192 , 1 , 7 ) if not hasattr ( sequences , '__len__ ' ) : zca_whitening : Boolean . Apply ZCA whitening . x = Reshape ( ( classes , ) , name='reshape_2 ' ) ( x ) DenseNet201 = densenet.DenseNet201 If a float , ` [ lower , upper ] = [ 1-zoom_range , 1+zoom_range ] ` . zoom_range=0.2 ) 'should have the same length . ' [ Deep Residual Learning for Image Recognition ] ( https : //arxiv.org/abs/1512.03385 ) brightness = None img = img.convert ( ' L ' ) x = Conv2D ( # no alpha applied to last conv as stated in the paper : def random_zoom ( x , zoom_range , row_axis=1 , col_axis=2 , channel_axis=0 , new preprocessing methods , etc ... Only words known by the tokenizer will be taken into account . x = Conv2D ( 256 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block3_conv3 ' ) ( x ) block_id='normal_right1_ % s ' % block_id ) model.load_weights ( weights_path ) containing images from each class ( e.g . ` [ `` dogs '' , `` cats '' ] ` ) . branch_1 = conv2d_bn ( branch_1 , 192 , [ 7 , 1 ] ) filters : Integer , the dimensionality of the output space x = conv_block ( x , 3 , [ 512 , 512 , 2048 ] , stage=5 , block= ' a ' ) if self.x.shape [ channels_axis ] not in { 1 , 3 , 4 } : encodes the probability to sample a word of rank i . self.length = length branch3x3 = conv2d_bn ( x , 192 , 1 , 1 ) raise ValueError ( 'Truncating type `` % s '' ' except ImportError : strides : Strides for the first conv layer in the block . with K.name_scope ( 'block_3 ' ) : x = Activation ( 'relu ' , name='block1_conv1_act ' ) ( x ) ' ( 192 , 192 ) , or ( 224 , 224 ) ) . ' kernel : An integer or tuple/list of 2 integers , specifying the x = Flatten ( name='flatten ' ) ( x ) whitex = np.dot ( flatx , self.principal_components ) if blocks == [ 6 , 12 , 24 , 16 ] : self.image_shape = ( 1 , ) + self.target_size featurewise_center : Boolean . x = SeparableConv2D ( 256 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block3_sepconv2 ' ) ( x ) # Ensure that the model takes into account in reverse chronological order . start_index : Data points earlier than ` start_index ` will not be used self.index_array = None x_max = np.max ( x ) Xception = xception.Xception 'bilinear ' : pil_image.BILINEAR , p2 = ZeroPadding2D ( padding= ( ( 0 , 1 ) , ( 0 , 1 ) ) ) ( p ) start_index=0 , activation : activation in ` Conv2D ` . It defaults to the ` image_data_format ` value found in your ( i.e. , `` linear '' activation : ` a ( x ) = x ` ) . blocks : integer , the number of building blocks . file_hash='64373286793e3c8b2b4e3219cbf3544b ' ) import numpy as np 'will cause distortion . ' counts = { } raise ValueError ( 'If using ` weights ` as ImageNet with ` include_top ` ' ' ( one of ( 96 , 96 ) , ( 128 , 128 ) , ( 160 , 160 ) , ' the number of multiply-adds and thereby By default , ` `` nearest '' ` is used . num_col : width of the convolution kernel . y = y [ : split_idx ] reduce inference cost on mobile devices . fill_mode=fill_mode , cval=cval ) x = layers.add ( [ x , input_tensor ] ) 'keras_preprocessing==1.0.1 ' ] , seq : List of lists , where each sublist is a sequence . x = img_to_array ( x ) def MobileNet ( input_shape=None , shuffle : Whether to shuffle the data ( default : True ) 'densenet121_weights_tf_dim_ordering_tf_kernels.h5 ' , 'before using tfidf mode . ' ) elif blocks == [ 6 , 12 , 32 , 32 ] : is by default ` ( 331 , 331 , 3 ) ` for NASNetLarge and else : raise ValueError ( 'Unknown Inception-ResNet block type . ' x = SeparableConv2D ( filters , kernel_size , depth_multiplier=depth_multiplier , x = BatchNormalization ( name='block2_sepconv2_bn ' ) ( x ) if ( type ( x ) is tuple ) or ( type ( x ) is list ) : import random for j , c in list ( counts.items ( ) ) : expansion=6 , block_id=10 ) ` ( samples , rows , cols , channels ) ` if data_format='channels_last ' . to infer input_shape from an input_tensor . ( https : //arxiv.org/abs/1707.07012 ) ' ; expected `` training '' or `` validation '' . ' ) 'Input shape provided = % s ' % ( input_shape , ) ) pooling , classes ) x , p0 = _reduction_a_cell ( x , p , filters * filter_multiplier , `` `` '' For python 2.x . x = AveragePooling2D ( ( 7 , 7 ) , name='avg_pool ' ) ( x ) maxlen = np.max ( lengths ) x = SeparableConv2D ( 728 , ( 3 , 3 ) , padding='same ' , use_bias=False , name=prefix + '_sepconv3 ' ) ( x ) file_hash='d81d89dc07e6e56530c4e77faddd61b5 ' ) Sequences longer than ` num_timesteps ` are truncated self.stride = stride x = MaxPooling2D ( ( 2 , 2 ) , strides= ( 2 , 2 ) , name='block3_pool ' ) ( x ) branch1x1 = conv2d_bn ( x , 192 , 1 , 1 ) applied , for debugging purposes . 4D tensor with shape : `` `` '' A dense block . x = SeparableConv2D ( 128 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block2_sepconv2 ' ) ( x ) filters : a string where each element is a character that will be def conv_block ( x , growth_rate , name ) : if self.sample_weight is not None : theta = np.deg2rad ( theta ) branches = [ branch_0 , branch_1 , branch_pool ] subset=subset ) class_indices , follow_links ) : Supported methods are ` `` nearest '' ` , ` `` bilinear '' ` , except ValueError : model = Model ( inputs , x , name='vgg16 ' ) window_start = max ( 0 , i - window_size ) num_col , return samples , targets By default , all punctuation is removed , turning the texts into `` `` '' Converts a text to a sequence of indexes in a fixed-size hashing space . Required before using ` sequences_to_matrix ` featurewise_std_normalization : Boolean . under ` directory ` , where each subdirectory will return DenseNet ( [ 6 , 12 , 24 , 16 ] , from .. import backend self.class_indices = dict ( zip ( classes , range ( len ( classes ) ) ) ) use_bias=False , x = Conv2D ( filters3 , ( 1 , 1 ) , name=conv_name_base + '2c ' ) ( x ) with K.name_scope ( 'adjust_reduction_block_ % s ' % block_id ) : ValueError : if ` zoom_range ` is n't a tuple . zx : Zoom in x direction . targets = np.array ( [ [ i ] for i in range ( 50 ) ] ) sorted_voc = [ wc [ 0 ] for wc in wcounts ] label : List where each element is an integer . # Raises when ` include_top ` is ` False ` . model.fit_generator ( | [ mobilenet_v2_0.5_128 ] | 32 | 1.95 | 57.7 | 80.8 | return np.empty ( samples_shape ) , np.empty ( targets_shape ) if hasattr ( pil_image , 'HAMMING ' ) : shuffle : Boolean ( default : True ) . interpolation='nearest ' ) : np.clip ( x_channel + intensity , ` 'zx ' ` : Float . Zoom in the x direction . shear : Shear angle in degrees . filters= ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , padding='valid ' , directory : absolute path to a directory containing the files to list . if y is not None : img_input = input_tensor if seed is not None : name='normal_conv_1_ % s ' % block_id , x1 = Conv2D ( growth_rate , 3 , padding='same ' , use_bias=False , for e in range ( epochs ) : elif hash_function == 'md5 ' : num_negative_samples = int ( len ( labels ) * negative_samples ) input_tensor=input_tensor , training/validation . block_id='normal_left1_ % s ' % block_id ) fill_mode=self.fill_mode , cval=self.cval ) sample_weight : Numpy array of sample weights . the word i-th most common word in a dataset theta : Rotation angle in degrees . | [ mobilenet_v2_0.5_96 ] | 18 | 1.95 | 51.2 | 75.8 | np.array ( [ [ [ 0 ] , [ 2 ] , [ 4 ] , [ 6 ] , [ 8 ] ] , ' Weights for input shape ' md5_hash='a7b3fe01876f51b976af0dea6bc144eb ' ) and that the input preprocessing function is also different ( same as Xception ) . old_data_format = None classes : Optional number of classes to classify images Here we consider NASNet-A , the highest performance model that was found x = Activation ( 'relu ' , name='conv1/relu ' ) ( x ) 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5 ' , x = x [ 0 ] 'Invalid interpolation method { } specified . Supported ' ( len ( x ) , len ( xx ) ) ) name='mixed7 ' ) The dimensions to which all images found will be resized . res.append ( vect ) the directory . branch7x7dbl = conv2d_bn ( branch7x7dbl , 192 , 7 , 1 ) [ TF Slim Implementation ] | NASNet-A ( 6 @ 4032 ) | 82.7 % | 96.2 % | 23.8 B | 88.9 | weights_path = get_file ( x = identity_block ( x , 3 , [ 128 , 128 , 512 ] , stage=3 , block= ' b ' ) objects ` relu6 ` and pass them to the ` custom_objects ` parameter . name='reduction_conv_1_ % s ' % block_id , # Reference paper block_id='reduce_ % d ' % ( 2 * num_blocks ) ) batch normalization , relu6 , pointwise convolution , 'densenet169_weights_tf_dim_ordering_tf_kernels.h5 ' , pooling=None , ` model.evaluate_generator ( ) ` , etc . ) . `` `` '' MobileNet v2 models for Keras . stem_block_filters=96 , conv_name_base = 'res ' + str ( stage ) + block + '_branch ' cols = input_shape [ col_axis ] ` rows ` and ` cols ` values might have changed due to stride . # we create two instances with the same arguments if blocks == [ 6 , 12 , 24 , 16 ] : self.batch_size * ( idx + 1 ) ] raise ValueError ( 'input_tensor specified : ' , input_tensor , branch1x1 = conv2d_bn ( x , 192 , 1 , 1 ) engine=engine , def InceptionV3 ( include_top=True , u , s , _ = linalg.svd ( sigma ) fill_mode='nearest ' , cval=0 . ) : pad either before or after each sequence . str ( x.shape ) + ' ( ' + str ( x.shape [ self.channel_axis ] ) self.total_batches_seen += 1 `` `` '' Base class for image data iterators . if categorical : def preprocess_input ( x , data_format=None ) : x = apply_affine_transform ( x , theta=theta , channel_axis=channel_axis , If None or 0 , no rescaling is applied , x = BatchNormalization ( axis=bn_axis , scale=False , name=bn_name ) ( x ) branch_1 = conv2d_bn ( x , 128 , 1 ) o_x = float ( x ) / 2 + 0.5 classes=None , class_mode='categorical ' , if not use_bias : if np.max ( self.width_shift_range ) < 1 : decode_predictions = inception_resnet_v2.decode_predictions x : Input tensor . Must be 3D . use_bias=False , kernel_initializer='he_normal ' ) ( p ) def random_brightness ( x , brightness_range ) : x = np.asarray ( x , dtype=K.floatx ( ) ) 1 for same number as positive samples . flip_vertical = ( np.random.random ( ) < 0.5 ) * self.vertical_flip residual = Conv2D ( 1024 , ( 1 , 1 ) , strides= ( 2 , 2 ) , if not hasattr ( x , '__len__ ' ) : Only required if ` featurewise_center ` or # Block 2 ' '' 'The model being returned right now will expect inputs ' x = inception_resnet_block ( x , if min_value is None : cval=cval ) for x_channel in x ] for i in range ( x.shape [ 0 ] ) : key=lambda x : x [ 0 ] ) dtype : Type of the output sequences . x = Activation ( 'relu ' , name=prefix + '_sepconv3_act ' ) ( x ) pooling : Optional pooling mode for feature extraction default_size=299 , of RGB data , it should have value 3 . if input_shape is not None and input_tensor is not None : x3_1 = AveragePooling2D ( ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='same ' , return apply_channel_shift ( x , intensity , channel_axis=channel_axis ) trunc = np.asarray ( trunc , dtype=dtype ) x = Activation ( 'relu ' , name=prefix + '_sepconv1_act ' ) ( x ) x = Activation ( 'relu ' , name='block13_sepconv1_act ' ) ( x ) backend that does not support separable convolutions . model_name = 'mobilenet_ % s_ % d_tf_no_top.h5 ' % ( alpha_text , rows ) return DirectoryIterator ( pooling : optional pooling mode for feature extraction tf = 1 + np.log ( c ) K.set_image_data_format ( 'channels_last ' ) | [ mobilenet_v2_0.75_128 ] | 69 | 2.61 | 63.2 | 85.3 | str ( alpha ) + ' _ ' + str ( rows ) + '_no_top ' + '.h5 ' validation_split is set in ImageDataGenerator . self.horizontal_flip = horizontal_flip ` p ( word ) = min ( 1 , sqrt ( word_frequency / sampling_factor ) / ( word_frequency / sampling_factor ) ) ` rank [ 0 ] = 1 Pre-padding is the default . lower : boolean . Whether to convert the texts to lowercase . return ( self.n + self.batch_size - 1 ) // self.batch_size # round up import string import keras_applications Default is 'nearest ' . or alternatively you could specify class subdirectories This class allows to vectorize a text corpus , by turning each `` `` '' A transition block . from .. import regularizers include_top=True , x2 = add ( [ x2_1 , x2_2 ] , name='normal_add_2_ % s ' % block_id ) from .. layers import ZeroPadding2D x , steps_per_epoch=2000 , [ Densely Connected Convolutional Networks ] intensity : Transformation intensity . x4 = AveragePooling2D ( ( 3 , 3 ) , strides= ( 1 , 1 ) , padding='same ' , input_tensor : optional Keras tensor ( i.e . output of ` layers.Input ( ) ` ) 'with shape ' , self.x.shape ) elif self.class_mode == 'binary ' : def _recursive_list ( subpath ) : str ( alpha ) + ' _ ' + str ( rows ) + '.h5 ' i = self.word_index.get ( w ) def text_to_word_sequence ( text , if 'nb_words ' in kwargs : branch_2 = conv2d_bn ( x , 256 , 1 ) # increase the number of output channels which is useful to use with ` model.predict_generator ( ) ` , warnings.warn ( 'The MobileNet family of models is only available ' ` ( 224 , 224 , 3 ) ` for NASNetMobile . def DenseNet201 ( include_top=True , x1 = Conv2D ( 4 * growth_rate , 1 , use_bias=False , self.rotation_range = rotation_range from __future__ import print_function ' '' Adds a Normal cell for NASNet-A ( Fig . 4 in the paper ) . alpha=1.0 , warnings.warn ( 'This ImageDataGenerator specifies ' text_to_word_sequence = text.text_to_word_sequence The below table describes the performance on ImageNet 2012 : so that they fit the desired length . for i in range ( num_negative_samples ) ] self.char_level = char_level [ branch3x3_1 , branch3x3_2 ] , axis=channel_axis , name='mixed9_ ' + str ( i ) ) Set input mean to 0 over the dataset , feature-wise . `` `` '' Generates skipgram word pairs . for i in range ( blocks ) : if maxlen is None : filters = int ( filters * alpha ) skip_reduction : Whether to skip the reduction step at the tail DENSENET169_WEIGHT_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5 ' depth_multiplier=1 , if padding == 'post ' : cols = input_shape [ 2 ] E.g . ` ( 160 , 160 , 3 ) ` would be one valid value . # but target PIL image has format ( width , height , channel ) 'zx ' : zx , are integers ` [ -1 , 0 , +1 ] ` , [ Learning Transferable Architectures for Scalable Image Recognition ] samplewise_std_normalization=False , block_id='reduction_right1_ % s ' % block_id ) use_bias=False , x = _inverted_res_block ( x , filters=64 , alpha=alpha , stride=2 , random_shear = image.random_shear ac_name = None if name is None else name + '_ac ' translate_map = maketrans ( translate_dict ) A list of words ( or tokens ) . self.start_index = start_index + length if ` include_top ` is False ( otherwise the input shape class_mode : One of `` categorical '' , `` binary '' , `` sparse '' , BASE_WEIGHT_URL + fname , self.zoom_range = zoom_range It should have exactly 3 inputs channels ( 224 , 224 , 3 ) . ` labels ` are either 0 or 1 . the network structure in the residual branch . if self.seed is not None : def random_shear ( x , intensity , row_axis=1 , col_axis=2 , channel_axis=0 , save_prefix : Str . Prefix to use for filenames of saved pictures the RGB values from [ 0 , 255 ] to [ -1 , 1 ] . Note that this preprocessing self.brightness_range [ 1 ] ) wcounts = list ( self.word_counts.items ( ) ) transform_matrix = rotation_matrix rows = 224 `` `` '' Utility class for generating batches of temporal data . default_size=224 ) axis=channel_axis , seed ) x = self.image_data_generator.apply_transform ( containing consecutive data points ( timesteps ) . ' ( channels on axis ' + str ( channels_axis ) if block_type == 'block35 ' : x = x.swapaxes ( 0 , axis ) x -= np.mean ( x , keepdims=True ) fill_mode='nearest ' , cval=0 . ) : x = Activation ( 'relu ' ) ( ip ) x = _inverted_res_block ( x , filters=64 , alpha=alpha , stride=1 , 'input must have a static square shape ' 'ppm ' , 'tif ' , 'tiff ' } self.total_batches_seen = 0 input_tensor : input tensor The window of a word ` w_i ` will be [ Xception : Deep Learning with Depthwise Separable Convolutions ] ( https : //arxiv.org/abs/1610.02357 ) kernel_initializer='he_normal ' ) ( p2 ) [ branch1x1 , branch7x7 , branch7x7dbl , branch_pool ] , channel_axis : Index of axis for channels in the input tensor . x = layers.add ( [ x , shortcut ] ) InceptionV3 = inception_v3.InceptionV3 row_axis : Index of axis for rows in the input tensor . will be the 4D tensor output of the And the shortcut should have strides= ( 2 , 2 ) as well This file contains building code for MobileNetV2 , based on # mixed 0 , 1 , 2 : 35 x 35 x 256 name=prefix + 'project_BN ' ) ( x ) num_samples = len ( sequences ) with K.name_scope ( 'adjust_block ' ) : x = conv2d_bn ( x , 64 , 3 ) x = identity_block ( x , 3 , [ 128 , 128 , 512 ] , stage=3 , block='d ' ) if K.image_data_format == 'channels_first ' : random_brightness = image.random_brightness mask_datagen = ImageDataGenerator ( * * data_gen_args ) ' channels ) . ' ) 'which overrides setting of ' ` data [ i ] ` , ` data [ i-r ] ` , ... ` data [ i - length ] ` x = dense_block ( x , blocks [ 0 ] , name='conv2 ' ) samplewise_center=False , fill_mode : Points outside the boundaries of the input `` hamming '' are also supported . By default , `` nearest '' is used . class subdirectories ( default : False ) . 'Expected input to be images ( as Numpy array ) ' x = Activation ( 'relu ' , name='block14_sepconv1_act ' ) ( x ) # This method is new in version 1.1.3 ( 2013 ) . inside each of the subdirectories directory tree hash_function : defaults to python ` hash ` function , can be 'md5 ' or filters , method . strides= ( 2 , 2 ) , block_id=6 ) batch_size=32 , shuffle=True , seed=None , x = np.rollaxis ( x , channel_axis , 0 ) | [ mobilenet_v2_0.35_160 ] | 30 | 1.66 | 55.7 | 79.1 | rescale=None , It should have exactly 3 input channels , The function should take one argument : if input_tensor._keras_shape [ 2 ] ! = input_shape [ 1 ] : 'nasnet_mobile.h5 ' , of word indices ( integers ) . If using a ` sampling_table ` , has to be ` ( 331 , 331 , 3 ) ` for NASNetLarge . 'data format `` channels_first '' ( channels , width , height ) . ' if pil_image is not None : if class_mode not in { 'categorical ' , 'binary ' , 'sparse ' , if sys.version_info < ( 3 , ) : Note that 'hash ' is not a stable hashing function , so if img.mode ! = ' L ' : self.std = None `` binary '' will be 1D binary labels , if self.samplewise_std_normalization : self.mean = np.reshape ( self.mean , broadcast_shape ) flatx = np.reshape ( x , ( -1 , np.prod ( x.shape [ -3 : ] ) ) ) weights=weights ) x = GlobalAveragePooling2D ( name='avg_pool ' ) ( x ) ' ( channel after row and column ) or ' return DenseNet ( [ 6 , 12 , 48 , 32 ] , x = BatchNormalization ( axis=channel_dim , momentum=0.9997 , The following table describes the size and accuracy of the 100 % MobileNet collisions by the hashing function . if w in self.word_counts : branch7x7dbl = conv2d_bn ( branch7x7dbl , 160 , 7 , 1 ) `` `` '' DenseNet models for Keras . It should have exactly 3 inputs channels . shear = np.deg2rad ( shear ) branch3x3dbl_2 = conv2d_bn ( branch3x3dbl , 384 , 3 , 1 ) inferred from the subdirectory names/structure 'is not a keras tensor ' ) default : ` ( 256 , 256 ) ` . from .. import layers x = GlobalAveragePooling2D ( ) ( x ) model = Model ( inputs , x , name='densenet121 ' ) if rows == cols and rows in [ 96 , 128 , 160 , 192 , 224 ] : branch_pool = conv2d_bn ( branch_pool , 64 , 1 ) x = SeparableConv2D ( 1536 , ( 3 , 3 ) , padding='same ' , use_bias=False , name='block14_sepconv1 ' ) ( x ) [ 0 , zy , 0 ] , return transform_matrix target_size=self.target_size , brightness = np.random.uniform ( self.brightness_range [ 0 ] , activation=None , ' '' Instantiates a NASNet model in ImageNet mode . | [ mobilenet_v2_0.75_192 ] | 153 | 2.61 | 68.7 | 88.9 | self.word_index = dict ( list ( zip ( sorted_voc , list ( range ( 1 , len ( sorted_voc ) + 1 ) ) ) ) ) default_size = 224 'separable convolution . ' ) x , p = _normal_a_cell ( x , p , filters , block_id= ' % d ' % ( i ) ) A randomly transformed version of the input ( same shape ) . raise ValueError ( 'input_tensor : ' , input_tensor , pointwise_conv_filters : Integer , the dimensionality of the output space than 32 x 32 , with larger image sizes input_shape : Optional shape tuple , the input shape transform_parameters : Dictionary with string - parameter pairs Resolution | ImageNet Acc | Multiply-Adds ( M ) | Params ( M ) x = Conv2D ( filters2 , kernel_size , if i is not None : The number of parameters and number of multiply-adds default_size = 331 elif p_shape [ img_dim ] ! = ip_shape [ img_dim ] : x = conv2d_bn ( x , 32 , 3 , 3 , padding='valid ' ) img_to_array = image.img_to_array 'All of the arrays in ` x ` ' x : a 3D or 4D numpy array consists of RGB values within [ 0 , 255 ] . width multiplier in the MobileNetV2 paper . block_id='stem_2 ' ) batches = 0 raise ValueError ( ' ` x ` ( images tensor ) and ` sample_weight ` ' and width and height should be no smaller than 139 . | [ mobilenet_v2_0.35_192 ] | 43 | 1.66 | 58.2 | 81.2 | subset : Subset of data ( ` `` training '' ` or ` `` validation '' ` ) if return sorted ( os.walk ( subpath , followlinks=follow_links ) , self.y = np.asarray ( y ) branch_2 = conv2d_bn ( branch_2 , 288 , 3 ) or a list of list of strings . x_misc = [ np.asarray ( xx [ : split_idx ] ) for xx in x_misc ] To load a MobileNetV2 model via ` load_model ` , import the custom x = GlobalMaxPooling2D ( name='max_pool ' ) ( x ) include_top , weights , 'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5 ' , x2_1 = MaxPooling2D ( ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='same ' , x = BatchNormalization ( axis=bn_axis , name='bn_conv1 ' ) ( x ) x2_2 = _separable_conv_block ( p , filters , ( 7 , 7 ) , strides= ( 2 , 2 ) , target_size : Tuple of integers ` ( height , width ) ` , strides= ( 1 , 1 ) , def DenseNet169 ( include_top=True , transformation . def make_sampling_table ( size , sampling_factor=1e-5 ) : x = Dense ( 4096 , activation='relu ' , name='fc1 ' ) ( x ) [ Efficient Estimation of Word Representations in name='separable_conv_2_ % s ' % block_id , skip_reduction=False , if rows == cols and rows in [ 96 , 128 , 160 , 192 , 224 ] : row_axis , col_axis = ( 0 , 1 ) `` `` '' Convert a list of texts to a Numpy matrix . classes=1000 ) : ( see [ activations ] ( .. /activations.md ) ) . block_id='normal_right2_ % s ' % block_id ) if i is None : self.oov_token = oov_token pooling=None , x1_1 = _separable_conv_block ( h , filters , ( 5 , 5 ) , strides= ( 2 , 2 ) , img_channel_axis ) # https : //en.wikipedia.org/wiki/Tf % E2 % 80 % 93idf ' ` zca_whitening ` , but it hasn\'t ' strides= ( 2 , 2 ) , padding='valid ' ) array_to_img = image.array_to_img batch_size , for x_channel in x ] image_datagen = ImageDataGenerator ( * * data_gen_args ) 'Expects `` block35 '' , `` block17 '' or `` block8 '' , ' for block_idx in range ( 1 , 10 ) : x = conv2d_bn ( x , 192 , 3 , padding='valid ' ) x = imgenhancer_Brightness.enhance ( brightness ) shuffle : Boolean , whether to shuffle the data between epochs . # ( std , mean , and principal components if ZCA whitening is applied ) kernel_size : default 3 , the kernel size of middle conv layer at main path for subdir in classes ) ) ) x = concatenate ( [ x2 , x3 , x4 , x5 ] , axis=channel_dim , it is not consistent across different runs , while 'md5 ' dtype=K.floatx ( ) ) The paper demonstrates the performance of MobileNets using ` alpha ` values of width_shift_range=0.1 , if self.principal_components is not None : def reset ( self ) : x3 = add ( [ x3 , p ] , name='normal_add_3_ % s ' % block_id ) rotation_range=90. , DENSENET201_WEIGHT_PATH , | [ mobilenet_v2_1.0_96 ] | 56 | 3.47 | 60.3 | 83.2 | 'has length { length } '.format ( idx=idx , within sequences . For rate ` r ` , timesteps ty : Heigh shift . 'constant ' : kkkkkkkk|abcd|kkkkkkkk ( cval=k ) is the probability that a word of rank i should be sampled . x = _depthwise_conv_block ( x , 512 , alpha , depth_multiplier , block_id=11 ) 'data/train ' , data_format = K.image_data_format ( ) warnings.warn ( output += ( self.y [ index_array ] , ) if new_v < 0.9 * v : for each input channel . # Determine proper input shape and default size . With ` height_shift_range=2 ` possible values 'tx ' : tx , ' You should set ` image_data_format= '' channels_last '' ` ' MobileNetV2 is very similar to the original MobileNet , decode_predictions = xception.decode_predictions 'densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5 ' , Inception-ResNet-B : ` block_type='block17 ' ` y : Numpy array of targets data . If ` filter_multiplier ` = 1 , default number of filters from the 'is type : ' , type ( input_tensor ) , train_generator , trunc = s [ -maxlen : ] ` ( batch , channels , rows , cols ) ` if data_format='channels_first ' residual = x | [ mobilenet_v2_0.75_96 ] | 39 | 2.61 | 58.8 | 81.6 | for i , seq in enumerate ( sequences ) : first_block_filters = _make_divisible ( 32 * alpha , 8 ) ValueError : if ` block_type ` is not one of ` 'block35 ' ` , import os x = Concatenate ( axis=channel_axis , name='mixed_6a ' ) ( branches ) trunc = s [ : maxlen ] `` `` '' Fairly basic set of tools for real-time data augmentation on image data . apply_channel_shift = image.apply_channel_shift This function builds 3 types of Inception-ResNet blocks mentioned 'densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5 ' , branch3x3dbl = conv2d_bn ( x , 448 , 1 , 1 ) min_value = divisor return Add ( name=prefix + 'add ' ) ( [ inputs , x ] ) follow_links : Boolean . x = self.x [ j ] horizontal_flip=False , y_train = np_utils.to_categorical ( y_train , num_classes ) try : # 1-D array-like or int i = 0 A list of integer word indices ( unicity non-guaranteed ) . branch_pool = conv2d_bn ( branch_pool , 192 , 1 , 1 ) x = Activation ( relu6 , name=prefix + 'depthwise_relu ' ) ( x ) mode=fill_mode , x = Conv2D ( 128 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block2_conv2 ' ) ( x ) 'which is not a valid type ' ) ( strictly between 0 and 1 ) . to which to save the augmented pictures being generated where ` gamma ` is the Euler-Mascheroni constant . def inception_resnet_block ( x , scale , block_type , block_idx , activation='relu ' ) : ImportError : if PIL is not available . from PIL import ImageEnhance labels += [ 0 ] * num_negative_samples counts [ j ] += 1 Inception-ResNet-A : ` block_type='block35 ' ` `` `` '' VGG16 model for Keras . translate_dict = dict ( ( c , split ) for c in filters ) if len ( x ) < maxlen : def preprocess_input ( x ) : # 10x block35 ( Inception-ResNet-A block ) : 35 x 35 x 320 the one specified in your Keras config at ` ~/.keras/keras.json ` . ' ` brightness_range should be tuple or list of two floats . ' return x.astype ( np.float32 ) ' as true , ` classes ` should be 1000 ' ) transform_parameters.get ( 'shear ' , 0 ) , class Iterator ( Sequence ) : in the paper , controlled by the ` block_type ` argument ( which is the strides= ( 2 , 2 ) , block_id=4 ) has to be ` ( 224 , 224 , 3 ) ` for NASNetMobile x = Conv2D ( 64 , 7 , strides=2 , use_bias=False , name='conv1/conv ' ) ( x ) preprocess_input = nasnet.preprocess_input x = MaxPooling2D ( ( 2 , 2 ) , strides= ( 2 , 2 ) , name='block2_pool ' ) ( x ) On ImageNet , this model gets to a top-1 validation accuracy of 0.790 Mobile Vision Applications ] ( https : //arxiv.org/pdf/1704.04861.pdf ) ) layer at the top of the network . self.samples = 0 def list_pictures ( directory , ext='jpg|jpeg|bmp|png|ppm ' ) : # Depthwise ty = np.random.uniform ( -wrg , wrg ) * w from .. layers import Concatenate if len ( zoom_range ) ! = 2 : elif padding == 'pre ' : ' floats . Received : ' , zoom_range ) zca_epsilon : epsilon for ZCA whitening . Default is 1e-6 . if name is not None : NASNET_MOBILE_WEIGHT_PATH_NO_TOP , rg : Rotation range , in degrees . `` `` '' Takes numpy data & label arrays , and generates batches of augmented data . NASNET_MOBILE_WEIGHT_PATH_NO_TOP = 'https : //github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile-no-top.h5 ' 'the input_shape argument must be static ' branch_pool = MaxPooling2D ( ( 3 , 3 ) , strides= ( 2 , 2 ) ) ( x ) fill_mode='nearest ' , cval=0 . ) : stem_block_filters : Number of filters in the initial stem block x = apply_affine_transform ( x , zx=zx , zy=zy , channel_axis=channel_axis , fill_mode : One of { `` constant '' , `` nearest '' , `` reflect '' or `` wrap '' } . block_id= ' % d ' % ( num_blocks + i + 1 ) ) x = Conv2D ( 64 , ( 3 , 3 ) , activation='relu ' , padding='same ' , name='block1_conv2 ' ) ( x ) texts : list of strings . ( word , word in the same window ) , with label 1 ( positive samples ) . with K.name_scope ( 'normal_A_block_ % s ' % block_id ) : self.batch_index = 0 ' ` `` channels_first '' ` ( channel before row and column ) . ' x = BatchNormalization ( epsilon=1e-3 , momentum=0.999 , name='bn_Conv1 ' ) ( x ) # optionally save augmented images to disk for debugging purposes `` `` '' Pads sequences to the same length . follow_links=follow_links , of a token in a dictionary ) or into a vector where the coefficient zoom_range=0. , branch_1 = conv2d_bn ( branch_1 , 32 , 3 ) block_name = block_type + ' _ ' + str ( block_idx ) branch3x3dbl = conv2d_bn ( x , 64 , 1 , 1 ) import scipy.ndimage as ndi expansion=6 , block_id=6 ) seq = text_to_word_sequence ( text , Divide inputs by std of the dataset , feature-wise . model = Model ( inputs , x , name='densenet169 ' ) Output tensor after applying ` Conv2D ` and ` BatchNormalization ` . MobileNet on various input sizes : | 1.0 MobileNet-192 | 69.1 % | 529 | 4.2 | branch_pool = AveragePooling2D ( 3 , strides=1 , padding='same ' ) ( x ) for w , c in list ( self.word_docs.items ( ) ) : x = img_to_array ( img , data_format=self.data_format ) datagen = ImageDataGenerator ( x_misc = [ np.asarray ( x [ 1 ] ) ] name='mixed ' + str ( 9 + i ) ) x /= ( np.std ( x , keepdims=True ) + K.epsilon ( ) ) yield root , fname p : Input tensor ` p ` bn_name = None if name is None else name + '_bn ' zoom_range=0.2 , scale=True , * * kwargs ) : x = AveragePooling2D ( 2 , strides=2 , name=name + '_pool ' ) ( x ) name='mixed1 ' ) `` `` '' Performs a channel shift . p = ip raise ValueError ( 'Input to ` .fit ( ) ` should have rank 4 . ' padding='same ' , samplewise_std_normalization : Boolean . Divide each input by its std . if self.batch_index == 0 : depth_multiplier : depth multiplier for depthwise convolution be centered around ` data [ i ] ` , ` data [ i+s ] ` , ` data [ i+2 * s ] ` , etc . The next batch . [ 0 , 0 , 1 ] ] ) } self.data = data strides= ( 1 , 1 ) , input_tensor=None , from keras_applications import xception x = BatchNormalization ( name='block3_sepconv1_bn ' ) ( x ) raise ValueError ( 'Unsupported channel number : ' , x.shape [ 2 ] ) rotation_range=20 , | 1.0 MobileNet-160 | 67.2 % | 529 | 4.2 | batch_size : Integer , size of a batch . maxlen : Int , maximum length of the output sequences . preprocess_input = resnet50.preprocess_input output_shape=K.int_shape ( x ) [ 1 : ] , x = x + max ( -np.min ( x ) , 0 ) if img.mode ! = 'RGB ' : from .imagenet_utils import preprocess_input like to use a model with an input img resolution that is not model = load_model ( 'mobilenet.h5 ' , custom_objects= { y = y [ split_idx : ] return img img = img.resize ( width_height_tuple , resample ) _iter_valid_files ( `` `` '' Utility function to apply conv + BN . ' Weights for input shape ( 224 , 224 ) will be loaded . ' ) Specifying any stride value ! = 1 is incompatible with specifying shear_range=0.2 , parameter should always be used . text = text.lower ( ) strides= ( 2 , 2 ) , block_id=2 ) self.targets = targets input_shape will be used if they match , if the shapes zca_whitening=False , model = Model ( inputs , x , name='inception_resnet_v2 ' ) if include_top : up = conv2d_bn ( mixed , p2 = Conv2D ( filters // 2 , ( 1 , 1 ) , padding='same ' , all spatial dimensions . def img_to_array ( img , data_format=None ) : raise ValueError ( ' ` sequences ` must be a list of iterables . ' tuple ( [ rounds * x.shape [ 0 ] ] + list ( x.shape ) [ 1 : ] ) , self.class_indices , follow_links ) ) ) for block_idx in range ( 1 , 21 ) : batch_y = np.zeros ( file_hash='bcf9965cf5064a5f9eb6d7dc69386f43 ' ) if ` include_top ` is ` False ` ( otherwise the input shape `` `` '' Instantiates the Inception-ResNet v2 architecture . 'not understood ' % truncating ) supported . If PIL version 3.4.0 or newer is installed , `` box '' and be kept . x_channel , from .. layers import Dropout `` `` '' Loads an image into PIL format . raise ValueError ( fill_mode='nearest ' , cval=0 . ) : def flow ( self , x , y=None , batch_size=32 , shuffle=True , sample_weight=None , seed=None , save_format : one of `` png '' , `` jpeg '' ( one of ` { 'constant ' , 'nearest ' , 'reflect ' , 'wrap ' } ` ) . self.shear_range ) include_top=True , To load a MobileNet model via ` load_model ` , import the custom img_col_axis = self.col_axis - 1 if pooling == 'avg ' : DenseNet121 = densenet.DenseNet121 raise ValueError ( 'Unknown vectorization mode : ' , mode ) `` `` '' Instantiates the Xception architecture . alpha_text = '2_5 ' 'reflect ' : abcddcba|abcd|dcbaabcd x1_2 = _separable_conv_block ( p , filters , epsilon=1e-3 , # If input_shape is None and no input_tensor self.principal_components = None block_id='reduction_left4_ % s ' % block_id ) raise ValueError ( 'Shape of sample % s of sequence at position % s ' a directory to which to save rank = np.arange ( size ) padding='valid ' , alpha_text = '7_5 ' if self.seed is not None : x = Activation ( activation , name=block_name + '_ac ' ) ( x ) if input_shape is None and not K.is_keras_tensor ( input_tensor ) : kernel_size : kernel size as in ` Conv2D ` . current_index + self.batch_size ] `` categorical '' will be 2D one-hot encoded labels , batch_size : Number of timeseries samples in each batch 'would be left to be used as current step . ' branch_2 = conv2d_bn ( branch_2 , 320 , 3 , strides=2 , padding='valid ' ) from .. import constraints The position where padding or truncation happens is determined by K.int_shape ( x ) [ channel_axis ] , min_size=32 , p2 ) name='mixed4 ' ) def _get_batches_of_transformed_samples ( self , index_array ) : 'but the Sequence ' branch3x3_1 = conv2d_bn ( branch3x3 , 384 , 1 , 3 ) x = MaxPooling2D ( ( 3 , 3 ) , strides= ( 2 , 2 ) ) ( x ) filters : Number of output filters def __getitem__ ( self , index ) : ` featurewise_std_normalization ` or ` zca_whitening ` are set to True . for i , wi in enumerate ( sequence ) : layer_utils.convert_all_kernels_in_model ( model ) h , w = x.shape [ row_axis ] , x.shape [ col_axis ] x = np.asarray ( x , dtype=K.floatx ( ) ) target_size=target_size , color_mode=color_mode , This allows you to optionally specify `` `` '' Performs a random brightness shift . interpolation : Interpolation method used to seed : Int ( default : None ) . `` `` '' One-hot encodes a text into a list of word indexes of size n . n : int . Size of vocabulary . `` `` '' Inception-ResNet V2 model for Keras . featurewise_center=True , ' ` samplewise_std_normalization ` , ' x1 = BatchNormalization ( axis=bn_axis , epsilon=1.001e-5 , [ 0 , 1 , ty ] , x = x [ split_idx : ] 'input ' , None } : def pad_sequences ( sequences , maxlen=None , dtype='int32 ' , def __init__ ( self , p = Conv2D ( filters , ( 1 , 1 ) , strides= ( 1 , 1 ) , padding='same ' , cval : Value used for points outside the boundaries i = self.start_index + self.batch_size * self.stride * index either `` channels_first '' or `` channels_last '' . white_list_formats=white_list_formats , translate_map = maketrans ( filters , split * len ( filters ) ) batch_x_miscs = [ xx [ index_array ] for xx in self.x_misc ] # Final convolution block : 8 x 8 x 1536 of the input if ` mode='constant ' ` . target_size : Either ` None ` ( default to original size ) size : Int , number of possible words to sample . self.document_count = 0 https : //github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md NASNET_LARGE_WEIGHT_PATH_NO_TOP , strides : Strided convolution for downsampling name : string , block label . # grayscale self.shuffle = shuffle NumpyArrayIterator = image.NumpyArrayIterator # mixed 5 , 6 : 17 x 17 x 768 ` 'flip_horizontal ' ` : Boolean . Horizontal flip . strides=1 , elif alpha == 0.75 : `` `` '' Adds a depthwise convolution block . if K.backend ( ) == 'theano ' : branches = [ branch_0 , branch_1 , branch_2 ] WEIGHTS_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5 ' # Provide the same seed and keyword arguments to the fit and flow methods if y is not None and len ( x ) ! = len ( y ) : self.channel_axis = 3 sample_shape = np.asarray ( s ) .shape [ 1 : ] 1.0 ( also called 100 % MobileNet ) , 0.75 , 0.5 and 0.25 . Note that the data format convention used by the model is if alpha not in [ 0.35 , 0.50 , 0.75 , 1.0 , 1.3 , 1.4 ] : x : Input data . Numpy array of rank 4 or a tuple . x = _depthwise_conv_block ( x , 512 , alpha , depth_multiplier , block_id=10 ) break ( words maybe include the ` ' ` character ) . These sequences are then default_size=224 , the augmented pictures being generated | [ mobilenet_v2_1.0_224 ] | 300 | 3.47 | 71.8 | 91.0 | Keras config file at ` ~/.keras/keras.json ` . `` channels_first '' mode means that the images should have shape print ( 'Epoch ' , e ) branch7x7 = conv2d_bn ( x , 192 , 1 , 1 ) def flow_from_directory ( self , directory ,","['.travis.yml', 'keras/applications/__init__.py', 'keras/applications/densenet.py', 'keras/applications/inception_resnet_v2.py', 'keras/applications/inception_v3.py', 'keras/applications/mobilenet.py', 'keras/applications/mobilenetv2.py', 'keras/applications/nasnet.py', 'keras/applications/resnet50.py', 'keras/applications/vgg16.py', 'keras/applications/vgg19.py', 'keras/applications/xception.py', 'keras/preprocessing/__init__.py', 'keras/preprocessing/image.py', 'keras/preprocessing/sequence.py', 'keras/preprocessing/text.py', 'setup.py']",Split ` applications ` and ` preprocessing ` modules . ( # 10339 )
425,a3664246de4bb8ef3232dd84ff47fc2694c69e58,2018-06-01 16:13:24-07:00,"`` `` '' Computes an output mask tensor for Embedding layer tensor_shape = K.shape ( tensor ) def compute_mask ( self , inputs , mask=None ) : model.add ( layers.SimpleRNN ( 1 , return_sequences=False ) ) output_mask = K.any ( output_mask , axis=-1 ) y._keras_shape = shape model_input = np.random.randint ( low=1 , high=5 , size= ( 10 , 3 , 4 ) ) for i , s in enumerate ( int_shape ) : assert np.array_equal ( mask_outputs_val [ 0 ] , np.any ( model_input , axis=-1 ) ) # Arguments if int_shape is None : if output_mask is None : model.add ( wrappers.TimeDistributed ( layers.SimpleRNN ( 8 , return_sequences=False ) ) ) else : input_shape= ( None , None ) ) ) # N by t_1 by t_2 by 6 y._keras_shape = shape `` `` '' y._uses_learning_phase = False inner_mask_shape = self._get_shape_tuple ( ( -1 , ) , mask , 2 ) int_shape [ i ] = tensor_shape [ start_idx + i ] # input_mask is not None , and output_mask is None : output_mask_shape = self._get_shape_tuple ( if inner_mask is not None : # Masking layer and Embedding layer with mask_zero if hasattr ( x , '_keras_shape ' ) : return init_tuple + int_shape output_mask_int_shape = K.compute_output_shape ( input_shape ) [ : -1 ] return K.not_equal ( inputs , 0 ) of the output shape if has_arg ( self.layer.call , 'mask ' ) and mask is not None : assert np.array_equal ( mask_outputs_val [ 1 ] , np.any ( model_input , axis=-1 ) ) keras_shape_list = list ( x._keras_shape ) the corresponding dimension from K.shape ( tensor ) and replaces them by the corresponding dynamic shapes of the tensor . return mask reason='Unknown timestamps for RNN not supported in CNTK . ' ) Reduce the input_mask to 2 dimensions and return it . output_mask = self.layer.compute_mask ( inner_inputs , inner_mask ) If batch size is specified : output_mask = K.reshape ( output_mask , output_mask_shape ) ( -1 , input_length ) , output_mask , 1 , output_mask_int_shape [ 1 : ] ) `` `` '' Finds non-specific dimensions in the static shapes # test with unspecified shape and Embeddings with mask_zero y._uses_learning_phase = x._uses_learning_phase inputs = K.reshape ( inputs , inner_input_shape ) # test with Masking layer for i in range ( 3 ) : ( E.g. , inner layer is Masking or RNN ) if keepdims : self.supports_masking = mask_zero ( -1 , input_length ) , y , 1 , output_shape [ 2 : ] ) output_mask = K.not_equal ( inputs , 0 ) axis_list = list ( set ( int ( a ) for a in axis ) ) model.add ( wrappers.TimeDistributed ( layers.Masking ( mask_value=0. , ) , for i in range ( 4 ) : and the last part from either ` int_shape ` ( if provided ) shape = tuple ( x if isinstance ( x , int ) and x > 0 else None for x in shape ) if not keras_shape_list : tensor : the tensor from which to get the ( static and dynamic ) shapes Otherwise ( both the output mask and the input mask are ` None ` ) : mask_outputs.append ( layer.compute_mask ( layer.input , mask_outputs [ -1 ] ) ) input_uid = object_list_uid ( inputs ) mask_outputs = [ model.layers [ 0 ] .compute_mask ( model.input ) ] model_input [ i , i : , i : ] = 0 return y Simply return the input ` mask ` . ( An rnn-based implementation with model_input [ i , i : , : ] = 0 . def test_TimeDistributed_with_masked_embedding_and_unspecified_shape ( ) : for a in axis_list : keras_shape_list = ( 1 , ) output_shape = self._get_shape_tuple ( mask_outputs += [ model.layers [ 1 ] .compute_mask ( model.layers [ 1 ] .input , mask_outputs [ -1 ] ) ] model.add ( wrappers.TimeDistributed ( layers.SimpleRNN ( 7 , return_sequences=True ) ) ) if _is_explicit_shape ( shape ) : y = K.reshape ( y , output_shape ) assert mask_outputs [ -1 ] is None # final layer else : else : return output_mask if not any ( not s for s in int_shape ) : if input_shape [ 0 ] : elif hasattr ( y , '_keras_shape ' ) : if hasattr ( x , '_uses_learning_phase ' ) : model = Sequential ( ) model.fit ( model_input , # if the output_mask does not have a static shape , inner_inputs = self._input_map [ input_uid ] return K.any ( K.not_equal ( inputs , self.mask_value ) , axis=-1 ) If the output mask at each time step is not ` None ` : init_tuple : a tuple , the first part of the output shape if axis is None : If the output mask at each time step is ` None ` and the input mask is not ` None ` : The new int_shape with the first part from init_tuple int_shape : an alternative static shape to take as the last part input_length = K.shape ( inputs ) [ 1 ] np.random.random ( ( 10 , 1 ) ) , epochs=1 , batch_size=10 ) if mask is None : func = K.function ( [ model.input ] , mask_outputs [ : -1 ] ) # batch size matters , we currently do not handle mask explicitly np.random.random ( ( 10 , 3 , 5 ) ) , epochs=1 , batch_size=6 ) y._uses_learning_phase = x._uses_learning_phase input_shape = K.int_shape ( inputs ) for layer in model.layers [ 1 : ] : if not s : mask : Tensor model.add ( wrappers.TimeDistributed ( layers.Dense ( 5 ) ) ) y._keras_shape = tuple ( keras_shape_list ) Otherwise we call ` compute_mask ` of the inner layer at each time step . None or a tensor inner_mask = mask output_mask_int_shape = K.int_shape ( mask ) return T.any ( x , axis=axis , keepdims=keepdims ) return init_tuple + tuple ( int_shape ) ref_mask_val_2 = np.any ( ref_mask_val_1 , axis=-1 ) # second RNN layer y = T.any ( x , axis=axis , keepdims=keepdims ) # its shape must be the same as mask 's func = K.function ( [ model.input ] , mask_outputs ) model.add ( wrappers.TimeDistributed ( layers.Embedding ( 5 , 6 , mask_zero=True ) , output_mask = mask keras_shape_list [ a ] = 1 # Returns Concatenate all of them and return the concatenation . int_shape = K.int_shape ( tensor ) [ start_idx : ] ( E.g. , ` mask ` is not used at all ) ( E.g. , inner layer is Dense ) kwargs [ 'mask ' ] = K.reshape ( mask , inner_mask_shape ) input_shape= ( None , 4 ) ) ) z = T.neq ( x , y ) keras_shape_list.pop ( a ) the static shape of the tensor # cases need to call the layer.compute_mask when input_mask is None : y = K.reshape ( y , ( -1 , input_length ) + output_shape [ 2 : ] ) return T.neq ( x , y ) as the last part of the output shape ref_mask_val_1 = ref_mask_val_0 # first RNN layer int_shape = list ( int_shape ) or K.int_shape ( tensor ) , where every ` None ` is replaced by for a in axis_list [ : :-1 ] : more than one rnn inputs is required but not supported in Keras yet . ) y._uses_learning_phase = False inner_input_shape = self._get_shape_tuple ( ( -1 , ) , inputs , 2 ) output_mask = K.any ( K.not_equal ( inputs , self.mask_value ) , axis=-1 ) else : if isinstance ( axis , int ) : assert np.array_equal ( mask_outputs_val [ i ] , ref_mask_val [ i ] ) return None shape = tuple ( x if x ! = -1 else None for x in shape ) def _get_shape_tuple ( self , init_tuple , tensor , start_idx , int_shape=None ) : based on the inputs , mask , and the inner layer . return z axis_list = [ axis ] inputs = K.reshape ( inputs , ( -1 , ) + input_shape [ 2 : ] ) inputs : Tensor # output_mask is not None . We need to reshape it model_input = np.random.randint ( low=1 , high=5 , size= ( 10 , 3 , 4 ) , dtype='int32 ' ) z._keras_shape = x._keras_shape Return ` None ` . if output_mask_int_shape is None : # replace all None in int_shape by K.shape def test_TimeDistributed_with_masking_layer ( ) : output_mask_int_shape = K.int_shape ( output_mask ) model.compile ( optimizer='rmsprop ' , loss='mse ' ) for _ in range ( 2 , len ( K.int_shape ( mask ) ) ) : inner_mask_shape = self._get_shape_tuple ( ( -1 , ) , mask , 2 ) input_length = input_shape [ 1 ] ref_mask_val_0 = model_input > 0 # embedding layer y._keras_shape = ( 1 , ) * len ( x._keras_shape ) if keepdims else ( 1 , ) start_idx : int , which indicate the first dimension to take from # we should return a not-None mask if mask is not None : if not input_length : inner_mask = K.reshape ( inner_mask , inner_mask_shape ) z._keras_shape = y._keras_shape if hasattr ( x , '_uses_learning_phase ' ) : ref_mask_val = [ ref_mask_val_0 , ref_mask_val_1 , ref_mask_val_2 ] mask_outputs_val = func ( [ model_input ] ) else :","['keras/backend/theano_backend.py', 'keras/layers/core.py', 'keras/layers/embeddings.py', 'keras/layers/wrappers.py', 'tests/keras/layers/wrappers_test.py']",Handle ` mask ` in ` TimeDistributed ` wrapper . ( # 10242 )
426,7365a99f6e832847808c7aa28718d32fbc744b21,2018-05-31 12:45:26-07:00,"will be applied to the output of the the output of the model will be a when ` include_top ` is ` False ` . To load a MobileNetV2 model via ` load_model ` , import the custom object ` relu6 ` and pass it to the ` custom_objects ` parameter . last convolutional layer , and thus keras.applications.mobilenetv2 ( input_shape=None , alpha=1.0 , depth_multiplier=1 , include_top=True , weights='imagenet ' , input_tensor=None , classes=1000 ) MobileNetV2 model , with weights pre-trained on ImageNet . To load a MobileNet model via ` load_model ` , import the custom object ` relu6 ` and pass it to the ` custom_objects ` parameter . 2D tensor . keras.applications.mobilenetv2.MobileNetV2 ( input_shape=None , alpha=1.0 , depth_multiplier=1 , include_top=True , weights='imagenet ' , input_tensor=None , pooling=None , classes=1000 ) 'relu6 ' : mobilenet.relu6 } ) ` 'max ' ` means that global max pooling will ` 'avg ' ` means that global average pooling be applied . last convolutional layer . 'relu6 ' : mobilenetv2.relu6 } ) ` None ` means that the output of the model pooling : Optional pooling mode for feature extraction MobileNet model , with weights pre-trained on ImageNet . will be the 4D tensor output of the",['docs/templates/applications.md'],Fix doc ( # 10327 )
427,fe066966b5afa96f2f6b9f71ec0c71158b44068d,2018-05-30 13:49:44+09:00,"` None ` means that the output of the model will be the 4D tensor output of the last convolutional layer . pooling : Optional pooling mode for feature extraction if app == applications.MobileNet : from .. layers import GlobalMaxPooling2D 2D tensor . when ` include_top ` is ` False ` . pooling=None , x = GlobalMaxPooling2D ( ) ( x ) _test_app_pooling ( app , last_dim ) ` avg ` means that global average pooling _test_app_pooling ( app , last_dim ) last convolutional layer , and thus x = GlobalAveragePooling2D ( ) ( x ) elif pooling == 'max ' : be applied . else : the output of the model will be a will be applied to the output of the ` max ` means that global max pooling will if pooling == 'avg ' :","['keras/applications/mobilenetv2.py', 'tests/keras/applications/applications_test.py']",Add pooling options in MobileNetV2 ( # 10313 )
428,315a80ae3cb65da2aa6308abd483f2f8643f1c8b,2018-05-29 09:43:37-07:00,"Note that the data format convention used by the model is therefore it only works with the data format MobileNetV2 is a general architecture and can be used for multiple use cases . Note that when using TensorFlow , for best performance you should Neural Architecture Search Network ( NASNet ) model , with weights pre-trained on ImageNet . CNTK backends . The data format convention used by the model is with this model . Use ` preprocess_input ( ) ` defined in this module instead ) . TensorFlow , Theano , and CNTK . The data format The number of parameters and number of multiply-adds MobileNet model , with weights pre-trained on ImageNet . The default input size for this model is 224x224 . different width factors . This allows different width models to reduce the one specified in your Keras config file . The model and the weights are compatible with can be modified by using the ` alpha ` parameter , with ` 'channels_first ' ` data format ( channels , height , width ) or ` 'channels_last ' ` data format ( height , width , channels ) . 'relu6 ' : mobilenet.relu6 } ) For ` Keras < 2.1.7 ` , The Xception model is only available for TensorFlow , due to its reliance on ` SeparableConvolution ` layers . E.g . Optionally loads weights pre-trained the data format ` 'channels_last ' ` ( height , width , channels ) . All of these architectures are compatible with all the backends ( TensorFlow , Theano , and CNTK ) , and upon instantiation the models will be built according to the image data format set in your Keras configuration file at ` ~/.keras/keras.json ` . For instance , if you have set ` image_data_format=channels_last ` , then any model loaded from this repository will get built according to the TensorFlow data format convention , `` Height-Width-Depth '' . Note that the default input image size for this model is 299x299 . To load a MobileNet model via ` load_model ` , import the custom objects ` relu6 ` and ` DepthwiseConv2D ` and pass them to the ` custom_objects ` parameter . This model and can be built both with ` 'channels_first ' ` data format ( channels , height , width ) or ` 'channels_last ' ` data format ( height , width , channels ) . For ` Keras < 2.1.5 ` , The MobileNet model is only available for TensorFlow , due to its reliance on ` DepthwiseConvolution ` layers . rows when weights='imagenet the number of multiply-adds and thereby Note that this model only supports the data format ` 'channels_last ' ` ( height , width , channels ) . of 224x224 as in the VGG16 and ResNet models . Also , the input preprocessing The model and the weights are compatible with both reduce inference cost on mobile devices . at ~/.keras/keras.json . 'DepthwiseConv2D ' : mobilenet.DepthwiseConv2D } ) the one specified in your Keras config at ` ~/.keras/keras.json ` . Note that this model is only available for the TensorFlow backend , at ` ~/.keras/keras.json ` . TensorFlow and Theano . The data format Note that : Neural Architecture Search Network ( NASNet ) models , with weights pre-trained on ImageNet . set ` `` image_data_format '' : `` channels_last '' ` in the config . function is different ( i.e. , do not use ` imagenet_utils.preprocess_input ( ) ` convention used by the model is the one on ImageNet . Note that when using TensorFlow , The model and the weights are compatible with which increases/decreases the number of filters in each layer . The MobileNet model is only available for TensorFlow , due to its reliance on ` DepthwiseConvolution ` layers . `` ` Optionally loads weights pre-trained ` image_data_format='channels_last ' ` in your Keras config Depending on the use case , it can use different input layer size and This model is available for both the Theano and TensorFlow backend , and can be built both for best performance you should set at ~/.keras/keras.json . The Xception model is only available for TensorFlow , due to its reliance on ` SeparableConvolution ` layers . ` image_data_format='channels_last ' ` in your Keras config at ` ~/.keras/keras.json ` . Optionally loads weights pre-trained on ImageNet . TensorFlow , Theano , and CNTK . The data format Note that only TensorFlow is supported for now , 'relu6 ' : mobilenet.relu6 , on ImageNet . Note that when using TensorFlow , for best performance you should set To load a MobileNet model via ` load_model ` , import the custom object ` relu6 ` and pass it to the ` custom_objects ` parameter . The model and the weights are compatible with TensorFlow , Theano and ` image_data_format='channels_last ' ` in your Keras config rows when weights='imagenet ' This model is available for Theano , TensorFlow and CNTK backends , and can be built both set ` `` image_data_format '' : `` channels_last '' ` in your Keras config specified in your Keras config file . All of these architectures ( except Xception and MobileNet ) are compatible with both TensorFlow and Theano , and upon instantiation the models will be built according to the image data format set in your Keras configuration file at ` ~/.keras/keras.json ` . For instance , if you have set ` image_data_format=channels_last ` , then any model loaded from this repository will get built according to the TensorFlow data format convention , `` Height-Width-Depth '' . Note that the default input image size for this model is 299x299 , instead Note that only TensorFlow is supported for now , specified in your Keras config file . `` ` python due to its reliance on ` SeparableConvolution ` layers . Additionally it only supports therefore it only works with the data format When using TensorFlow , for best performance you should This model can be built both with ` 'channels_first ' ` data format ( channels , height , width ) or ` 'channels_last ' ` data format ( height , width , channels ) . model = load_model ( 'mobilenet_v2.h5 ' , custom_objects= { convention used by the model is the one DenseNet models , with weights pre-trained on ImageNet .","['docs/templates/applications.md', 'keras/applications/densenet.py', 'keras/applications/inception_resnet_v2.py', 'keras/applications/inception_v3.py', 'keras/applications/nasnet.py', 'keras/applications/resnet50.py', 'keras/applications/vgg16.py', 'keras/applications/vgg19.py']",Improve docstrings of applications ( # 10310 )
429,84aa7b5f4167afaa8e4b24f2449ac76c11d5e8df,2018-05-25 14:20:57-07:00,"# The CPU implementation of FusedBatchNorm only support NHWC beta , tf_data_format = None # default if beta is None : ( typically the features axis ) . axis : Integer , the axis that should be normalized . def batch_normalization ( x , mean , var , beta , gamma , epsilon=1e-3 ) : axis=self.axis , else : mean=mean , data_format=tf_data_format , tf_data_format = 'NHWC ' gamma , return y is_training=False variance=var , if axis == 1 : if gamma is None : y , _ , _ = tf.nn.fused_batch_norm ( if ndim ( x ) == 4 : x , tf_data_format = 'NCHW ' ) epsilon=epsilon , def batch_normalization ( x , mean , var , beta , gamma , axis=-1 , epsilon=1e-3 ) : beta = zeros_like ( mean ) gamma = ones_like ( mean ) elif axis == 3 : if tf_data_format == 'NHWC ' or tf_data_format == 'NCHW ' and _has_nchw_support ( ) :","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/layers/normalization.py']",Non training Batch Norm operator has bad performance for it running into tensorflow 's non fused batch norm API ( # 10207 )
430,e6d21795d363165b36962e71ea7ba7948185e26f,2018-05-25 14:19:38-07:00,"trainable_weights : List of variables . ` layer.get_input_shape_for ( input_shape ) ` , or ill-defined ( e.g . a shared layer with multiple input and will raise an exception . In such cases , use additional non-weight state . Used in , for instance , RNN compute_mask ( x , mask ) requesting ` input_shape ` will raise an Exception . outbound_nodes : List of nodes . an Exception . Prefer using get_output_mask_at ( node_index ) input_shape : Shape tuple . Provided for convenience , but note count_params ( ) set_weights ( weights ) ` layer.get_input_at ( node_index ) ` . ` layer.get_input_at ( node_index ) ` . ` layer.get_input_shape_at ( node_index ) ` . get_input_shape_at ( node_index ) get_output_mask_at ( node_index ) name : String , must be unique within a model . output_shape : Shape tuple . See ` input_shape ` . inbound_nodes : List of nodes . build ( input_shape ) trainable_weights : List of variables . get_weights ( ) supports_masking : Boolean indicator of whether the layer get_weights ( ) compute_output_shape ( input_shape ) compute_output_shape ( input_shape ) input , output : Input/output tensor ( s ) . Note that if the layer is used and will raise an exception . In such cases , use or ` layer.get_input_shape_at ( node_index ) ` . get_input_mask_at ( node_index ) stateful : Boolean indicating whether the layer carries but note that there may be cases in which this input , output . shapes ) , in which case requesting ` input_shape ` will raise input_mask , output_mask : Same as above , for masks . attribute is ill-defined ( e.g . a shared layer set_weights ( weights ) non_trainable_weights : List of variables . cells to carry information between batches . count_params ( ) name : String , must be unique within a model . output_shape : Shape tuple . See above . get_input_shape_at ( node_index ) input_shape : Shape tuple . Provided for convenience , sequence . with multiple input shapes ) , in which case is used more than once ( shared layer ) , this is ill-defined non_trainable_weights : List of variables . Prefer using ` layer.get_input_shape_for ( input_shape ) ` , input , output : Input/output tensor ( s ) . Note that if the layer input_mask , output_mask : Mask tensors . Same caveats apply as that there may be cases in which this attribute is compute_mask ( x , mask ) get_input_mask_at ( node_index ) more than once ( shared layer ) , this is ill-defined build ( input_shape ) supports masking , typically for unused timesteps in a",['keras/engine/base_layer.py'],Adds to and alphabetizes documentation of Layer base class . ( # 10282 )
431,5eecd55a6f1d6d149b42f9b76aa53d4c5ab8d3eb,2018-05-24 13:54:20-07:00,"weight_value_tuples.append ( ( symbolic_weights [ i ] , if skip_mismatch : ' , but the saved weight has shape ' if skip_mismatch : if K.int_shape ( symbolic_weights [ i ] ) ! = weight_values [ i ] .shape : weight_values [ i ] ) ) weight_value_tuples.append ( ( symbolic_weights [ i ] , ' has shape { } '.format ( K.int_shape ( symbolic_weights [ i ] ) ) if K.int_shape ( symbolic_weights [ i ] ) ! = weight_values [ i ] .shape : else : ' '' ) , weight ' weight_values [ i ] ) ) str ( weight_values [ i ] .shape ) + ' . ' ) raise ValueError ( 'Layer # ' + str ( k ) ' ( named `` ' + layer.name str ( symbolic_weights [ i ] ) else :",['keras/engine/saving.py'],load_weights will fail if shape mismatch ( # 10266 )
432,a07d9f3f3665ed79401f37f0c5759f271268a34f,2018-05-24 10:50:21-07:00,"spaces = re.search ( r'\S ' , line ) ` batch_input_shape= ( ... ) ` to the first layer in your model . which is useful to use with ` model.predict_generator ( ) ` , print ( ' ... inserting autogenerated content into template : ' , path ) layer = RNN ( cells ) ` 'flip_horizontal ' ` : Boolean . Horizontal flip . To enable statefulness : lines = [ re.sub ( r'^ ' , `` , line ) for line in lines ] lines = [ re.sub ( top_level_regex , top_level_replacement , line ) for line in lines ] from the dictionary are used : This layer supports masking for input data with a variable number anchor_pos = docstring.find ( anchor ) shift , functions = page_data.get ( 'functions ' , [ ] ) # Examples prev_output = states [ 0 ] This argument ( or alternatively , ` constants ` . Such constants can be used to condition the cell ` 'brightness ' ` : Float . Brightness shift intensity . ` Flatten ` then ` Dense ` layers upstream # Note on specifying the initial state of RNNs if os.path.exists ( path ) : same as with ` width_shift_range= [ -1 , 0 , +1 ] ` , __Note on using statefulness in RNNs__ a specific layer , or on your entire model . the initial state of the RNN layer . describing the transformation . Currently , the following parameters To reset the states of your model , call ` .reset_states ( ) ` on either computed for the samples in one batch will be reused as initial states with open ( path , ' w ' ) as f : page_data [ 'page ' ] ) blocks = [ ] # These have to be removed , but first the list roots have to be detected . ` 'zx ' ` : Float . Zoom in the x direction . methods = collect_class_methods ( cls , element [ 1 ] ) if cls not in module_classes : raise RuntimeError ( 'Found no content for page ' while with ` height_shift_range=1.0 ` possible values are floats in ( the generator will only yield batches of image data , def process_list_block ( docstring , anchor , marker ) : print ( ' ... creating new page with autogenerated content : ' , path ) To enable statefulness : while with ` width_shift_range=1.0 ` possible values are floats in element = ( element , [ ] ) in the output sequences . This is useful to reserve part of the for the samples in the next batch . This assumes a one-to-one mapping if ` return_sequences ` : 3D tensor with shape page_name = page_data [ 'page ' ] This is the expected shape of your inputs calling them with the keyword argument ` initial_state ` . when it is constant . with open ( 'sources/index.md ' , ' w ' ) as f : # Let 's use this cell in a RNN layer : __Arguments__ # White spaces to be removed in order to correctly align the block . or later than ` end_index ` will not be used in the output sequences . It should be a tuple of integers , e.g . ` ( 32 , 10 , 100 ) ` . input_dim : dimensionality of the input ( integer ) . Two : The value of ` initial_state ` should be a tensor or list of for marker , content in sections.items ( ) : __cell__ : A RNN cell instance . A RNN cell is a class that has : block = `` '' module_functions.sort ( key=lambda x : id ( x ) ) list of numpy arrays representing mkdown = template.replace ( ' { { autogenerated } } ' , mkdown ) the data still needs to reside in a subdirectory lines [ i ] = '\n ' + line This : requires that the ` cell.call ` method accepts the same keyword argument ` 'ty ' ` : Float . Shift in the y direction . Unrolling is only suitable for short sequences . # Either insert content into existing page , This is useful to reserve part of the data for test or validation . test_doc1 = { for each sample at index i in a batch will be used as initial __go_backwards__ : Boolean ( default False ) . else a symbolic loop will be used . text_block = False `` ` between samples in different successive batches . block = '\n'.join ( lines ) ending_point = docstring.find ( '\n\n ' , starting_point ) marker = ' $ ' + anchor.replace ( ' ' , ' _ ' ) + ' $ ' set to ` True ` . if inspect.isclass ( module_member ) : The value of indent = spaces.start ( ) __input_dim__ : dimensionality of the input ( integer ) . subblocks.append ( ' # # ' + cls.__name__ + ' methods\n ' ) for element in classes : if ` return_state ` : a list of tensors . The first tensor is for page_data in PAGES : the output . The remaining tensors are the last states , You can specify the initial state of RNN layers numerically by : print ( ' ... creating new page with autogenerated content : ' , path ) else , 2D tensor with shape ` ( batch_size , units ) ` . self.kernel = self.add_weight ( shape= ( input_shape [ -1 ] , self.units ) , pytest.main ( [ __file__ ] ) index = read_file ( 'templates/index.md ' ) in your model , you would need to specify the input length 'result ' : `` 'Base class for recurrent layers . for function in functions : Note that if the recurrent layer is not the first layer if inspect.isfunction ( module_member ) : from the dictionary are used : ` Flatten ` then ` Dense ` layers upstream leading_spaces , subblocks.append ( ' # # ' + cls.__name__ + ' methods\n ' ) subblocks.append ( ' # # # ' + cls.__name__ + '\n ' ) end_index : Data points later than ` end_index ` will not be used ` 'tx ' ` : Float . Shift in the x direction . __Two__ : The value of ` initial_state ` should be a tensor or list of ` 'channel_shift_intencity ' ` : Float . Channel shift intensity . indent = spaces.start ( ) + 1 for the samples in the next batch . This assumes a one-to-one mapping index = index.replace ( ' { { autogenerated } } ' , readme [ readme.find ( ' # # ' ) : ] ) for module in page_data.get ( 'all_module_functions ' , [ ] ) : `` ` python the size of the cell output . for module in page_data.get ( 'all_module_functions ' , [ ] ) : # Arguments With the keyword argument ` states ` . a.k.a . an attention mechanism . function = module_member which is useful to use with ` model.predict_generator ( ) ` , else : subblocks.append ( process_docstring ( docstring ) ) a ` call ( input_at_t , states_at_t ) ` method , returning of timesteps . To introduce masks to your data , # If it is a list element whitespace_n = re.search ( `` [ ^\s ] '' , block ) .start ( ) module_functions.append ( function ) module_functions = [ ] the size of the cell output . shift += section_idx.end ( ) template = read_file ( path ) sections = { } functions = page_data.get ( 'functions ' , [ ] ) return output , [ output ] block = docstring [ starting_point : docstring.find ( `` \n\n '' , starting_point ) ] top_level_regex = r'^ ( [ ^\s\\\ ( ] + ) : ( . * ) ' subblocks.append ( ' # # ' + cls.__name__ + ' class\n ' ) docstring = docstring.replace ( block , marker ) if methods : same as with ` width_shift_range= [ -1 , 0 , +1 ] ` , 3D tensor with shape ` ( batch_size , timesteps , input_dim ) ` . __keyword__ : argument of ` RNN.__call__ ` ( as well as ` RNN.call ` ) method . # Place marker for later reinjection . class_to_source_link ( cls ) + ' < /span > ' ) tensors representing output = h + K.dot ( prev_output , self.recurrent_kernel ) classes = page_data.get ( 'classes ' , [ ] ) ` 'zy ' ` : Float . Zoom in the y direction . layer = RNN ( cells ) ( ` state_size [ 0 ] ` ) should be the same as if __name__ == '__main__ ' : def __init__ ( self , units , * * kwargs ) : return output , [ output ] for name in dir ( module ) : cls = element [ 0 ] * including the batch size * . __Note on passing external constants to RNNs__ top_level_regex = r'^ ( [ ^\s\\\ ( ] + ) : ( . * ) ' describing the transformation . Currently , the following parameters __Examples__ mkdown = template.replace ( ' { { autogenerated } } ' , mkdown ) blocks.append ( '\n'.join ( subblocks ) ) name='kernel ' ) subblocks.append ( ' < span style= '' float : right ; '' > ' classes += module_classes the keyword argument ` input_shape ` ) ` 'zx ' ` : Float . Zoom in the x direction . # Output shape # Reinject list blocks . if not os.path.exists ( subdir ) : leading_spaces = len ( section_idx.group ( 1 ) ) indent = 0 else : module_functions.sort ( key=lambda x : id ( x ) ) # Here 's how to use the cell to build a stacked RNN : subblocks.append ( code_snippet ( signature ) ) computed for the samples in one batch will be reused as initial states functions += module_functions The value of x = keras.Input ( ( None , 5 ) ) # First , let 's define a RNN Cell , as a layer subclass . ` batch_shape= ( ... ) ` to all the first layers in your model . from docs import autogen # Note on using statefulness in RNNs subdir = os.path.dirname ( path ) __return_state__ : Boolean . Whether to return the last state between samples in different successive batches . __Note on specifying the initial state of RNNs__ lines = block.split ( '\n ' ) classes += module_classes if not os.path.exists ( subdir ) : os.makedirs ( subdir ) in which cases the cells get stacked on after the other in the RNN , layer = RNN ( cell ) of ` directory ` for it to work correctly . ( which should be the same as the size of the cell output ) . functions += module_functions layer = RNN ( cell ) for element in classes : shape= ( self.units , self.units ) , ( single state ) in which case it is cls = module_member One : calling ` reset_states ` It is also possible for ` cell ` to be a list of RNN cell instances , docstring = cls.__doc__ # Note on passing external constants to RNNs although it tends to be more memory-intensive . if not blocks : You can set RNN layers to be 'stateful ' , which means that the states docstring , content = process_list_block ( docstring , subblocks.append ( '\n -- - ' ) element = ( element , [ ] ) [ render_function ( method , method=True ) for method in methods ] ) ) if docstring : # Strip all remaining leading spaces . if sequential model : ( e.g . via the ` input_shape ` argument ) the keyword argument ` input_shape ` ) if os.path.exists ( path ) : if line [ spaces.start ( ) ] == '- ' : for module in page_data.get ( 'all_module_classes ' , [ ] ) : # Either insert content into existing page , self.recurrent_kernel = self.add_weight ( subdir = os.path.dirname ( path ) if name [ 0 ] == ' _ ' or name in EXCLUDE : if sequential model : This layer supports masking for input data with a variable number initializer='uniform ' , reversed sequence . print ( 'Starting autogeneration . ' ) module_classes.append ( cls ) line = lines [ i ] ` 'shear ' ` : Float . Shear angle in degrees . self.units = units You can set RNN layers to be 'stateful ' , which means that the states the initial state of the RNN layer . def __init__ ( self , units , * * kwargs ) : x = keras.Input ( ( None , 5 ) ) This argument is required if you are going to connect else , 2D tensor with shape ` ( batch_size , units ) ` . # First , let 's define a RNN Cell , as a layer subclass . readme = read_file ( ' .. /README.md ' ) path = os.path.join ( 'sources ' , page_name ) path = os.path.join ( 'sources ' , page_name ) transformation on additional static inputs ( not changing over time ) , module_functions = [ ] docstring = docstring.replace ( `` $ RETURNS $ '' , returns ) text_block = False marker ) at the level of the first layer lines = [ re.sub ( '^ ' + ' ' * leading_spaces , `` , line ) for line in lines ] classes = page_data.get ( 'classes ' , [ ] ) # Masking You can pass `` external '' constants to the cell using the ` constants ` cell can also take the optional argument ` constants ` , see __Input shape__ ' but missing { { autogenerated } } tag . ' ) module_member = getattr ( module , name ) # Place marker for later reinjection . docstring = docstring.replace ( block , marker ) subblocks.append ( '\n -- -\n'.join ( h = K.dot ( inputs , self.kernel ) # Fix text lines after lists if not isinstance ( element , ( list , tuple ) ) : unroll : Boolean ( default False ) . sections [ marker ] = content the initial state of the RNN layer . for module in page_data.get ( 'all_module_classes ' , [ ] ) : specify ` shuffle=False ` when calling fit ( ) . a specific layer , or on your entire model . class MinimalRNNCell ( keras.layers.Layer ) : # All the other lines get simply the 4 leading space ( if present ) removed use an [ Embedding ] ( embeddings.md ) layer with the ` mask_zero ` parameter y = layer ( x ) shutil.copyfile ( ' .. /CONTRIBUTING.md ' , 'sources/contributing.md ' ) of ` directory ` for it to work correctly . if inspect.isfunction ( module_member ) : if ` return_sequences ` : 3D tensor with shape raise RuntimeError ( 'Found no content for page ' return_state : Boolean . Whether to return the last state specify ` shuffle=False ` when calling fit ( ) . with open ( path , ' w ' ) as f : Note that if the recurrent layer is not the first layer output = h + K.dot ( prev_output , self.recurrent_kernel ) if spaces : transformation on additional static inputs ( not changing over time ) , if function not in module_functions : module_member = getattr ( module , name ) subblocks = [ ] if element [ 1 ] : initializer='uniform ' , ` 'channel_shift_intencity ' ` : Float . Channel shift intensity . docstring = cls.__doc__ ` 'theta ' ` : Float . Rotation angle in degrees . docstring = docstring.replace ( `` $ ARGUMENTS $ '' , arguments ) else for functional model with 1 or more Input layers : def test_doc_lists ( ) : the initial state of the RNN layer . elif spaces.start ( ) < indent : name='recurrent_kernel ' ) if anchor_pos > -1 : subblocks.append ( code_snippet ( signature ) ) a ` state_size ` attribute . This can be a single integer 'doc ' : `` '' '' Base class for recurrent layers . ` ( batch_size , timesteps , units ) ` . data for test or validation . def build ( self , input_shape ) : With the keyword argument ` states ` . section `` Note on passing external constants '' below . assert ' { { autogenerated } } ' in template , ( 'Template found for ' + path __This__ : requires that the ` cell.call ` method accepts the same keyword argument if ` return_state ` : a list of tensors . The first tensor is if module.__name__ in function.__module__ : template = read_file ( path ) # Let 's use this cell in a RNN layer : if name [ 0 ] == ' _ ' or name in EXCLUDE : page_name = page_data [ 'page ' ] although it tends to be more memory-intensive . each with shape ` ( batch_size , units ) ` . def call ( self , inputs , states ) : stateful : Boolean ( default False ) . If True , the last state specify ` stateful=True ` in the layer constructor . cell can also take the optional argument ` constants ` , see module_classes.append ( cls ) It should be a tuple of integers , e.g . ` ( 32 , 10 , 100 ) ` . Unrolling is only suitable for short sequences . top_level_replacement = r'- __\1__ : \2 ' list of numpy arrays representing # save module page . in the output sequence , or the full sequence . readme = read_file ( ' .. /README.md ' ) signature = get_class_signature ( cls ) in addition to the output . ( the generator will only yield batches of image data , blocks.append ( render_function ( function , method=False ) ) docstring , arguments = process_list_block ( docstring , `` # Arguments '' , `` $ ARGUMENTS $ '' ) shift = 0 while section_idx and section_idx.group ( 2 ) : index = index.replace ( ' { { autogenerated } } ' , readme [ readme.find ( ' # # ' ) : ] ) This can also be a list/tuple of integers # Strip all leading spaces . else a symbolic loop will be used . for page_data in PAGES : set to ` True ` . One : You can specify the initial state of RNN layers symbolically by if cls.__module__ == module.__name__ : the data still needs to reside in a subdirectory cell : A RNN cell instance . A RNN cell is a class that has : Note : that tensors representing section_idx = re.search ( section_regex , docstring [ shift : ] ) # Format docstring lists # Remove the computed number of leading white spaces from each line . lines = [ re.sub ( top_level_regex , top_level_replacement , line ) for line in lines ] continue You can pass `` external '' constants to the cell using the ` constants ` with open ( 'sources/index.md ' , ' w ' ) as f : initializer='uniform ' , # Remove the computed number of leading white spaces from each line . for name in dir ( module ) : self.state_size = units the size of the recurrent state for function in functions : the interval [ -1.0 , +1.0 ) . self.built = True at the level of the first layer specify ` stateful=True ` in the layer constructor . If True , process the input sequence backwards and return the # Output shape __One__ : You can specify the initial state of RNN layers symbolically by This argument is required if you are going to connect ` ( output_at_t , states_at_t_plus_1 ) ` . The call method of the ` 'zy ' ` : Float . Zoom in the y direction . cells = [ MinimalRNNCell ( 32 ) , MinimalRNNCell ( 64 ) ] ` batch_input_shape= ( ... ) ` to the first layer in your model . cells = [ MinimalRNNCell ( 32 ) , MinimalRNNCell ( 64 ) ] if module.__name__ in function.__module__ : is required when using this layer as the first layer in a model . If True , the network will be unrolled , indent = 0 subblocks.append ( '\n -- -\n'.join ( text_block = False cell = MinimalRNNCell ( 32 ) the size of the recurrent state in which cases the cells get stacked on after the other in the RNN , return_sequences : Boolean . Whether to return the last output You can specify the initial state of RNN layers numerically by : subblocks.append ( ' # # ' + cls.__name__ + ' class\n ' ) a ` state_size ` attribute . This can be a single integer shutil.copyfile ( ' .. /CONTRIBUTING.md ' , 'sources/contributing.md ' ) while with ` height_shift_range=1.0 ` possible values are floats in self.built = True Unrolling can speed-up a RNN , ` ( batch_size , timesteps , units ) ` . # Reinject arguments and returns blocks . ` ( output_at_t , states_at_t_plus_1 ) ` . The call method of the cell = MinimalRNNCell ( 32 ) blocks.append ( render_function ( function , method=False ) ) # Usually lines have at least 4 additional leading spaces . anchor = section_idx.group ( 2 ) block = '\n'.join ( lines ) section `` Note on passing external constants '' below . start_index : Data points earlier than ` start_index ` will not be used ` 'theta ' ` : Float . Rotation angle in degrees . state for the sample of index i in the following batch . ` model.evaluate_generator ( ) ` , etc . ) . while with ` width_shift_range=1.0 ` possible values are floats in if docstring : if function not in module_functions : continue ' '' } state for the sample of index i in the following batch . name='recurrent_kernel ' ) of timesteps . To introduce masks to your data , ` 'tx ' ` : Float . Shift in the x direction . if text_block : specify a fixed batch size for your model , by passing when it is constant . self.kernel = self.add_weight ( shape= ( input_shape [ -1 ] , self.units ) , ( single state ) in which case it is name='kernel ' ) __Output shape__ ` states ` should be a numpy array or ( one size per state ) . In this case , the first entry ` 'flip_vertical ' ` : Boolean . Vertical flip . same as with ` height_shift_range= [ -1 , 0 , +1 ] ` , mkdown = '\n -- -- \n\n'.join ( blocks ) a.k.a . an attention mechanism . methods = collect_class_methods ( cls , element [ 1 ] ) go_backwards : Boolean ( default False ) . subblocks.append ( process_docstring ( docstring ) ) in addition to the output . # or create page otherwise ` 'shear ' ` : Float . Shear angle in degrees . in your model , you would need to specify the input length is required when using this layer as the first layer in a model . if cls not in module_classes : print ( 'Starting autogeneration . ' ) each with shape ` ( batch_size , units ) ` . self.units = units def call ( self , inputs , states ) : This is the expected shape of your inputs if not blocks : __return_sequences__ : Boolean . Whether to return the last output the output . The remaining tensors are the last states , ` constants ` . Such constants can be used to condition the cell reversed sequence . shape= ( self.units , self.units ) , module_classes.sort ( key=lambda x : id ( x ) ) section_idx = re.search ( section_regex , docstring ) text_block = True # Input shape self.state_size = units same as with ` height_shift_range= [ -1 , 0 , +1 ] ` , __stateful__ : Boolean ( default False ) . If True , the last state prev_output = states [ 0 ] f.write ( index ) top_level_replacement = r'- __\1__ : \2 ' index = read_file ( 'templates/index.md ' ) if not isinstance ( element , ( list , tuple ) ) : __Masking__ function = module_member keyword : argument of ` RNN.__call__ ` ( as well as ` RNN.call ` ) method . cls = module_member module_classes.sort ( key=lambda x : id ( x ) ) ( which should be the same as the size of the cell output ) . __unroll__ : Boolean ( default False ) . ` 'brightness ' ` : Float . Brightness shift intensity . `` `` '' , else : This can also be a list/tuple of integers assert ' { { autogenerated } } ' in template , ( 'Template found for ' + path for each sample at index i in a batch will be used as initial if cls.__module__ == module.__name__ : signature = get_class_signature ( cls ) ` states ` should be a numpy array or ( without it , the shape of the dense outputs can not be computed ) . class MinimalRNNCell ( keras.layers.Layer ) : implementing an efficient stacked RNN . __input_length__ : Length of input sequences , to be specified f.write ( mkdown ) blocks = [ ] os.makedirs ( subdir ) initializer='uniform ' , ` model.evaluate_generator ( ) ` , etc . ) . docstring , returns = process_list_block ( docstring , `` # Returns '' , `` $ RETURNS $ '' ) assert docstring == test_doc1 [ 'result ' ] def process_list_block ( docstring , starting_point , leading_spaces , marker ) : module_classes = [ ] ( e.g . via the ` input_shape ` argument ) __One__ : calling ` reset_states ` page_data [ 'page ' ] ) ( one size per state ) . In this case , the first entry docstring = autogen.process_docstring ( test_doc1 [ 'doc ' ] ) input_length : Length of input sequences , to be specified specify a fixed batch size for your model , by passing calling them with the keyword argument ` initial_state ` . self.recurrent_kernel = self.add_weight ( lines [ i ] = '\n ' + line else for functional model with 1 or more Input layers : mkdown = '\n -- -- \n\n'.join ( blocks ) Please note that in case of class_mode None , implementing an efficient stacked RNN . `` ` super ( MinimalRNNCell , self ) .__init__ ( * * kwargs ) ` 'flip_horizontal ' ` : Boolean . Horizontal flip . Unrolling can speed-up a RNN , This argument ( or alternatively , # save module page . If True , process the input sequence backwards and return the if inspect.isclass ( module_member ) : Note : that y = layer ( x ) To reset the states of your model , call ` .reset_states ( ) ` on either ` 'flip_vertical ' ` : Boolean . Vertical flip . docstring = docstring.replace ( marker , content ) Please note that in case of class_mode None , module_functions.append ( function ) for i in range ( len ( lines ) ) : def build ( self , input_shape ) : the interval [ -1.0 , +1.0 ) . lines = [ re.sub ( '^ ' + ' ' * whitespace_n , `` , line ) for line in block.split ( '\n ' ) ] use an [ Embedding ] ( embeddings.md ) layer with the ` mask_zero ` parameter starting_point = anchor_pos + len ( anchor ) + 1 # or create page otherwise cls = element [ 0 ] ` batch_shape= ( ... ) ` to all the first layers in your model . ' but missing { { autogenerated } } tag . ' ) super ( MinimalRNNCell , self ) .__init__ ( * * kwargs ) start_index , end_index : Data points earlier than ` start_index ` if element [ 1 ] : block = docstring [ starting_point : None if ending_point == -1 else ending_point - 1 ] a ` call ( input_at_t , states_at_t ) ` method , returning ( without it , the shape of the dense outputs can not be computed ) . If True , the network will be unrolled , class_to_source_link ( cls ) + ' < /span > ' ) 3D tensor with shape ` ( batch_size , timesteps , input_dim ) ` . in the output sequence , or the full sequence . ` 'ty ' ` : Float . Shift in the y direction . subblocks.append ( ' # # # ' + cls.__name__ + '\n ' ) * including the batch size * . # Here 's how to use the cell to build a stacked RNN : ( ` state_size [ 0 ] ` ) should be the same as subblocks.append ( '\n -- - ' ) f.write ( index ) blocks.append ( '\n'.join ( subblocks ) ) `` ` python section_regex = r'\n ( + ) # ( . * ) \n ' module_classes = [ ] [ render_function ( method , method=True ) for method in methods ] ) ) subblocks = [ ] subblocks.append ( ' < span style= '' float : right ; '' > ' print ( ' ... inserting autogenerated content into template : ' , path ) import pytest It is also possible for ` cell ` to be a list of RNN cell instances , h = K.dot ( inputs , self.kernel ) f.write ( mkdown ) if methods :","['docs/__init__.py', 'docs/autogen.py', 'keras/layers/convolutional_recurrent.py', 'keras/preprocessing/image.py', 'keras/preprocessing/sequence.py', 'tests/test_doc_auto_generation.py']",Fixes automatic doc generation problem with nested lists . Adds a new test ( # 10212 )
433,c77267af5fff5642951ae4a2c9bb09fb6d698c30,2018-05-23 11:54:36-07:00,model._make_train_function ( ) model.model._make_train_function ( ) if model.__class__.__name__ == 'Sequential ' : model._make_train_function ( ) else :,['keras/engine/saving.py'],Remove deprecated model.model from engine/saving ( # 10275 )
434,08c873669f39b37743014db99fcd2d308f8ea5ea,2018-05-22 14:03:51-07:00,"assert transform_dict [ 'channel_shift_intensity ' ] ! = transform_dict2 [ 'channel_shift_intensity ' ] x = img_to_array ( x ) A ransformed version of the input ( same shape ) . [ 1. , 1. , 1 . ] , np.clip ( x_channel + np.random.uniform ( -intensity , intensity ) , ValueError if ` brightness_range ` is n't a tuple . assert transform_dict [ 'channel_shift_intensity ' ] ! = 0 zx : Zoom in x direction . x = apply_transform ( x , transform_matrix , img_channel_axis , assert transform_dict [ 'zy ' ] ! = transform_dict2 [ 'zy ' ] brightness : Float . The new brightness value . `` `` '' fill_mode='constant ' ) 'flip_vertical ' : flip_vertical , assert transform_dict [ 'shear ' ] ! = 0 [ 0 , zy , 0 ] , [ 0 , 0 , 1 ] ] ) cval=cval ) for x_channel in x ] height_shift_range=0.1 , self.shear_range ) ) x = self.image_data_generator.apply_transform ( img_channel_axis = self.channel_axis - 1 channel_images = [ ndi.interpolation.affine_transform ( if self.channel_shift_range ! = 0 : raise ValueError ( brightness = np.random.uniform ( self.brightness_range [ 0 ] , transform_matrix , h , w ) transform_matrix = transform_matrix_offset_center ( rotation_matrix , h , w ) np.clip ( x_channel + intensity , x = apply_affine_transform ( x , zx=zx , zy=zy , channel_axis=channel_axis , def random_channel_shift ( x , intensity , channel_axis=0 ) : [ 0 , zy , 0 ] , x : 3D tensor , single image . transform_matrix = shear_matrix if transform_matrix is None else np.dot ( transform_matrix , shear_matrix ) transform_parameters.get ( 'shear ' , 0 ) , assert transform_dict [ 'theta ' ] ! = 0 tx * = img_shape [ img_row_axis ] x = flip_axis ( x , img_row_axis ) flip_horizontal = ( np.random.random ( ) < 0.5 ) * self.horizontal_flip transform_matrix = rotation_matrix x = img_to_array ( x ) x = flip_axis ( x , img_row_axis ) transform_matrix = shear_matrix if transform_matrix is None else np.dot ( transform_matrix , shear_matrix ) fill_mode='constant ' ) , x_rotated ) `` `` '' Generates random parameters for a transformation . tx : Width shift . x = random_brightness ( x , self.brightness_range ) rotation_range=90 , assert transform_dict [ 'shear ' ] ! = transform_dict2 [ 'shear ' ] ` 'ty ' ` : Float . Shift in the y direction . if transform_parameters.get ( 'flip_vertical ' , False ) : `` `` '' Performs a random channel shift . ` 'flip_horizontal ' ` : Boolean . Horizontal flip . row_axis : Index of axis for rows in the input image . A randomly transformed version of the input ( same shape ) . theta = np.deg2rad ( np.random.uniform ( if shear ! = 0 : transform_dict = generator.get_random_transform ( x.shape , seed ) channel_images = [ ndi.interpolation.affine_transform ( def apply_affine_transform ( x , theta=0 , tx=0 , ty=0 , shear=0 , zx=1 , zy=1 , # Raises x_rotated = np.array ( [ [ [ 0. , 0. , 0 . ] , # Test get_random_transform without any randomness 'zx ' : zx , x = apply_channel_shift ( x , x.astype ( K.floatx ( ) ) ) params = self.get_random_transform ( x.shape , seed ) transform_parameters [ 'channel_shift_intensity ' ] , [ 0 , zy , 0 ] , assert np.allclose ( generator.apply_transform ( x , { 'theta ' : 45 } ) , assert transform_dict [ 'zy ' ] ! = 0 fill_mode=self.fill_mode , cval=self.cval ) if self.vertical_flip : transform_matrix = transform_matrix_offset_center ( zoom_matrix , h , w ) # Arguments theta = np.random.uniform ( -rg , rg ) channel_shift_range=0.1 , shear = np.deg2rad ( shear ) shift_matrix = np.array ( [ [ 1 , 0 , tx ] , shear : Shear angle in degrees . def apply_channel_shift ( x , intensity , channel_axis=0 ) : h , w = x.shape [ row_axis ] , x.shape [ col_axis ] shear_matrix = np.array ( [ [ 1 , -np.sin ( shear ) , 0 ] , ' ` brightness_range should be tuple or list of two floats . ' `` `` '' Applies a random transformation to an image . transform_parameters.get ( 'zx ' , 1 ) , x = imgenhancer_Brightness = ImageEnhance.Brightness ( x ) assert np.allclose ( image.apply_affine_transform ( x , theta=45 , channel_axis=2 , [ np.sin ( theta ) , np.cos ( theta ) , 0 ] , x = apply_affine_transform ( x , tx=tx , ty=ty , channel_axis=channel_axis , shear = np.random.uniform ( -intensity , intensity ) return self.apply_transform ( x , params ) shift_matrix = np.array ( [ [ 1 , 0 , tx ] , ` 'tx ' ` : Float . Shift in the x direction . zoom_range=0.2 , intensity_range : Transformation intensity . describing the transformation . Currently , the following parameters ty : Heigh shift . def apply_transform ( self , x , transform_parameters ) : transform_matrix : Numpy array specifying the geometric transformation . h , w = x.shape [ img_row_axis ] , x.shape [ img_col_axis ] generator = image.ImageDataGenerator ( # Arguments if tx ! = 0 or ty ! = 0 : order=1 , final_affine_matrix , [ 1. , 1. , 1 . ] ] ] ) `` `` '' Randomly augments a single image tensor . channel_shift_intensity = np.random.uniform ( -self.channel_shift_range , seed : Random seed . 'shear ' : shear , vertical_flip=True ) assert transform_dict [ 'zx ' ] ! = transform_dict2 [ 'zx ' ] mode=fill_mode , if self.channel_shift_range ! = 0 : generator = image.ImageDataGenerator ( ) [ 0 , 0 , 1 ] ] ) final_affine_matrix , params = self.image_data_generator.get_random_transform ( x.shape ) horizontal_flip=True , def apply_transform ( x , ` 'theta ' ` : Float . Rotation angle in degrees . 'brightness ' : brightness } x = array_to_img ( x ) theta = np.deg2rad ( theta ) img_shape : Tuple of integers . Shape of the image that is transformed . `` `` '' Applies a transformation to an image according to given parameters . intensity = np.random.uniform ( -intensity_range , intensity_range ) x = random_channel_shift ( x , zoom_matrix = np.array ( [ [ zx , 0 , 0 ] , assert transform_dict [ 'zx ' ] ! = 0 x_channel , order=1 , assert np.allclose ( generator.apply_transform ( x , { 'flip_horizontal ' : True } ) , transform_matrix = translation_matrix # no need to do offset def random_channel_shift ( x , intensity_range , channel_axis=0 ) : return x zy : Zoom in y direction h , w = x.shape [ row_axis ] , x.shape [ col_axis ] [ 1. , 1. , 1 . ] ] , assert transform_dict [ 'shear ' ] == 0 img_row_axis = self.row_axis - 1 # x is a single image , so it does n't have image number at index 0 channel_axis=img_channel_axis , transform_matrix = None x = np.rollaxis ( x , 0 , channel_axis + 1 ) x = flip_axis ( x , img_col_axis ) x_rotated ) channel_axis : Index of axis for channels in the input tensor . width_shift_range=0.1 , fill_mode=self.fill_mode , cval=self.cval ) ` 'brightness ' ` : Float . Brightness shift intensity . brightness = None if zx ! = 1 or zy ! = 1 : transform_dict2 = generator.get_random_transform ( x.shape , seed * 2 ) [ [ 0. , 0. , 0 . ] , 'flip_horizontal ' : flip_horizontal , self.channel_shift_range , transform_matrix = transform_matrix_offset_center ( x = np.rollaxis ( x , channel_axis , 0 ) [ 0 , 0 , 1 ] ] ) final_affine_matrix = transform_matrix [ :2 , :2 ] brightness_range= ( 1 , 5 ) , transform_matrix , transform_parameters = { 'theta ' : theta , if tx ! = 0 or ty ! = 0 : [ 0 , 0 , 1 ] ] ) channel_axis=0 , shear = np.random.uniform ( transform_matrix = zoom_matrix if transform_matrix is None else np.dot ( transform_matrix , zoom_matrix ) final_affine_matrix = transform_matrix [ :2 , :2 ] theta : Rotation angle in degrees . transform_matrix = shift_matrix if transform_matrix is None else np.dot ( transform_matrix , shift_matrix ) if self.brightness_range is not None : img_channel_axis ) zoom_matrix = np.array ( [ [ zx , 0 , 0 ] , x = apply_affine_transform ( x , shear=shear , channel_axis=channel_axis , def get_random_transform ( self , img_shape , seed=None ) : [ 0 , 0 , 1 ] ] ) transform_matrix = None transformation . A randomly transformed version of the input ( same shape ) . transform_matrix = zoom_matrix if transform_matrix is None else np.dot ( transform_matrix , zoom_matrix ) mode=fill_mode , if self.brightness_range is not None : A dictionary containing randomly chosen parameters describing the if transform_parameters.get ( 'flip_horizontal ' , False ) : x = imgenhancer_Brightness = ImageEnhance.Brightness ( x ) return apply_channel_shift ( x , intensity , channel_axis=channel_axis ) x = np.rollaxis ( x , 0 , channel_axis + 1 ) channel_axis : Index of axis for channels in the input image . ` 'zx ' ` : Float . Zoom in the x direction . transform_matrix = transform_matrix_offset_center ( shear = np.deg2rad ( np.random.uniform ( assert transform_dict [ 'ty ' ] ! = transform_dict2 [ 'ty ' ] assert transform_dict [ 'ty ' ] == 0 # Use composition of homographies x.astype ( K.floatx ( ) ) , params ) ` 'shear ' ` : Float . Shear angle in degrees . [ 0 , 1 , ty ] , def apply_brightness_shift ( x , brightness ) : if len ( self.brightness_range ) ! = 2 : 'channel_shift_intensity ' : channel_shift_intensity , `` `` '' Applies the image transformation specified by a matrix . [ 0 , 0 , 1 ] ] ) img_channel_axis = self.channel_axis - 1 x = np.rollaxis ( x , channel_axis , 0 ) if theta ! = 0 : col_axis : Index of axis for columns in the input image . [ 0 , np.cos ( shear ) , 0 ] , Numpy image tensor . shear = np.deg2rad ( np.random.uniform ( -intensity , intensity ) ) x = np.random.random ( ( 32 , 32 , 3 ) ) theta = np.deg2rad ( np.random.uniform ( -rg , rg ) ) row_axis=0 , col_axis=1 , channel_axis=2 , if zx ! = 1 or zy ! = 1 : def random_transform ( self , x , seed=None ) : assert np.allclose ( generator.apply_transform ( x , { 'flip_vertical ' : True } ) , transform_matrix = transform_matrix_offset_center ( shear_matrix , h , w ) # to generate final transform that needs to be applied [ 0 , 1 , ty ] , [ 0 , np.cos ( shear ) , 0 ] , x = apply_affine_transform ( x , theta=theta , channel_axis=channel_axis , assert transform_dict [ 'channel_shift_intensity ' ] is None [ 0. , 0. , 0 . ] , tx * = x.shape [ img_row_axis ] if theta ! = 0 : ` 'zy ' ` : Float . Zoom in the y direction . [ 0 , 0 , 1 ] ] ) [ 0 , 0 , 1 ] ] ) x [ : , : :-1 , : ] ) if np.random.random ( ) < 0.5 : x = array_to_img ( x ) `` `` '' Applies an affine transformation specified by the parameters given . transform_matrix , h , w ) x = np.ones ( ( 32 , 32 , 3 ) ) assert transform_dict [ 'theta ' ] == 0 transform_matrix = rotation_matrix if self.horizontal_flip : rotation_matrix = np.array ( [ [ np.cos ( theta ) , -np.sin ( theta ) , 0 ] , assert transform_dict [ 'zx ' ] == 1 assert transform_dict [ 'brightness ' ] ! = 0 x : Input tensor . Must be 3D . ty * = img_shape [ img_col_axis ] self.channel_shift_range ) # x is a single image , so it does n't have image number at index 0 flip_vertical = ( np.random.random ( ) < 0.5 ) * self.vertical_flip x = imgenhancer_Brightness.enhance ( u ) x : 3D tensor , single image . assert transform_dict [ 'tx ' ] ! = 0 transform_parameters.get ( 'zy ' , 1 ) , final_offset = transform_matrix [ :2 , 2 ] [ 0 , np.cos ( shear ) , 0 ] , ty * = x.shape [ img_col_axis ] x [ : :-1 , : , : ] ) return apply_brightness_shift ( x , u ) def random_transform ( self , x , seed=None ) : [ np.sin ( theta ) , np.cos ( theta ) , 0 ] , ` 'flip_vertical ' ` : Boolean . Vertical flip . # Test get_random_transform with predefined seed x = apply_brightness_shift ( x , transform_parameters [ 'brightness ' ] ) transform_parameters.get ( 'ty ' , 0 ) , 'ty ' : ty , x = np.ones ( ( 3 , 3 , 3 ) ) final_offset , assert transform_dict [ 'zy ' ] == 1 theta = np.random.uniform ( transform_parameters.get ( 'tx ' , 0 ) , from the dictionary are used : 'tx ' : tx , fill_mode='nearest ' , cval=0 . ) : assert transform_dict [ 'brightness ' ] ! = transform_dict2 [ 'brightness ' ] row_axis=img_row_axis , col_axis=img_col_axis , x = apply_affine_transform ( x , transform_parameters.get ( 'theta ' , 0 ) , `` `` '' Performs a random channel shift . channel_shift_intensity = None if transform_parameters.get ( 'brightness ' ) is not None : self.rotation_range ) ) [ 0 , 0 , 1 ] ] ) assert transform_dict [ 'tx ' ] ! = transform_dict2 [ 'tx ' ] `` `` '' Performs a channel shift . x_channel , zoom_matrix = np.array ( [ [ zx , 0 , 0 ] , ` 'channel_shift_intencity ' ` : Float . Channel shift intensity . self.brightness_range [ 1 ] ) if transform_matrix is not None : x = flip_axis ( x , img_col_axis ) assert transform_dict [ 'brightness ' ] is None if transform_matrix is not None : rotation_matrix = np.array ( [ [ np.cos ( theta ) , -np.sin ( theta ) , 0 ] , transform_parameters : Dictionary with string - parameter pairs x = np.stack ( channel_images , axis=0 ) fill_mode=fill_mode , cval=cval ) img_col_axis = self.col_axis - 1 assert transform_dict [ 'ty ' ] ! = 0 x = self.image_data_generator.random_transform ( x ) rotation_range=90. , assert transform_dict [ 'theta ' ] ! = transform_dict2 [ 'theta ' ] `` `` '' `` `` '' Performs a brightness shift . def test_deterministic_transform ( self ) : final_offset , if transform_parameters.get ( 'channel_shift_intensity ' ) is not None : return transform_parameters # Returns assert transform_dict [ 'tx ' ] == 0 cval=0 . ) : # Returns seed = 1 rotation_matrix = np.array ( [ [ np.cos ( theta ) , -np.sin ( theta ) , 0 ] , x = apply_transform ( x , transform_matrix , channel_axis , fill_mode , cval ) self.rotation_range ) [ 0 , 1 , ty ] , img_channel_axis ) [ np.sin ( theta ) , np.cos ( theta ) , 0 ] , cval=cval ) for x_channel in x ] [ 0 , 0 , 1 ] ] ) shear_range=0.5 , self.shear_range ) transform_matrix = shift_matrix if transform_matrix is None else np.dot ( transform_matrix , shift_matrix ) 'zy ' : zy , x = imgenhancer_Brightness.enhance ( brightness ) return x x = self.image_data_generator.random_transform ( shear_matrix = np.array ( [ [ 1 , -np.sin ( shear ) , 0 ] , translation_matrix = np.array ( [ [ 1 , 0 , tx ] , channel_axis : Index of axis for channels in the input tensor . x = np.stack ( channel_images , axis=0 ) if shear ! = 0 : fill_mode='nearest ' , shear_matrix = np.array ( [ [ 1 , -np.sin ( shear ) , 0 ] , x = self.image_data_generator.apply_transform ( x , params ) 'Received : % s ' % brightness_range ) final_offset = transform_matrix [ :2 , 2 ]","['keras/preprocessing/image.py', 'tests/keras/preprocessing/image_test.py']",Refactor ImageDataGenerator ( # 10130 )
435,25a8973dfce5251c0bd527ffa16ce17174b94218,2018-05-22 12:18:46-07:00,"if num_dynamic_axis > 0 : x = _padding ( x , padding [ 2 ] , 3 ) x = _padding ( x , padding [ 2 ] , 2 ) else : x = _padding ( x , padding [ 1 ] , 1 ) if num_dynamic_axis > 0 : x = C.pad ( x , pattern= [ [ 0 , 0 ] , list ( padding [ 0 ] ) , list ( padding [ 1 ] ) , [ 0 , 0 ] ] ) if hasattr ( C , 'pad ' ) : x = _padding ( x , padding , 0 ) pattern = [ [ 0 , 0 ] ] + pattern assert len ( base_shape ) == 4 return x base_shape = x.shape x = C.pad ( x , pattern= [ ( 0 , 0 ) , padding , ( 0 , 0 ) ] ) if num_dynamic_axis == 0 : return C.pad ( x , pattern=pattern ) assert len ( base_shape ) == 3 assert len ( base_shape ) == 3 x = _padding ( x , padding [ 1 ] , 2 ) x = C.pad ( x , pattern= [ [ 0 , 0 ] , list ( padding [ 0 ] ) , list ( padding [ 1 ] ) , list ( padding [ 2 ] ) ] ) x = _padding ( x , padding [ 0 ] , 2 ) if hasattr ( C , 'pad ' ) : x = _padding ( x , padding , 1 ) else : assert len ( base_shape ) == 5 x = _padding ( x , padding [ 0 ] , 1 ) assert len ( base_shape ) == 2 if hasattr ( C , 'pad ' ) : x = C.pad ( x , pattern= [ list ( padding [ 0 ] ) , list ( padding [ 1 ] ) , [ 0 , 0 ] ] ) pattern = [ list ( p ) for p in pad_info ] return pad ( x , padding , data_format , num_dynamic_axis ) assert len ( x.shape ) == 5 - ( 1 if num_dynamic_axis > 0 else 0 ) x = C.pad ( x , pattern= [ [ 0 , 0 ] , [ 0 , 0 ] , list ( padding [ 0 ] ) , list ( padding [ 1 ] ) , list ( padding [ 2 ] ) ] ) x = C.pad ( x , pattern= [ [ 0 , 0 ] , list ( padding [ 0 ] ) , list ( padding [ 1 ] ) , list ( padding [ 2 ] ) , [ 0 , 0 ] ] ) x = _padding ( x , padding [ 2 ] , 4 ) x = C.pad ( x , pattern= [ [ 0 , 0 ] , [ 0 , 0 ] , list ( padding [ 0 ] ) , list ( padding [ 1 ] ) ] ) if data_format == 'channels_first ' : for ( a , p ) in enumerate ( pad_info ) : else : else : else : if data_format == 'channels_first ' : return pad ( x , [ padding ] , 'channels_last ' , num_dynamic_axis ) def pad ( x , pad_info , data_format , num_dynamic_axis ) : x = _padding ( x , p , assert len ( x.shape ) == 4 - ( 1 if num_dynamic_axis > 0 else 0 ) x = _padding ( x , padding [ 1 ] , 3 ) x = C.pad ( x , pattern= [ [ 0 , 0 ] , list ( padding [ 0 ] ) , list ( padding [ 1 ] ) ] ) a + ( 1 if num_dynamic_axis == 0 else 0 ) ( 1 if data_format == 'channels_first ' else 0 ) ) return x x = C.pad ( x , pattern= [ list ( padding [ 0 ] ) , list ( padding [ 1 ] ) , list ( padding [ 2 ] ) , [ 0 , 0 ] ] ) assert len ( x.shape ) == 3 - ( 1 if num_dynamic_axis > 0 else 0 ) x = C.pad ( x , pattern= [ padding , ( 0 , 0 ) ] ) x = _padding ( x , padding [ 0 ] , 0 ) pattern = pattern + [ [ 0 , 0 ] ]",['keras/backend/cntk_backend.py'],Increase test coverages by factorizing CNTK pads ( # 10259 )
436,2eed2a5d688e8720740f30d281a18fb2c13a15fd,2018-05-21 11:11:43-07:00,"data_format='channels_middle ' ) K.variable ( np.ones ( ( 1 , 1 , 12 , 7 ) ) ) , if K.backend ( ) == 'cntk ' : dummy_w_3d = K.variable ( np.ones ( ( 2 , 2 , 2 , 3 , 4 ) ) ) K.separable_conv2d ( dummy_x_2d , dummy_w_2d , dummy_w1x1_2d , dummy_w_2d = K.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) K.conv1d ( dummy_x_1d , dummy_w_1d , data_format='channels_middle ' ) dummy_w1x1_2d = K.variable ( np.ones ( ( 1 , 1 , 12 , 7 ) ) ) K.conv3d ( dummy_x_3d , dummy_w_3d , data_format='channels_middle ' ) K.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , dummy_x_2d = K.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) dilation_rate= ( 1 , 2 ) ) K.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , dummy_x_3d = K.variable ( np.ones ( ( 2 , 3 , 4 , 5 , 4 ) ) ) K.separable_conv2d ( K.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , with pytest.raises ( ValueError ) : K.conv2d ( dummy_x_2d , dummy_w_2d , data_format='channels_middle ' ) K.variable ( np.ones ( ( 3 , 2 , 3 ) ) ) , dummy_w_1d = K.variable ( np.ones ( ( 3 , 2 , 3 ) ) ) dummy_x_1d = K.variable ( np.ones ( ( 4 , 8 , 2 ) ) ) strides= ( 2 , 2 ) , dilation_rate= ( 1 , 2 ) ) K.depthwise_conv2d ( dummy_x_2d , dummy_w_2d , K.conv2d ( K.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , K.depthwise_conv2d ( dummy_x_2d , dummy_w_2d , K.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , K.conv1d ( K.variable ( np.ones ( ( 4 , 8 , 2 ) ) ) , K.depthwise_conv2d ( K.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , K.variable ( np.ones ( ( 2 , 2 , 2 , 3 , 4 ) ) ) , K.conv3d ( K.variable ( np.ones ( ( 2 , 3 , 4 , 5 , 4 ) ) ) ,",['tests/keras/backend/backend_test.py'],Increase test coverages by adding invalid CNTK usecases ( # 10236 )
437,bf1378f39d02b7d0b53ece5458f9275ac8208046,2018-05-18 09:59:16-07:00,"# duplication . input_shape = K.int_shape ( x ) [ 1 : ] 'parts ' : num_gpus } ) ( x ) with tf.device ( x.device ) : # In-place input splitting which is not only # 5 % ~ 12 % faster but also less GPU memory input_shape = K.int_shape ( x ) [ 1 : ] arguments= { ' i ' : i , slice_i = Lambda ( get_slice , output_shape=input_shape , arguments= { ' i ' : i , 'parts ' : num_gpus } ) ( x ) output_shape=input_shape , inputs.append ( slice_i ) slice_i = Lambda ( get_slice , inputs.append ( slice_i )",['keras/utils/multi_gpu_utils.py'],In-place split to avoid inter-device duplication ( # 10230 )
438,14ff5175181e680d75b8d4ad6fe0d7d4dd35beff,2018-05-18 11:06:39+09:00,"model_name = 'nasnet_mobile_no_top.h5 ' weights_file = get_file ( model_name , weight_path , if not skip_reduction : NASNET_MOBILE_WEIGHT_PATH_NO_TOP , NASNET_MOBILE_WEIGHT_PATH_NO_TOP = 'https : //github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile-no-top.h5 ' x , p = _reduction_a_cell ( x , p , filters // filter_multiplier , block_id='reduction_1_ % s ' % block_id ) x , p = _reduction_a_cell ( x , p , filters // ( filter_multiplier * * 2 ) , NASNET_MOBILE_WEIGHT_PATH = 'https : //github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile.h5 ' x = Conv2D ( stem_block_filters , ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='valid ' , x = Conv2D ( stem_block_filters , ( 3 , 3 ) , strides= ( 2 , 2 ) , padding='valid ' , weight_path = NASNET_MOBILE_WEIGHT_PATH_NO_TOP weights_path = get_file ( file_hash='1ed92395b5b598bdda52abe5c0dbfd63 ' ) weight_path = NASNET_MOBILE_WEIGHT_PATH model_name = 'nasnet_large_no_top.h5 ' x , p = _reduction_a_cell ( x , p , filters // ( filter_multiplier * * 2 ) , 'nasnet_large.h5 ' , NASNET_LARGE_WEIGHT_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/NASNet-large-no-top.h5 ' cache_subdir='models ' ) model.load_weights ( weights_path ) skip_reduction=True , block_id='stem_1 ' ) NASNET_LARGE_WEIGHT_PATH , weight_path = NASNET_LARGE_WEIGHT_PATH_NO_TOP file_hash='020fb642bf7360b370c678b08e0adf61 ' ) end of the network . Set to ` False ` for CIFAR models . 'nasnet_mobile_no_top.h5 ' , NASNET_MOBILE_WEIGHT_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/NASNet-mobile.h5 ' model_name = 'nasnet_mobile.h5 ' file_hash='11577c9a518f0070763c2b964a382f17 ' ) weight_path = NASNET_LARGE_WEIGHT_PATH NASNET_LARGE_WEIGHT_PATH = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/NASNet-large.h5 ' NASNET_MOBILE_WEIGHT_PATH , 'nasnet_large_no_top.h5 ' , block_id='stem_1 ' ) file_hash='d81d89dc07e6e56530c4e77faddd61b5 ' ) cache_subdir='models ' , block_id='stem_2 ' ) use_bias=False , name='stem_conv1 ' , NASNET_MOBILE_WEIGHT_PATH_NO_TOP = 'https : //github.com/fchollet/deep-learning-models/releases/download/v0.8/NASNet-mobile-no-top.h5 ' x , p = _reduction_a_cell ( x , p , filters // filter_multiplier , block_id='reduction_right1_ % s ' % block_id ) NASNET_LARGE_WEIGHT_PATH_NO_TOP = 'https : //github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-large-no-top.h5 ' block_id='stem_2 ' ) use_bias=False , name='stem_conv1 ' , kernel_initializer='he_normal ' ) ( img_input ) 'nasnet_mobile.h5 ' , else : model_name = 'nasnet_large.h5 ' skip_reduction=False , kernel_initializer='he_normal ' ) ( img_input ) x = Conv2D ( stem_block_filters , ( 3 , 3 ) , strides= ( 1 , 1 ) , padding='same ' , NASNET_LARGE_WEIGHT_PATH = 'https : //github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-large.h5 ' model.load_weights ( weights_file ) NASNET_LARGE_WEIGHT_PATH_NO_TOP , end of the network . if not skip_reduction : # imagenet / mobile mode",['keras/applications/nasnet.py'],Fix NASNet ( # 10209 )
439,3e9048bccaba33805d96ec29f5139e5d6ec40358,2018-05-17 09:56:40-07:00,return ( self.end_index - self.start_index self.batch_size * self.stride ) // ( self.batch_size * self.stride ) return int ( np.ceil ( ( self.batch_size * self.stride ) ) ) ( self.end_index - self.start_index + 1 ) /,['keras/preprocessing/sequence.py'],Replace np.ceil ( ) with a faster operation ( # 10184 )
440,44fd649069bac653accd0a47550619d69015a91b,2018-05-15 11:26:10-07:00,"do not match then we will throw an error . If ` alpha ` > 1.0 , proportionally increases the number Depending on the use case , it can use different input layer size and depth_multiplier : depth multiplier for depthwise convolution 'imagenet ' ( pre-training on ImageNet ) , weights : one of ` None ` ( random initialization ) , to use as image input for the model . You can also omit this option if you would like classes : optional number of classes to classify images MobileNetV2 is a general architecture and can be used for multiple use cases . # # # Returns [ MobileNetV2 : Inverted Residuals and Linear Bottlenecks ] ( https : //arxiv.org/abs/1801.04381 ) width multiplier in the MobileNetV2 paper . E.g . ` ( 160 , 160 , 3 ) ` would be one valid value . are used at each layer . can be modified by using the ` alpha ` parameter , If ` alpha ` < 1.0 , proportionally decreases the number If you choose to include both input_tensor and input_shape then # # # Raises # # # License input_shape will be used if they match , if the shapes into , only to be specified if ` include_top ` is True , and ( 224 , 224 , 3 ) . ` layers.Input ( ) ` ) include_top : whether to include the fully-connected layer at the top of the network . # # mobilenet v2 `` ` It should have exactly 3 inputs channels ( 224 , 224 , 3 ) . `` ` python input_shape : optional shape tuple , to be specified if you would The number of parameters and number of multiply-adds to infer input_shape from an input_tensor . or the path to the weights file to be loaded . A Keras model instance . If ` alpha ` = 1 , default number of filters from the paper input_tensor : optional Keras tensor ( i.e . output of different width factors . This allows different width models to reduce if no ` weights ` argument is specified . like to use a model with an input img resolution that is not of filters in each layer . which increases/decreases the number of filters in each layer . or invalid input shape or invalid depth_multiplier , alpha , ValueError : in case of invalid argument for ` weights ` , # # # Arguments These weights are released under [ the Apache License ] ( https : //github.com/tensorflow/models/blob/master/LICENSE ) . the number of multiply-adds and thereby rows when weights='imagenet ( also called the resolution multiplier ) # # # References keras.applications.mobilenetv2 ( input_shape=None , alpha=1.0 , depth_multiplier=1 , include_top=True , weights='imagenet ' , input_tensor=None , classes=1000 ) reduce inference cost on mobile devices . alpha : controls the width of the network . This is known as the",['docs/templates/applications.md'],Mobilenetv2 explanation ( # 10174 )
441,8532744ad3de309e807ade1b9b6eff9603305b1a,2018-05-15 11:09:09-07:00,"'wrap ' : abcdabcd|abcd|abcdabcd starting_point = anchor_pos + len ( anchor ) + 1 # Reinject arguments and returns blocks . height_shift_range : Float , 1-D array-like or int docstring , returns = process_list_block ( docstring , `` # Returns '' , `` $ RETURNS $ '' ) if anchor_pos > -1 : block = docstring [ starting_point : docstring.find ( `` \n\n '' , starting_point ) ] 'constant ' : kkkkkkkk|abcd|kkkkkkkk ( cval=k ) block = '\n'.join ( lines ) return docstring , block def process_list_block ( docstring , anchor , marker ) : docstring = docstring.replace ( `` $ ARGUMENTS $ '' , arguments ) the interval [ -1.0 , +1.0 ) . # White spaces to be removed in order to correctly align the block . are integers ` [ -1 , 0 , +1 ] ` , docstring , arguments = process_list_block ( docstring , `` # Arguments '' , `` $ ARGUMENTS $ '' ) float : fraction of total width , if < 1 , or pixels if > = 1 . whitespace_n = re.search ( `` [ ^\s ] '' , block ) .start ( ) float : fraction of total height , if < 1 , or pixels if > = 1 . r ' - __\1__ : \2\n ' , With ` height_shift_range=2 ` possible values 'reflect ' : abcddcba|abcd|dcbaabcd 'wrap ' : abcdabcd|abcd|abcdabcd 1-D array-like : random elements from the array . docstring = re.sub ( r ' ( [ ^\s\\\ ( ] + ) : ( . * ) \n ' , int : integer number of pixels from interval 'constant ' : kkkkkkkk|abcd|kkkkkkkk ( cval=k ) 'nearest ' : aaaaaaaa|abcd|dddddddd 1-D array-like : random elements from the array . # Place marker for later reinjection . 'reflect ' : abcddcba|abcd|dcbaabcd top_level_regex = r'^ ( [ ^\s\\\ ( ] + ) : ( . * ) ' block = `` '' # Format docstring lists anchor_pos = docstring.find ( anchor ) # Format docstring lists . docstring = docstring.replace ( `` $ RETURNS $ '' , returns ) same as with ` height_shift_range= [ -1 , 0 , +1 ] ` , If None , no labels are returned docstring = docstring.replace ( block , marker ) lines = [ re.sub ( top_level_regex , top_level_replacement , line ) for line in lines ] 'nearest ' : aaaaaaaa|abcd|dddddddd If None , no labels are returned float : fraction of total width , if < 1 , or pixels if > = 1 . int : integer number of pixels from interval With ` width_shift_range=2 ` possible values docstring ) # Format docstring lists . With ` width_shift_range=2 ` possible values top_level_replacement = r'- __\1__ : \2 ' ` ( -height_shift_range , +height_shift_range ) ` lines = [ re.sub ( '^ ' + ' ' * whitespace_n , `` , line ) for line in block.split ( '\n ' ) ] # Remove the computed number of leading white spaces from each line . while with ` height_shift_range=1.0 ` possible values are floats in","['docs/autogen.py', 'keras/preprocessing/image.py']",Adds height_shift_range to preprocessing.image doc and adds support for indentation in auto-generated doc ( # 10194 )
442,50411ccb0e7059ad2bd9286b64ffc2cda8957025,2018-05-15 10:08:54-07:00,"_keras_dir = os.path.join ( _keras_base_dir , '.keras ' ) if not os.access ( _keras_base_dir , os.W_OK ) : _keras_base_dir = '/tmp ' if 'KERAS_HOME ' in os.environ : _keras_base_dir = '/tmp ' # Obtain Keras base dir path : either ~/.keras or /tmp . # Set Keras base dir path given KERAS_HOME env variable , if applicable . if not os.access ( _keras_base_dir , os.W_OK ) : _keras_dir = os.environ.get ( 'KERAS_HOME ' ) _keras_dir = os.path.join ( _keras_base_dir , '.keras ' ) _keras_base_dir = os.path.expanduser ( '~ ' ) _keras_base_dir = os.path.expanduser ( '~ ' ) else : # Otherwise either ~/.keras or /tmp .",['keras/backend/__init__.py'],Allow project specific config files in Keras ( # 10183 )
443,bcf0031b54d555179be81c088cc3df0a723d7907,2018-05-14 16:52:41-07:00,"layer.output ) layer.output ) input_shape= ( input_dim , input_dim ) , input_shape= ( input_dim , ) , inp_pair = Lambda ( lambda x : x ) ( [ inp_3d , inp_2d ] ) # test a layer with a list of output tensors tf.summary.histogram ( ' { } _out'.format ( layer.name ) , tf.summary.histogram ( ' { } _out_ { } '.format ( layer.name , i ) , output ) hidden = Dense ( num_hidden , activation='relu ' ) ( hidden ) hidden = dot ( inp_pair , axes=-1 ) else : inp_2d = GlobalAveragePooling1D ( ) ( inp_3d ) inp2 = Input ( ( input_dim , input_dim ) ) from keras.layers import Input , Dense , Dropout , add hidden = Dense ( num_hidden , activation='relu ' ) ( inp ) if isinstance ( layer.output , list ) : tf.summary.histogram ( ' { } _out'.format ( layer.name ) , for i , output in enumerate ( layer.output ) : inp2 = Input ( ( input_dim , ) ) inp_3d = add ( [ inp1 , inp2 ] ) inp1 = Input ( ( input_dim , input_dim ) ) inp1 = Input ( ( input_dim , ) ) inp = add ( [ inp1 , inp2 ] ) from keras.layers.pooling import MaxPooling2D , GlobalAveragePooling1D , GlobalAveragePooling2D from keras.layers.pooling import MaxPooling2D , GlobalAveragePooling2D from keras.layers import Input , Dense , Dropout , add , dot , Lambda","['keras/callbacks.py', 'tests/keras/test_callbacks.py']",fix TensorBoard callback with unit test ( # 10173 )
444,d7884570b10951d156aa086ee29a4df9eab79cf3,2018-05-14 15:51:20-07:00,"return False current_uses_correlation = True if K.backend ( ) ! = 'tensorflow ' : return uses_correlation [ original_backend ] ! = uses_correlation [ K.backend ( ) ] return uses_correlation [ original_backend ] ! = current_uses_correlation if K.backend ( ) in [ 'cntk ' , 'theano ' ] : # By default , do not convert the kernels if the original backend is unknown else : if original_backend not in uses_correlation : if K.backend ( ) in uses_correlation : # Assume unknown backends use correlation current_uses_correlation = uses_correlation [ K.backend ( ) ]","['keras/applications/nasnet.py', 'keras/engine/saving.py']",Allow dynamic backends in _need_convert_kernel ( # 10111 )
445,a6afe7c8149c61372b2aae6ae331d1584eb8a19e,2018-05-14 12:30:54-07:00,"except TypeError : def int_or_none ( value ) : # As of Keras 2.0.0 , all kernels are normalized depthwise_kernel = reshape ( depthwise_kernel , depthwise_kernel_shape ) depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape ( depthwise_kernel_shape , data_format ) def _preprocess_conv2d_depthwise_filter_shape ( filter_shape , data_format ) : # Theano expects ` ( input_depth * depth , 1 , rows , cols ) ` for depthwise convolution . depthwise_kernel = _preprocess_conv2d_kernel ( depthwise_kernel , data_format ) return kernel kernel = reshape ( kernel , kernel_shape ) depthwise_kernel = _preprocess_conv2d_depthwise_kernel ( depthwise_kernel , depthwise_kernel_shape , data_format ) def _preprocess_conv2d_depthwise_kernel ( kernel , kernel_shape , data_format ) : depthwise_kernel_shape = _preprocess_conv2d_filter_shape ( depthwise_kernel_shape , data_format ) input_depth = depthwise_kernel_shape [ 1 ] if filter_shape : return None return filter_shape depthwise_kernel_shape = ( input_depth * output_depth , 1 ) + depthwise_kernel_shape [ 2 : ] filter_shape = tuple ( int_or_none ( v ) for v in filter_shape ) output_depth = depthwise_kernel_shape [ 0 ] filter_shape [ 0 ] , filter_shape [ 1 ] ) try : depthwise_kernel = depthwise_kernel.dimshuffle ( ( 1 , 0 , 2 , 3 ) ) # Theano might not accept long type filter_shape = ( filter_shape [ 3 ] * filter_shape [ 2 ] , 1 , return int ( value ) kernel = kernel [ : :-1 , : :-1 , : , : ] # independently of ` data_format ` . num_groups=input_depth ) kernel = kernel.dimshuffle ( ( 2 , 3 , 0 , 1 ) ) # on the format ` ( rows , cols , input_depth , depth ) ` , num_groups=image_shape [ 1 ] ) depthwise_kernel = depthwise_kernel [ : , : , : :-1 , : :-1 ] if filter_shape is not None :",['keras/backend/theano_backend.py'],Clean up preprocessing of ` depthwise_kernel ` for Theano ( # 10189 )
446,36176924bb5d0fe248a56362c6f776799434ebc0,2018-05-14 11:24:51-07:00,"` build ( input_shape ) ` : this is where you will define your weights . This method must set ` self.built = True ` at the end , which can be done by calling ` super ( [ Layer ] , self ) .build ( ) ` . ` build ( input_shape ) ` : this is where you will define your weights . This method must set ` self.built = True ` , which can be done by calling ` super ( [ Layer ] , self ) .build ( ) ` . super ( MyLayer , self ) .build ( input_shape ) # Be sure to call this at the end super ( MyLayer , self ) .build ( input_shape ) # Be sure to call this somewhere !",['docs/templates/layers/writing-your-own-keras-layers.md'],Best to set self.built=True at the end of build ( ) ( # 10191 )
447,cbadaf00e28f7fe42762b55f52294e3a7bb90515,2018-05-11 18:04:00+09:00,"x = Activation ( relu6 , name='conv_dw_ % d_relu ' % block_id ) ( x ) kernel_size=1 , name='mobl % d_conv_depthwise ' % block_id ) ( inputs ) use_bias=False , padding='same ' , if block_id : x = Conv2D ( pointwise_filters , def _first_inverted_res_block ( inputs , expansion , stride , use_bias=False , activation=None , x = _first_inverted_res_block ( x , x = Activation ( relu6 , name=prefix + 'depthwise_relu ' ) ( x ) return x name='bn % d_conv_depthwise ' % block_id ) ( x ) stride=1 , filters=16 , name=prefix + 'depthwise_BN ' ) ( x ) prefix = 'block_ { } _'.format ( block_id ) return Add ( name=prefix + 'add ' ) ( [ inputs , x ] ) dropout=1e-3 , name='bn % d_conv_bn_expand ' % name=prefix + 'project ' ) ( x ) block_id ) ( x ) name='mobl % d_conv_expand ' % block_id ) ( inputs ) pointwise_conv_filters = int ( filters * alpha ) use_bias=False , activation=None , x = BatchNormalization ( epsilon=1e-3 , momentum=0.999 , prefix = 'expanded_conv_ ' alpha , filters , block_id ) : dropout : dropout rate , dropout is currently not in use x = BatchNormalization ( epsilon=1e-3 , momentum=0.999 , x = Activation ( relu6 , name=prefix + 'expand_relu ' ) ( x ) # Depthwise use_bias=False , x = _inverted_res_block ( x , filters=16 , alpha=alpha , stride=1 , block_id=0 ) x = Conv2D ( expansion * in_channels , kernel_size=1 , padding='same ' , name=prefix + 'expand ' ) ( x ) name=prefix + 'project_BN ' ) ( x ) name='bn % d_conv_bn_project ' % block_id ) ( x ) expansion=1 , name='mobl % d_conv_depthwise ' % block_id ) ( x ) name='bn % d_conv_depthwise ' % pointwise_filters = _make_divisible ( pointwise_conv_filters , 8 ) alpha=alpha , x = Conv2D ( expansion * in_channels , kernel_size=1 , padding='same ' , x = DepthwiseConv2D ( kernel_size=3 , x = Activation ( relu6 , name='conv_ % d_relu ' % block_id ) ( x ) x = inputs in_channels = inputs._keras_shape [ -1 ] block_id ) ( x ) # Expand strides=stride , activation=None , return Add ( name='res_connect_ ' + str ( block_id ) ) ( [ inputs , x ] ) # Expand name='mobl % d_conv_project ' % block_id ) ( x ) name=prefix + 'depthwise ' ) ( x ) activation=None , else : name='bn % d_conv_project ' % expansion=1 , block_id=0 ) padding='same ' , prefix = 'features . ' + str ( block_id ) + '.conv . ' if in_channels == pointwise_filters and stride == 1 : # Project name='mobl % d_conv_project ' % name=prefix + 'expand_BN ' ) ( x )",['keras/applications/mobilenetv2.py'],Merge 2 functions together in applications MobileNetV2 ( # 10163 )
448,0ba6d95e768eb7a0d74a6fda3ea893e7fb2d7a67,2018-05-10 13:38:01-07:00,"_x_decoded_mean = decoder_mean ( _h_decoded ) from __future__ import division plt.figure ( figsize= ( 6 , 6 ) ) _x_decoded_mean_squash = decoder_mean_squash ( _x_decoded_relu ) # instead of sampling from Q ( z|X ) , sample eps = N ( 0 , I ) stddev=epsilon_std ) latent_inputs = Input ( shape= ( latent_dim , ) , name='z_sampling ' ) kernel_size=num_conv , x_test = np.reshape ( x_test , [ -1 , original_dim ] ) ' '' This script demonstrates how to build a variational autoencoder with Keras . z_log_var = Dense ( latent_dim ) ( h ) plt.show ( ) plt.yticks ( pixel_range , sample_range_y ) ( x_train , _ ) , ( x_test , y_test ) = mnist.load_data ( ) plt.scatter ( x_test_encoded [ : , 0 ] , x_test_encoded [ : , 1 ] , c=y_test ) conv_2 = Conv2D ( filters , vae.compile ( optimizer='rmsprop ' ) hidden = Dense ( intermediate_dim , activation='relu ' ) ( flat ) vae.summary ( ) vae.add_loss ( vae_loss ) vae.add_loss ( vae_loss ) plt.imshow ( figure , cmap='Greys_r ' ) from keras.layers import Reshape , Conv2DTranspose padding='same ' , activation='relu ' ) ( x ) padding='same ' ) ( x ) batch_size=batch_size ) x_test = x_test.reshape ( ( len ( x_test ) , np.prod ( x_test.shape [ 1 : ] ) ) ) reconstruction_loss = mse ( inputs , outputs ) h_decoded = decoder_h ( z ) batch_size=batch_size , ' '' Example of VAE on MNIST dataset using MLP conv_3 = Conv2D ( filters , decoder_hid = Dense ( intermediate_dim , activation='relu ' ) x_decoded_mean_squash = decoder_mean_squash ( x_decoded_relu ) original_dim = image_size * image_size conv_4 = Conv2D ( filters , _deconv_2_decoded = decoder_deconv_2 ( _deconv_1_decoded ) grid_y = norm.ppf ( np.linspace ( 0.05 , 0.95 , n ) ) batch = K.shape ( z_mean ) [ 0 ] validation_data= ( x_test , None ) ) original_img_size = ( img_rows , img_cols , img_chns ) # instantiate VAE model from keras.layers import Input , Dense , Lambda , Flatten , Reshape end_range = n * digit_size + start_range + 1 z = Lambda ( sampling , output_shape= ( latent_dim , ) ) ( [ z_mean , z_log_var ] ) ' '' Example of VAE on MNIST dataset using CNN # reparameterization trick vae = Model ( x , x_decoded_mean ) else : _h_decoded = decoder_h ( decoder_input ) padding='same ' ) ( x ) _x_decoded_relu = decoder_deconv_3_upsamp ( _deconv_2_decoded ) data ( tuple ) : test data and label strides=2 , # Compute VAE loss x_train = x_train.reshape ( ( len ( x_train ) , np.prod ( x_train.shape [ 1 : ] ) ) ) The decoder can be used to generate MNIST digits by sampling the `` -- mse '' , latent vector from a Gaussian distribution with mean=0 and std=1 . _hid_decoded = decoder_hid ( decoder_input ) plt.figure ( figsize= ( 10 , 10 ) ) model_name= '' vae_mnist '' ) : vae = vae.load_weights ( args.weights ) original_img_size = ( img_chns , img_rows , img_cols ) sample_range_y = np.round ( grid_y , 1 ) input_shape = ( original_dim , ) from keras import metrics kl_loss = - 0.5 * K.sum ( 1 + z_log_var - K.square ( z_mean ) - K.exp ( z_log_var ) , axis=-1 ) outputs = Conv2DTranspose ( filters=1 , # Reference # Reference to_file='vae_mlp.png ' , decoder_mean_squash = Conv2D ( img_chns , x_train = np.reshape ( x_train , [ -1 , image_size , image_size , 1 ] ) # train the autoencoder strides=1 , epochs = 50 z_mean = Dense ( latent_dim ) ( hidden ) x_test , y_test = data xent_loss = img_rows * img_cols * metrics.binary_crossentropy ( figure [ i * digit_size : ( i + 1 ) * digit_size , filters * = 2 digit = x_decoded [ 0 ] .reshape ( digit_size , digit_size ) input_shape = ( image_size , image_size , 1 ) the encoder can be used to generate latent vectors . # display a 2D plot of the digit classes in the latent space strides=2 , https : //arxiv.org/abs/1312.6114 args ( tensor ) : mean and log of variance of Q ( z|X ) # display a 30x30 2D manifold of digits kl_loss = K.sum ( kl_loss , axis=-1 ) z_sample = np.tile ( z_sample , batch_size ) .reshape ( batch_size , 2 ) data = ( x_test , y_test ) encoder.summary ( ) kernel_size= ( 2 , 2 ) , # build decoder model x_test = x_test.reshape ( ( x_test.shape [ 0 ] , ) + original_img_size ) return z_mean + K.exp ( z_log_var / 2 ) * epsilon x = Conv2D ( filters=filters , x = Input ( shape= ( original_dim , ) ) ' '' This script demonstrates how to build a variational autoencoder num_conv = 3 padding='same ' , # then z = z_mean + sqrt ( var ) * eps `` `` '' Reparameterization trick by sampling fr an isotropic unit Gaussian . # of digit classes in the latent space from keras import backend as K h = Dense ( intermediate_dim , activation='relu ' ) ( x ) _deconv_1_decoded = decoder_deconv_1 ( _reshape_decoded ) if __name__ == '__main__ ' : output_shape = ( batch_size , 29 , 29 , filters ) plt.colorbar ( ) kernel_size=kernel_size , vae.fit ( x_train , digit = x_decoded [ 0 ] .reshape ( digit_size , digit_size ) encoder , decoder = models # generate latent vector Q ( z|X ) original_dim = 784 x_train = x_train.astype ( 'float32 ' ) / 255 . x_test = x_test.astype ( 'float32 ' ) / 255 . grid_x = np.linspace ( -4 , 4 , n ) are 3 models that share weights . After training the VAE model , strides= ( 2 , 2 ) , _up_decoded = decoder_upsample ( _hid_decoded ) epochs=epochs , x_test_encoded = encoder.predict ( x_test , batch_size=batch_size ) digit_size = 28 plot_model ( vae , to_file='vae_cnn.png ' , show_shapes=True ) strides= ( 2 , 2 ) ) ( conv_1 ) from keras.models import Model The VAE has a modular design . The encoder , decoder and VAE x_decoded = generator.predict ( z_sample , batch_size=batch_size ) vae_loss = K.mean ( reconstruction_loss + kl_loss ) outputs ) intermediate_dim = 512 j * digit_size : ( j + 1 ) * digit_size ] = digit epochs = 50 help=help_ , action='store_true ' ) padding='valid ' , epsilon = K.random_normal ( shape= ( batch , dim ) ) from keras.layers import Conv2D , Flatten , Lambda vae.compile ( optimizer='adam ' ) batch_size = 100 # train the VAE on MNIST digits x_test = np.reshape ( x_test , [ -1 , image_size , image_size , 1 ] ) epochs = 30 K.flatten ( x_decoded_mean_squash ) ) kl_loss * = -0.5 kernel_size=kernel_size , # so you could write ` Lambda ( sampling ) ( [ z_mean , z_log_var ] ) ` import argparse x_train = x_train.reshape ( ( x_train.shape [ 0 ] , ) + original_img_size ) plt.colorbar ( ) https : //arxiv.org/abs/1312.6114 n = 30 decoder.summary ( ) grid_y = np.linspace ( -4 , 4 , n ) [ : :-1 ] image_size = x_train.shape [ 1 ] epochs=epochs , plt.xticks ( pixel_range , sample_range_x ) kl_loss = 1 + z_log_var - K.square ( z_mean ) - K.exp ( z_log_var ) def plot_results ( models , from keras.layers import Input , Dense , Lambda data , return z_mean + K.exp ( z_log_var ) * epsilon models ( tuple ) : encoder and decoder models img_rows , img_cols , img_chns = 28 , 28 , 1 x = Input ( shape=original_img_size ) n = 15 # figure with 15x15 digits parser.add_argument ( `` -m '' , plt.ylabel ( `` z [ 1 ] '' ) from keras.datasets import mnist decoder_deconv_1 = Conv2DTranspose ( filters , https : //arxiv.org/abs/1312.6114 for j , xi in enumerate ( grid_y ) : generator = Model ( decoder_input , _x_decoded_mean ) print ( 'x_train.shape : ' , x_train.shape ) plt.show ( ) # instantiate VAE model shape = K.int_shape ( x ) import os figure [ i * digit_size : ( i + 1 ) * digit_size , strides=1 ) ( conv_3 ) from scipy.stats import norm z ( tensor ) : sampled latent vector generator = Model ( decoder_input , _x_decoded_mean_squash ) vae.compile ( optimizer='rmsprop ' ) os.makedirs ( model_name , exist_ok=True ) # note that `` output_shape '' is n't necessary with the TensorFlow backend plt.figure ( figsize= ( 10 , 10 ) ) show_shapes=True ) hid_decoded = decoder_hid ( z ) from keras.layers import Conv2D , Conv2DTranspose reconstruction_loss * = original_dim xent_loss = original_dim * metrics.binary_crossentropy ( x , x_decoded_mean ) name='decoder_output ' ) ( x ) # Arguments : # VAE model = encoder + decoder activation='sigmoid ' ) if args.mse : batch_size=batch_size , ( x_train , y_train ) , ( x_test , y_test ) = mnist.load_data ( ) from keras.losses import mse , binary_crossentropy # build a model to project inputs on the latent space plt.scatter ( z_mean [ : , 0 ] , z_mean [ : , 1 ] , c=y_test ) # use reparameterization trick to push the sampling out as input intermediate_dim = 256 sample_range_x = np.round ( grid_x , 1 ) digit_size = 28 z_mean = Dense ( latent_dim ) ( h ) vae.fit ( x_train , validation_data= ( x_test , None ) ) vae = Model ( inputs , outputs , name='vae ' ) strides=1 ) ( conv_2 ) vae_loss = K.mean ( xent_loss + kl_loss ) filters //= 2 # shape info needed to build decoder model x = Dense ( intermediate_dim , activation='relu ' ) ( latent_inputs ) dim = K.int_shape ( z_mean ) [ 1 ] x = Dense ( intermediate_dim , activation='relu ' ) ( inputs ) mean=0. , stddev=epsilon_std ) vae = Model ( x , x_decoded_mean_squash ) filters = 64 # build a digit generator that can sample from the learned distribution activation='relu ' ) x = inputs plt.xlabel ( `` z [ 0 ] '' ) intermediate_dim = 128 # linearly spaced coordinates corresponding to the 2D plot activation='relu ' ) # number of convolutional filters to use decoder_upsample = Dense ( filters * 14 * 14 , activation='relu ' ) kernel_size = 3 reconstruction_loss = binary_crossentropy ( K.flatten ( inputs ) , x_train = np.reshape ( x_train , [ -1 , original_dim ] ) epsilon_std = 1.0 plot_results ( models , with Keras and deconvolution layers . x = Dense ( shape [ 1 ] * shape [ 2 ] * shape [ 3 ] , activation='relu ' ) ( latent_inputs ) for j , xi in enumerate ( grid_x ) : filename = os.path.join ( model_name , `` digits_over_latent.png '' ) x = Conv2DTranspose ( filters=filters , from __future__ import absolute_import # VAE loss = mse_loss or xent_loss + kl_loss deconv_2_decoded = decoder_deconv_2 ( deconv_1_decoded ) z = Lambda ( sampling , output_shape= ( latent_dim , ) , name= ' z ' ) ( [ z_mean , z_log_var ] ) encoder = Model ( x , z_mean ) outputs = Dense ( original_dim , activation='sigmoid ' ) ( x ) x_decoded_relu = decoder_deconv_3_upsamp ( deconv_2_decoded ) def sampling ( args ) : plt.figure ( figsize= ( 12 , 10 ) ) vae.save_weights ( 'vae_cnn_mnist.h5 ' ) help_ = `` Load h5 model trained weights '' for i , yi in enumerate ( grid_y ) : ( x_train , y_train ) , ( x_test , y_test ) = mnist.load_data ( ) from keras.models import Model if K.image_data_format ( ) == 'channels_first ' : plot_model ( encoder , to_file='vae_mlp_encoder.png ' , show_shapes=True ) # input image dimensions from keras.datasets import mnist decoder_mean = Dense ( original_dim , activation='sigmoid ' ) # we instantiate these layers separately so as to reuse them later parser.add_argument ( `` -m '' , `` -- mse '' , help=help_ , action='store_true ' ) plot_results ( models , data , batch_size=batch_size , model_name= '' vae_cnn '' ) # by default , random_normal has mean=0 and std=1.0 plt.imshow ( figure , cmap='Greys_r ' ) grid_x = norm.ppf ( np.linspace ( 0.05 , 0.95 , n ) ) activation='sigmoid ' , for i , yi in enumerate ( grid_x ) : batch_size=128 , z_sample = np.array ( [ [ xi , yi ] ] ) plot_model ( encoder , to_file='vae_cnn_encoder.png ' , show_shapes=True ) # display a 2D manifold of the digits plot_model ( decoder , to_file='vae_cnn_decoder.png ' , show_shapes=True ) for i in range ( 2 ) : # z = z_mean + sqrt ( var ) * eps # linearly spaced coordinates on the unit square were transformed through the inverse CDF ( ppf ) of the Gaussian reconstruction_loss = mse ( K.flatten ( inputs ) , K.flatten ( outputs ) ) _reshape_decoded = decoder_reshape ( _up_decoded ) z_mean , _ , _ = encoder.predict ( x_test , x = Dense ( 16 , activation='relu ' ) ( x ) inputs = Input ( shape=input_shape , name='encoder_input ' ) plot_model ( decoder , to_file='vae_mlp_decoder.png ' , show_shapes=True ) reshape_decoded = decoder_reshape ( up_decoded ) model_name= '' vae_mlp '' ) decoder_deconv_3_upsamp = Conv2DTranspose ( filters , # build encoder model pixel_range = np.arange ( start_range , end_range , digit_size ) encoder = Model ( inputs , [ z_mean , z_log_var , z ] , name='encoder ' ) z_log_var = Dense ( latent_dim ) ( hidden ) kernel_size=num_conv , plot_model ( vae , z_mean = Dense ( latent_dim , name='z_mean ' ) ( x ) Auto-Encoding Variational Bayes return z_mean + K.exp ( 0.5 * z_log_var ) * epsilon `` `` '' Plots labels and MNIST digits as function of 2-dim latent vector def sampling ( args ) : x_train = x_train.astype ( 'float32 ' ) / 255 help_ = `` Use mse loss instead of binary cross entropy ( default ) '' flat = Flatten ( ) ( conv_4 ) batch_size ( int ) : prediction batch size else : decoder_input = Input ( shape= ( latent_dim , ) ) # note that `` output_shape '' is n't necessary with the TensorFlow backend padding='same ' , activation='relu ' , output_shape = ( batch_size , filters , 14 , 14 ) # MNIST dataset Auto-Encoding Variational Bayes latent_dim = 2 epsilon = K.random_normal ( shape= ( K.shape ( z_mean ) [ 0 ] , latent_dim ) , mean=0. , reconstruction_loss = binary_crossentropy ( inputs , decoder_h = Dense ( intermediate_dim , activation='relu ' ) # convolution kernel size output_shape = ( batch_size , filters , 29 , 29 ) filename = os.path.join ( model_name , `` vae_mean.png '' ) x_decoded = generator.predict ( z_sample ) [ 1 ] Kingma , Diederik P. , and Max Welling . model_name ( string ) : which model is using this function args = parser.parse_args ( ) vae.save_weights ( 'vae_mlp_mnist.h5 ' ) # to produce values of the latent variables z , since the prior of the latent space is Gaussian parser.add_argument ( `` -w '' , `` -- weights '' , help=help_ ) parser = argparse.ArgumentParser ( ) from keras import backend as K kernel_size=kernel_size , models = ( encoder , decoder ) j * digit_size : ( j + 1 ) * digit_size ] = digit conv_1 = Conv2D ( img_chns , # network parameters z_sample = np.array ( [ [ xi , yi ] ] ) from keras.layers import Dense , Input latent_dim = 2 decoder_deconv_2 = Conv2DTranspose ( filters , filters = 16 outputs = decoder ( encoder ( inputs ) [ 2 ] ) # display a 2D plot of the digit classes in the latent space `` Auto-encoding variational bayes . '' `` `` '' plt.savefig ( filename ) activation='relu ' , x = Reshape ( ( shape [ 1 ] , shape [ 2 ] , shape [ 3 ] ) ) ( x ) deconv_1_decoded = decoder_deconv_1 ( reshape_decoded ) decoder_reshape = Reshape ( output_shape [ 1 : ] ) x_test = x_test.astype ( 'float32 ' ) / 255 epochs = 5 # instantiate decoder model start_range = digit_size // 2 # instantiate encoder model kernel_size= ( 3 , 3 ) , # Returns : output_shape = ( batch_size , 14 , 14 , filters ) if args.weights : vae = Model ( inputs , outputs , name='vae_mlp ' ) batch_size=batch_size , from keras.layers import Lambda , Input , Dense figure = np.zeros ( ( digit_size * n , digit_size * n ) ) up_decoded = decoder_upsample ( hid_decoded ) from keras.utils import plot_model reconstruction_loss * = image_size * image_size K.flatten ( x ) , x_decoded_mean = decoder_mean ( h_decoded ) decoder = Model ( latent_inputs , outputs , name='decoder ' ) shuffle=True , K.flatten ( outputs ) ) z_log_var = Dense ( latent_dim , name='z_log_var ' ) ( x ) vae.summary ( ) epsilon = K.random_normal ( shape= ( K.shape ( z_mean ) [ 0 ] , latent_dim ) , figure = np.zeros ( ( digit_size * n , digit_size * n ) ) batch_size = 128 x_decoded = decoder.predict ( z_sample ) x = Flatten ( ) ( x ) padding='same ' , kernel_size=2 , padding='valid ' , activation='relu ' ,","['examples/variational_autoencoder.py', 'examples/variational_autoencoder_deconv.py']",vae examples fixes ( # 10062 )
449,978efb61b9cea47ddc2b5226cefee2a890693557,2018-05-10 09:49:03-07:00,"if the data types are compatible . To avoid this x2 = utils.preprocess_input ( x , mode=mode ) if not issubclass ( x.dtype.type , np.floating ) : xint2 = utils.preprocess_input ( xint ) for mode in [ 'torch ' , 'tf ' ] : x2 = utils.preprocess_input ( x , data_format='channels_last ' , mode='caffe ' ) assert_allclose ( x , x2 ) xint2 = utils.preprocess_input ( xint ) # Caffe mode works differently from the others x = x.astype ( K.floatx ( ) ) # Test that writing over the input data works predictably assert xint.astype ( 'float ' ) .max ( ) ! = xint2.max ( ) The preprocessed data is written over the input data x = np.random.uniform ( 0 , 255 , ( 2 , 10 , 10 , 3 ) ) assert_allclose ( x , x2 [ ... , : :-1 ] ) behaviour , ` numpy.copy ( x ) ` can be used . x = np.random.uniform ( 0 , 255 , ( 2 , 10 , 10 , 3 ) ) xint = x.astype ( 'int ' ) x = x.astype ( K.floatx ( ) , copy=False ) assert xint.astype ( 'float ' ) .max ( ) ! = xint2.max ( ) xint = x.astype ( 'int ' )","['keras/applications/imagenet_utils.py', 'tests/keras/applications/imagenet_utils_test.py']",Fix undefined behaviour : preprocess_input copying/not copying the input arrays ( # 10153 )
450,528542176f70f9a889e84e4494032f42a53bec67,2018-05-10 09:47:00-07:00,"K.dtype ( self.decay ) ) ) ) lr = lr * ( 1 . / ( 1 . + self.decay * K.cast ( self.iterations , K.dtype ( self.decay ) ) ) ) lr * = ( 1 . / ( 1 . + self.decay * K.cast ( self.iterations ,",['keras/optimizers.py'],Avoid a warning by using ' x = x * y ' instead of ' x * = y ' ( # 10158 )
451,a80ecd78daf7d464a6ff915111f62b8cd21c8b6b,2018-05-09 11:06:07-07:00,'when doing step-wise training . ' ) 'to perform validation ' elif do_validation : raise ValueError ( 'Must specify ` validation_steps ` ' if steps_per_epoch :,['keras/engine/training_arrays.py'],Add exceptions for ` fit_loop ` ( # 10145 )
452,895dba6c92ad4fd9bc98975b0ae23ee6230a8b38,2018-05-07 12:33:46-07:00,"assert np.max ( rand ) < = max_val rand = k.eval ( k.truncated_normal ( ( 300 , 200 ) , mean=mean , stddev=std , seed=1337 ) ) scale = np.sqrt ( 1 . / fan_in ) assert np.abs ( np.std ( rand ) - std * 0.87962 ) < 0.015 # Poor man 's truncated normal : we literally clip the tensor scale = np.sqrt ( 2 . / fan_in ) for k in BACKENDS : try : std = np.sqrt ( 2 . / ( fan_in + fan_out ) ) return T.clip ( normal_t , mean - 2 * stddev , mean + 2 * stddev ) fan_in , _ = initializers._compute_fans ( tensor_shape ) assert rand.shape == ( 300 , 200 ) target_mean=0. , target_max=scale , target_min=-scale ) scale = np.sqrt ( 2 . / ( fan_in + fan_out ) ) fan_in , _ = initializers._compute_fans ( tensor_shape ) min_val = -2 . def test_lecun_normal ( tensor_shape ) : assert np.min ( rand ) > = min_val stddev = np.sqrt ( scale ) def test_lecun_normal ( tensor_shape ) : normal_tensor = rng.normal ( size=shape , avg=mean , std=stddev , dtype=dtype ) std = np.sqrt ( 1 . / fan_in ) return rng.normal ( size=shape , avg=mean , std=stddev , dtype=dtype , return T.clip ( normal_tensor , mean - 2 * stddev , mean + 2 * stddev ) target_mean=0. , target_std=None , target_max=2 ) # assumption in initializers.VarianceScaling target_mean=0. , target_max=2 , target_min=-2 ) scale = np.sqrt ( 6 . / fan_in ) target_mean=0. , target_std=scale ) # 0.879 ... = scipy.stats.truncnorm.std ( a=-2 , b=2 , loc=0. , scale=1 . ) # Poor man 's truncated normal : we literally clip the tensor mean = 0 . normal_t = rng.normal ( size=shape , avg=mean , std=stddev , dtype=dtype ) def test_truncated_normal ( self ) : truncate=True ) max_val = 2 . std = np.sqrt ( 2 . / fan_in ) target_mean=0. , target_std=None , target_max=2 * scale ) @ pytest.mark.parametrize ( 'tensor_shape ' , [ FC_SHAPE , CONV_SHAPE ] , ids= [ 'FC ' , 'CONV ' ] ) scale = np.sqrt ( 6 . / ( fan_in + fan_out ) ) _runner ( initializers.lecun_normal ( ) , tensor_shape , stddev = np.sqrt ( scale ) / .87962566103423978 scale = np.sqrt ( 3 . / fan_in ) _runner ( initializers.lecun_normal ( ) , tensor_shape , target_mean=0. , target_std=std ) std = 1 . assert np.abs ( np.mean ( rand ) - mean ) < 0.015 except TypeError :","['keras/backend/theano_backend.py', 'keras/initializers.py', 'tests/keras/backend/backend_test.py', 'tests/keras/initializers_test.py']",Fix initializers ( # 9963 )
453,c8728e4e47569a2eb918600fe75f36a6dae4e623,2018-05-07 11:02:03-07:00,"If ` y ` is None , only the numpy array ` x ` is returned . 'should have the same length . ' break assert list ( w ) == [ 1 , 2 , 3 ] def flow ( self , x , y=None , batch_size=32 , shuffle=True , seed=None , if self.sample_weight is not None : assert list ( y ) == [ 0 , 1 , 2 ] if sample_weight is not None : batch_size=32 , shuffle=False , sample_weight=None , sample_weight=np.arange ( images.shape [ 0 ] ) + 1 , sample_weight : Numpy array of sample weights . self.sample_weight = np.asarray ( sample_weight ) sample_weight : Sample weights . else : batch_size=3 ) : for x , y , w in generator.flow ( images , np.arange ( images.shape [ 0 ] ) , save_to_dir=str ( tmpdir ) , shuffle=False , output += ( self.sample_weight [ index_array ] , ) def flow ( self , x , y=None , batch_size=32 , shuffle=True , sample_weight=None , seed=None , raise ValueError ( ' ` x ` ( images tensor ) and ` sample_weight ` ' assert x.shape == images [ :3 ] .shape batch_size=32 , shuffle=False , seed=None , sample_weight=sample_weight , data_format=None , if sample_weight is not None and len ( x ) ! = len ( sample_weight ) : seed=None , data_format=None , the yielded tuples are of the form ` ( x , y , sample_weight ) ` . of corresponding labels . # Test with sample weights self.sample_weight = None of corresponding labels . If 'sample_weight ' is not None , ( np.asarray ( x ) .shape , np.asarray ( sample_weight ) .shape ) ) 'Found : x.shape = % s , sample_weight.shape = % s ' %","['keras/preprocessing/image.py', 'tests/keras/preprocessing/image_test.py']",Sample weighted ImageDataGenerator ( # 10092 )
454,f6fabd1a370c27aff4e8a82e5a3edce52aaf9c2b,2018-05-07 11:00:09-07:00,"_test_application_variable_input_channels ( app , last_dim ) app = applications.MobileNetV2 def test_mobilenetv2 ( ) : _test_application_basic ( app ) _test_application_notop ( app , last_dim ) _test_app_pooling ( app , last_dim ) MOBILENET_LIST = [ ( applications.MobileNet , 1024 ) , if app == applications.MobileNet : app = applications.MobileNet ( applications.MobileNetV2 , 1280 ) ] _test_app_pooling ( app , last_dim ) app , last_dim = random.choice ( MOBILENET_LIST ) last_dim = 1024 last_dim = 1280",['tests/keras/applications/applications_test.py'],Make MobileNet tests random ( # 10132 )
455,84e168b5fa55933e02e767ff7c86fcc0232aecc6,2018-05-07 11:07:48-04:00,"assert utils.preprocess_input ( xint ) .shape == xint.shape out1int = utils.preprocess_input ( xint , 'channels_last ' ) assert_allclose ( out1int , out2int.transpose ( 1 , 2 , 0 ) ) out2int = utils.preprocess_input ( np.transpose ( xint , ( 2 , 0 , 1 ) ) , xint = x.astype ( 'int32 ' ) 'channels_first ' ) # Test image batch out2int = utils.preprocess_input ( np.transpose ( xint , ( 0 , 3 , 1 , 2 ) ) , assert_allclose ( out1int , out2int.transpose ( 0 , 2 , 3 , 1 ) ) # Test image batch with float and int image input x = x.astype ( K.floatx ( ) )","['keras/applications/imagenet_utils.py', 'tests/keras/applications/imagenet_utils_test.py']",Fix preprocess_input not working with int arrays ( # 10134 )
456,b470a595f7278acf5e7e47521edf25d3c4f479f1,2018-05-06 12:11:46-07:00,"filter_shape=pointwise_kernel_shape , return squeeze ( x , spatial_start_dim ) strides = ( 1 , ) + strides + ( 1 , ) if data_format == 'channels_last ' : pointwise_kernel = _preprocess_conv2d_kernel ( pointwise_kernel , data_format ) depthwise_kernel = reshape ( depthwise_kernel , depthwise_kernel_shape ) raise NotImplementedError raise ValueError ( 'Unknown data_format ' , data_format ) depthwise_kernel_shape = ( input_depth * output_depth , 1 ) + depthwise_kernel_shape [ 2 : ] depthwise_kernel = depthwise_kernel.dimshuffle ( ( 1 , 0 , 2 , 3 ) ) num_groups=input_depth ) border_mode=th_padding , subsample= ( 1 , 1 ) , if dilation_rate == ( 1 , 1 ) : padding : string , ` `` same '' ` or ` `` valid '' ` . spatial_start_dim = 3 if data_format not in { 'channels_first ' , 'channels_last ' } : dilation_rate = ( dilation_rate , ) x = C.convolution ( depthwise_kernel , x , depthwise_kernel = _preprocess_conv2d_kernel ( depthwise_kernel , data_format ) input_depth = depthwise_kernel_shape [ 1 ] conv_out = T.nnet.conv2d ( conv_out , pointwise_kernel , depthwise_kernel_shape = depthwise_kernel.eval ( ) .shape # in case of a shared variable input_shape=image_shape , padding = _preprocess_border_mode ( padding ) x = _postprocess_conv2d_output ( x , data_format ) x = _preprocess_conv2d_input ( x , data_format ) ( 'separable_conv1d ' , ( 2 , 8 , 2 ) , ( 3 , ) , 1 , 'same ' , 'channels_last ' ) , raise ValueError ( 'CNTK Backend : non-square dilation_rate is ' conv_out = T.nnet.conv2d ( x , depthwise_kernel , strides = ( strides , ) if dilation_rate [ 0 ] ! = dilation_rate [ 1 ] : dilation_rate = ( 1 , ) + dilation_rate raise ValueError ( 'Invalid strides for dilated convolution ' ) pointwise_kernel_shape , pointwise_kernel : kernel for the 1x1 convolution . if dilation_rate ! = 1 and K.backend ( ) == 'cntk ' : pointwise_kernel_shape = _preprocess_conv2d_filter_shape ( pointwise_kernel_shape , data_format ) raise ValueError ( 'Unknown data_format ' + str ( data_format ) ) spatial_start_dim = 2 pointwise_kernel_shape = int_shape ( pointwise_kernel ) image_shape = _preprocess_conv2d_image_shape ( int_shape ( x ) , data_format ) if data_format is None : `` `` '' 1D convolution with separable filters . x = expand_dims ( x , spatial_start_dim ) dilation_rate : integer dilation rate . input_shape=None , groups=x.shape [ 0 ] ) subsample=strides , def test_separable_conv ( self , op , input_shape , kernel_shape , depth_multiplier , padding , data_format ) : strides : strides integer . pointwise_kernel_shape = pointwise_kernel.eval ( ) .shape # in case of a shared variable if depthwise_kernel_shape is None : strides , data_format ) else : x = C.convolution ( pointwise_kernel , x , ( 'separable_conv1d ' , ( 1 , 8 , 2 ) , ( 3 , ) , 2 , 'valid ' , 'channels_last ' ) , if strides ! = ( 1 , 1 ) : conv_out = _postprocess_conv2d_output ( conv_out , x , padding , auto_padding= [ False ] ) Output tensor . filter_dilation=dilation_rate ) continue `` `` '' return conv_out strides = strides + ( 1 , ) if isinstance ( strides , int ) : strides=strides , depthwise_kernel_shape = int_shape ( depthwise_kernel ) 'not supported . ' ) strides= ( 1 , 1 , 1 ) , # Raises def test_separable_conv2d ( self , op , input_shape , kernel_shape , depth_multiplier , padding , data_format ) : if pointwise_kernel_shape is None : _ , pointwise = parse_shape_or_val ( ( 1 , ) * len ( kernel_shape ) + ( input_depth * depth_multiplier , 7 ) ) # Returns th_padding = _preprocess_padding ( padding ) ( -1 , 1 ) + depthwise_kernel.shape [ 2 : ] ) output_depth = depthwise_kernel_shape [ 0 ] x : input tensor depthwise_kernel = depthwise_kernel [ : , : , : :-1 , : :-1 ] depthwise_kernel : convolution kernel for the depthwise convolution . data_format : string , ` `` channels_last '' ` or ` `` channels_first '' ` . data_format = image_data_format ( ) # Arguments ValueError : if ` data_format ` is neither ` `` channels_last '' ` or ` `` channels_first '' ` . if isinstance ( dilation_rate , int ) : depthwise_kernel_shape = _preprocess_conv2d_filter_shape ( depthwise_kernel_shape , data_format ) filter_dilation=dilation_rate , conv_out = squeeze ( conv_out , spatial_start_dim ) auto_padding= [ False , padding , padding ] , _ , pointwise = parse_shape_or_val ( ( 1 , 1 ) + ( input_depth * depth_multiplier , 7 ) ) pointwise_kernel = expand_dims ( pointwise_kernel , 1 ) filter_shape=depthwise_kernel_shape , depthwise_kernel = expand_dims ( depthwise_kernel , 1 ) depthwise_kernel = C.reshape ( C.transpose ( depthwise_kernel , ( 1 , 0 , 2 , 3 ) ) , dilation_rate = dilation_rate + ( 1 , ) @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , reason='Requires TF backend ' )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py', 'tests/keras/layers/convolutional_test.py']",Add ` separable_conv1d ` ( # 10125 )
457,aac81c29a762fdaef6718bbc4883f208283aac9c,2018-05-06 10:54:59-07:00,"units = 2 keras.engine.saving.preprocess_weights_for_loading ( skipif_no_tf_gpu = pytest.mark.skipif ( return layer weights=weights [ : num_weights ] , def convert_model ( source_model , target_model ) : if num_weights > 0 : num_weights = len ( sublayer.trainable_weights ) elif layer.__class__.__name__ in [ 'Model ' , 'Sequential ' ] : for n in group.attrs [ ' % s % d ' % ( name , chunk_id ) ] ] ) input_size = 10 if to_cudnn : weights : List of weights values ( Numpy arrays ) . for sublayer in layer.layers : `` `` '' Converts layers nested in ` Model ` or ` Sequential ` by ` preprocess_weights_for_loading ( ) ` . assert_not_compatible ( gru ( reset_after=True ) , gru ( ) , def initialize_weights ( layer ) : target_layer.set_weights ( weights ) num_samples = 32 'implementation ' : implementation elif model_type == 'seq ' : target_model.load_weights ( fname ) assert_not_compatible ( gru ( cudnn=True ) , gru ( ) , if bidirectional : @ pytest.mark.skipif ( not keras.backend.tensorflow_backend._get_available_gpus ( ) , layer_class = keras.layers.CuDNNGRU if cudnn else keras.layers.GRU 'recurrent_activation ' : 'sigmoid ' , rnn_layer_kwargs = { # trainable weights def make_nested_func_model ( input_shape , layer , level=1 ) : # Arguments num_weights = len ( [ l for l in sublayer.weights # ensure biases are non-zero and properly converted assert_not_compatible ( gru ( ) , gru ( cudnn=True ) , `` `` '' new_weights = [ ] Loading weights between incompatible layers should fail fast with an exception . @ pytest.mark.parametrize ( 'bidirectional ' , [ False , True ] , ids= [ 'single ' , 'bidirectional ' ] ) for n in group.attrs [ ' % s % d ' % ( name , chunk_id ) ] ] ) assert message in ex.value.message 'recurrent_activation ' : 'sigmoid ' , A list of weights values ( Numpy arrays ) . original_backend=original_backend ) ) model_nest_level , model_type ) : `` `` '' Converts layers weights from Keras 1 format to Keras 2 and also weights of CuDNN layers in Keras 2 . original_backend=original_backend ) ) _ , fname = tempfile.mkstemp ( '.h5 ' ) return make_nested_func_model ( input_shape , layer , level ) def gru ( cudnn=False , * * kwargs ) : model = _make_nested_model ( input_shape , layer , model_nest_level , model_type ) new_weights.extend ( preprocess_weights_for_loading ( import keras.backend as K rnn_layer_kwargs = { weights = new_weights ( K.backend ( ) ! = 'tensorflow ' ) or ( not K.tensorflow_backend._get_available_gpus ( ) ) , } convert_weights ( layer , cudnn_layer ) def test_load_weights_between_noncudnn_rnn ( rnn_type , to_cudnn , bidirectional , implementation , layer=sublayer , 'CuDNNGRU is not compatible with GRU ( reset_after=False ) ' ) def convert_weights ( source_layer , target_layer ) : if l not in sublayer.trainable_weights ] ) @ keras_test weights = weights [ num_weights : ] units = 2 layer=sublayer , 'GRU ( reset_after=False ) is not compatible with CuDNNGRU ' ) # non-trainable weights @ pytest.mark.parametrize ( 'implementation ' , [ 1 , 2 ] , ids= [ 'impl1 ' , 'impl2 ' ] ) inputs = np.random.random ( ( num_samples , timesteps , input_size ) ) if num_weights > 0 : cudnn_rnn_layer_class = keras.layers.CuDNNGRU def initialize_weights ( layer ) : for i in range ( level ) : # Both transformation should be ran for both Keras 1- > 2 conversion reason='Requires TensorFlow backend ' ) weights = convert_nested_bidirectional ( weights ) from keras.engine.saving import preprocess_weights_for_loading convert_model ( cudnn_model , model ) assert message in ex.value.message layers = [ InputLayer ( input_shape ) , model ] if ( i == 1 ) else [ model ] cudnn_model = _make_nested_model ( input_shape , cudnn_layer , model_nest_level , model_type ) return new_weights weights = source_layer.get_weights ( ) input_layer = keras.layers.InputLayer ( input_shape ) layer_class = CuDNNGRU if cudnn else GRU weights = keras.engine.saving.preprocess_weights_for_loading ( target_layer , weights ) _ = Sequential ( [ layer ] ) assert_allclose ( model.predict ( inputs ) , cudnn_model.predict ( inputs ) , atol=1e-4 ) original_keras_version=original_keras_version , from keras.layers import Dense , Lambda , RepeatVector , TimeDistributed , Bidirectional , GRU , LSTM , CuDNNGRU , CuDNNLSTM _ = keras.models.Sequential ( [ layer ] ) 'implementation ' : implementation preprocess_weights_for_loading ( dest , initialize_weights ( src ) .get_weights ( ) ) input_size = 10 num_weights = len ( [ l for l in sublayer.weights rnn_layer_class = keras.layers.LSTM weights = convert_nested_model ( weights ) 'GRU ( reset_after=True ) is not compatible with GRU ( reset_after=False ) ' ) timesteps = 6 rnn_layer_class = GRU @ pytest.mark.skipif ( ( K.backend ( ) ! = 'tensorflow ' ) , reason='Requires TensorFlow backend ' ) cudnn_layer = cudnn_rnn_layer_class ( units ) return layer_class ( 2 , input_shape= [ 3 , 5 ] , * * kwargs ) else : else : rnn_layer_class = keras.layers.GRU weights = forward_weights + backward_weights 'GRU ( reset_after=True ) is not compatible with GRU ( reset_after=False ) ' ) def assert_not_compatible ( src , dest , message ) : rnn_layer_class = LSTM inputs = np.random.random ( ( num_samples , timesteps , input_size ) ) # non-trainable weights cudnn_rnn_layer_class = CuDNNGRU for i in range ( 1 , level + 1 ) : return layer weights = weights [ num_weights : ] cudnn_layer = cudnn_rnn_layer_class ( units ) def _make_nested_model ( input_shape , layer , level=1 , model_type='func ' ) : from keras.layers import Dense , Lambda , RepeatVector , TimeDistributed , LSTM layer = rnn_layer_class ( units , * * rnn_layer_kwargs ) def test_preprocess_weights_for_loading_gru_incompatible ( ) : biases = weights [ 2 ] .reshape ( ( 2 , -1 ) if from_cudnn else -1 ) 'GRU ( reset_after=False ) is not compatible with GRU ( reset_after=True ) ' ) Loading weights between incompatible layers should fail fast with an exception . reason='Requires TensorFlow backend and a GPU ' ) # trainable weights # and for conversion of CuDNN layers . cudnn_rnn_layer_class = keras.layers.CuDNNLSTM 'CuDNNGRU is not compatible with GRU ( reset_after=False ) ' ) # A model is needed to initialize weights . 'GRU ( reset_after=False ) is not compatible with CuDNNGRU ' ) def convert_nested_bidirectional ( weights ) : return forward_weights + backward_weights convert_model ( model , cudnn_model ) assert_allclose ( model.predict ( inputs ) , cudnn_model.predict ( inputs ) , atol=1e-4 ) new_weights.extend ( preprocess_weights_for_loading ( if layer.__class__.__name__ == 'Bidirectional ' : `` `` '' layer = rnn_layer_class ( units , * * rnn_layer_kwargs ) num_weights = len ( sublayer.trainable_weights ) # convert CuDNN layers cudnn_rnn_layer_class = CuDNNLSTM # example : make_nested_seq_model ( ( 1 , ) , Dense ( 10 ) , level=2 ) .summary ( ) # Convert layers nested in Bidirectional/Model/Sequential . reason='Requires GPU ' ) return model with pytest.raises ( ValueError ) as ex : source_model.save_weights ( fname ) `` `` '' def test_preprocess_weights_for_loading_gru_incompatible ( ) : @ pytest.mark.skipif ( not K.tensorflow_backend._get_available_gpus ( ) , reason='Requires GPU ' ) # Returns for sublayer in layer.layers : input_shape = ( timesteps , input_size ) model = keras.models.Sequential ( [ input_layer , layer ] ) if rnn_type == 'LSTM ' : rnn_layer_kwargs [ 'reset_after ' ] = True @ pytest.mark.parametrize ( 'rnn_type ' , [ 'LSTM ' , 'GRU ' ] , ids= [ 'LSTM ' , 'GRU ' ] ) if to_cudnn : with pytest.raises ( ValueError ) as ex : return make_nested_seq_model ( input_shape , layer , level ) original_keras_version=original_keras_version , def test_load_weights_between_noncudnn_rnn ( rnn_type , to_cudnn , bidirectional , implementation ) : if model_type == 'func ' : layer = keras.layers.Bidirectional ( layer ) 'bias_initializer ' : 'random_uniform ' , layer = Bidirectional ( layer ) num_samples = 32 # A model is needed to initialize weights . assert_not_compatible ( gru ( ) , gru ( reset_after=True ) , def convert_nested_model ( weights ) : if l not in sublayer.trainable_weights ] ) new_weights = [ ] # example : make_nested_func_model ( ( 1 , ) , Dense ( 10 ) , level=2 ) .summary ( ) assert_not_compatible ( gru ( reset_after=True ) , gru ( ) , @ pytest.mark.skipif ( ( keras.backend.backend ( ) ! = 'tensorflow ' ) , @ pytest.mark.parametrize ( 'to_cudnn ' , [ False , True ] , ids= [ 'from_cudnn ' , 'to_cudnn ' ] ) # ensure biases are non-zero and properly converted convert_weights ( cudnn_layer , layer ) model = Sequential ( layers ) 'bias_initializer ' : 'random_uniform ' , from keras.layers import Input , InputLayer model = Model ( input , model ( input ) ) `` `` '' Converts layers weights from Keras 1 format to Keras 2 . assert_not_compatible ( gru ( ) , gru ( reset_after=True ) , def make_nested_seq_model ( input_shape , layer , level=1 ) : weights=weights [ : num_weights ] , return layer_class ( 2 , input_shape= [ 3 , 5 ] , * * kwargs ) input = Input ( input_shape ) 'GRU ( reset_after=False ) is not compatible with GRU ( reset_after=True ) ' ) cudnn_layer = keras.layers.Bidirectional ( cudnn_layer ) cudnn_model = keras.models.Sequential ( [ input_layer , cudnn_layer ] ) from keras.layers import Input os.remove ( fname ) cudnn_layer = Bidirectional ( cudnn_layer ) model = layer biases = np.array ( weights [ 2 ] ) .reshape ( ( 2 , -1 ) if from_cudnn else -1 ) timesteps = 6 assert_not_compatible ( gru ( cudnn=True ) , gru ( ) , rnn_layer_kwargs [ 'reset_after ' ] = True def assert_not_compatible ( src , dest , message ) : if layer.__class__.__name__ == 'Bidirectional ' : if layer.__class__.__name__ in [ 'Model ' , 'Sequential ' ] : `` `` '' Converts layers nested in ` Bidirectional ` wrapper by ` preprocess_weights_for_loading ( ) ` . def gru ( cudnn=False , * * kwargs ) : input_shape = ( timesteps , input_size ) if rnn_type == 'LSTM ' : assert_not_compatible ( gru ( ) , gru ( cudnn=True ) , } if bidirectional : dest , initialize_weights ( src ) .get_weights ( ) )","['keras/engine/saving.py', 'tests/keras/engine/test_topology.py', 'tests/keras/layers/cudnn_recurrent_test.py', 'tests/test_model_saving.py']",# 10080 Convert CuDNN weights in nested Model . ( # 10081 )
458,97d5fa920e4f8248128f7c1b460fd9bb20d3478f,2018-05-04 13:49:11-07:00,"last_output_list.append ( last_y2 ) { 'go_backwards ' : True , 'mask ' : None } , assert len ( h_k ) == 2 timesteps = 6 def rnn_fn ( x_k , h_k ) : x_k = K.variable ( x ) state_list = [ ] h21 = h21 * np.expand_dims ( mask [ : , -1 ] , -1 ) mask_k = K.variable ( mask ) assert_allclose ( last_y1 , last_y2 , atol=1e-05 ) { 'go_backwards ' : True , 'mask ' : None , 'unroll ' : True , 'input_length ' : timesteps } , kwargs_list = [ h12 = np.concatenate ( [ h1 [ : , -1 ] , h1 [ : , -1 ] ] , axis=-1 ) h11 = h1 [ : , -1 ] # implement a simple RNN with an additional state last_y1 , y1 , h1 = reference_operations.rnn ( x , [ wi , wh , None ] , h0 , * * kwargs ) last_y2 = last_y2 * np.expand_dims ( mask [ : , -1 ] , -1 ) { 'go_backwards ' : False , 'mask ' : mask_k , 'unroll ' : True , 'input_length ' : timesteps } , h22 = h22 * np.expand_dims ( mask [ : , -1 ] , -1 ) ] { 'go_backwards ' : False , 'mask ' : None , 'unroll ' : True , 'input_length ' : timesteps } , new_states [ i ] , states [ i ] ) for i in range ( len ( states ) ) for ( i , kwargs ) in enumerate ( kwargs_list ) : outputs_list = [ ] outputs_list.append ( y2 ) if i % 2 == 0 : h21 = K.eval ( h2 [ 0 ] ) # test default setup y2 = K.eval ( y2 ) last_y1 = last_y1 * np.expand_dims ( mask [ : , -1 ] , -1 ) y1 = y1 * np.expand_dims ( mask , -1 ) y_k = K.dot ( x_k , wi_k ) + K.dot ( h_k [ 0 ] , wh_k ) _ , wh = parse_shape_or_val ( ( output_dim , output_dim ) ) input_dim = 5 wi_k = K.variable ( wi ) else : assert_allclose ( state_list [ i - 1 ] [ 1 ] , state_list [ i ] [ 1 ] , atol=1e-05 ) wh_k = K.variable ( wh ) # whose shape is different from that of the output last_y2 = K.eval ( last_y2 ) assert_allclose ( state_list [ i - 1 ] [ 0 ] , state_list [ i ] [ 0 ] , atol=1e-05 ) ] _ , x = parse_shape_or_val ( ( num_samples , timesteps , input_dim ) ) y2 = y2 * np.expand_dims ( mask , -1 ) output_dim = 3 last_output_list = [ ] h11 = h11 * np.expand_dims ( mask [ : , -1 ] , -1 ) state_list.append ( ( h21 , h22 ) ) h12 = h12 * np.expand_dims ( mask [ : , -1 ] , -1 ) new_states = [ new_states = [ tf.where ( tiled_mask_t , new_states [ i ] , states [ i ] ) for i in range ( len ( states ) ) ] { 'go_backwards ' : False , 'mask ' : mask_k } , assert_allclose ( last_output_list [ i - 1 ] , last_output_list [ i ] , atol=1e-05 ) return y_k , [ y_k , K.concatenate ( [ y_k , y_k ] , axis=-1 ) ] _ , h0 = parse_shape_or_val ( ( num_samples , output_dim ) ) tf.where ( tf.tile ( mask_t , tf.stack ( [ 1 , tf.shape ( new_states [ i ] ) [ 1 ] ] ) ) , num_samples = 4 _ , wi = parse_shape_or_val ( ( input_dim , output_dim ) ) def test_rnn_additional_states ( self ) : if kwargs [ 'mask ' ] is not None : assert_allclose ( y1 , y2 , atol=1e-05 ) { 'go_backwards ' : False , 'mask ' : None } , mask = np.random.randint ( 2 , size= ( num_samples , timesteps ) ) assert len ( h2 ) == 2 h22 = K.eval ( h2 [ 1 ] ) assert_allclose ( h12 , h22 , atol=1e-05 ) h0_k = [ K.variable ( h0 ) , K.variable ( np.concatenate ( [ h0 , h0 ] , axis=-1 ) ) ] assert_allclose ( h11 , h21 , atol=1e-05 ) last_y2 , y2 , h2 = K.rnn ( rnn_fn , x_k , h0_k , * * kwargs ) assert_allclose ( outputs_list [ i - 1 ] , outputs_list [ i ] , atol=1e-05 )","['keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Fix shape mismatch in ` rnn ( ) ` of tensorflow_backend ( # 10038 )
459,24e7ac0756aeba06ce1cf7f7e72219a11b14b101,2018-05-03 10:56:12-07:00,"# Test with dictionary inputs return self._call ( inputs ) data = [ data [ x ] .values if data [ x ] .__class__.__name__ == 'DataFrame ' raise ValueError ( 'Some keys in session_kwargs are not ' if hasattr ( get_session ( ) , '_make_callable_from_options ' ) : session = get_session ( ) else data [ x ] for x in names ] feed_arrays = [ ] 'for each key in : ' + str ( names ) ) 'The list you passed was : ' + str ( data ) [ :200 ] ) def test_training_and_eval_methods_on_symbolic_tensors_single_io ( ) : if hasattr ( ins [ 0 ] , 'shape ' ) : fetched = self._callable_fn ( * array_vals ) assert 'should specify the ` steps_per_epoch ` ' in str ( excinfo.value ) raise TypeError ( ' ` inputs ` should be a list or tuple . ' ) def _make_callable ( self , feed_arrays , feed_symbols , symbol_vals , session ) : validation_data= ( inputs , targets ) , validation_steps=2 ) model.train_on_batch ( [ input_a_tf , input_b_tf ] , [ output_d_tf , output_e_tf ] ) 'supported at this ' feed_symbols.append ( tensor ) metrics = [ 'mae ' ] symbol_vals.append ( value ) epochs=2 , for x in feed_arrays : def __call__ ( self , inputs ) : symbol_vals = [ ] connection.from_tensor = from_tensor.name # Data tensor # Prepare callable options . if data [ x ] .__class__.__name__ == 'DataFrame ' else data [ x ] raise ValueError ( 'No data provided for `` ' + e.args [ 0 ] 'If your data is in the form of symbolic tensors , ' [ input_a_tf , input_b_tf ] , [ output_d_tf , output_e_tf ] , self.fetches = [ tf.identity ( x ) for x in self.fetches ] reason='Requires TF 1.8 or higher ' ) if py_any ( is_tensor ( x ) for x in inputs ) : if x is None : # Arguments # ( since the outputs of fetches are never returned ) . d = dense ( b ) self._callable_fn = callable_fn self._feed_arrays = None else : 'dropout ' : output_e_tf } , `` `` '' # Cache parameters corresponding to the generated callable , so that * * session_kwargs ) : ' '' . Need data ' # Refresh callable if anything has changed . { 'dense ' : output_d_tf , if py_any ( is_sparse ( x ) for x in self.inputs ) : with pytest.raises ( ValueError ) as excinfo : ] for x , y in zip ( feed_symbols , symbol_vals ) : callable_opts.fetch.append ( x.name ) output_d_tf = keras.backend.zeros ( shape= ( 10 , 4 ) ) if x.dtype ! = y.dtype : y = keras.layers.Dense ( 4 , name='dense ' ) ( x ) if is_tensor ( value ) : return fetched [ : len ( self.outputs ) ] ' : data should be a Numpy array , or list/dict of ' batch_size=5 , 'because symbolic tensors are expected to produce ' ' is set , the ` batch_size ` must be None . ' ) x = np.expand_dims ( x , 1 ) # Create callable . validation_data= ( { 'input_a ' : input_a_tf , model = keras.models.Model ( [ a , b ] , [ d , e ] ) if shapes [ i ] is not None and not K.is_tensor ( data [ i ] ) : '_make_callable_from_options ' ) ) , output_e_tf = keras.backend.zeros ( shape= ( 10 , 4 ) ) optimizer = 'rmsprop ' raise ValueError ( # This requires us to wrap fetches in ` identity ` ops . # Handle updates . 'time : % s ' , session_kwargs.keys ( ) ) else x for x in data ] 'When feeding symbolic tensors to a model , we expect the ' name=None , 'you can not use ` validation_split ` . ' ) model.predict ( inputs , steps=2 ) indices = np.concatenate ( ( np.expand_dims ( sparse_coo.row , 1 ) , session : Session to use to generate the callable . e = keras.layers.Dropout ( 0.5 , name='dropout ' ) ( c ) validation_steps=2 ) 'Error when checking model ' + exception_prefix indices = np.concatenate ( ' : data should be a Numpy array , or list/dict of ' if steps is not None and batch_size is not None : for key in sorted ( self.feed_dict.keys ( ) ) : elif K.is_tensor ( x ) : shape = K.int_shape ( x ) # ` callable_fn ` only supports exact matches . return None # Edge case where ins == [ static_learning_phase ] a = keras.layers.Input ( shape= ( 3 , ) , name='input_a ' ) 'input_b ' : input_b_tf } , 'Got tensor with shape : % s ' % str ( shape ) ) # we can detect future mismatches and refresh the callable . feed_arrays : List of input tensors to be fed # Handle fetches . 'If your data is in the form of symbolic tensors , ' return None ' should be specified . ' ) input_b_tf = keras.backend.zeros ( shape= ( 10 , 3 ) ) raise ValueError ( from tensorflow.core.protobuf import config_pb2 str ( data ) [ :200 ] ) { 'dense ' : output_d_tf , raise ValueError ( 'Either the input data should have ' np.expand_dims ( sparse_coo.col , 1 ) ) , 1 ) self._symbol_vals = None if not ins or any ( K.is_tensor ( x ) for x in ins ) : return self._legacy_call ( inputs ) for x in self.outputs + self.fetches : 'No data provided for `` ' + e.args [ 0 ] + ' '' . Need data ' if py_any ( is_tensor ( x ) for x in inputs ) : steps_per_epoch=2 , from_tensor = y ` options ` , ` run_metadata ` symbol_vals : List of symbolic tensors to be fed to ` feed_symbols ` . data = [ x.values if x.__class__.__name__ == 'DataFrame ' callable_opts = config_pb2.CallableOptions ( ) not hasattr ( K.get_session ( ) , data = [ x.values if x.__class__.__name__ == 'DataFrame ' def _legacy_call ( self , inputs ) : return int ( ins [ 0 ] .shape [ 0 ] ) else : data = [ standardize_single_array ( x ) for x in data ] 'you should specify the ` ' + steps_name + ' ` argument ' model.train_on_batch ( dtype=tensor.dtype.base_dtype.name ) ) [ output_d_tf , output_e_tf ] ) , return x ( np.expand_dims ( sparse_coo.row , 1 ) , ' Numpy arrays instead . ' if steps is not None : steps_per_epoch=2 , model.test_on_batch ( [ input_a_tf , input_b_tf ] , [ output_d_tf , output_e_tf ] ) if isinstance ( data [ 0 ] , list ) : 'tensors to have a static batch size . ' 'If ' + steps_name + ' is set , the ` batch_size ` must be None . ' ) return self._legacy_call ( inputs ) num_samples = ins [ 0 ] .shape [ 0 ] return num_samples [ input_a_tf , input_b_tf ] , [ output_d_tf , output_e_tf ] , { 'input_a ' : input_a_tf , self._session = None epochs=1 , y = tf.cast ( y , dtype=x.dtype ) self._symbol_vals = symbol_vals # at this level , since np.asarray ( value , raise ValueError ( 'If ' + steps_name raise ValueError ( num_samples = None raise TypeError ( 'Error when checking model ' + exception_prefix raise ValueError ( return set ( [ 0 if y is None else y.shape [ 0 ] for y in x ] ) if session_kwargs : # Handle symbolic feed . assert 'you can not use ` validation_split ` ' in str ( excinfo.value ) session_kwargs : arguments to ` tf.Session.run ( ) ` : ` fetches ` , ` feed_dict ` , session ! = self._session ) : callable_fn = session._make_callable_from_options ( callable_opts ) connection.to_tensor = x.name # Placeholder if steps is None : elif ins and hasattr ( ins [ 0 ] , 'shape ' ) : for x in names if value is None : feed_symbols , for tensor , value in zip ( self.inputs , inputs ) : def __call__ ( self , inputs ) : ` options ` , ` run_metadata ` ` fetches ` , ` feed_dict ` , x = keras.layers.Input ( shape= ( 3 , ) , name='input ' ) if self.feed_dict : model.compile ( optimizer , loss , metrics=metrics , loss_weights=loss_weights ) split_at = int ( x [ 0 ] .shape [ 0 ] * ( 1 . - validation_split ) ) `` `` '' Generates a callable that runs the graph . # Test with validation data feed_arrays.append ( tensor ) model.compile ( optimizer , loss , metrics=metrics ) steps=2 , verbose=0 ) validation_data= ( [ input_a_tf , input_b_tf ] , def standardize_single_array ( x ) : model.fit ( inputs , targets , epochs=1 , steps_per_epoch=2 , verbose=0 ) model.train_on_batch ( inputs , targets ) elif x.ndim == 1 : 'Feeding from symbolic tensors is not ' 'dropout ' : output_e_tf } ) # Returns verbose=0 , ' Numpy arrays instead . The list you passed was : ' if len ( names ) == 1 and data and isinstance ( data [ 0 ] , ( float , int ) ) : elif len ( names ) == 1 and isinstance ( data [ 0 ] , ( float , int ) ) : raise ValueError ( # Test evaluation / prediction methods def _call ( self , inputs ) : self._make_callable ( feed_arrays , raise TypeError ( 'Numpy arrays . Found : ' + str ( data ) [ :200 ] + ' ... ' ) symbolic tensors at runtime . return set ( [ 0 if y is None else int ( y.shape [ 0 ] ) for y in x ] ) model.predict ( [ input_a_tf , input_b_tf ] , steps=2 ) np.expand_dims ( sparse_coo.col , 1 ) ) , 1 ) verbose=0 ) array_vals = [ ] self._feed_symbols = feed_symbols callable_opts.feed.append ( key.name ) feed_symbols ! = self._feed_symbols or 'Numpy arrays . Found : ' + str ( data ) [ :200 ] + ' ... ' ) session_kwargs : arguments to ` tf.Session.run ( ) ` : array_vals.append ( def __init__ ( self , inputs , outputs , updates=None , name=None , * * session_kwargs ) : # Test with validation split feed_symbols : List of input tensors to be fed loss_weights = [ 1. , 0.5 ] 'dropout ' : output_e_tf } ) , callable_opts.target.append ( self.updates_op.name ) feed_symbols = [ ] data = [ np.asarray ( d ) for d in data ] model.test_on_batch ( inputs , targets ) 'supported with sparse inputs . ' ) Numpy arrays at runtime . continue 'In order to feed symbolic tensors to a Keras model ' epochs=1 , steps_per_epoch=2 , verbose=1 , # The main use case of ` fetches ` being passed to a model is the ability # We need to do array conversion and type casting session ) if any ( K.is_tensor ( t ) for t in x ) : # Case : feeding symbolic tensor . return x if ( self._callable_fn is None or feed_arrays ! = self._feed_arrays or updates=None , else : data [ x ] .values model = keras.Model ( x , y ) # Handle external-data feed . if shapes [ i ] is not None : dense = keras.layers.Dense ( 4 , name='dense ' ) 'input_b ' : input_b_tf } , validation_split=0.2 , symbol_vals ! = self._symbol_vals or if from_tensor is None : if not isinstance ( inputs , ( list , tuple ) ) : callable_opts.feed.append ( x.name ) else x for x in data model.fit ( inputs = keras.backend.zeros ( shape= ( 10 , 3 ) ) def __init__ ( self , inputs , outputs , model.fit ( dtype=key.dtype.base_dtype.name ) ) validation_steps=2 , np.asarray ( self.feed_dict [ key ] , connection = callable_opts.tensor_connection.add ( ) split_at = int ( int ( x [ 0 ] .shape [ 0 ] ) * ( 1 . - validation_split ) ) verbose=0 ) from_tensor = tf_ops._as_graph_element ( y ) self._feed_arrays = feed_arrays 'in TensorFlow , you need tensorflow 1.8 or higher . ' ) else x for x in data ] model.evaluate ( inputs , targets , steps=2 , verbose=0 ) if shape is None or shape [ 0 ] is None : self._session = session 'for each key in : ' + str ( names ) ) model.evaluate ( [ input_a_tf , input_b_tf ] , [ output_d_tf , output_e_tf ] , ' ( instead of the ` batch_size ` argument , ' c = dense ( a ) model.fit ( inputs , targets , b = keras.layers.Input ( shape= ( 3 , ) , name='input_b ' ) self._feed_symbols = None targets = keras.backend.zeros ( shape= ( 10 , 4 ) ) epochs=1 , 'batches of input data ) . ' ) # to run custom updates loss = 'mse ' input_a_tf = keras.backend.zeros ( shape= ( 10 , 3 ) ) def test_training_and_eval_methods_on_symbolic_tensors_multi_io ( ) : Function that runs the graph according to the above options . if batch_size is not None : symbol_vals , self._callable_fn = None data = [ np.expand_dims ( x , 1 ) if x is not None and x.ndim == 1 ' a defined shape , or ' + steps_name","['keras/backend/tensorflow_backend.py', 'keras/engine/training.py', 'keras/engine/training_utils.py', 'tests/keras/engine/test_training.py']",[ RELNOTES ] Allow symbolic tensors to be fed to models ( with TF backend ) ( # 10087 )
460,c98093f6ac852534493956ce6668f7294d608c82,2018-05-03 17:41:38+09:00,"if K.image_data_format ( ) ! = 'channels_last ' : from __future__ import division app = applications.MobileNetV2 # Expand to use as image input for the model . x = _inverted_res_block ( x , filters=96 , alpha=alpha , stride=1 , from .. utils import conv_utils expansion=6 , block_id=12 ) default_size = 224 | [ mobilenet_v2_1.4_224 ] | 582 | 6.06 | 75.0 | 92.5 | if no ` weights ` argument is specified . x = DepthwiseConv2D ( kernel_size=3 , min_size=32 , from .. layers import Reshape ' ( one of ( 96 , 96 ) , ( 128 , 128 ) , ( 160 , 160 ) , ' 'The model being returned right now will expect inputs ' | [ mobilenet_v2_0.35_128 ] | 20 | 1.66 | 50.8 | 75.0 | rows = input_shape [ 1 ] MobileNets support any input size greater from .mobilenetv2 import MobileNetV2 different width factors . This allows different width models to reduce def MobileNetV2 ( input_shape=None , try : expansion=6 , block_id=14 ) [ MobileNetV2 : Inverted Residuals and Linear Bottlenecks ] ( https : //arxiv.org/abs/1801.04381 ) alpha , filters , block_id ) : name='bn % d_conv_depthwise ' % the RGB values from [ 0 , 255 ] to [ -1 , 1 ] . Note that this preprocessing 'Input shape provided = % s ' % ( input_shape , ) ) the number of multiply-adds and thereby By altering the image size and ` alpha ` parameter , def _first_inverted_res_block ( inputs , block_id=0 ) 'which is not a valid type ' ) MobileNetV2 is very similar to the original MobileNet , if input_tensor is None : filters=16 , BASE_WEIGHT_PATH = 'https : //github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/ ' alpha=alpha , return K.relu ( x , max_value=6 ) ' You should set ` image_data_format= '' channels_last '' ` ' default_size = 224 if alpha > 1.0 : | [ mobilenet_v2_1.0_128 ] | 99 | 3.47 | 65.3 | 86.9 | # https : //github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py if rows is None : input_shape : optional shape tuple , to be specified if you would 'imagenet ' ( pre-training on ImageNet ) , else : than 32 x 32 , with larger image sizes input_tensor=None , last_block_filters = _make_divisible ( 1280 * alpha , 8 ) If ` alpha ` > 1.0 , proportionally increases the number if rows ! = cols or rows not in [ 96 , 128 , 160 , 192 , 224 ] : from .. layers import Dropout if K.image_data_format == 'channels_first ' : require_flatten=include_top , _test_application_notop ( app , last_dim ) _test_application_variable_input_channels ( app , last_dim ) K.set_image_data_format ( old_data_format ) if alpha not in [ 0.35 , 0.50 , 0.75 , 1.0 , 1.3 , 1.4 ] : raise ValueError ( 'input_shape : ' , input_shape , include_top=True , old_data_format = 'channels_first ' inputs = img_input # Reference ' ` 0.25 ` , ` 0.50 ` , ` 0.75 ` or ` 1.0 ` only . ' ) def test_mobilenetv2 ( ) : The number of parameters and number of multiply-adds from .. import constraints # Raises try : 'is type : ' , type ( input_tensor ) , # It can be seen here : of filters in each layer . x = GlobalAveragePooling2D ( ) ( x ) default_size=default_size , reduce inference cost on mobile devices . x = DepthwiseConv2D ( kernel_size=3 , strides=stride , activation=None , # If input_shape is None and no input_tensor use_bias=False , pointwise_conv_filters = int ( filters * alpha ) from .. utils.data_utils import get_file img_input = Input ( shape=input_shape ) # Arguments elif input_shape is None and K.is_keras_tensor ( input_tensor ) : # Ensure that the model takes into account else : ' ( width , height , channels ) . ' x = Activation ( relu6 , name='out_relu ' ) ( x ) from .. import initializers str ( alpha ) + ' _ ' + str ( rows ) + '_no_top ' + '.h5 ' ( 224 , 224 , 3 ) . if not K.is_keras_tensor ( input_tensor ) : Classification Checkpoint| MACs ( M ) | Parameters ( M ) | Top 1 Accuracy| Top 5 Accuracy if K.image_data_format ( ) == 'channels_first ' : weights_path = get_file ( model_name , weigh_path , use_bias=True , name='Logits ' ) ( x ) default_size = rows expansion=6 , block_id=11 ) expansion=6 , block_id=1 ) | [ mobilenet_v2_1.0_96 ] | 56 | 3.47 | 60.3 | 83.2 | return Add ( name='res_connect_ ' + str ( block_id ) ) ( [ inputs , x ] ) else : expansion=6 , block_id=9 ) min_value = divisor classes : optional number of classes to classify images x = _inverted_res_block ( x , filters=160 , alpha=alpha , stride=1 , classes=1000 ) : if rows == cols and rows in [ 96 , 128 , 160 , 192 , 224 ] : are used at each layer . 'depth multiplier must be 1 ' ) to infer input_shape from an input_tensor . from .. layers import DepthwiseConv2D warnings.warn ( 'MobileNet shape is undefined . ' | -- -- -- -- -- -- | -- -- -- -- -- -- -- -| -- -- -- -- -| -- -- | if input_tensor._keras_shape [ 2 ] ! = input_shape [ 1 ] : Depending on the use case , it can use different input layer size and A Keras model instance . kernel_size=1 , padding='same ' , use_bias=False , activation=None , x = _inverted_res_block ( x , filters=64 , alpha=alpha , stride=2 , or invalid input shape or invalid depth_multiplier , alpha , 'to follow the `` channels_last '' data format . ' ) if old_data_format : | [ mobilenet_v2_0.75_128 ] | 69 | 2.61 | 63.2 | 85.3 | `` `` '' Instantiates the MobileNetV2 architecture . from __future__ import print_function rows = 224 x = _inverted_res_block ( x , filters=32 , alpha=alpha , stride=1 , expansion=6 , block_id=10 ) stride=1 , name='mobl % d_conv_expand ' % block_id ) ( inputs ) weights='imagenet ' , # Make sure that round down does not go down by more than 10 % . ' ( 224 , 224 ) will be loaded . ' ) x = Activation ( relu6 , name='conv_dw_ % d_relu ' % block_id ) ( x ) from .. layers import Add found at [ mobilenet_v2_keras ] ( https : //github.com/JonathanCMitchell/mobilenet_v2_keras ) raise ValueError ( 'Weights for `` channels_first '' format ' x = _inverted_res_block ( x , filters=24 , alpha=alpha , stride=2 , which increases/decreases the number of filters in each layer . 'for the input data format `` channels_last '' ' expansion=6 , block_id=16 ) name='mobl % d_conv_depthwise ' % This file contains building code for MobileNetV2 , based on padding='same ' , # It ensures that all layers have a channel number that is divisible by 8 if K.image_data_format ( ) == 'channels_last ' : MACs stands for Multiply Adds new_v = max ( min_value , int ( v + divisor / 2 ) // divisor * divisor ) if depth_multiplier ! = 1 : from . import imagenet_utils if input_shape is None and not K.is_keras_tensor ( input_tensor ) : # If input_shape is None , infer shape from input_tensor except ValueError : cache_subdir='models ' ) alpha=1.0 , warnings.warn ( 'The MobileNet family of models is only available ' are provided ( 224 , 192 , 160 , 128 , and 96 ) . expansion=6 , block_id=3 ) if input_shape is not None and input_tensor is not None : rows when weights='imagenet ' expansion=1 , x = Conv2D ( pointwise_filters , ( also called the resolution multiplier ) | [ mobilenet_v2_0.75_224 ] | 209 | 2.61 | 69.8 | 89.6 | img_input = Input ( tensor=input_tensor , shape=input_shape ) import h5py block_id ) ( x ) from .. layers import Flatten | [ mobilenet_v2_1.0_160 ] | 154 | 3.47 | 68.8 | 89.0 | if weights == 'imagenet ' : raise ValueError ( 'If imagenet weights are being loaded , ' def _make_divisible ( v , divisor , min_value=None ) : if rows == cols and rows in [ 96 , 128 , 160 , 192 , 224 ] : If ` alpha ` < 1.0 , proportionally decreases the number expansion=6 , block_id=4 ) from .imagenet_utils import decode_predictions kernel_size=3 , `` `` '' name='bn % d_conv_bn_project ' % block_id ) ( x ) `` `` '' MobileNet v2 models for Keras . alpha : controls the width of the network . This is known as the get_source_inputs ( input_tensor ) ) rows = input_tensor._keras_shape [ 2 ] | [ mobilenet_v2_0.35_224 ] | 59 | 1.66 | 60.3 | 82.9 | expansion=6 , block_id=13 ) # Depthwise 'input must have a static square shape ' except that it uses inverted residual blocks with import os weights=weights ) if input_tensor._keras_shape [ 1 ] ! = input_shape [ 1 ] : if input_tensor is not None : | [ mobilenet_v2_0.35_192 ] | 43 | 1.66 | 58.2 | 81.2 | rows = input_shape [ row_axis ] expansion , stride , # If both input_shape and input_tensor are used , they should match name='mobl % d_conv_project ' % block_id ) ( x ) dropout=1e-3 , width multiplier in the MobileNetV2 paper . def relu6 ( x ) : | [ mobilenet_v2_0.75_192 ] | 153 | 2.61 | 68.7 | 88.9 | is_input_t_tensor = K.is_keras_tensor ( input_tensor ) new_v += divisor is_input_t_tensor = K.is_keras_tensor ( | [ mobilenet_v2_0.5_96 ] | 18 | 1.95 | 51.2 | 75.8 | offering better performance . row_axis , col_axis = ( 0 , 1 ) return model depth_multiplier : depth multiplier for depthwise convolution last_block_filters = 1280 'relu6 ' : mobilenet.relu6 } ) 'in your Keras config located at ~/.keras/keras.json . ' The following table describes the performance of Preprocessed array . If ` alpha ` = 1 , default number of filters from the paper `` `` '' Preprocesses a numpy array encoding a batch of images . It should have exactly 3 inputs channels ( 224 , 224 , 3 ) . weigh_path = BASE_WEIGHT_PATH + model_name | [ mobilenet_v2_0.5_128 ] | 32 | 1.95 | 57.7 | 80.8 | 'as true , ` classes ` should be 1000 ' ) 'is not type input_tensor ' ) # increase the number of output channels if is_input_t_tensor : ' ` None ` ( random initialization ) , ` imagenet ` ' default_size = rows model_name = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_ ' + \ x = BatchNormalization ( epsilon=1e-3 , momentum=0.999 , name='bn_Conv1 ' ) ( x ) if weights == 'imagenet ' and include_top and classes ! = 1000 : expansion=6 , block_id=8 ) weights : one of ` None ` ( random initialization ) , K.set_image_data_format ( 'channels_last ' ) ' ( pre-training on ImageNet ) , ' E.g . ` ( 160 , 160 , 3 ) ` would be one valid value . use_bias=False , padding='same ' , all 22 models from the paper can be built , with ImageNet weights provided . if include_top : 'do not meet the same shape requirements ' ) from .. layers import BatchNormalization x = Conv2D ( expansion * in_channels , kernel_size=1 , padding='same ' , _test_application_basic ( app ) x = _inverted_res_block ( x , filters=64 , alpha=alpha , stride=1 , import warnings # if the width multiplier is greater than 1 we ` layers.Input ( ) ` ) raise ValueError ( 'input_tensor : ' , input_tensor , # TODO Change path to v1.1 model = load_model ( 'mobilenet.h5 ' , custom_objects= { For each of these ` alpha ` values , weights for 5 different input image sizes from .. import regularizers name='Conv_1 ' ) ( x ) x = _first_inverted_res_block ( x , | [ mobilenet_v2_0.5_160 ] | 50 | 1.95 | 61.0 | 83.2 | MobileNetV2 is a general architecture and can be used for multiple use cases . from __future__ import absolute_import | [ mobilenet_v2_0.5_192 ] | 71 | 1.95 | 63.9 | 85.4 | except ValueError : 'and input_tensor : ' , input_tensor , raise ValueError ( 'input_tensor : ' , input_tensor , import numpy as np expansion=6 , block_id=15 ) ValueError : in case of invalid argument for ` weights ` , from .. models import Model cols = input_shape [ col_axis ] in_channels = inputs._keras_shape [ -1 ] parameter count than the original MobileNet . if min_value is None : depth_multiplier=1 , objects ` relu6 ` and pass them to the ` custom_objects ` parameter . data_format=K.image_data_format ( ) , | [ mobilenet_v2_0.35_160 ] | 30 | 1.66 | 55.7 | 79.1 | if not ( weights in { 'imagenet ' , None } or os.path.exists ( weights ) ) : input_tensor : optional Keras tensor ( i.e . output of into , only to be specified if ` include_top ` is True , and input_shape will be used if they match , if the shapes x = _inverted_res_block ( x , filters=24 , alpha=alpha , stride=1 , strides=stride , activation=None , if new_v < 0.9 * v : # no alpha applied to last conv as stated in the paper : from TensorFlow checkpoints found at expansion=6 , block_id=5 ) inputs = get_source_inputs ( input_tensor ) activation=None , name='bn % d_conv_project ' % name='bn % d_conv_bn_expand ' % from .. layers import Dense dropout : dropout rate , dropout is currently not in use x = Conv2D ( last_block_filters , # Project input_shape = _obtain_input_shape ( input_shape , use_bias=False , activation=None , pointwise_filters = _make_divisible ( pointwise_conv_filters , 8 ) kernel_size=1 , bottlenecking features . It has a drastically lower | [ mobilenet_v2_0.35_96 ] | 11 | 1.66 | 45.5 | 70.4 | https : //github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md strides= ( 2 , 2 ) , padding='same ' , from .imagenet_utils import _obtain_input_shape rows = input_shape [ 0 ] block_id ) ( inputs ) 'is not a keras tensor ' ) 'or the path to the weights file to be loaded . ' ) cols = input_shape [ 1 ] # load weights cols = input_shape [ 2 ] like to use a model with an input img resolution that is not model = Model ( inputs , x , name='mobilenetv2_ % 0.2f_ % s ' % ( alpha , rows ) ) include_top : whether to include the fully-connected def _inverted_res_block ( inputs , expansion , stride , alpha , filters , block_id ) : x = _inverted_res_block ( x , filters=32 , alpha=alpha , stride=2 , layer at the top of the network . x /= 128 . prefix = 'features . ' + str ( block_id ) + '.conv . ' elif weights is not None : 'alpha can be one of ' ' ( 192 , 192 ) , or ( 224 , 224 ) ) . ' # If input_shape is not None , assume default size If you choose to include both input_tensor and input_shape then x = _inverted_res_block ( x , filters=320 , alpha=alpha , stride=1 , | [ mobilenet_v2_0.75_160 ] | 107 | 2.61 | 66.4 | 87.3 | expansion=6 , block_id=7 ) use_bias=False , name='Conv1 ' ) ( img_input ) elif input_shape is None : The paper demonstrates the performance of MobileNets using ` alpha ` values of 1.0 ( also called 100 % MobileNet ) , 0.35 , 0.5 , 0.75 , 1.0 , 1.3 , and 1.4 Tests comparing this model to the existing Tensorflow model can be from .. engine import get_source_inputs from .. import backend as K cols = input_tensor._keras_shape [ 2 ] x = BatchNormalization ( epsilon=1e-3 , momentum=0.999 , cols = input_tensor._keras_shape [ 3 ] x = _inverted_res_block ( x , filters=160 , alpha=alpha , stride=2 , def preprocess_input ( x ) : raise ValueError ( 'If imagenet weights are being loaded , ' from .. layers import Conv2D | [ mobilenet_v2_1.0_224 ] | 300 | 3.47 | 71.8 | 91.0 | expansion=6 , block_id=2 ) function is different from ` imagenet_utils.preprocess_input ( ) ` . str ( alpha ) + ' _ ' + str ( rows ) + '.h5 ' row_axis , col_axis = ( 1 , 2 ) x : a 4D numpy array consists of RGB values within [ 0 , 255 ] . rows = input_tensor._keras_shape [ 1 ] raise ValueError ( 'If using ` weights ` as ImageNet with ` include_top ` ' from .. layers import Input The weights for all 16 models are obtained and translated from the Tensorflow checkpoints ' Weights for input shape ' # any potential predecessors of ` input_tensor ` . 'data format `` channels_first '' ( channels , width , height ) . ' # Determine proper input shape and default size . return new_v model.load_weights ( weights ) block_id ) ( x ) # This function is taken from the original tf repo . x = Activation ( relu6 , name='conv_ % d_relu ' % block_id ) ( x ) first_block_filters = _make_divisible ( 32 * alpha , 8 ) | [ mobilenet_v2_0.75_96 ] | 39 | 2.61 | 58.8 | 81.6 | raise ValueError ( 'input_tensor specified : ' , input_tensor , can be modified by using the ` alpha ` parameter , K.is_keras_tensor ( input_tensor ) or the path to the weights file to be loaded . `` `` '' if input_shape is None and input_tensor is not None : if K.image_data_format ( ) == 'channels_first ' : | [ mobilenet_v2_1.3_224 ] | 509 | 5.34 | 74.4 | 92.1 | name='mobl % d_conv_depthwise ' % block_id ) ( x ) name='bn % d_conv_depthwise ' % block_id ) ( x ) x = Dense ( classes , activation='softmax ' , # Create model . E.g . do not match then we will throw an error . # Returns MobileNet on various input sizes : | [ mobilenet_v2_1.0_192 ] | 221 | 3.47 | 70.7 | 90.1 | old_data_format = None name='mobl % d_conv_project ' % if in_channels == pointwise_filters and stride == 1 : expansion=6 , block_id=6 ) x = BatchNormalization ( epsilon=1e-3 , momentum=0.999 , name='Conv_1_bn ' ) ( x ) from .. layers import GlobalAveragePooling2D x -= 1 . To load a MobileNetV2 model via ` load_model ` , import the custom model.load_weights ( weights_path ) img_input = input_tensor This function applies the `` Inception '' preprocessing which converts from .. engine.base_layer import InputSpec return x You can also omit this option if you would like 'However your settings specify the default ' from .. layers import Activation if include_top : | [ mobilenet_v2_0.5_224 ] | 97 | 1.95 | 65.4 | 86.4 | x = Activation ( relu6 , name='Conv1_relu ' ) ( x ) raise ValueError ( 'The ` weights ` argument should be either ' x = Conv2D ( first_block_filters , last_dim = 1280 return x.astype ( np.float32 ) 'are not available . ' ) default_size = 224","['keras/applications/__init__.py', 'keras/applications/mobilenetv2.py', 'tests/keras/applications/applications_test.py']",Adds MobiletNetV2 to applications ( # 10047 )
461,19ce1ca783a995bbd0765dfd9d89f2866fccbc09,2018-05-03 15:27:44+09:00,"acc_levels = [ 0.55 , 0.74 , 0.81 , 0.81 ] break self.best = self.baseline acc_levels = [ 0.55 , 0.76 , 0.81 , 0.81 ] def __init__ ( self ) : baseline : baseline value for the monitored quantity to reach baseline_met = baseline_tester ( acc_levels ) if self.baseline > 0 : baseline_not_met = baseline_tester ( acc_levels ) epochs_trained += 1 # All epochs should run because baseline was met in second epoch early_stop.on_train_begin ( ) self.stop_training = False def __init__ ( self , monitor='val_loss ' , def __init__ ( self , monitor='val_loss ' , baseline=0 , assert baseline_not_met == 2 epochs_trained = 0 else : self.best = np.Inf if self.monitor_op == np.less else -np.Inf return epochs_trained early_stop.on_epoch_end ( epoch , logs= { 'val_acc ' : acc_levels [ epoch ] } ) for epoch in range ( len ( acc_levels ) ) : self.baseline = baseline self.best = np.Inf if self.monitor_op == np.less else -np.Inf def baseline_tester ( acc_levels ) : class DummyModel ( object ) : early_stop.model = DummyModel ( ) def test_EarlyStopping_baseline ( ) : assert baseline_met == 4 # Baseline was not met by second epoch and should stop early_stop = callbacks.EarlyStopping ( monitor='val_acc ' , baseline=0.75 , patience=2 ) if early_stop.model.stop_training :","['keras/callbacks.py', 'tests/keras/test_callbacks.py']",New Callback : EarlyBaselineStopping ( # 10061 )
462,49b9682b3570211c7d8f619f8538c08fd5d8bdad,2018-05-01 10:57:24-07:00,"raise ValueError ( 'Invalid backend . Missing required entry : ' + e ) try : assert _backend in { 'theano ' , 'tensorflow ' , 'cntk ' } # Try and load external backend . namespace [ k ] = v import importlib # Check if valid backend . sys.stderr.write ( 'Using ' + _BACKEND + ' backend.\n ' ) entries = backend_module.__dict__ required_entries = [ 'placeholder ' , 'variable ' , 'function ' ] for e in required_entries : if k not in namespace : # Module is a valid backend if it has the required entries . # Make sure we do n't override any entries from common , such as epsilon . namespace = globals ( ) raise ValueError ( 'Unable to import backend : ' + str ( _BACKEND ) ) backend_module = importlib.import_module ( _BACKEND ) raise ValueError ( 'Unknown backend : ' + str ( _BACKEND ) ) for k , v in entries.items ( ) : if e not in entries : except ImportError :",['keras/backend/__init__.py'],[ RELNOTES ] Allow loading external backends ( # 10034 )
463,ba8b3fca6e9e4e863bb80d134946be1b60463718,2018-05-01 10:31:21-07:00,"def save_img ( img , fname ) : scipy.misc.imsave ( fname , pil_img ) data_format=None , from keras.preprocessing.image import load_img , save_img , img_to_array x , scale=True , * * kwargs ) : `` `` '' Save an image stored as a Numpy array to a path or file object . save_img ( fname , img ) save_img ( 'stitched_filters_ % dx % d.png ' % ( n , n ) , stitched_filters ) def save_img ( path , # Arguments parameter should always be used . from keras.preprocessing.image import save_img imsave ( fname , img ) save_img ( result_prefix + '.png ' , deprocess_image ( np.copy ( img ) ) ) from keras.preprocessing.image import load_img , img_to_array data_format : Image data format . One of { `` channels_first '' or `` channels_last '' } . `` `` '' img = array_to_img ( x , data_format=data_format , scale=scale ) x : Numpy array . scale : Whether to rescale image values to be within [ 0 , 255 ] . imsave ( 'stitched_filters_ % dx % d.png ' % ( n , n ) , stitched_filters ) Default is 'channels_last ' . save_img ( img , fname=result_prefix + '.png ' ) file_format : Optional file format override . If omitted , the ref_img = imread ( target_mask_path ) file_format=None , * * kwargs : Additional keyword arguments passed to PIL.Image.save ( ) . img.save ( path , format=file_format , * * kwargs ) from scipy.misc import imsave ref_img = img_to_array ( load_img ( target_mask_path ) ) pil_img = deprocess_image ( np.copy ( img ) ) path : Path or file object . from scipy.misc import imread , imsave from keras.preprocessing.image import load_img , img_to_array format to use is determined from the filename extension . save_img ( fname , img ) from keras.preprocessing.image import load_img , save_img , img_to_array imsave ( fname , img ) If a file object was used instead of a filename , this","['examples/conv_filter_visualization.py', 'examples/deep_dream.py', 'examples/neural_doodle.py', 'examples/neural_style_transfer.py', 'keras/preprocessing/image.py']",[ RELNOTES ] Introduce ` preprocessing.image.save_img ` and remove deprecated imsave method in neural style transfer example ( # 9996 )
464,7ff9303041729bb69bd416e2ccb6e7a87ee7cc2a,2018-05-01 10:18:25-07:00,"` 0.2 * x + 0.5 ` if ` -2.5 < = x < = 2.5 ` . ` alpha * x ` if ` x < 0 ` . If ` max_value ` is defined , the result `` `` '' Softplus activation function . correctly ( see ` lecun_normal ` initialization ) and the number of inputs `` `` '' Scaled Exponential Linear Unit ( SELU ) . Hard sigmoid activation : # Arguments chosen so that the mean and variance of the inputs are preserved `` `` '' Exponential linear unit . SELU is equal to : ` scale * elu ( x , alpha ) ` , where alpha and scale is truncated to this value . between two consecutive layers as long as the weights are initialized ` 1 ` if ` x > 2.5 ` The softplus activation : ` log ( exp ( x ) + 1 ` . are pre-defined constants . The values of ` alpha ` and ` scale ` are `` `` '' x : Input tensor . The softplus activation : ` x / ( abs ( x ) + 1 ) ` . # References alpha : A scalar , slope of negative section . The ( leaky ) rectified linear unit activation : ` x ` if ` x > 0 ` , ` alpha * ( exp ( x ) -1 ) ` if ` x < 0 ` . max_value : Maximum value for the output . The scaled exponential unit activation : ` scale * elu ( x , alpha ) ` . is `` large enough '' ( see references for more information ) . `` `` '' Linear ( e.g . unit ) activation function . Tensor with the same shape and dtype as ` x ` . `` `` '' Hard sigmoid activation function . `` `` '' Hyperbolic tangent activation function . The exponential linear activation : ` x ` if ` x > 0 ` and # Returns x : Tensor . `` `` '' Scaled Exponential Linear Unit . ( Klambauer et al. , 2017 ) . `` `` '' Softsign activation function . `` `` '' ( Leaky ) Rectified Linear Unit . `` `` '' Sigmoid activation function . ` 0 ` if ` x < -2.5 ` alpha : Slope of the negative part . Defaults to zero . [ Fast and Accurate Deep Network Learning by Exponential Faster to compute than sigmoid activation . Linear Units ( ELUs ) ] ( https : //arxiv.org/abs/1511.07289 )",['keras/activations.py'],Add documentation to several activation functions ( # 10066 )
465,a637960fab61b66848a36e6a5caf0204c155af01,2018-05-01 10:14:38-07:00,"model.compile ( 'rmsprop ' , 'mse ' ) right = Sequential ( name='branch_2 ' ) ( one array per model weight ) . model.add ( Merge ( [ left , right ] , mode='dot ' , dot_axes= [ 1 , 1 ] ) ) on this data at the end of each epoch . return self.model.get_weights ( ) model.add ( Dense ( num_classes ) ) class_weight=None , for layer in merge.layers : generator : generator yielding batches of input samples . self._output_tensor_cache = self.model._output_tensor_cache model_from_yaml ( yaml_str ) model = models.Model ( x , y ) will default to an array of 0s if not provided . model_sum.fit ( [ rand ( 2 , 3 ) , rand ( 2 , 3 ) ] , [ rand ( 2 , 3 ) ] , epochs=1 ) of ` Sequence ` ( ` keras.utils.Sequence ` ) . metrics : List of metrics to be evaluated by the model `` `` '' Single gradient update over one batch of samples . return self.model.evaluate ( x , y , output_shape_type = 'raw ' if not isinstance ( layer , ( InputLayer , legacy_layers.Merge ) ) : node_indices , tensor_indices ) : steps_per_epoch=None , config [ 'output_mask ' ] = output_mask indefinitely . An epoch finishes when ` steps_per_epoch ` model.add ( Dense ( 32 ) ) return self._layers node_indices= [ ] , output_mask_type = 'lambda ' @ keras_test if merge mode is a lambda/function ) . If the latter case , it should model.summary ( ) # equivalent to : self.model = Model ( self.inputs , self.outputs [ 0 ] , A ` History ` object . Its ` History.history ` attribute is model = models.Sequential ( [ legacy_layers.Merge ( [ branch_1_2 , branch_3 ] , mode='concat ' ) ] , name='final ' ) the ` len ( generator ) ` as a number of steps . epochs : Integer . Number of epochs to train the model . input_a = layers.Input ( shape= ( 3 , ) ) embedding = layers.Embedding ( 3 , 4 , mask_zero=True ) # embeddings output = K.expand_dims ( output , 1 ) # ( no input shape specified ) , if not tensor_indices : optimizer : String ( name of optimizer ) or optimizer object . 'mode_type ' : mode_type , input_b = layers.Input ( shape= ( 3 , ) , dtype='int32 ' ) # Instantiate the input layer . 'argument was provided . ' batch . Therefore , all arrays in this tuple must have the same righter = Sequential ( ) outer_model = models.Model ( [ a , b , c ] , o ) raise ValueError ( 'Invalid format for dot_axes - ' ( useful for resuming a previous training run ) . ' ` Sequential ` is a subclass of ` Model ` , you can ' self.model.compile ( optimizer , loss , arguments [ 'mask ' ] = mask The model will not be trained on this data . merge_config = self.layers [ 0 ] .get_config ( ) `` `` '' Evaluates the model on a data generator . model.add ( layer ) the output of ` model.get_weights ( ) ` . model_concat = models.Model ( [ input_a , input_b ] , [ merged_concat ] ) Typically you will use ` metrics= [ 'accuracy ' ] ` . from .. utils.generic_utils import func_dump , func_load , has_arg tensor_indices= [ ] , node_index = node_indices [ i ] return { 'name ' : self.name , output_mask : Mask or lambda/function to compute the output mask ( only from keras.models import model_from_json , model_from_yaml such as ` metrics= { 'output_a ' : 'accuracy ' } ` . if self.inputs : if self._layers and isinstance ( self._layers [ 0 ] , InputLayer ) : shape2.pop ( self.dot_axes [ 1 ] ) layers.append ( layer ) model._make_train_function ( ) you should not pass non-picklable arguments to the generator return x * a + y * b # rnn # test lambda with output_shape lambda [ How can I install HDF5 or h5py to save my models in Keras ? ] ( order of horizontal graph traversal ( bottom-up ) . the training samples , used for weighting the loss function self.outputs = [ layer._inbound_nodes [ -1 ] .output_tensors [ 0 ] ] input_dim = 16 model.add ( merge ) 'mode ' : mode , sample_weight=None ) : from __future__ import absolute_import of Numpy arrays with shapes and types matching def train_on_batch ( self , x , y , class_weight=None , for layer in self.layers [ 1 : ] : return self.mode ( inputs , * * arguments ) model.add ( Dense ( 32 , input_shape= ( 500 , ) ) ) return verbose : 0 , 1 , or 2 . Verbosity mode . to a weight ( float ) value , used for weighting the loss function tensor_indices = [ None for _ in range ( len ( layers ) ) ] if 'layer_names ' not in f.attrs and 'model_weights ' in f : self.uses_learning_phase = False str ( layers ) ) or a lambda/function @ classmethod def fit ( self , batch_size=None , when using process-based threading . model.train_on_batch ( x_train [ :32 ] , y_train [ :32 ] ) else : self.build ( ) `` ` python # actually create the model masks.append ( K.expand_dims ( mask_i ) ) The use of ` keras.utils.Sequence ` guarantees the ordering in the ` x ` and ` y ` data provided , before shuffling . return layers RuntimeError : if the model was never compiled . input_masks = [ ] sample_weight_mode=None , output_mask = self._output_mask merged_model = Sequential ( ) # more advanced model with multiple branches globs=globals ( ) ) `` ` /getting-started/faq/ mode = config [ 'mode ' ] self.node_indices = node_indices config.append ( { 'class_name ' : layer.__class__.__name__ , # Corner case where the user passes an InputLayer via ` add ` . output = K.batch_dot ( l1 , l2 , self.dot_axes ) A layer instance . # test serialization ` None ` defaults to sample-wise weights ( 1D ) . layers = self.layers saving.save_weights_to_hdf5_group ( f , layers ) a = layers.Masking ( ) ( input_a ) steps , batch_shape=batch_shape , if sublayer not in layers : mode=lambda tup : K.concatenate ( [ tup [ 0 ] , tup [ 1 ] ] ) , set_inputs = True self._layers.append ( layer ) concat_axis=concat_axis , workers : maximum number of processes to spin up # also possible ( equivalent to the above ) : 'list elements should be `` int '' . ' ) self.mode = mode if output_shape_type == 'function ' : for layer in layers : if output_mask_type == 'function ' : # In that case the model gets built the first time you call ` fit ` ( or other return ( input_shape [ 0 ] [ 0 ] , ) + tuple ( self._output_shape ) # implemented as a wrapper for ` Model ` which maintained # test concatenation with masked and non-masked inputs with open ( path ) as f : if set_inputs : from keras import models name=self.name + '_input ' ) elif self.mode == 'cos ' : it should take as input a list of shape tuples else : def losses ( self ) : 'before being used . ' ) if not self.inputs or not self.outputs : If the output layer in the model is named , you can also pass a `` `` '' Trains the model for a fixed number of epochs ( iterations on a dataset ) . tensor_indices = [ 0 for _ in range ( len ( layers ) ) ] elif callable ( self._output_mask ) : self.concat_axis = concat_axis # Example for i , layer in enumerate ( layers ) : validation_steps=None , input_b = layers.Input ( shape=input_shapes [ 1 ] [ 1 : ] ) return self.model.predict_generator ( generator , steps , # test sum callbacks=callbacks , max_queue_size=10 , workers=1 , self._non_trainable_weights = [ ] output_mask = globals ( ) [ config [ 'output_mask ' ] ] weights : Should be a list all_keras_tensors = True dtype = layer.dtype metrics= [ 'accuracy ' ] ) # inside the functional API elif self.mode == 'concat ' : for sublayer in layer._flattened_layers : self._input_coordinates = self.model._input_coordinates output_masks= [ None ] , `` pay more attention '' to samples from an under-represented class . import types as python_types batch_size=batch_size , framework-native tensors ( e.g . TensorFlow data tensors ) . an ` input_dim ` argument . merged = legacy_layers.merge ( [ input_a , input_b ] , mode=fn_mode , output_shape=lambda s : s [ 0 ] , arguments= { ' a ' : 0.7 , ' b ' : 0.3 } ) skip_mismatch=skip_mismatch , from __future__ import print_function model.add ( Dense ( 32 , input_dim=500 ) ) model.add ( Dense ( 32 , input_shape= ( 500 , ) ) ) # Optionally , the first layer can receive an ` input_shape ` argument : layer = layer_module.deserialize ( conf , custom_objects=custom_objects ) is a generator . Total number of steps ( batches of samples ) model = Sequential ( ) return weights # Note # First , we need to infer the expected input shape and dtype . def in_tmpdir ( tmpdir ) : # and we only need a specific one . merge_inputs.append ( merge_input ) merge_layer = Merge ( input_layers , mode=mode , If ` True ` , use process-based threading . if mask is None or all ( [ m is None for m in mask ] ) : for i in range ( 1 , len ( inputs ) ) : sample_weight : sample weights , as a Numpy array . model.predict ( x_test , verbose=0 ) if callable ( self.mode ) : pytest.main ( [ __file__ ] ) if name in layer_cache : x : the input data , as a Numpy array . shuffle : Boolean ( whether to shuffle the order of the batches at assert not model.model.updates or for some type of layers ( recurrent , Dense ... ) config = model.get_config ( ) model = cls ( ) input_layers = [ ] intermediate = Sequential ( ) verbose=1 , s = inputs [ 0 ] if all_keras_tensors : output_tensors=self.outputs , initial_epoch=initial_epoch ) 'output_shape_type ' : output_shape_type , model.fit ( [ x_train , x_train ] , y_train , batch_size=batch_size , epochs=epochs , verbose=0 , validation_data= ( [ x_test , x_test ] , y_test ) ) if not self.outputs : # Instantiate the input layer . node_indices : Optional list of integers containing # Case : `` mode '' is a lambda or function . to compute output_shape ( only if merge mode is a lambda/function ) . all_keras_tensors = False a list of layer instances . Must be more self._is_graph_network = True # to a training/evaluation method ( since it is n't yet built ) : mode_type = config.pop ( 'mode_type ' ) self._per_input_losses = { } self.dot_axes = [ dot_axes % n1 , dot_axes % n2 ] return layers fname = 'test_merge_recursivity_temp.h5 ' The model will set apart this fraction of the training data , not trained for n steps given by epochs , but until the s = K.maximum ( s , inputs [ i ] ) def save_weights ( self , filepath , overwrite=True ) : def fn_output_mask ( tup ) : 'Use instead layers from ` keras.layers.merge ` , ' for sublayer in layer.layers : if layer not in layers : mode=fn_mode , The input samples are processed batch by batch . layers = legacy_models.legacy_sequential_layers ( self ) layers.append ( self.layers [ 0 ] ) y_train = np_utils.to_categorical ( y_train ) raise ImportError ( ' ` save_weights ` requires h5py . ' ) layer_output_shape = layer_output_shape [ tensor_indices [ i ] ] shape1.pop ( self.dot_axes [ 0 ] ) epochs : Integer , total number of iterations on the data . the loss and any model metrics raise ValueError ( 'All layers in a Sequential model ' or ( inputs , targets , sample_weights ) model.fit ( x , y , batch_size=32 , epochs=10 ) with h5py.File ( filepath , ' w ' ) as f : When using the TensorFlow backend , def _arguments_validation ( self , layers , mode , concat_axis , dot_axes , # will call the parent Sequential model . arguments=None , name=None ) : # as you are adding layers : return self.model.train_on_batch ( x , y , # choose to manually build your model by calling classification=True , layer_output_shape = layer.get_output_shape_at ( node_indices [ i ] ) input_shapes.append ( layer_output_shape ) ' ` batch_input_shape ` argument . ' ) model.fit ( [ x_train , x_train , x_train ] , y_train , batch_size=batch_size , epochs=epochs , verbose=0 , validation_data= ( [ x_test , x_test , x_test ] , y_test ) ) TensorFlow data tensors , the default ` None ` is equal to class_weight=None , sample_weight=None , self ( input_tensors , mask=input_masks ) self.outputs = [ x ] ( ` keras.utils.Sequence ` ) object in order to avoid duplicate data self.outputs [ 0 ] ._keras_shape ] shuffle=True , use_multiprocessing=use_multiprocessing ) # And to the following : model.fit ( [ x , y , z ] , labels , epochs=1 ) self._nodes_by_depth = self.model._nodes_by_depth epochs=1 , layer = get_or_create_layer ( conf ) layer ( x ) super ( Sequential , self ) .__init__ ( name=name ) input_a = layers.Input ( shape= ( 3 , ) , dtype='int32 ' ) model = Sequential ( ) validation_data : This can be either if legacy_models.needs_legacy_support ( self ) : mask_output = model.layers [ -1 ] ._output_mask ( mask_input_placeholders ) expected_mask_output = np.concatenate ( mask_inputs , axis=-1 ) ( in case some input layer node returns multiple tensors ) . mask_input_placeholders = [ K.placeholder ( shape=input_shape [ : -1 ] ) for input_shape in input_shapes ] if layer not in layers : model = models.Model ( [ input_a , input_b ] , merged ) # Legacy support # Afterwards , we do automatic shape inference : elif self.mode in [ 'cos ' , 'dot ' ] : validation dataset divided by the batch size . a tuple ` ( inputs , targets ) ` # This will build the current layer if self.layers : model.add ( layers.Dense ( 16 , name='dense_final ' ) ) # which means any batch size will be accepted by the model . self.sample_weights = self.model.sample_weights if isinstance ( self.layers [ 0 ] , legacy_layers.Merge ) : if isinstance ( self._output_shape , python_types.LambdaType ) : actual_output_shape = model.predict ( inputs ) .shape The generator is run in parallel to the model , for efficiency . input_shapes = input_shape # and create the node connecting the current layer model.add ( Merge ( [ left , right ] , mode='sum ' ) ) self.arguments = arguments if arguments else { } if hasattr ( layer , 'layers ' ) : model.compile ( loss='categorical_crossentropy ' , optimizer='rmsprop ' ) def compile ( self , optimizer , loss , model.predict_classes ( [ x_test , x_test , x_test ] , verbose=0 ) model_sum = models.Model ( [ input_a , input_b ] , [ merged_sum ] ) dtype=dtype , def compute_mask ( self , inputs , mask=None ) : if isinstance ( model.layers [ 0 ] , Merge ) : ( 1:1 mapping between weights and samples ) , labels = np.random.random ( ( 100 , 16 ) ) raise ValueError ( 'All layers in a Sequential model ' output_shape=fn_output_shape ) 'arguments ' : self.arguments } intermediate.add ( Activation ( 'relu ' ) ) self.supports_masking = self.model.supports_masking if not self._layers : of index ` epochs ` is reached . if layers : yield None the beginning of each epoch . Only used with instances `` `` '' Returns predictions for a single batch of samples . model.fit ( x_train , y_train , batch_size=batch_size , epochs=epochs , verbose=2 , validation_split=0.1 ) The loaded Model . name=None ) : outer_model.fit ( [ x , y , z ] , labels , epochs=1 ) right.add ( Activation ( 'relu ' ) ) * * kwargs ) n1 = len ( shape1 ) verbose : Integer . 0 , 1 , or 2 . Verbosity mode . def get_updates_for ( self , inputs ) : A Numpy array of predictions . config.append ( { 'class_name ' : self.layers [ 0 ] .__class__.__name__ , If unspecified , ` use_multiprocessing ` will default to ` False ` . def _gather_list_attr ( self , attr ) : self.optimizer = None concat_axis : Integer , axis to use in mode ` concat ` . validation_data=None , self._input_layers = self.model._input_layers merged = legacy_layers.merge ( [ input_a , input_b ] , mode=mode ) return ( s1 [ 0 ] , s1 [ 1 ] + s2 [ 1 ] ) + s1 [ 2 : ] def evaluate ( self , x=None , y=None , import pytest for input_i , mask_i in zip ( inputs , mask ) : tensor_a = Input ( shape= ( 32 , ) ) return self.model.uses_learning_phase elif self.mode == 'dot ' : 'Make sure to pass a shape tuple ' input_shapes = [ ( 3 , 2 ) , ( 3 , 2 ) ] model.fit ( [ x_train , x_train , x_train ] , y_train , batch_size=batch_size , epochs=epochs , verbose=0 , validation_split=0.1 ) `` `` '' Functional merge , to apply to Keras tensors ( NOT layers ) . if input_shape and not self.inputs : using ` use_multiprocessing=True ` . os.remove ( fname ) def test_sequential_regression ( ) : return copy.deepcopy ( config ) 'use the functional API . ' ) model2.add ( Dense ( 32 , input_dim=32 ) ) Note : Please also see output_shape_type = 'lambda ' preds = self.predict ( x , batch_size , verbose ) self.outputs , raise RuntimeError ( 'The model needs to be compiled ' mask_inputs = ( np.zeros ( input_shapes [ 0 ] [ : -1 ] ) , np.ones ( input_shapes [ 1 ] [ : -1 ] ) ) proba = self.predict ( x , batch_size=batch_size , verbose=verbose , x = Input ( batch_shape=batch_shape , # test weight saving self.trainable = True metrics= [ 'accuracy ' ] ) first_layer_config = first_layer [ 'config ' ] If unspecified , ` workers ` will default to 1 . If 0 , will a generator for the validation data def test_merge_overlap ( in_tmpdir ) : ` ( x_val , y_val , val_sample_weights ) ` on which to evaluate when using multiprocessing . def predict_classes ( self , x , batch_size=32 , verbose=0 ) : def layers ( self ) : return [ ] batch_size : Integer . If unspecified , it will default to 32 . model.fit ( [ x_train , x_train ] , y_train , batch_size=batch_size , epochs=epochs , verbose=0 , shuffle=False ) output_mask=output_mask , dictionary or a list of modes . `` ` steps=None ) : input_shapes = [ ] # This should have been caught earlier . return self.model.get_losses_for ( inputs ) # create Numpy arrays of input data workers=workers , left.add ( Dense ( num_hidden , input_shape= ( input_dim , ) ) ) output_shapes= [ self.outputs [ 0 ] ._keras_shape ] ) model.add ( Merge ( [ left , right ] , mode='dot ' , dot_axes=1 ) ) fname = 'test_merge_concat_temp.h5 ' verbose=verbose , ( for a single-output ` Sequential ` model ) . proba = self.predict ( x , batch_size=batch_size , verbose=verbose ) # is the same as the corresponding input . if len ( shape_set ) > 1 : warnings.warn ( 'The ` nb_epoch ` argument in ` fit ` ' the number of samples in your dataset divided by arguments=None , node_indices=None , tensor_indices=None , If the model has multiple outputs , you can use a different loss 'Layer shapes : % s ' % ( input_shapes ) ) finished and starting the next epoch . It should typically are passed into ` K.function ` . workers=workers , be equal to the number of samples of your dataset 'batch ' is a special option for dealing with the righter.add ( Activation ( 'relu ' ) ) max_queue_size=10 , if not isinstance ( layers , ( list , tuple ) ) or len ( layers ) < 2 : will then be the sum of all individual losses . input_tensors.append ( inbound_node.output_tensors [ tensor_index ] ) # inputs layer = get_or_create_layer ( first_layer ) 'For multi-output layers , ' shape1 = list ( input_shapes [ 0 ] ) self._inbound_nodes [ 0 ] .output_shapes = [ sample_weight=sample_weight ) raise ValueError ( 'Dimension incompatibility using dot mode : ' # test if Sequential can be called in the functional API self.built = True model.compile ( loss='mse ' , optimizer='sgd ' ) x = Input ( batch_shape=batch_shape , return output_shape epochs is to be understood as `` final epoch '' . The model is ( if the model has multiple inputs ) . # We create an input node , which we will keep updated input_tensors = [ ] right.add ( Dense ( num_hidden , input_shape= ( input_dim , ) ) ) masks = [ ] merge_inputs = [ ] l2 = inputs [ 1 ] [ a , b ] , mode=lambda tup : K.concatenate ( [ tup [ 0 ] , tup [ 1 ] ] , axis=1 ) , if callable ( self._output_mask ) : # Case : callable self._output_shape . batch_shape = first_layer.batch_input_shape return K.concatenate ( [ x , y ] , axis=1 ) raise TypeError ( 'Merge must be called on a list of tensors ' model_layers = model.layers use_multiprocessing : if True , use process based threading . model.add ( Merge ( [ left , left ] , mode='sum ' ) ) test_samples = 50 An epoch is an iteration over the entire ` x ` and ` y ` This will override ` validation_split ` . model.compile ( optimizer=optimizer , loss=loss ) # First , we need to infer its expected input shape and dtype . used for scaling the loss function ( during training only ) . if len ( self.dot_axes ) ! = 2 : # We keep it for compatibility reasons . if len ( layer._inbound_nodes [ -1 ] .output_tensors ) ! = 1 : Returns a Keras tensor . return self.model.regularizers If lambda/function , it should take as input a list of tensors raise TypeError ( ' A Merge should only be applied to a list of ' layers.append ( layer ) mode : String or lambda/function . If string , must be one before declaring the evaluation round finished . @ property def updates ( self ) : 'config ' : layer.get_config ( ) into a single tensor , following some merge ` mode ` . input_b = layers.Input ( shape=input_shapes [ 1 ] [ 1 : ] ) * * kwargs ) : model2 = Sequential ( ) ( same convention as the ` compute_output_shape ` method of layers ) . name=layer.name + '_input ' ) layers.append ( sublayer ) branch_1_2 = models.Sequential ( [ legacy_layers.Merge ( [ branch_1 , branch_2 ] , mode='concat ' ) ] , name='branch_1_2 ' ) first_layer_config [ 'layers ' ] = merge_inputs if 'class_name ' not in conf : shape2.pop ( 0 ) masks.append ( K.ones_like ( input_i , dtype='bool ' ) ) self._outbound_nodes = [ ] steps : Integer or ` None ` . mode_type = 'lambda ' model.fit ( [ x_train , x_train , x_train ] , y_train , batch_size=batch_size , epochs=epochs , verbose=0 ) as appropriate . model.fit ( x_train , y_train , batch_size=batch_size , epochs=epochs , verbose=0 ) def fn_mode ( tup , a , b ) : merge_layer = Merge ( mode=mode , model.compile ( optimizer='rmsprop ' , multi-output model , you could also pass a dictionary , self._inbound_nodes = [ ] at successive epochs , as well as validation loss values model.add ( Dense ( num_classes , name='final_dense ' ) ) ' % s ! = % s . ' % ( shape1 [ self.dot_axes [ 0 ] ] , shape2 [ self.dot_axes [ 1 ] ] ) callbacks=None , limitations of HDF5 data ; it shuffles in batch-sized chunks . dictionary mapping the input name to a Numpy array . branch_2.add ( layers.Dense ( 32 , input_shape= ( 8 , ) , name='dense_2 ' ) ) self.input_spec = None # Compatible with anything . output_shape = self._output_shape.__name__ The attribute ` model.metrics_names ` will give you masked_b = layers.Masking ( mask_value=0 ) ( input_b ) # Pre-defined merge modes . name='embed_1 ' ) ) # test Merge ( # 2460 ) # Returns : raise TypeError ( 'Sequential model can not be built : model is empty . ' 'should have a single output tensor . ' model_concat.fit ( [ rand ( 2 , 3 ) , rand ( 2 , 3 ) ] , [ rand ( 2 , 6 ) ] , epochs=1 ) # start with a basic example of using a Sequential model # Whereas if you specify the input shape , the model gets built continuously output_shape = self._output_shape ( input_shape ) for layer in self._layers : def normalize_legacy_config ( conf ) : s * = inputs [ i ] 'config ' : conf } target_tensors=target_tensors , elif output_shape_type == 'lambda ' : model.compile ( 'rmsprop ' , 'mse ' ) self.supports_masking = True def predict_classes ( self , x , batch_size=None , verbose=0 , steps=None ) : next epoch . When training with input tensors such as merge = model.layers [ 0 ] the ` len ( validation_data ) ` as a number of steps . use_multiprocessing=False , verbose=0 ) : raise ValueError ( 'Only layers of same output shape can ' loss = model.evaluate ( x_test , y_test , verbose=0 ) return self.model.state_updates input_a = layers.Input ( shape=input_shapes [ 0 ] [ 1 : ] ) generator , model.save_weights ( fname , overwrite=True ) o = model ( [ a , b , c ] ) name=name ) have different sizes . For example , the last batch of the epoch if h5py is None : y_test = np_utils.to_categorical ( y_test ) initial_epoch=0 ) : # test concatenation mode_type = 'function ' b = layers.Masking ( ) ( input_b ) model.model._make_train_function ( ) a record of training loss values and metrics values The validation data is selected from the last samples saving.load_weights_from_hdf5_group_by_name ( f , layers , If unspecified , it will default to 32 . ` epochs ` is to be understood as `` final epoch '' . self.model = None # Internal Model instance . 'and we can not infer its output shape ' ( during training only ) . This can be useful to tell the model to Note that because this implementation relies on multiprocessing , input_tensors=self.inputs , sample_weight=None ) : reshape=reshape ) for i in range ( len ( reduced_inputs_shapes ) ) : rnn_a = rnn ( embedding_a ) return cls.legacy_from_config ( config ) node_indices=node_indices , model.predict ( [ x_test , x_test , x_test ] , verbose=0 ) def model ( self ) : if not overwrite and os.path.isfile ( filepath ) : is specified . Total number of steps ( batches of samples ) batch_size=None , from .. utils.io_utils import ask_to_proceed_with_overwrite elif self.outputs : def regularizers ( self ) : break to yield from ` generator ` before declaring one epoch ( during training only ) . You can either pass a flat ( 1D ) to consider for merging denominator = K.maximum ( denominator , K.epsilon ( ) ) # Make sure child model callbacks use_multiprocessing=use_multiprocessing , self._layers = [ ] # Stack of layers . branch_3 = models.Sequential ( name='branch_3 ' ) def legacy_from_config ( cls , config , layer_cache=None ) : if 'nb_epoch ' in kwargs : self.inputs = None 'should have a single output tensor . ' output_shape = globals ( ) [ config [ 'output_shape ' ] ] an under-represented class . before declaring the prediction round finished . if hasattr ( layer , '_flattened_layers ' ) : self.constraints = { } 'get an ` input_shape ` or ' @ pytest.fixture if not proceed : output_shape=output_shape , input_masks.append ( inbound_node.output_masks [ tensor_index ] ) take as input a list of masks and return a single mask . def call ( self , inputs , mask=None ) : It should be a single tensor input_b = layers.Input ( shape= ( 3 , ) ) a = layers.Input ( shape= ( 2 , ) , dtype='int32 ' ) steps_per_epoch : Total number of steps ( batches of samples ) c = layers.Input ( shape= ( 6 , ) ) dtype = K.floatx ( ) dtype=dtype , from keras.layers import Dense , Activation # know about its input shape . Otherwise , that 's an error . The first layer passed to a Sequential model for layer in self._flattened_layers : output_shape = shape1 + shape2 if not layer_cache : assert hasattr ( self.layers [ 0 ] , 'layers ' ) layer.set_weights ( weights [ : nb_param ] ) ` sample_weight_mode ` on each output by passing a def compute_output_shape ( self , input_shape ) : `` `` '' Saves the weights of a model . model.fit ( [ rand ( 2 , 3 ) , rand ( 2 , 3 ) ] , [ rand ( 2 , 3 , 6 ) ] ) the batch size , or 1 if that can not be determined . model.weights # returns list of length 4 max_queue_size : maximum size for the generator queue break return merge_layer._inbound_nodes [ 0 ] .output_tensors [ 0 ] validation_split : Float between 0 and 1 . The output of the generator must be either ` compute_output_shape ` method of layers ) . tensor_index = tensor_indices [ i ] ` sample_weight_mode= '' temporal '' ` in ` compile ( ) ` . `` `` '' nb_param = len ( layer.weights ) seq.add ( layers.Dense ( 10 , input_shape= ( 10 , ) ) ) shape2 = list ( input_shapes [ 1 ] ) ( only if merge mode is a lambda/function ) . is not divisible by the batch size . # test modes : 'sum ' , 'mul ' , 'concat ' , 'ave ' , 'cos ' , 'dot ' . batch_size = 32 self._losses = [ ] if by_name : config : dictionary with configuration . model = Sequential ( ) # no model-level masking for now with h5py.File ( filepath , mode= ' r ' ) as f : dot_axes=-1 , output_shape=None , output_mask=None , self.built = False # Note that when using this delayed-build pattern def get_layer ( self , name=None , index=None ) : loss : String ( name of objective function ) or objective function . class_name = conf [ 'name ' ] to compute ` output_shape ` can specify them via the ` target_tensors ` argument . node_indices = [ 0 for _ in range ( len ( layers ) ) ] arguments = self.arguments def merge ( inputs , mode='sum ' , concat_axis=-1 , model.build ( ( None , 500 ) ) self.build ( ) use_multiprocessing=False , def generate_arrays_from_file ( path ) : ' Add some layers first . ' ) } ) input_layer , node_index , tensor_index = x._keras_history initial_epoch=initial_epoch , # build the model lazily on ` fit ` /etc . `` `` '' A ` Merge ` layer can be used to merge a list of tensors sample weighting ( 2D weights ) , set this to ` `` temporal '' ` . dot_axes=dot_axes , including the batch size from keras.utils.test_utils import get_test_data , keras_test # By default we connect to # and create the node connecting the current layer assert len ( layer._inbound_nodes [ -1 ] .output_tensors ) == 1 branch_1_2.add ( layers.Dense ( 16 , name='dense_1_2-0 ' ) ) print ( model.layers ) z = np.random.random ( ( 100 , 6 ) ) conf [ 'name ' ] = name node_indices = [ ] assert np.all ( model.predict ( inputs ) == output ) with tmpdir.as_cwd ( ) : def fn_output_shape ( tup ) : than one layer/tensor . validation_steps=validation_steps , List of callbacks to apply during training . epochs=1 , steps_per_epoch=1000 , epochs=10 ) def test_merge_mask_2d ( ) : metrics=None , n2 = len ( shape2 ) merged = legacy_layers.merge ( if dot_axes < 0 : ` y ` can be ` None ` ( default ) if feeding from Numpy data for these targets at training time ) , you # merge = Merge ( layers=None ) `` `` '' Retrieves the model configuration as a Python list . left.add ( Activation ( 'relu ' , name='relu_1 ' ) ) and validation metrics values ( if applicable ) . axes to use in mode ` dot ` or ` cos ` . self.weighted_metrics = self.model.weighted_metrics steps_per_epoch=None , model.add ( Dense ( 32 , input_shape= ( 500 , ) ) ) See [ optimizers ] ( /optimizers ) . masks.append ( mask_i ) Number of samples per gradient update . RuntimeError : If the model was never compiled . class_weight=class_weight ) 'config ' : self.layers [ 0 ] .get_config ( ) } ) tensor_indices = [ ] return self._trainable name = conf.get ( 'custom_name ' ) return self.model.updates raise ValueError ( ' '' concat '' mode can only merge ' # test function with output_shape function model.add ( Dense ( 10 , activation='softmax ' ) ) fname = 'test_merge_sum_temp.h5 ' elif self.mode == 'max ' : model1 = Sequential ( ) if mode in { 'cos ' , 'dot ' } : ( x_train , y_train ) , ( x_test , y_test ) = get_test_data ( num_train=train_samples , For instance , this allows you to do real-time data augmentation self.built = False 'output_mask_type ' : output_mask_type , config = [ ] model.add ( Dense ( 32 , batch_input_shape= ( None , 500 ) ) ) or ` batch_input_shape ` argument , divided by the batch size . Ignored with the default value of ` None ` . Total number of steps ( batches of samples ) if not isinstance ( self.dot_axes , ( list , tuple ) ) : raise ValueError ( 'Unknown merge mode . ' ) self._feed_input_names = self.model._feed_input_names `` `` '' Configures the model for training . initial_epoch=0 , elif callable ( self._output_shape ) : def test_merge ( ) : for layer in model.layers [ 1 : ] : `` pay more attention '' to samples from # This exists for backwards compatibility . merged_sum = legacy_layers.merge ( [ masked_a , masked_b ] , mode='sum ' ) elif hasattr ( layer , 'layers ' ) : model_layers = model.layers before declaring one epoch finished and starting the sample_weight=sample_weight , tensor_indices=tensor_indices , if self.mode == 'sum ' or self.mode == 'ave ' : merged_concat_mixed = legacy_layers.merge ( [ masked_a , input_b ] , mode='concat ' , concat_axis=1 ) arguments=arguments , shuffle=True , name=self.name + '_model ' ) from .. legacy import interfaces 0 = silent , 1 = progress bar , 2 = one line per epoch . merged = legacy_layers.merge ( [ input_a , input_b ] , self.dot_axes = dot_axes use_multiprocessing : Boolean . weighted_metrics : List of metrics to be evaluated and weighted output = K.batch_dot ( l1 , l2 , self.dot_axes ) / denominator input_shapes_set = set ( input_shapes ) model.compile ( optimizer='rmsprop ' , righter.add ( Dense ( num_hidden , input_shape= ( input_dim , ) ) ) from keras import layers config [ 'output_shape ' ] = output_shape if mode in { 'sum ' , 'mul ' , 'ave ' , 'cos ' , 'max ' } : del reduced_inputs_shapes [ i ] [ self.concat_axis ] inputs = [ np.random.random ( shape ) for shape in input_shapes ] weighted_metrics=weighted_metrics , return all_attrs num_test=test_samples , # Set model name . self.total_loss = self.model.total_loss loss='categorical_crossentropy ' , loss='categorical_crossentropy ' , masked_a = layers.Masking ( mask_value=0 ) ( input_a ) model = Sequential ( ) metrics= [ 'accuracy ' ] ) raise ImportError ( ' ` load_weights ` requires h5py . ' ) training . If instead you would like to use your own merge_config [ 'layers ' ] = layers epochs = 1 ( in case some input layers have multiple output nodes ) . 'Layer shapes : % s ' % input_shapes ) as accepted by ` test_on_batch ` . self._initial_weights = None use_multiprocessing=use_multiprocessing , callbacks=callbacks , output_dim=10 , def fn_mode ( tup ) : return model The model is not trained for a number of iterations self.layers.pop ( ) model.add ( Activation ( 'softmax ' , name='softmax ' ) ) while True : def trainable ( self , value ) : or its index in the graph . Indices are based on `` ` python A flat list of Numpy arrays dictionary mapping the output name to a Numpy array . steps : Total number of steps ( batches of samples ) if len ( layers ) > 2 : sample_weight=None , output = model.predict ( inputs ) return self.model.fit_generator ( generator , raise TypeError ( 'Unrecognized keyword arguments : ' + str ( kwargs ) ) elif mode == 'concat ' : branch_2 = models.Sequential ( name='branch_2 ' ) model 's target , which will be fed with the target data during all_attrs += getattr ( layer , attr , [ ] ) output_shape += [ 1 ] left.add ( Dense ( num_hidden , input_shape= ( input_dim , ) , name='dense_1 ' ) ) model.add ( Dense ( 32 ) ) Numpy array with the same length as the input samples def __init__ ( self , layers=None , mode='sum ' , concat_axis=-1 , 'Layer shapes : % s , % s ' % ( shape1 , shape2 ) ) if not isinstance ( layer , InputLayer ) : layer ( x ) # the 1st output stream in the input layer . self._arguments_validation ( layers , mode , if kwargs : self._feed_inputs = self.model._feed_inputs class Merge ( Layer ) : # Make a list of masks while making sure def load_weights ( self , filepath , by_name=False , skip_mismatch=False , reshape=False ) : model_concat = models.Model ( [ input_a , input_b ] , [ merged_concat_mixed ] ) from keras import backend as K inbound_node = layer._inbound_nodes [ node_index ] if not hasattr ( layer , 'batch_input_shape ' ) : sample_weight_mode=sample_weight_mode , f = f [ 'model_weights ' ] if isinstance ( self.layers [ 0 ] , legacy_layers.Merge ) : # time dimension is required for masking of : 'sum ' , 'mul ' , 'concat ' , 'ave ' , 'cos ' , 'dot ' , 'max ' . 'output_shape ' : output_shape , self._output_mask_cache = self.model._output_mask_cache model.load_weights ( fname ) to yield from ` validation_data ` generator before stopping target_tensors=None , elif self._output_shape is not None : base_layer.Node ( outbound_layer=self , initial_epoch : Epoch at which to start training `` ` python `` ` python x , y = process_line ( line ) and guarantees the single use of every input per epoch when given by ` epochs ` , but merely until the epoch def trainable_weights ( self ) : globs=globals ( ) ) model = models.Model.from_config ( config ) relies on multiprocessing , you should not pass max_queue_size=10 , workers=1 , return None model1.add ( Dense ( 32 , input_dim=32 ) ) if tensor_indices is None : for merge_input_config in first_layer_config.pop ( 'layers ' ) : return self.model.call ( inputs , mask ) output_mask = func_dump ( self._output_mask ) layers.append ( layer ) ` ( samples , sequence_length ) ` , weights = weights [ nb_param : ] dot_axes : Integer or tuple of integers , layer = layer_module.deserialize ( conf , output_shape = self._output_shape 'Sequential model must ' y=None , self.model.trainable = value self.model.callback_model = self If the input layer in the model is named , you can also pass a else : return s model_concat.compile ( loss='mse ' , optimizer='sgd ' ) nloss = model.evaluate ( x_test , y_test , verbose=0 ) first_layer = layer saving.load_weights_from_hdf5_group ( f , layers , reshape=reshape ) def non_trainable_weights ( self ) : # test function with output_mask function `` `` '' Load a model from a legacy configuration . # here the batch dimension is None , max_queue_size=max_queue_size , if mode_type == 'function ' : embedding_b = embedding ( input_b ) be equal to the number of samples of your self._per_input_updates = { } ( 1:1 mapping to input tensors ) and return a single shape tuple , return self.model.get_updates_for ( inputs ) output_mask=lambda tup : K.concatenate ( [ tup [ 0 ] , tup [ 1 ] ] ) ) Scalar training loss ( if the model has no metrics ) config = outer_model.get_config ( ) # via ` add ` , and omits the auto-generated ` InputLayer ` sample_weight=sample_weight , merged_concat = legacy_layers.merge ( [ masked_a , masked_b ] , mode='concat ' , concat_axis=1 ) `` `` '' self.loss = self.model.loss the display labels for the scalar outputs . output_shape_type = 'function ' If the argument is a tuple , # update self._inbound_nodes assert model.model.updates If the argument is callable , workers=workers , and what the model expects . if mode not in { 'sum ' , 'mul ' , 'concat ' , 'ave ' , 'cos ' , 'dot ' , 'max ' } : # concatenation layers.append ( layer ) sample_weight=sample_weight , def trainable ( self ) : validation_steps=validation_steps ) self._trainable_weights = [ ] if legacy_models.needs_legacy_support ( model ) : 'layers with matching ' # Support for legacy behavior # training and evaluation methods ) . output_mask_type = 'raw ' class_weight : Optional dictionary mapping class indices ( integers ) from .. legacy import layers as legacy_layers See [ callbacks ] ( /callbacks ) . if not callable ( mode ) : 'Use instead layers from ` keras.layers.merge ` , ' return layer self.layers = layers dtype=dtype , # Returns merged_concat = legacy_layers.merge ( [ rnn_a , rnn_b ] , mode='concat ' , concat_axis=-1 ) Note that because this implementation for i , layer in enumerate ( layers ) : elif mode_type == 'lambda ' : def state_updates ( self ) : on images on CPU in parallel to training your model on GPU . output_shape = func_load ( config [ 'output_shape ' ] , merged = legacy_layers.merge ( [ a , b ] , mode=fn_mode , output_shape=fn_output_shape , output_mask=fn_output_mask ) raise ValueError ( 'The Merge layer ' + self.name if not isinstance ( self.dot_axes [ 0 ] , int ) or not isinstance ( self.dot_axes [ 1 ] , int ) : batch_size : integer . # Historically , ` Sequential ` was once workers=1 , self.dot_axes = [ dot_axes , ] * 2 validation_data=validation_data , # Model attributes . self.outputs = [ ] # List of length 1 : the output tensor ( unique ) . length ( equal to the size of this batch ) . Different batches may ValueError : In case of mismatch between the provided input data 'For multi-output layers , ' before each epoch ) or str ( for 'batch ' ) . raise ValueError ( 'Invalid merge mode : { } '.format ( self.mode ) ) 'output shapes except for the concat axis . ' mode = self.mode.__name__ # test functional API loss = model.evaluate ( [ x_test , x_test ] , y_test , verbose=0 ) output_shape=lambda tup : ( tup [ 0 ] [ 0 ] , tup [ 0 ] [ 1 ] + tup [ 1 ] [ 1 ] ) + tup [ 0 ] [ 2 : ] , if 'class_name ' not in config [ 0 ] or config [ 0 ] [ 'class_name ' ] == 'Merge ' : else : l1 = inputs [ 0 ] first_layer = normalize_legacy_config ( first_layer ) # that comes at the bottom of the stack . intermediate.add ( Merge ( [ left , right ] , mode='sum ' ) ) self.layers.append ( layer ) in the FAQ for instructions on how to install ` h5py ` . callbacks=None , model.fit ( x_train , y_train , batch_size=batch_size , epochs=epochs , verbose=1 , validation_data= ( x_test , y_test ) ) if self.mode == 'ave ' : # test whether impromptu input_shape breaks the model left = Sequential ( ) 'concat_axis ' : self.concat_axis , config.append ( { Checks that the directory is empty afterwards . batch_shape = None from __future__ import division s += inputs [ i ] 'config ' : layer.get_config ( ) } self.name = name all_attrs = [ ] If the model has multiple outputs , you can use a different return trainable_weights + weights `` ` prefix = self.__class__.__name__.lower ( ) # Arguments x = layers.Input ( shape= ( 10 , ) ) self._output_shape = output_shape loss='categorical_crossentropy ' , 'config ' : layer.get_config ( ) } ) outer_model = models.Model.from_config ( config ) denominator = K.sqrt ( K.batch_dot ( l1 , l1 , self.dot_axes ) * self.metrics_names = self.model.metrics_names # Arguments target tensor ( in turn , Keras will not expect external as they ca n't be passed easily to children processes . This can be useful to tell the model to # as we add more layers # We were passed a regular layer , and it should for x in inputs : Note that in conjunction with ` initial_epoch ` , layers = [ ] # mirror model attributes return conf or list of scalars ( if the model computes other metrics ) . mode = globals ( ) [ config [ 'mode ' ] ] num_classes = 4 assert hasattr ( mask , '__len__ ' ) and len ( mask ) == len ( inputs ) self._inbound_nodes [ 0 ] .output_tensors = self.outputs # Legacy support model = models.Model.from_config ( config ) print ( model.trainable_weights ) sample_weight_mode : If you need to do timestep-wise self.model.set_weights ( weights ) branch_3.add ( layers.Dense ( 32 , input_shape= ( 6 , ) , name='dense_3 ' ) ) return self._layers [ 1 : ] max_queue_size=max_queue_size , nloss = model.evaluate ( [ x_test , x_test , x_test ] , y_test , verbose=0 ) return self model_from_json ( json_str ) to yield from ` generator ` before stopping . for x in inputs : if legacy_models.needs_legacy_support ( self ) : config = config.copy ( ) def predict_proba ( self , x , batch_size=32 , verbose=0 ) : # and labels , from each line in the file assert not tmpdir.listdir ( ) def fit_generator ( self , return K.concatenate ( [ x_mask , y_mask ] ) return self.model.losses self.output_names = self.model.output_names layer_config = { 'class_name ' : layer.__class__.__name__ , layers = legacy_models.legacy_sequential_layers ( self ) steps=steps ) execute the generator on the main thread . def get_weights ( self ) : model.predict_proba ( [ x_test , x_test ] , verbose=0 ) if has_arg ( self.mode , 'mask ' ) : x = ( 100 * np.random.random ( ( 100 , 2 ) ) ) .astype ( 'int32 ' ) prefix = 'sequential_ ' def from_config ( cls , config ) : return self.model.test_on_batch ( x , y , if mask_i is None : model_layers = legacy_models.legacy_sequential_layers ( model ) branch_1.add ( layers.Embedding ( input_dim=100 , # Example if output_shape [ self.concat_axis ] is None or shape [ self.concat_axis ] is None : ValueError : In case the generator yields data in an invalid format . # Mask is smaller than the input , expand it shape1 = input_shapes [ 0 ] use_multiprocessing=False ) : self.built = False model.fit ( [ x_train , x_train ] , y_train , batch_size=batch_size , epochs=epochs , verbose=0 , validation_split=0.1 ) max_queue_size=max_queue_size , self._output_layers = self.model._output_layers def get_or_create_layer ( layer_data ) : masks = [ K.expand_dims ( m , 0 ) for m in mask if m is not None ] self.built = True weighted_metrics=None , x : input data , as a Numpy array or list of Numpy arrays s /= len ( inputs ) else : target_tensors : By default , Keras will create a placeholder for the 'should contain two elements . ' ) verbose=1 , output_shape_type = config.pop ( 'output_shape_type ' , None ) callbacks : List of ` keras.callbacks.Callback ` instances . ' ` output_shape ` to Merge . ' ) A list of dicts ( each dict is a layer config ) . custom_objects=custom_objects ) # Historically , ` sequential.layers ` only returns layers that were added tensor_indices : Optional list of indices of output tensors model.fit_generator ( generate_arrays_from_file ( '/my_file.txt ' ) , self.outputs = [ ] def predict_generator ( self , generator , steps=None , ValueError : In case of invalid arguments for epochs=epochs , self.outputs = [ layer._inbound_nodes [ -1 ] .output_tensors [ 0 ] ] Has no effect when ` steps_per_epoch ` is not ` None ` . y = seq ( x ) # Create an input tensor and call ` layer ` on the input tensor . # to the input layer we just created . ' ( at least 2 ) . Got : ' + str ( inputs ) ) self._trainable = value output_shape : Shape tuple ( tuple of integers ) , or lambda/function 'use the functional API . ' ) return tuple ( output_shape ) # its underlying ` Model ` as the ` model ` property . verbose=verbose , if self.built : s1 , s2 = tup output_mask = config.get ( 'output_mask ' ) name : string , name of layer . generator : Generator yielding tuples ( inputs , targets ) batch_size : Integer or ` None ` . See [ losses ] ( /losses ) . input_masks= [ None for _ in self.inputs ] , self.optimizer = self.model.optimizer `` `` '' Generates predictions for the input samples from a data generator . # Layer parameters . max_queue_size : Integer . Maximum size for the generator queue . batches have been seen by the model . return self.legacy_get_config ( ) steps=steps ) # Must have multiple input shape tuples . b = layers.Input ( shape= ( 8 , ) ) `` `` '' Sets the weights of the model . class_weight : dictionary mapping classes to a weight value , y = np.random.random ( ( 100 , 8 ) ) if self.mode in [ 'sum ' , 'mul ' , 'ave ' , 'max ' ] : outer_model.compile ( optimizer='rmsprop ' , tensor_indices.append ( tensor_index ) def test_merge_concat ( in_tmpdir ) : output_shape [ self.concat_axis ] = None return input_shapes [ 0 ] output_shape : Either a shape tuple ( tuple of integers ) , 'and will be removed after 08/2017 . ' # The layer does n't know about its expected shape . # Note that you can also omit the ` input_shape ` argument : input_length=2 , workers : Integer . Maximum number of processes to spin up if not node_indices : if hasattr ( first_layer , 'batch_input_shape ' ) : proceed = ask_to_proceed_with_overwrite ( filepath ) shape_set = set ( ) batch_shape = layer.batch_input_shape batch size ( same convention as the 'class_name ' : layer.__class__.__name__ , model = models.Model ( [ input_a , input_b ] , [ merged_concat ] ) import numpy as np def get_losses_for ( self , inputs ) : ' e.g . ` add ` , ` concatenate ` , etc . ' , stacklevel=2 ) def test_on_batch ( self , x , y , # afterwards , Keras does automatic shape inference x=None , yield ( x , y ) if not isinstance ( inputs , list ) or len ( inputs ) < = 1 : self._outbound_nodes = [ ] shuffle=shuffle , def _get_test_data ( ) : for line in f : model.set_weights ( weights ) left.add ( Activation ( 'relu ' ) ) else : self.sample_weight_mode = self.model.sample_weight_mode model.fit ( x_train , y_train , batch_size=batch_size , epochs=epochs , verbose=1 , shuffle=False ) validation_split=validation_split , epochs , verbose=verbose , self.inputs = [ ] # List of input tensors def predict_on_batch ( self , x ) : name = prefix + ' _ ' + str ( K.get_uid ( prefix ) ) return self._gather_list_attr ( 'trainable_weights ' ) rnn_b = rnn ( embedding_b ) name = layer_data [ 'config ' ] .get ( 'name ' ) # call compile method of Model class # the model does n't have any weights until the first call The generator should return the same kind of data print ( mode ) set_inputs = True return self.model.predict_on_batch ( x ) input_layers.append ( input_layer ) 'has been renamed ` epochs ` . ' ) The generator is expected to loop over its data def needs_legacy_support ( model ) : mode_type = 'raw ' elif self.mode in [ 'dot ' , 'cos ' ] : # first layer must have a defined input shape `` `` '' Computes the loss on some input data , batch by batch . return output # Raises rnn = layers.SimpleRNN ( 3 , return_sequences=True ) np.random.seed ( 1234 ) # Create an input layer . # If file exists and should not be overwritten : is commonly smaller than the others , if the size of the dataset dtype = first_layer.dtype layer_cache [ name ] = layer def predict ( self , x , batch_size=None , verbose=0 , steps=None ) : verbose=verbose ) return isinstance ( model.layers [ 0 ] , Merge ) The generator should return the same kind of data as accepted by self.outputs = None return self.model.evaluate_generator ( generator , return K.concatenate ( inputs , axis=self.concat_axis ) initial_epoch : Integer . else : warnings.warn ( ' ` Sequential.model ` is deprecated . ' num_hidden = 8 model.predict_classes ( [ x_test , x_test ] , verbose=0 ) 'should be a list . ' ) return self.model.predict ( x , batch_size=batch_size , verbose=verbose , from keras.legacy.layers import Merge non picklable arguments to the generator epochs = kwargs.pop ( 'nb_epoch ' ) for layer in self.layers [ 1 : ] : merge_input = layer_module.deserialize ( merge_input_config ) left = Sequential ( name='branch_1 ' ) Epoch at which to start training To specify different metrics for different outputs of a these arguments are passed into ` tf.Session.run ` . config.append ( { 'class_name ' : 'Merge ' , 'config ' : merge_config } ) # This builds the model for the first time : shuffle=shuffle , def legacy_sequential_layers ( model ) : self._network_nodes = self.model._network_nodes This tuple ( a single output of the generator ) makes a single config [ 'mode ' ] = mode model.add ( Dense ( 32 , batch_input_shape= ( None , 500 ) ) ) `` `` '' Generates output predictions for the input samples . raise ValueError ( 'Invalid merge mode : ' + str ( mode ) ) if not self.built : `` `` '' Runs a function in a temporary directory . ` optimizer ` , ` loss ` , ` metrics ` or ` sample_weight_mode ` . assert expected_output_shape == actual_output_shape assert expected_output_shape == actual_output_shape preds = self.predict ( x , batch_size , verbose , steps=steps ) Returns a layer based on either its name ( unique ) concatenated = K.concatenate ( masks , axis=self.concat_axis ) def uses_learning_phase ( self ) : return super ( Merge , cls ) .from_config ( config ) # This is identical to the following : `` `` '' Fits the model on data generated batch-by-batch by a Python generator . # When using the delayed-build pattern ( no input shape specified ) , you can shape2 = input_shapes [ 1 ] return self.model.fit ( x , y , shuffle : Boolean ( whether to shuffle the training data `` `` '' Retrieves the weights of the model . def get_config ( self ) : layers.append ( layer_config ) @ trainable.setter for layer in merge.layers : @ interfaces.legacy_generator_methods_support def set_weights ( self , weights ) : by sample_weight or class_weight during training and testing . node_indices , tensor_indices ) * * kwargs : When using the Theano/CNTK backends , these arguments steps_per_epoch=steps_per_epoch , validation_steps=None , 'just use your ` Sequential ` instance directly . ' ) return self.model.get_layer ( name , index ) return layer_cache [ name ] validation_data=None , 'and will be removed after 08/2017 . ' name = prefix + str ( K.get_uid ( prefix ) ) to validate before stopping . # Case : the layer has multiple output tensors merged_tensor = merge ( [ tensor_a , tensor_b ] , mode='concat ' , concat_axis=1 ) self._init_graph_network ( self.inputs , output_mask_type = config.pop ( 'output_mask_type ' , None ) # the model we will return return self._output_mask ( mask ) dot_axes=-1 , output_shape=None , output_mask=None , expected_output_shape = model.compute_output_shape ( input_shapes ) if model.layers : self.inputs = network.get_source_inputs ( self.outputs [ 0 ] ) from keras.utils import np_utils layers.append ( model.layers [ 0 ] ) return K.all ( concatenated , axis=-1 , keepdims=False ) # create the underlying model conf = normalize_legacy_config ( conf ) If the latter case , it should take as input a list of shape tuples loss = model.evaluate ( [ x_test , x_test , x_test ] , y_test , verbose=0 ) validation_data=validation_data , if model.layers [ 0 ] not in layers : the loss and any model metrics at the end of each epoch . if first_layer [ 'class_name ' ] == 'Merge ' : nloss = model.evaluate ( [ x_test , x_test ] , y_test , verbose=0 ) model.predict_proba ( [ x_test , x_test , x_test ] , verbose=0 ) fname = 'test_merge_overlap_temp.h5 ' layer_cache : cache to draw pre-existing layer . expected_output_shape = model.compute_output_shape ( input_shapes ) actual_output_shape = model.predict ( inputs ) .shape def test_merge_mask_3d ( ) : ' e.g . ` add ` , ` concatenate ` , etc . ' , stacklevel=2 ) if sublayer not in layers : at the end of every epoch . It should typically for sublayer in layer.layers : return weights self.stateful = False self._output_mask = output_mask input_shapes = [ ( 4 , 3 , 2 ) , ( 4 , 3 , 2 ) ] input_shapes= [ x._keras_shape for x in self.inputs ] , If unspecified , ` max_queue_size ` will default to 10 . # test with arguments 'because no ` output_shape ` ' A ` History ` object . layers.append ( sublayer ) [ input_a , input_b ] , The loss value that will be minimized by the model # masks # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) ( 1:1 mapping to input tensors ) self._updates = [ ] on each output by passing a dictionary or a list of losses . for mode in [ 'sum ' , 'mul ' , 'concat ' , 'ave ' , 'max ' ] : y : labels , as a Numpy array . if len ( output_shape ) == 1 : validation_data : tuple ` ( x_val , y_val ) ` or tuple f.flush ( ) if layer not in layers : right.add ( Activation ( 'relu ' , name='relu_2 ' ) ) self.loss_weights = self.model.loss_weights layers = [ ] x : Numpy array of training data . # We will have to node_indices.append ( node_index ) train_samples = 100 weights.append ( layer.get_weights ( ) ) input_shape= ( input_dim , ) , model.weights # returns [ ] assert isinstance ( input_shape , list ) during training and testing . right.add ( Dense ( num_hidden , input_shape= ( input_dim , ) , name='dense_2 ' ) ) if not name : raise ValueError ( mode + ' merge takes exactly 2 layers ' ) model.add ( Merge ( [ intermediate , righter ] , mode='sum ' ) ) output_shape = func_dump ( self._output_shape ) merged = legacy_layers.Merge ( mode=mode ) ( [ input_a , input_b ] ) class_weight=class_weight , 'be merged using ' + mode + ' mode . ' x = Input ( from .. legacy import models as legacy_models # This will build the current layer y : Numpy array of target ( label ) data . from . import base_layer verbose : verbosity mode , 0 or 1 . model = models.Model ( [ input_a , input_b ] , merged ) batch_size=batch_size , num_classes=num_classes ) branch_1 = models.Sequential ( name='branch_1 ' ) Optional for ` Sequence ` : if unspecified , will use ` predict_on_batch ` . should have a defined input shape . What that self.model.trainable = self.trainable # test lambda with output_mask lambda * * kwargs ) : Fraction of the training data to be used as validation data . `` `` '' Validates user-passed arguments and raises exceptions validation_steps : Only relevant if ` steps_per_epoch ` model.add ( Dense ( 32 , input_dim=500 ) ) if isinstance ( self._output_mask , python_types.LambdaType ) : shape_set.add ( tuple ( reduced_inputs_shapes [ i ] ) ) # the dimensionality of each mask return merge_layer ( inputs ) # output = merge ( [ input_tensor_1 , input_tensor_2 ] ) layers = [ ] # All tuples in input_shapes should be the same . assert np.all ( K.function ( mask_input_placeholders , [ mask_output ] ) ( mask_inputs ) [ 0 ] == expected_mask_output ) self.input_names = self.model.input_names name=layer.name + '_input ' ) output_shape = list ( input_shapes [ 0 ] ) embedding_a = embedding ( input_a ) a tuple ` ( inputs , targets , sample_weights ) ` . validation_split=0. , merge = legacy_layers.Merge.from_config ( first_layer_config ) index : integer , index of layer . class_weight=class_weight , output_mask = self._output_mask.__name__ seq = models.Sequential ( ) mode = func_dump ( self.mode ) import os steps_per_epoch , if layer not in layers : self.inputs = network.get_source_inputs ( self.outputs [ 0 ] ) layers = self.layers model.add ( Merge ( [ left , right ] , mode='concat ' , name='merge ' ) ) Scalar test loss ( if the model has no metrics ) data provided . self.build ( ) def test_merge_recursivity ( in_tmpdir ) : elif callable ( self.mode ) : model.fit ( [ x_train , x_train ] , y_train , batch_size=batch_size , epochs=epochs , verbose=0 ) # ` build ( batch_input_shape ) ` : it should be expected output shape , * not * including the batch size if isinstance ( layer_output_shape , list ) : def legacy_get_config ( self ) : self.targets = self.model.targets model.fit ( [ x_train , x_train , x_train ] , y_train , batch_size=batch_size , epochs=epochs , verbose=0 , shuffle=False ) or in the case of temporal data , `` `` '' Evaluates the model over a single batch of samples . return self._output_mask return ( x_train , y_train ) , ( x_test , y_test ) raise ValueError ( 'The first layer in a ' `` ` from .layers import Merge self._output_coordinates = self.model._output_coordinates layers : Can be a list of Keras tensors or elif self.mode == 'mul ' : if len ( layer._inbound_nodes [ -1 ] .output_tensors ) ! = 1 : output_shape [ self.concat_axis ] += shape [ self.concat_axis ] if not hasattr ( x , '_keras_history ' ) : model.get_config ( ) if not self.trainable : def _flattened_layers ( self ) : output_mask = func_load ( config [ 'output_mask ' ] , def test_merge_dot ( ) : inbound_layers= [ ] , assert ( loss == nloss ) warnings.warn ( 'The ` Merge ` layer is deprecated ' model.predict_proba ( x_test , verbose=0 ) self.built = True and return a single shape tuple , including the model = Sequential ( name='merged_branches ' ) json_str = model.to_json ( ) weights = [ ] Note that in conjunction with initial_epoch , the parameter `` `` '' Retrieve a layer that is part of the model . ` x ` can be ` None ` ( default ) if feeding from concat_axis , dot_axes , sample_weight : Optional Numpy array of weights for if isinstance ( self.mode , python_types.LambdaType ) : self.inputs = [ x ] def predict_proba ( self , x , batch_size=None , verbose=0 , steps=None ) : self._layers.pop ( ) mode = func_load ( config [ 'mode ' ] , globs=globals ( ) ) rand = lambda * shape : np.asarray ( np.random.random ( shape ) > 0.5 , dtype='int32 ' ) elif K.ndim ( mask_i ) < K.ndim ( input_i ) : if __name__ == '__main__ ' : for conf in config [ 1 : ] : # to the input layer we just created . def evaluate_generator ( self , generator , steps=None , output_mask_type = 'function ' if self.layers [ 0 ] not in layers : self._trainable = True self.metrics = self.model.metrics name=self.name ) warnings.warn ( 'The ` merge ` function is deprecated ' weights = model.get_weights ( ) and return a single tensor . the output node index for each input layer for shape in input_shapes [ 1 : ] : validation_steps : Only relevant if ` validation_data ` you can pass a 2D array with shape input_a = layers.Input ( shape=input_shapes [ 0 ] [ 1 : ] ) output_shape = config.get ( 'output_shape ' ) means is that it should have received an ` input_shape ` mode = self.mode self._inbound_nodes = [ ] steps=steps ) x = layer ( x ) intermediate.add ( Dense ( num_hidden ) ) def test_merge_sum ( in_tmpdir ) : yaml_str = model.to_yaml ( ) metrics=metrics , In this case you should make sure to specify ( x_train , y_train ) , ( x_test , y_test ) = _get_test_data ( ) return { 'class_name ' : class_name , x_mask , y_mask = tup first_layer = config [ 0 ] branch_1_2.add ( layers.Dense ( 16 , input_shape= ( 16 , ) , name='dense_1_2-1 ' ) ) if callable ( self._output_shape ) : weights = self._gather_list_attr ( 'non_trainable_weights ' ) self.metrics_tensors = self.model.metrics_tensors layer = layer_module.deserialize ( layer_data ) generator : A generator or an instance of ` Sequence ` right = Sequential ( ) layer_cache = { } epoch epochs is reached . self._output_shape_cache = self.model._output_shape_cache will not train on it , and will evaluate merged_model.add ( Merge ( [ model1 , model2 ] , mode='concat ' , concat_axis=1 ) ) model.predict_classes ( x_test , verbose=0 ) branch_1.add ( layers.LSTM ( 32 , name='lstm_1 ' ) ) merge = self.layers [ 0 ] 'layers with at least 2 elements . Found : ' ( same convention as the ` input_shape ` argument in layers ) . return K.all ( K.concatenate ( masks , axis=0 ) , axis=0 , keepdims=False ) for layer in self.layers [ 0 ] .layers : to apply a different weight to every timestep of every sample . if isinstance ( dot_axes , int ) : x , y = tup elif output_mask_type == 'lambda ' : config = model.get_config ( ) batch_shape = tuple ( input_shape ) # Input is unmasked . Append all 1s to masks , from keras.models import Sequential reduced_inputs_shapes = [ list ( shape ) for shape in input_shapes ] model.predict ( [ x_test , x_test ] , verbose=0 ) ' ( or callable ) ' K.batch_dot ( l2 , l2 , self.dot_axes ) ) 'dot_axes ' : self.dot_axes , output_shape=lambda tup : tup [ 0 ] [ : -1 ] + ( tup [ 0 ] [ -1 ] + tup [ 1 ] [ -1 ] , ) ) if len ( input_shapes_set ) > 1 : raise TypeError ( 'Invalid type for dot_axes - ' ' has a callable ` mode ` argument , ' verbose=1 , # three different types of merging 'output_mask ' : output_mask , if shape1 [ self.dot_axes [ 0 ] ] ! = shape2 [ self.dot_axes [ 1 ] ] : model_sum.compile ( loss='mse ' , optimizer='sgd ' ) model.add ( Activation ( 'softmax ' ) ) Sequential.from_config ( config ) tensor_b = Input ( shape= ( 32 , ) ) set_inputs = False from . import saving trainable_weights = self._gather_list_attr ( 'trainable_weights ' ) # Support for legacy models ( during training only ) .","['keras/engine/saving.py', 'keras/engine/sequential.py', 'keras/legacy/layers.py', 'keras/legacy/models.py', 'tests/keras/legacy/layers_test.py', 'tests/keras/legacy/models_test.py', 'tests/keras/test_sequential_model.py', 'tests/test_model_saving.py']",[ RELNOTES ] Simplify implementation of Sequential and remove legacy Merge support ( # 10077 )
466,3b440235e237ef59ec5763c413e7f4292dab5d79,2018-04-26 18:10:09-07:00,"self._feed_input_names = [ ] self._feed_input_shapes.append ( shape ) ( layer._inbound_nodes and y = [ y ] if not hasattr ( x , '_keras_history ' ) : self.optimizer = None # Collect losses that are dependent on inputs elif ( not hasattr ( loss_fn , '__name__ ' ) or for x in output_tensors ] ' ` tf.layers.Input ` ' self._layers_by_depth = layers_by_depth 'training/testing . ' # hence the same layer might appear twice ) # here we order them by traversal order . node_key = self._node_key ( layer , node_index ) # If ` x ` and ` y ` were all symbolic , exception_prefix='target ' ) # Build self._input_layers : if depth not in nodes_by_depth : assert tensor_index == 0 # All layers in order of horizontal graph traversal . self._inbound_nodes = [ ] self._is_graph_network = False self._expects_training_arg = False # retrieve it from there instead of recomputing it . # and outputs , nor their shapes and names . if all_names.count ( name ) ! = 1 : cache_key = object_list_uid ( inputs ) `` `` '' ' Always start with this line . ' ) str ( x.name ) ) # Check that all arrays have the same length . computable_tensors.append ( x ) self._feed_output_names ) # Check that x is a Keras tensor . inputs = list ( inputs ) with pytest.raises ( TypeError ) : ' ; y= ' + str ( y ) ) fields = [ name + ' ( ' + cls_name + ' ) ' , node_index = self._output_coordinates [ i ] [ 1 ] if not isinstance ( y , np.ndarray ) and not K.is_tensor ( y ) : target_tensors = None self.output_layers.append ( layer ) try : def build_map_of_graph ( tensor , finished_nodes , nodes_in_progress , layers_by_depth [ depth ] = [ ] batch_size=None ) : self._layers = [ ] feed_output_shapes = [ ] ValueError : In case the network is not valid ( e.g . disconnected graph ) . # If ` loss_fn ` is not a function ( e.g . callable class ) name = 'input_ % d ' % ( i + 1 ) # self.losses # thus it is already built . # Collect updates that are dependent on inputs 'as model inputs . ' ) C.ops.functions.Function ) ) tensor_index = self._output_coordinates [ i ] [ 2 ] return self._layers tensor_index : Tensor_index from which ` tensor ` comes from . y = standardize_input_data ( y , layer._inbound_nodes [ 0 ] .inbound_layers ) ) : for node in nodes_by_depth [ depth ] : nodes_by_depth [ depth ] = [ ] # Private attributes to implement compatibility with Layer . # Collect unconditional losses . nodes_by_depth : dict mapping ints ( depth ) to lists of node instances . sequential_like = True layer.count_params ( ) , # The `` depth '' of a node is the max of the depths super ( Network , self ) .__setattr__ ( name , value ) feed_output_names , feed_sample_weight_modes ) self.trainable = True with K.name_scope ( 'activity_regularizer ' ) : to lists of layer instances . 'Note that input tensors are ' # TODO : support it in subclassd networks after inputs are set . layer_indices = { } # dict { layer : index in traversal } y , self._feed_loss_fns , feed_output_shapes ) def _standardize_user_data ( self , x , cls_name = self.__class__.__name__ raise ValueError ( 'Model inputs are already set . ' ) tensor_indices= [ ] , input_tensors=self.inputs , depth = max ( depth , previous_depth ) name = model._input_layers [ i ] .name return hasattr ( x , '_keras_history ' ) def layers ( self ) : nodes_depths [ node ] = depth # about it . print ( [ layer.name for layer in recreated_model.output_layers ] ) `` `` '' Builds a map of the graph of layers . check_batch_axis=False , # Do n't enforce the batch size . if not layer or node_index is None or tensor_index is None : ' must be ' layers_for_depth = layers_by_depth [ depth ] 'Total params : { : , } '.format ( trainable_count + non_trainable_count ) ) # Update network_nodes . str ( inbound_tensor_index ) + ' ] ' ) if isinstance ( value , ( Layer , Network ) ) : # Ensure name unicity , which will be crucial for serialization 'Here , a tensor specified as ' raise NotImplementedError print ( 'final_model layers : ' , [ layer.name for layer in final_model.layers ] ) self._internal_output_shapes = [ x._keras_shape for x in self.outputs ] # Network_nodes : set of nodes included in the graph if node_key in self._network_nodes : # Standardize the inputs . regularization_losses = [ # This is for performance optimization raise ValueError ( 'Output tensors to a ' + cls_name except AttributeError : for i in range ( len ( self.input_layers ) ) : self.input_names.append ( name ) 'You passed : x= ' + str ( x ) print ( [ layer.name for layer in recreated_model.layers ] ) inbound_layer = node.inbound_layers [ i ] if depth not in layers_by_depth : 'Use ` model.compile ( optimizer , loss ) ` . ' ) str ( len ( self.input_layers ) ) + ' tensor inputs . ' ) # Model is not compilable because layer = self._output_layers [ i ] exception_prefix='input ' ) if not hasattr ( self , 'optimizer ' ) : self._feed_loss_fns , C.ops.functions.Function ) ) : # Subclassed networks are not serializable warnings.warn ( cls_name + ' inputs must come from ' updates += layer.updates # A Network does not create weights of its own , layers_with_complete_input = [ ] # To provide a better error msg . # ( since serialized nodes refer to layers by their name ) . regularization_losses = [ layers_depths = { } # dict { layer : depth value } self.output_names = [ # Do n't reset optimizer if already set . 'array or a list of arrays . ' # Create the node linking internal inputs to internal outputs . layer_indices [ layer ] = len ( layer_indices ) self._init_graph_network ( * args , * * kwargs ) # Sample weighting not supported in this case . for x in node.input_tensors : # The model owns this layer node . if layer in self.input_layers : if depth not in layers_by_depth : for layer in layers_for_depth : # ` class_weight ` arguments . # Do n't repeat work for shared subgraphs # Arguments : # We fix the placeholder shape except the batch size . self._feed_input_shapes ) ' a previous non-Input layer . ' # Entries are unique . Includes input and output layers . # ( since serialized nodes refer to layers by their name ) . layer , layers = [ ] nodes_depths = { } # dict { node : depth value } check_batch_axis=False ) losses += layer.get_losses_for ( inputs ) # Build self.input_layers : # Collect updates that are dependent on inputs outputs = self.call ( self.inputs [ 0 ] , training=training ) 'Connected to ' ] from .. utils.generic_utils import object_list_uid # Raises loss = loss or { } feed_output_shapes.append ( None ) ' ( missing Keras metadata ) . ' ) feed_input_names = self._feed_input_names raise ValueError ( 'Unexpectedly found an instance of type ` ' + str ( type ( x ) ) + ' ` . ' finished_nodes.add ( node ) self.output_layers = self.model.output_layers # when calling the Network on new inputs . if data tensors : the model is built on top of these tensors . cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) if self.inputs : for i , v in enumerate ( inputs ) : print ( [ layer.name for layer in recreated_model.input_layers ] ) # User-provided arguments validation . placeholder = K.placeholder ( shape=shape , name=name ) nodes_by_depth = { } # Assumed tensor - TODO ( fchollet ) additional type check ? str ( all_names.count ( name ) ) `` `` '' Builds a map of the graph of layers . output_shape = K.int_shape ( self.outputs [ i ] ) # every time the Network is called on a set on input tensors , output_shapes , check_array_lengths=True , batch_size=None ) : print ( 'output_layers : ' , [ l.name for l in model.output_layers ] ) if layer not in layer_indices : # we have . The user should call ` model._set_inputs ( placeholders ) ` cache_key += ' _ ' + object_list_uid ( masks ) layers_depths [ node.outbound_layer ] = depth regularization_losses = [ def __setattr__ ( self , name , value ) : K.is_tensor ( v ) for v in y ) : if value not in self._layers : if not hasattr ( x , '_keras_history ' ) : feed_output_shapes , if not target_tensors : 'inputs ' in kwargs and 'outputs ' in kwargs ) : for node , depth in nodes_depths.items ( ) : for node_index , node in enumerate ( layer._inbound_nodes ) : # This is necessary for shared layers that have inputs at different network_nodes.add ( node_key ) elif not model._is_graph_network : output_shape , layer.count_params ( ) ] # This is necessary for shared layers that have inputs at different inbound_layer = node.inbound_layers [ i ] # Arguments # Check that x has appropriate ` _keras_history ` metadata . mask_cache_key = ' , '.join ( [ str ( id ( x ) ) for x in self.inputs ] ) # we should use that depth instead of the node depth . for layer in getattr ( self , '_input_layers ' , [ ] ) : recursion stack . Useful to detect cycles . for layer in self._output_layers : output_shape = self._internal_output_shapes [ i ] # List of initial layers ( 1 to 1 mapping with self.inputs , if isinstance ( inputs , ( list , tuple ) ) : self._set_inputs ( x ) 'training/testing . ' self._feed_input_shapes = [ ] # We need to use ` y ` to set the model targets . finished_nodes = set ( ) # Get sorted list of node depths . print ( 'model.input_layers_tensor_indices : ' , model.input_layers_tensor_indices ) else : mask_cache_key += ' _ ' + object_list_uid ( masks ) 'they can not be the output of ' for depth in depth_keys : if isinstance ( x , ( list , tuple ) ) : for i in range ( len ( self._output_layers ) ) : # ( unless serialization is implemented by self._output_layers = self.model._output_layers ' [ ' + str ( inbound_node_index ) + ' ] [ ' # self.updates def is_tensor ( x ) : class_weight=None , else : warnings.warn ( 'The list of outputs passed to the model ' sample_weights = [ standardize_weights ( ref , sw , cw , mode ) if not all ( K.is_tensor ( v ) for v in all_inputs ) : ValueError : if a cycle is detected . v = np.expand_dims ( v , 1 ) if not self._is_graph_network : ' times in the model . ' ' Found : ' + str ( self.outputs ) ) outputs = [ outputs ] self._feed_inputs.append ( layer.input ) zip ( y , sample_weights , class_weights , self._output_layers.append ( layer ) nodes_in_progress , hasattr ( self , 'compute_mask ' ) ) # Collect unconditional updates . all_inputs += list ( y ) 'You passed : y= ' + str ( y ) ) for i in range ( len ( node.inbound_layers ) ) : else : # Collect losses that are dependent on inputs # or by creating a placeholder if Numpy data was provided ) . inputs = inputs [ 0 ] target_tensors=target_tensors ) self.inputs = None # What follows is input validation and standardization to list format , loss=self.loss , updates 'All layer names should be unique . ' layers.extend ( layers_for_depth ) 'it was generated by layer ' return self.add_loss ( regularizer ( weight ) ) # computable_tensors : all tensors in the graph self.input_layers.append ( layer ) feed_output_shapes.append ( output_shape ) # Automatically track layers set as Model weights ( list of variables ) ' times in the model . ' feed_input_shapes , previous_depth = layers_depths.get ( node.outbound_layer , 0 ) 'The following previous layers ' self.input_layers_tensor_indices = [ ] layer , node_index , tensor_index = x._keras_history class_weights = standardize_class_weights ( elif isinstance ( x , dict ) : # Arguments # or lists of arrays . self._input_layers = [ ] warnings.warn ( cls_name + ' inputs must come from ' self._initial_weights = None finished_nodes.add ( node ) # Ensure name unicity , which will be crucial for serialization 'Found : ' + str ( x ) ) self._output_coordinates = self.model._output_coordinates tf.SparseTensor ) ) : else : layers_depths [ node.outbound_layer ] = depth print ( 'output_shape : ' , model.compute_output_shape ( [ ( None , 32 ) , ( None , 32 ) ] ) ) layer = self._input_layers [ i ] 'input to `` ' + self.name # It 's supposed to be an input layer , so only one node node_index = self.output_layers_node_indices [ i ] getattr ( losses , loss_fn.__name__ , None ) is None ) : layers_by_depth = { } # it does not know its number of inputs layers_by_depth [ depth ] .append ( layer ) node_index = node.node_indices [ i ] self.loss = loss or [ ] output_layers depth_keys = list ( nodes_by_depth.keys ( ) ) # Build the model using the retrieved inputs ( value or symbolic ) . `` `` '' Validates a network 's topology and gather its layers and nodes . for x in self.inputs : ] first_connection ] if node in nodes_in_progress : node_index = self._output_coordinates [ i ] [ 1 ] K.is_tensor ( v ) for v in x ) : self._feed_input_shapes , # Propagate to all previous tensors connected to this node . target_tensors = [ v for v in y if K.is_tensor ( v ) ] self._nodes_by_depth = nodes_by_depth if not all ( isinstance ( v , np.ndarray ) or for output_shape , loss_fn in zip ( self._feed_output_shapes , input_layers # time the model gets called on training data . self._feed_inputs.append ( layer.input ) # Note : we ca n't test whether the model input_shapes= [ x._keras_shape for x in self.inputs ] , tf_variables.Variable , finished_nodes : Set of nodes whose subgraphs have been traversed if v.ndim == 1 : output_masks= [ None for _ in self.outputs ] , self._layers.append ( value ) # No network-level masking for now . layer.name + '.\n ' inbound_layers= [ ] , 'as model targets . ' ) if include_optimizer and model.optimizer : # Signature detection # Graph network # Check that x is an input tensor . mask_cache_key = object_list_uid ( inputs ) # If the depth is not set , the node has no outbound nodes ( depth 0 ) . # Set self.layers and self._layers_by_depth . # Check that x is an input tensor . def __init__ ( self , inputs , outputs , name=None ) : layer = node.inbound_layers [ i ] 'array or a list of arrays . ' layers_by_depth = { } # Must be implemented by subclasses . 'The tensor ' + str ( tensor ) + ' at layer `` ' outputs : List of outputs tensors . self._input_layers = self.model._input_layers for x in inputs : inputs = node.input_tensors # TODO : consider supporting it . all_inputs.append ( x ) feed_output_shapes.append ( output_shape [ : -1 ] + ( 1 , ) ) outputs = self.call ( self.inputs [ 0 ] ) node_indices= [ ] , self._feed_output_names , nodes_in_progress.remove ( node ) sample_weights , for name in all_names : # depth levels in the graph . layers_with_complete_input.append ( layer.name ) `` `` '' Set model 's input and output specs based on the input data received . to_json output_shapes.append ( None ) self.weighted_metrics = weighted_metrics nodes : list of Node instances . cls_name = self.__class__.__name__ raise TypeError ( 'Output tensors to a ' + cls_name + ' must be ' # to match the value shapes . node = layer._inbound_nodes [ node_index ] nodes_in_progress.add ( node ) # then cache them here . When one of these output is queried later , # all layers in order of horizontal graph traversal . for i in range ( len ( self.output_layers ) ) : This recursively updates the map ` layer_indices ` , build_map_of_graph ( x , finished_nodes , nodes_in_progress , 'must come from ` tf.layers.Input ` . ' # User-provided argument validation . feed_input_names , 'Param # ' , fields = [ name + ' ( ' + cls_name + ' ) ' , output_shape , layer.count_params ( ) ] if isinstance ( inputs , list ) : regularization_losses = [ # This is for performance optimization when calling the Network on new inputs : List of input tensors . raise RuntimeError ( 'You must compile a model before ' 'The following previous layers ' raise ValueError ( 'Please provide as model targets ' raise ValueError ( 'The tensor ' + str ( tensor ) + ' at layer `` ' updates += layer.get_updates_for ( inputs ) computable_tensors.append ( x ) print ( 'recreated : ' ) if not isinstance ( y , ( list , tuple ) ) : self.output_layers_node_indices = self.model.output_layers_node_indices cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) for ( ref , sw , cw , mode ) in print ( 'input_layers : ' , [ l.name for l in model.input_layers ] ) if not self._is_graph_network : add_loss def build_map ( tensor , # Do n't repeat work for shared subgraphs # Get sorted list of node depths . # Build a dict { depth : list of layers with this depth } if self.__class__.__name__ == 'Sequential ' : self._feed_input_shapes.append ( K.int_shape ( v ) ) for node_index , node in enumerate ( layer._inbound_nodes ) : if node in nodes_in_progress : assert [ l.name for l in recreated_model._output_layers ] == [ 'dense_2 ' , 'dense_3 ' ] node_key = self._node_key ( layer , node_index ) self._output_coordinates = [ ] depth_keys = list ( nodes_by_depth.keys ( ) ) layers_by_depth : dict mapping ints ( depth ) feed_input_shapes = self._feed_input_shapes # computable_tensors : all tensors in the graph tensor : Some tensor in a graph . return if isinstance ( y , ( list , tuple ) ) : nodes_by_depth [ depth ] .append ( node ) if placeholders : the model is built on top of these placeholders , layer=None , node_index=None , tensor_index=None ) : shape = ( None , ) + v.shape [ 1 : ] self._layers = [ ] # Stack of layers . # or if it not in the ` losses ` module , then layer : Layer from which ` tensor ` comes from . If not provided , sample_weight , feed_output_names ) network_nodes = set ( ) # ids of all nodes relevant to the Network # self.non_trainable_weights # Get sorted list of layer depths . print ( model.compute_mask ( [ e , f ] , [ None , None ] ) ) if depth not in nodes_by_depth : self.weighted_metrics = weighted_metrics layer , node_index , tensor_index = x._keras_history for x in output_tensors ] self.output_layers_node_indices = [ ] get_updates_for self._compute_previous_mask = ( updates += layer.get_updates_for ( inputs ) finished_nodes : Set of nodes whose subgraphs depth = nodes_depths.setdefault ( node , 0 ) previous_depth = layers_depths.get ( node.outbound_layer , 0 ) str ( layers_with_complete_input ) ) # or if it not in the ` losses ` module , then if y is not None : str ( layers_with_complete_input ) ) # of the static batch size . updates += layer.get_updates_for ( None ) if self._is_graph_network : # and in general ` x [ 0 ] .shape [ 0 ] ! = self._feed_input_shapes [ 0 ] [ 0 ] ` self.inputs.append ( placeholder ) will be obtained from ` tensor._keras_history ` . all_inputs.append ( y ) # about it . self.output_layers_node_indices.append ( node_index ) def _map_graph_network ( inputs , outputs ) : tensor_index=tensor_index ) self.output_layers_tensor_indices = self.model.output_layers_tensor_indices 'Expected a symbolic tensor instance . ' ) nodes_depths [ inbound_node ] = max ( depth + 1 , previous_depth ) ' ( missing previous layer metadata ) . ' ) layers.append ( layer ) and we expect Numpy data to be fed for them when calling ` fit ` /etc . elif isinstance ( y , dict ) : if isinstance ( y , ( list , tuple ) ) : 'The tensor that caused the issue was : ' for i in range ( len ( self._input_layers ) ) : computable_tensors = [ ] x = standardize_input_data ( # Network-specific properties . 'it was generated by layer ' for layer in self.output_layers : self._outbound_nodes = [ ] if isinstance ( v , ( np.ndarray ) ) : all_inputs += list ( x ) for x in node.output_tensors : all_names = [ layer.name for layer in self.layers ] self._feed_input_names.append ( name ) outputs : Optional output tensors ( if already computed by running the model ) . self._base_init ( name=name ) node_index : Node index from which ` tensor ` comes from . self._internal_input_shapes = [ x._keras_shape for x in self.inputs ] output_masks= [ None for _ in self.outputs ] , str ( x.name ) ) # and one tensor output . node = layer._inbound_nodes [ node_index ] This is to be used for Model subclasses , which do not know at instantiation # Obtain symbolic outputs by calling the model . 'Use ` model.compile ( optimizer , loss ) ` . ' ) ' '' was not an Input tensor , ' # self.trainable_weights self._feed_sample_weight_modes ) ] with K.name_scope ( 'weight_regularizer ' ) : self._input_coordinates = [ ] nodes_depths = { } # dict { node : depth value } # Set self._network_nodes and self._nodes_by_depth . # Collect unconditional losses . if ( len ( args ) == 2 or layer = self.input_layers [ i ] 'All layer names should be unique . ' ) self.metrics = metrics or [ ] self.loss = loss # We type-check that ` x ` and ` y ` are either single arrays # In this case , we run extensive shape validation checks . sample_weights = standardize_sample_weights ( str ( x ) + ' at layer `` ' + layer.name + ' '' . ' assert [ l.name for l in model.input_layers ] == [ 'input_a ' , 'input_b ' ] self.layers = layers raise ValueError ( 'Graph disconnected : ' layers_for_depth.sort ( key=lambda x : layer_indices [ x ] ) self._init_subclassed_network ( * * kwargs ) nodes_in_decreasing_depth = [ ] # Create the node linking internal inputs to internal outputs . str ( len ( self._input_layers ) ) + ' tensor inputs . ' ) nodes_in_progress.remove ( node ) # Normalize and set self.inputs , self.outputs . if x not in computable_tensors : 'were accessed without issue : ' return isinstance ( x , ( C.variables.Constant , tensor_indices= [ ] , class_weights = standardize_class_weights ( class_weight , # Case : symbolic-mode graph network . fields = [ name whether to build the model 's graph in inference mode ( False ) , training ' a Keras Input layer , ' for depth , layers in model.layers_by_depth.items ( ) : tensor_index ) : self._uses_inputs_arg = True # Additional checks to avoid users mistakenly self._is_compiled = True if not isinstance ( x , ( tf.Tensor , self.input_layers_node_indices = [ ] in zip ( y , for x in self.outputs : if check_array_lengths : layer_indices [ layer ] = len ( layer_indices ) self.layers = [ ] # Stack of layers . sample_weights = [ # Build self._output_layers : 'Expected a symbolic tensor instance . ' ) nodes_in_progress.add ( node ) for original_input_layer , cloned_input_layer in zip ( model.input_layers , input_layers ) : self.input_layers_node_indices.append ( node_index ) outputs = list ( outputs ) # Store the traversal order for layer sorting . self._feed_inputs.append ( v ) raise ValueError ( 'Please do not pass a dictionary ' losses += layer.get_losses_for ( None ) 'the output of a TensorFlow ` Layer ` ' layer , node_index , tensor_index = tensor._keras_history arrays . We expect Numpy data to be fed for these placeholders print_fn ( 'Total params : { : , } '.format ( trainable_count + non_trainable_count ) ) print ( get_source_inputs ( c ) ) layer.name + ' '' . ' # attributes for subclassed Models . the list ` nodes_in_decreasing_depth ` and the set ` network_nodes ` . self.add_loss ( regularizer ( weight ) ) tensor_index = node.tensor_indices [ i ] # This acts just like the ` trainable ` attribute of any layer instance . self._output_coordinates.append ( ( layer , node_index , tensor_index ) ) if not self.inputs : T.sharedvar.TensorSharedVariable ) ) : # The following are implemented as property functions : return [ ] , [ ] , [ ] for node in reversed ( nodes_in_decreasing_depth ) : 'input to `` ' + self.name if not isinstance ( x , ( C.variables.Constant , for x in node.input_tensors : computable_tensors.append ( x ) sample_weights = [ ] layer = node.outbound_layer # This is suboptimal , but it is the best we can do with the info self._feed_inputs.append ( placeholder ) if loss_fn is losses.sparse_categorical_crossentropy : # may contain multiple batches layers_with_complete_input.append ( layer.name ) tensor : Some tensor in a graph . # since ` Sequential ` depends on ` Model ` . # Generate sample-wise weight values given the ` sample_weight ` and inbound_node = inbound_layer._inbound_nodes [ node_index ] self._layers = layers self._input_coordinates.append ( ( layer , node_index , tensor_index ) ) on the recursion stack . Useful to detect cycles . # inputs . Every time the Network is called on a set on input tensors , nodes_in_progress : Set of nodes that are currently active raise ValueError ( 'Input tensors to a ' + cls_name + ' ' inputs = [ inputs ] if any ( K.is_tensor ( v ) for v in all_inputs ) : # Arguments for layer in model.input_layers : inbound_nodes : list of nodes We do not expect any Numpy data to be provided when calling ` fit ` /etc . return isinstance ( x , tf_ops._TensorLike ) or tf_ops.is_dense_tensor_like ( x ) self.input_names = [ ] outputs = self.call ( self.inputs , training=training ) outputs = self.call ( self.inputs ) # We treat subclassed models as a simple sequence of layers , # Collect unconditional updates . for layer , depth in layers_depths.items ( ) : # Update the depth of inbound nodes . # Retrieve losses for all internal layers . def _init_subclassed_network ( self , name=None ) : assert tensor_index == 0 self._network_nodes = network_nodes # of all layers it is connected to . self.build ( input_shape= ( None , ) + inputs.shape [ 1 : ] ) fields = [ name + ' ( ' + cls_name + ' ) ' , output_shape , layer.count_params ( ) , first_connection ] __call__ layer.activity_regularizer ( x ) 'output_ % d ' % ( i + 1 ) for i in range ( len ( self.outputs ) ) ] `` `` '' self._feed_input_names , # Check that for stateful networks , number of samples is a multiple 'either a single ' connections.append ( inbound_layer str ( type ( x ) ) + ' ` . ' self.activity_regularizer ( x ) previous_depth = nodes_depths.get ( inbound_node , 0 ) # If values , then in symbolic-mode placeholders will be created for i , layer in enumerate ( self.input_layers ) : # We need to use ` x ` to set the model inputs . with K.name_scope ( 'activity_regularizer ' ) : # Handle target tensors if any passed . node_index=node_index , depth = max ( depth , previous_depth ) cache_key += ' _ ' + ' , '.join ( [ str ( id ( x ) ) for x in masks ] ) for i in range ( len ( node.inbound_layers ) ) : nodes_depths [ node ] = depth raise ValueError ( 'Do not pass inputs that mix Numpy ' self._uses_inputs_arg = has_arg ( self.call , 'inputs ' ) layer = self.input_layers [ i ] # Used only in conjonction with graph-networks raise ValueError ( 'The name `` ' + name + ' '' is used ' layers = [ ] # The following are implemented as property functions : mode ( True ) , or using the Keras learning phase ( None ) . # If we 've seen this layer before at a higher depth , # self.non_trainable_weights for x in self.inputs : for depth , layers in model._layers_by_depth.items ( ) : # Keep track of the network 's nodes and layers . node_index = node.node_indices [ i ] 'Keras tensors . Found : ' + str ( x ) ) self._outbound_nodes = [ ] # Appended to by calls to ` __call__ ` . nodes , nodes_by_depth , layers , layers_by_depth = _map_graph_network ( layer = node.inbound_layers [ i ] if ( len ( layer._inbound_nodes ) > 1 or if not is_graph_network : # On-the-fly compilation of the model . depth_keys.sort ( reverse=True ) T.sharedvar.TensorSharedVariable ) ) # Build a dict { depth : list of nodes with this depth } input_masks= [ None for _ in self.inputs ] , This recursively updates the map ` layer_indices ` , if all_names.count ( name ) ! = 1 : if node_key in self._network_nodes : tensor_index = self._input_coordinates [ i ] [ 2 ] tensor_index = self.input_layers_tensor_indices [ i ] self.input_layers_tensor_indices = self.model.input_layers_tensor_indices # We will compile after the first for x in to_list ( output ) ] build_map ( x , finished_nodes , nodes_in_progress , self._expects_training_arg = has_arg ( self.call , 'training ' ) layers_by_depth [ depth ] .append ( layer ) assert node_index == 0 nodes_in_progress : Set of nodes that are currently active on the for i in range ( len ( self._output_layers ) ) : if K.is_placeholder ( v ) : layers_by_depth [ depth ] = [ ] if not isinstance ( x , np.ndarray ) and not K.is_tensor ( x ) : feed_input_shapes = None def _base_init ( self , name=None ) : build_map_of_graph ( x , finished_nodes , nodes_in_progress ) # Subclassed network str ( all_names.count ( name ) ) reset_states if layer : sample_weights = standardize_sample_weights ( sample_weight , x , _ , _ = self._standardize_user_data ( x ) print ( 'model.output_layers ' , model.output_layers ) 'It looks like you are subclassing ` Model ` and you ' previous_depth = nodes_depths.get ( inbound_node , 0 ) for layer in model._input_layers : # Store the traversal order for layer sorting . # that are part of the model . computable_tensors = [ ] self._feed_inputs = [ ] import numpy as np output_shapes= [ x._keras_shape for x in self.outputs ] ) output_shapes = [ ] 'Note that input tensors are ' self._feed_loss_fns ) : for x in outputs : if node in finished_nodes : # If we 've seen this layer before at a higher depth , layer : Layer from which ` tensor ` comes from . If not provided , 'Graph disconnected : ' len ( args ) == 1 and 'outputs ' in kwargs or self.built = True return None build_map ( x , finished_nodes , nodes_in_progress , layer , if node in finished_nodes : return False state_updates finished_nodes , nodes_in_progress = set ( ) ' ` tensor = tf.layers.Input ( shape ) ` .\n ' 'instantiated via ` tensor = Input ( shape ) ` .\n ' # to specify custom placeholders if the need arises . # If ` loss_fn ` is not a function ( e.g . callable class ) self.outputs = None node_index : Node index from which ` tensor ` comes from . if not self.built : assert node_index == 0 # It 's supposed to be an input layer , so only one node completely . Useful to prevent duplicated work . if not hasattr ( self , 'optimizer ' ) : # self.trainable_weights # for its inputs , and no outbound nodes . # in the case where all inputs are value arrays . output_shapes= [ x._keras_shape for x in self.outputs ] ) input_tensors=self.inputs , def _standardize_user_data ( self , x , y , self.inputs = [ ] for x in self.outputs : layer = node.outbound_layer self.input_layers = self.model.input_layers # Prevent cycles . if any ( K.is_tensor ( v ) for v in all_inputs ) : self._inbound_nodes = [ ] # Appended to by calls to ` __call__ ` . node_indices= [ ] , x = node.input_tensors [ i ] exception_prefix='target ' ) 'arrays and symbolic tensors . ' for node in nodes_by_depth [ depth ] : 'either a single ' if layer : layer = self._input_layers [ i ] 'must be Keras tensors . Found : ' + str ( x ) raise RuntimeError ( 'The name `` ' + name + ' '' is used ' # that are part of the model . # we retrieve it from there instead of recomputing it . # Check that all tensors required are computable . 'is redundant . ' sample_weight=None , class_weight=None , 'forgot to call ` super ( YourClass , self ) .__init__ ( ) ` . ' ' ( thus holding past layer metadata ) . ' def __init__ ( self , * args , * * kwargs ) : # If the depth is not set , # Update the depth of the corresponding layer tensor_index = self.output_layers_tensor_indices [ i ] is_graph_network = self._is_graph_network # The new network starts with a single inbound node layer , node_index , tensor_index ) # Get sorted list of layer depths . # Note : in this case , ` any ` and ` all ` are equivalent since we disallow def _make_node_key ( layer_name , node_index ) : nodes_in_progress = set ( ) # here we order them by traversal order . for i , layer in enumerate ( self._input_layers ) : 'were accessed without issue : ' # It does not affect users of the underlying layers , only users of the self.outputs = outputs # Case : symbolic-mode subclassed network . output_shape , inbound_node = inbound_layer._inbound_nodes [ node_index ] # Standardize the outputs . class_weights , with pytest.raises ( RuntimeError ) : self._output_layers = [ ] for i in range ( len ( self.output_layers ) ) : nodes_in_decreasing_depth = [ ] if ( len ( layer._inbound_nodes ) > 1 or assert [ l.name for l in recreated_model._input_layers ] == [ 'input_a ' , 'input_b ' ] node_index , tensor_index ) raise ValueError ( 'Please do not pass a dictionary ' depth = nodes_depths.setdefault ( node , 0 ) return hasattr ( x , '_keras_history ' ) self.input_layers_tensor_indices.append ( tensor_index ) self.input_layers = [ ] 'The tensor that caused the issue was : ' # Network.layers needs to have a deterministic order : # self.input_spec for ( ref , sw , cw , mode ) ' ( thus holding past layer metadata ) , ' Node ( outbound_layer=self , raise TypeError ( 'Input tensors to a ' + cls_name + ' ' x , if not self.outputs : ' a previous non-Input layer . ' nodes_by_depth [ depth ] = [ ] inbound_layers= [ ] , node_index = self.output_layers_node_indices [ i ] # Prevent cycles . check_batch_axis=False , # Do n't enforce the batch size . add_update if loss_fn is losses.sparse_categorical_crossentropy : self.layers_by_depth = layers_by_depth # that can be computed from the inputs provided . # Update the depth of the corresponding layer # self.input_spec layer._inbound_nodes [ 0 ] .inbound_layers ) ) : feed_output_names = self._feed_output_names raise RuntimeError ( 'You passed : x= ' + str ( x ) ) if x not in computable_tensors : # list of layers ( 1 to 1 mapping with self.inputs , self._feed_output_names ) if layer not in layer_indices : # mixed symbolic/value inputs . check_batch_axis=False , losses all_inputs = [ ] layer = self.output_layers [ i ] self.compile ( optimizer=self.optimizer , 'instantiated via ' input_masks= [ None for _ in self.inputs ] , layers have been traversed completely . training : Boolean or None . Only relevant in symbolic mode . Specifies self.output_layers = [ ] updates += layer.get_updates_for ( None ) node_key = _make_node_key ( layer.name , node_index ) tensor_index = node.tensor_indices [ i ] Useful to prevent duplicated work . ' can not obtain value for tensor ' nodes_by_depth = { } assert len ( inputs ) == 1 if isinstance ( v , list ) : for depth in depth_keys : # Check for redundancy in outputs . # Network_nodes : set of nodes included in the graph of layers if layer in self._input_layers : # Raises # it is a user-defined loss and we make no assumptions self._nodes_by_depth = nodes_by_depth output_shapes.append ( output_shape ) self.optimizer = None # Update network_nodes . self._feed_input_names.append ( name ) x = node.input_tensors [ i ] output_tensors=self.outputs , # The model owns this layer node . # the author of the subclassed network ) . print ( 'model.input_layers : ' , model.input_layers ) self.metrics = metrics or [ ] layer , node_index , tensor_index = x._keras_history has_arg ( self.call , 'mask ' ) or # ( either by using the tensor provided , self.trainable = True for node in reversed ( nodes_in_decreasing_depth ) : 'Layer names : ' , all_names ) finished_nodes = set ( ) print ( final_model.compute_output_shape ( [ ( 10 , 32 ) , ( 10 , 32 ) ] ) ) node_index = self.input_layers_node_indices [ i ] layer.name + '.\n ' raise ValueError ( 'Unexpectedly found an instance of type ` ' if self._is_graph_network : from tensorflow.python.framework import ops as tf_ops self.activity_regularizer ( x ) depth_keys = list ( layers_by_depth.keys ( ) ) # it is a user-defined loss and we make no assumptions output_tensors=self.outputs , return layer_name + '_ib- ' + str ( node_index ) self._input_coordinates = self.model._input_coordinates network_nodes.add ( self._node_key ( layer , node_index ) ) get_losses_for # Entries are unique . Includes input and output layers . nodes_in_decreasing_depth.append ( node ) ' ( ' + cls_name + ' ) ' , # Check that all tensors required are computable . for layer , depth in layers_depths.items ( ) : cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) self.built = False raise ValueError ( 'Please provide as model inputs ' # then the model should not be fed any inputs and targets . for original_input_layer , cloned_input_layer in zip ( model._input_layers , input_layers ) : # are relevant to the current graph ) . nodes_by_depth [ depth ] .append ( node ) connections.append ( inbound_layer + ' [ ' + str ( inbound_node_index ) + ' ] [ ' + str ( inbound_tensor_index ) + ' ] ' ) tensor_index = self._output_coordinates [ i ] [ 2 ] layer = self._output_layers [ i ] nodes_depths [ inbound_node ] = max ( depth + 1 , previous_depth ) 'All outputs should only appear once . ' # Build self.output_layers : print_fn ( check_array_lengths=True , # using improper loss fns . layers_depths = { } # dict { layer : depth value } layer.name + ' '' is part of a cycle . ' ) # and one tensor output . str ( x ) + ' at layer `` ' will be obtained from ` tensor._keras_history ` . RuntimeError : if a cycle is detected . inputs : Single array , or list of arrays . The arrays could be placeholders , self.built = True # that can be computed from the inputs provided . layers_with_complete_input = [ ] # To provide a better error msg . when calling ` fit ` /etc . if Numpy data : we create placeholders matching the shape of the Numpy nodes_in_decreasing_depth.append ( node ) v = np.asarray ( v ) print ( 'model.input_layers_node_indices : ' , model.input_layers_node_indices ) print ( 'mask : ' , model.compute_mask ( [ a , b ] , [ None , None ] ) ) output_shapes.append ( output_shape [ : -1 ] + ( 1 , ) ) ( layer._inbound_nodes and # On-the-fly setting of symbolic model inputs # is ` Sequential ` via ` isinstance ` input_shapes= [ x._keras_shape for x in self.inputs ] , for output_shape , loss_fn in zip ( self._feed_output_shapes , self._feed_loss_fns ) : getattr ( losses , loss_fn.__name__ , None ) is None ) : cls_name = self.__class__.__name__ return isinstance ( x , ( T.TensorVariable , outbound_nodes : list of nodes node_index , ' can not obtain value for tensor ' Node ( outbound_layer=self , exception_prefix='input ' ) check_loss_and_target_compatibility ( y , to_display = [ 'Layer ( type ) ' , 'Here , a tensor specified as ' class_weight , feed_output_names ) if outputs is None : layer.name + ' '' is part of a cycle . ' ) standardize_weights ( ref , sw , cw , mode ) assert [ l.name for l in model._input_layers ] == [ 'input_a ' , 'input_b ' ] # the node has no outbound nodes ( depth 0 ) . inputs = node.input_tensors self._is_compiled = False # ( not all nodes included in the layers are relevant to the current graph ) . y=None , tensor_index = self.output_layers_tensor_indices [ i ] losses += layer.get_losses_for ( inputs ) # Build a dict { depth : list of nodes with this depth } y = [ ] self._feed_output_shapes ) self._losses = [ ] metrics=self.metrics , # Network.layers needs to have a deterministic order : if not self._is_compiled : depth_keys.sort ( reverse=True ) shape = self._internal_output_shapes [ i ] x = standardize_input_data ( x , `` `` '' # Update the depth of inbound nodes . cache_key = ' , '.join ( [ str ( id ( x ) ) for x in inputs ] ) feed_sample_weight_modes = [ None for _ in self.outputs ] if len ( self.inputs ) == 1 : layers : list of Layer instances . # No network-level masking for now . y , if isinstance ( outputs , ( list , tuple ) ) : name = model.input_layers [ i ] .name assert [ l.name for l in model.output_layers ] == [ 'dense_2 ' , 'dense_3 ' ] 'Output Shape ' , layer.activity_regularizer ( x ) assert [ l.name for l in model._output_layers ] == [ 'dense_2 ' , 'dense_3 ' ] 'they can not be the output of ' # we should use that depth instead of the node depth . with pytest.raises ( ValueError ) : check_loss_and_target_compatibility ( computable_tensors.append ( x ) losses += layer.losses for node , depth in nodes_depths.items ( ) : # Returns 'Received : ' + str ( x ) self.output_layers_tensor_indices.append ( tensor_index ) mask_cache_key += ' _ ' + ' , '.join ( [ str ( id ( x ) ) for x in masks ] ) for x in node.output_tensors : to_display = [ 'Layer ( type ) ' , 'Output Shape ' , 'Param # ' , 'Connected to ' ] def _init_graph_network ( self , inputs , outputs , name=None ) : the list ` nodes_in_decreasing_depth ` and the set ` network_nodes ` . # Set self.layers and self.layers_by_depth . if not is_tensor ( x ) : layer = self.output_layers [ i ] all_names = [ layer.name for layer in layers ] feed_output_shapes = None # depth levels in the graph . assert [ l.name for l in recreated_model.input_layers ] == [ 'input_a ' , 'input_b ' ] A tuple ` ( nodes , nodes_by_depth , layers , layers_by_depth ) ` . depth_keys = list ( layers_by_depth.keys ( ) ) # for logging purposes . assert [ l.name for l in recreated_model.output_layers ] == [ 'dense_2 ' , 'dense_3 ' ] for x in to_list ( output ) ] stateful for name in all_names : if len ( set ( self.outputs ) ) ! = len ( self.outputs ) : layer=layer , # Raises : layers_for_depth.sort ( key=lambda x : layer_indices [ x ] ) elif ( not hasattr ( loss_fn , '__name__ ' ) or # Network instance . # Do not do shape validation . # Build a dict { depth : list of layers with this depth } if not all ( isinstance ( v , np.ndarray ) or sample_weight=None , # ( not all nodes included in the layers if include_optimizer and hasattr ( model , 'optimizer ' ) : if len ( input_shapes ) ! = len ( self._input_layers ) : time what their inputs look like . tensor_index : Tensor_index from which ` tensor ` comes from . Numpy arrays , or data tensors . losses += layer.get_losses_for ( None ) if self._expects_training_arg : # Typecheck that all inputs are * either * value * or * symbolic . if not isinstance ( x , ( T.TensorVariable , for layer in getattr ( self , 'input_layers ' , [ ] ) : if y is not None : y = standardize_input_data ( node_index = self._input_coordinates [ i ] [ 1 ] network_nodes = set ( ) # ids of all nodes relevant to the Network raise RuntimeError ( 'You must compile a model before ' self._is_graph_network = True cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) if not self.optimizer : self.inputs , self.outputs ) # ` check_batch_axis ` is set to False since ` x ` self._updates = [ ] self.layers = [ ] self._input_layers.append ( layer ) print ( 'layers : ' , [ layer.name for layer in model.layers ] ) # Propagate to all previous tensors connected to this node . def _set_inputs ( self , inputs , outputs=None , training=None ) : # then cache them here . When any of these outputs is queried later , we self.inputs.append ( v ) ' '' was not an Input tensor , ' self.input_layers_node_indices = self.model.input_layers_node_indices self._network_nodes = nodes loss_weights=self.loss_weights , to_yaml layer_indices = { } # dict { layer : index in traversal } # Layer parameters . if len ( input_shapes ) ! = len ( self.input_layers ) : layers_for_depth = layers_by_depth [ depth ] feed_sample_weight_modes = self._feed_sample_weight_modes save return network_nodes , nodes_by_depth , layers , layers_by_depth self.output_layers_tensor_indices = [ ] shape = K.int_shape ( self.outputs [ i ] )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'keras/engine/base_layer.py', 'keras/engine/network.py', 'keras/engine/saving.py', 'keras/engine/sequential.py', 'keras/engine/training.py', 'keras/models.py', 'keras/utils/layer_utils.py', 'tests/keras/engine/test_topology.py']",Enable Model subclassing API ( # 10046 )
467,a1d544f4a156373b1c9e72d3af0bf87e4d81d90c,2018-04-26 12:13:58-04:00,"# Test cases with ` y = None ` break assert ( x [ 1 ] == x_misc1 [ :3 ] ) .all ( ) or a list of numpy arrays of miscellaneous data that gets passed to the output assert str ( e_info.value ) .find ( 'All of the arrays in ' ) ! = -1 if len ( x ) ! = len ( xx ) : return output x = generator.flow ( ( images , [ x_misc1 , x_misc2 ] ) , None , batch_x_miscs = [ xx [ index_array ] for xx in self.x_misc ] for x in generator.flow ( images , None , In case of grayscale data , the channels axis of the image array else : additional inputs ) and ` y ` is a numpy array of corresponding labels . '' '' '' shuffle=True , save_to_dir=str ( tmpdir ) , assert x.shape == images [ :3 ] .shape # Check that the sequence is shuffled . self.x_misc = x_misc assert type ( x ) is np.ndarray x_misc1 = np.random.random ( dsize ) x : Numpy array of input data or tuple . If tuple , the second elements is either if type ( x [ 1 ] ) is not list : the channels axis should have value 1 , and in case ( len ( x ) , len ( xx ) ) ) # Test without y x : data . Should have rank 4 . # Test with a single miscellaneous input data array # Test with two miscellaneous inputs batch_size=3 , shuffle=False ) .next ( ) assert ( x [ 2 ] == x_misc2 [ ( i * 2 ) : ( ( i + 1 ) * 2 ) ] ) .all ( ) output = ( batch_x if batch_x_miscs == [ ] else [ batch_x ] + batch_x_miscs , ) return output [ 0 ] x_misc_err = np.random.random ( ( dsize + 1 , 3 , 3 ) ) should contain the images and the second element another numpy array x = generator.flow ( ( images , x_misc1 ) , None , dsize = images.shape [ 0 ] An Iterator yielding tuples of ` ( x , y ) ` where ` x ` is a numpy array of image data and batch_y = self.y [ index_array ] generator.flow ( ( images , x_misc1 ) , np.arange ( dsize + 1 ) , batch_size=3 ) along with the images . In case of grayscale data , assert type ( x ) is np.ndarray x_misc = [ np.asarray ( x [ 1 ] ) ] for i , ( x , y ) in enumerate ( generator.flow ( ( images , [ x_misc1 , x_misc2 ] ) , 'Found a pair with : len ( x [ 0 ] ) = % s , len ( x [ ? ] ) = % s ' % generator.flow ( ( images , x_misc_err ) , np.arange ( dsize ) , batch_size=3 ) output += ( self.y [ index_array ] , ) np.arange ( dsize ) , should have value 1 , and in case assert ( x [ 1 ] == x_misc1 [ ( i * 2 ) : ( ( i + 1 ) * 2 ) ] ) .all ( ) shuffle=False , batch_size=2 ) ) : break assert x [ 0 ] .shape == images [ :2 ] .shape if ( type ( x ) is tuple ) or ( type ( x ) is list ) : assert x [ 0 ] .shape == images [ :3 ] .shape another numpy array or a list of numpy arrays , each of which gets passed through as an output without any modifications . assert type ( x ) is list assert str ( e_info.value ) .find ( ' ` x ` ( images tensor ) and ` y ` ( labels ) ' ) ! = -1 x_misc2 = np.random.random ( ( dsize , 3 , 3 ) ) for i , ( x , y ) in enumerate ( generator.flow ( ( images , x_misc1 ) , x_misc = [ np.asarray ( xx [ : split_idx ] ) for xx in x_misc ] x : data . Numpy array of rank 4 or a tuple . If tuple , the first element without any modifications . Can be used to feed the model miscellaneous data x_misc = [ np.asarray ( xx [ split_idx : ] ) for xx in x_misc ] else : batch_size=3 ) : x = x [ 0 ] return batch_x ` y ` is a numpy array of corresponding labels . '' '' '' x : Numpy array of input data . if i == 2 : assert ( x [ 2 ] == x_misc2 [ :3 ] ) .all ( ) return batch_x , batch_y x_misc = [ ] assert x.shape == images [ :3 ] .shape An Iterator yielding tuples of ` ( x , y ) ` where ` x ` is a numpy array of image data raise ValueError ( 'All of the arrays in ` x ` should have the same length . ' ( in the case of a single image input ) or a list of numpy arrays ( in the case with with pytest.raises ( ValueError ) as e_info : x = generator.flow ( images , None , batch_size=3 ) .next ( ) x_misc = [ np.asarray ( xx ) for xx in x [ 1 ] ] for xx in x_misc : # Test some failure cases :","['keras/preprocessing/image.py', 'tests/keras/preprocessing/image_test.py']",Add support for passthrough arguments to NumpyArrayIterator ( # 10035 )
468,24daab1c7b3ceebf04a19d3df3fdcdef11fe773b,2018-04-25 21:36:23-07:00,"if K.backend ( ) ! = 'tensorflow ' : Optionally loads weights pre-trained reason='Requires TensorFlow backend ' ) on ImageNet . This model is available for TensorFlow only , data format ` ( width , height , channels ) ` . due to its reliance on ` SeparableConvolution ` layers . from .. utils import layer_utils raise RuntimeError ( 'The Xception model is only available with ' layer_utils.convert_all_kernels_in_model ( model ) Optionally loads weights pre-trained on ImageNet . This model can @ pytest.mark.skipif ( ( K.backend ( ) ! = 'tensorflow ' ) , 'the TensorFlow backend . ' ) Also do note that this model is only available for the TensorFlow backend , and can only be used with inputs following the TensorFlow only be used with the data format ` ( width , height , channels ) ` . if K.backend ( ) == 'theano ' :","['keras/applications/xception.py', 'tests/keras/applications/applications_test.py']",Enable Xception to work on Theano and CNTK ( # 10024 )
469,918c5991faa21cbb48de07566124c57cf16c0cf0,2018-04-25 11:33:28-07:00,"'Provided : ' + str ( axes ) ) if py_any ( [ isinstance ( a , ( list , tuple ) ) for a in axes ] ) : py_any = any 'Expected : None , int , ( int , int ) , ' raise ValueError ( 'Multiple target dimensions are not supported . ' if b_any ( [ isinstance ( a , ( list , tuple ) ) for a in axes ] ) :","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py']",Add exceptions for ` batch_dot ` ( # 10020 )
470,14113279768e0511d6adb7b2ffad21863a7c969d,2018-04-25 10:09:11-07:00,"image_shape = _preprocess_conv2d_image_shape ( int_shape ( x ) , data_format ) if kernel_shape is None : # Will only work if ` pointwise_kernel ` is a shared variable . depthwise_kernel_shape = depthwise_kernel._keras_shape shape = None kernel_shape = kernel._keras_shape return None pointwise_kernel_shape = int_shape ( pointwise_kernel ) kernel_shape = None volume_shape = _preprocess_conv3d_volume_shape ( int_shape ( x ) , data_format ) shape = x._keras_shape if hasattr ( pointwise_kernel , '_keras_shape ' ) : image_shape = _preprocess_conv2d_image_shape ( int_shape ( x ) , data_format ) if hasattr ( kernel , '_keras_shape ' ) : if hasattr ( x , '_keras_shape ' ) : depthwise_kernel_shape = int_shape ( depthwise_kernel ) kernel_shape = int_shape ( kernel ) depthwise_kernel_shape = depthwise_kernel.eval ( ) .shape # in case of a shared variable # Will only work if ` kernel ` is a shared variable . shape = int_shape ( x ) volume_shape = _preprocess_conv3d_volume_shape ( int_shape ( x ) , data_format ) image_shape = None if hasattr ( depthwise_kernel , '_keras_shape ' ) : kernel_shape = kernel.eval ( ) .shape # in case of a shared variable pointwise_kernel_shape = pointwise_kernel.eval ( ) .shape # in case of a shared variable if pointwise_kernel_shape is None : if depthwise_kernel_shape is None : # Will only work if ` depthwise_kernel ` is a shared variable . else : kernel_shape = kernel.eval ( ) .shape raise TypeError ( 'Not a Keras tensor : ' , x ) depthwise_kernel_shape = depthwise_kernel.eval ( ) .shape pointwise_kernel_shape = pointwise_kernel._keras_shape pointwise_kernel_shape = pointwise_kernel.eval ( ) .shape volume_shape = None",['keras/backend/theano_backend.py'],Fix ` int_shape ` of Theano and Refactor associated lines ( # 10030 )
471,58fd1f0589d33aeb33c4129cfedfb7737495efc0,2018-04-24 21:06:54-07:00,"if steps is not None : 'you can convert them to the expected format via : \n ' from __future__ import division use_multiprocessing=use_multiprocessing ) losses.binary_crossentropy , weight_type : A string used purely for exception printing . model.fit ( test_inputs , test_outputs , epochs=1 , batch_size=2 , validation_split=0.5 ) 'should be None or `` temporal '' . ' ' the ` keras.utils.Sequence ` class . ' ) metric_fn = metrics_module.get ( metric ) value found in ` shapes ` . raise ValueError ( ' ` validation_data ` should be a tuple ' workers=1 , score_array = K.mean ( score_array , axis=list ( range ( weight_ndim , ndim ) ) ) if isinstance ( x_weight , dict ) : val_gen = ( hasattr ( validation_data , 'next ' ) or if batch_size == 0 : if y.shape [ 1 ] > 1 : weighted_metrics=None , if len ( outs ) == 1 : _callbacks = [ cbks.BaseLogger ( steps=steps ) if len ( sample_weight.shape ) > len ( y.shape ) : if isinstance ( metrics , list ) : def compile ( self , optimizer , loss=None , metrics=None , loss_weights=None , metric_name = base_metric_name + ' _ ' + str ( j ) return [ copy.copy ( metrics ) for _ in output_names ] # Compatibility with the generators # it 's possible to callback a different model than self : return arrays [ start : stop ] # case : categorical accuracy/crossentropy with sparse targets return x_weight if self.uses_learning_phase and not isinstance ( K.learning_phase ( ) , int ) : 'from keras.utils import to_categorical\n ' else : if verbose == 1 : output_shapes , raise ValueError ( ' ` validation_steps=None ` is only valid for a ' use_multiprocessing=use_multiprocessing , if do_validation : batch_outs = f ( ins_batch ) # Prepare display labels . return [ None if x is None else x [ start : stop ] for x in arrays ] epoch_logs [ 'val_ ' + l ] = o training_utils.check_array_length_consistency ( [ a_np ] , None , [ b_np ] ) # used for training . # we will see , we can not pre-allocate 'metrics ' : callback_metrics or [ ] , if not isinstance ( outs , list ) : # Check shapes compatibility . check_array_length_consistency ( x , y , sample_weights ) callback_model.stop_training = False str ( x_weight ) ) outs = [ ] return arrays [ start ] ` History ` object . losses.binary_crossentropy , before declaring ` predict_loop ` finished . def set_of_lengths ( x ) : # Dedupe name elif shuffle : verbose=0 ) ( where one can not access arbitrary indices ) . metric_fn = metrics_module.sparse_categorical_crossentropy set_x = set_of_lengths ( inputs ) exclusive . val_x , val_y , val_sample_weight = validation_data x1 , x2 , y = process_line ( line ) callback_model = model.callback_model # Step-based predictions . if hasattr ( model , 'metrics ' ) : if i not in stateful_metric_indices : if model.uses_learning_phase and not isinstance ( K.learning_phase ( ) , def _standardize_sample_or_class_weights ( x_weight , output_names , weight_type ) : raise ValueError ( 'Received an empty batch . ' ' Numpy arrays instead . The list you passed was : ' # different shapes , with None = > 0 'Expected a list or dictionary , found : ' weighted_metric_fn = _weighted_masked_objective ( metric_fn ) model.compile ( optimizer , loss='mse ' , losses.categorical_crossentropy ) 'the same number of samples as target arrays . Got ' ins_batch [ i ] = ins_batch [ i ] .toarray ( ) epoch_logs [ 'val_ ' + l ] = o return averages y_classes = y if do_validation : from .. utils.generic_utils import Progbar raise ValueError ( ' ` validation_steps=None ` is only valid for a ' enqueuer.stop ( ) validation_data= ( { 'input_a ' : input_a_np , 'input_b ' : input_b_np } , [ output_a_np , output_b_np ] ) ) outs = [ ] np.random.random ( ( batch_sz , 3 ) ) ] ) batch_count = int ( len ( index_array ) / batch_size ) _slice_arrays ( sample_weights , 0 , split_at ) , batch_size=batch_size , enqueuer = None that will be applied to the last 2 dimensions of _check_array_lengths ( [ a_np ] , None , [ b_np ] ) # Returns elif metric in ( 'crossentropy ' , 'ce ' ) : j = 1 verbose=0 ) Users may pass data as a list of arrays , dictionary of arrays , raise ValueError ( 'Output of generator should be ' outs.append ( 0 . ) 'Expected to see ' + str ( len ( names ) ) + ' array ( s ) , ' Users may pass data as a list of arrays , dictionary of arrays , if shapes : Assume that f returns a list , labeled by out_labels . str ( [ x.shape for x in inputs ] ) ) # Do not slice the training phase flag . for i in range ( len ( self._feed_inputs ) ) : out_labels = out_labels or [ ] batch_ids = index_array [ batch_start : batch_end ] batch_logs [ 'batch ' ] = step_index # Raises weights=weights , use_multiprocessing=use_multiprocessing , model._make_train_function ( ) `` `` '' Adds support for masking and sample-weighting to an objective function . ' based on the ` keras.utils.Sequence ` class . ' if len ( y.shape ) > 2 : sequence_length=sequence_length ) ) 'and ' + str ( list ( set_y ) [ 0 ] ) + ' target samples . ' ) ' ` keras.utils.Sequence ` ' if x is None : val_outs = [ val_outs ] batch_count = int ( len ( index_array ) / batch_size ) if steps_done == 1 : UserWarning ( 'Using a generator with ` use_multiprocessing=True ` ' or a list/array of indices stateful_metric_indices = [ ] the batch axis of the arrays matches the expected return outs elif isinstance ( metrics , dict ) : batch_size : Integer batch size or None if unknown . print ( 'Train on % d samples , validate on % d samples ' % batch_outs = f ( ins ) `` `` '' Determine the number of samples provided for training and evaluation . use_multiprocessing=use_multiprocessing , if hasattr ( self , 'metrics ' ) : if val_gen : self.sample_weight_modes [ i ] ) _check_loss_and_target_compatibility ( y , if ref_dim ! = dim and ref_dim : 'Expected to see ' + str ( len ( names ) ) + ' array ( s ) , ' max_queue_size=max_queue_size ) self._feed_loss_fns , x_weights = [ ] _check_loss_and_target_compatibility ( [ a ] , [ losses.categorical_crossentropy ] , [ ( 2 , None , 3 ) ] ) ' : the list of Numpy arrays that you are passing to ' ' was passed for an output of shape ' + str ( shape ) metric_fn = metrics_module.categorical_accuracy # build batch logs for i in range ( len ( model._feed_inputs ) ) : raise ValueError ( 'Found a sample_weight with shape ' for l , o in zip ( out_labels , outs ) : model._feed_targets raise ValueError ( 'All target arrays ( y ) should have ' index_array = np.arange ( num_samples ) 'which does expect integer targets . ' ) return training_generator.fit_generator ( data = data.values if data.__class__.__name__ == 'DataFrame ' else data callbacks.on_batch_end ( batch_index , batch_logs ) validation_data= ( [ input_a_np , input_b_np ] , callback_metrics=None , initial_epoch=0 , def weighted_masked_objective ( fn ) : self.loss_functions [ i ] == losses.binary_crossentropy ) : for batch_index , ( batch_start , batch_end ) in enumerate ( batches ) : output_shape = self._internal_output_shapes [ i ] `` `` '' Abstract fit function for ` f ( ins ) ` . if arrays is None : ndim=len ( shape ) , ' Please consider using the ` keras.utils.Sequence ' if ins and isinstance ( ins [ -1 ] , float ) : out_labels : List of strings , display names of ' 3+ dimensional targets . ' ) # No need for try/except because _slice_arrays ( input_a , 0 , 1 ) if len ( unconcatenated_outs ) == 1 : if is_sequence : return sample_weight for cbk in callbacks : progbar.update ( steps_done ) shape = shapes [ i ] if data [ i ] .ndim ! = len ( shape ) : if x_weight is None or len ( x_weight ) == 0 : else : 'input_b ' : input_b_np } ) 'or ` ( x , y ) ` . Found : ' 'do_validation ' : do_validation , if x is None or len ( x ) == 0 : out = single_output_model.predict_generator ( RandomSequence ( batch_size , if len ( y.shape ) < 3 : num_samples = None data = [ np.asarray ( data ) ] verbose=verbose , steps_per_epoch=4 , raise TypeError ( 'TypeError while preparing batch . ' A slice of the array ( s ) . raise ValueError ( 'Output of generator should be a tuple ' while steps_done < steps : for i in range ( len ( feed ) ) : if len ( data ) ! = len ( names ) : into a sample-weighted , cost-masked objective function outs.append ( 0 . ) raise ValueError ( ' ` steps=None ` is only valid for a generator ' for i , batch_out in enumerate ( batch_outs ) : 'If your targets are integer classes , ' for metric in metrics : weight_type + ' ` ' count_mode = 'steps ' inputs = ( self._feed_inputs exclusive . from an under-represented class . def test_slice_arrays ( ) : weights=weights , return np.concatenate ( all_outs [ 0 ] ) [ a ] , [ losses.categorical_crossentropy ] , [ a.shape ] ) steps_done = 0 if self.uses_learning_phase and not isinstance ( K.learning_phase ( ) , int ) : callbacks.set_params ( { if x_weight is None or len ( x_weight ) == 0 : for batch_out in batch_outs : [ x [ start : stop ] for x in arrays ] if ` arrays ` is a list x = generator_output self.train_function = K.function ( callback_model.stop_training = False raise TypeError ( 'Type of ` metrics ` argument not understood . ' self.stateful_metric_names.append ( metric_name ) batch_outs = f ( ins_batch ) do_validation = True return num_samples outs [ i ] += batch_out * len ( batch_ids ) output_names : a list of the names ( strings ) of model outputs . # score_array has ndim > = 2 start : can be an integer index ( start index ) if verbose : exception_prefix + ' : ' if steps_per_epoch is not None : raise ValueError ( 'Either the input data should have ' if len ( y.shape ) > 2 : output_names : List of output names ( strings ) in the model . _check_loss_and_target_compatibility ( [ a ] , [ losses.categorical_crossentropy ] , [ a.shape ] ) ' `` ` \n ' callback_model.stop_training = False # Need range check here as filling if isinstance ( batch_outs , list ) : str ( list ( set_y ) [ 0 ] ) + ' input samples and ' 'pass shuffle= '' batch '' . ' ) if len ( set_x ) > 1 : if len ( set_x ) > 1 : self.history = cbks.History ( ) x1 , x2 , y = process_line ( line ) from keras.engine.training import _slice_arrays if i not in stateful_metric_indices : batch_size = list ( x.values ( ) ) [ 0 ] .shape [ 0 ] if data is None : 'or ` ( x , y ) ` . Found : ' # reduce score_array to same ndim as weight array for target_dim , out_dim in zip ( y.shape [ 1 : ] , shape [ 1 : ] ) : not validation_steps ) : elif len ( validation_data ) == 3 : data = [ x.values if x.__class__.__name__ == 'DataFrame ' # Pre-allocate the results arrays . 'all classes in the data . ' if hasattr ( start , 'shape ' ) : steps=steps_per_epoch , yield ( { 'input_1 ' : x1 , 'input_2 ' : x2 } , { 'output ' : y } ) Array of predictions ( if the model has a single output ) with open ( path ) as f : if isinstance ( x_weight , list ) : callback_model.stop_training = False ' : the list of Numpy arrays that you are passing to ' if set_y and set_w and list ( set_y ) [ 0 ] ! = list ( set_w ) [ 0 ] : ins_batch [ i ] = ins_batch [ i ] .toarray ( ) else : verbose=1 , first input numpy array . When steps is not ` None ` and val_data += [ 0 . ] batch_size = x.shape [ 0 ] ' and multiple workers may duplicate your data . ' next epoch . Ignored with the default value of ` None ` . `` `` '' Trains the model for a fixed number of epochs ( iterations on a dataset ) . stateful_metric_indices = [ fn : The objective function to wrap , from .training_utils import batch_shuffle index_array = index_array.flatten ( ) 'from keras.utils import to_categorical\n ' `` `` '' steps_per_epoch=10000 , epochs=10 ) Ignored with the default value of ` None ` . score_array * = weights cbks.ProgbarLogger ( ' should be specified . ' ) raise TypeError ( 'TypeError while preparing batch . ' output_shapes : list of shapes of model outputs . class_weight=class_weight ) model._feed_sample_weights ) for batch_index , ( batch_start , batch_end ) in enumerate ( batches ) : metric_fn = metrics_module.categorical_crossentropy # fit_generator will throw an exception * * self._function_kwargs ) use_multiprocessing=False , outs [ i ] += batch_out weights=batch_sizes ) ) # score_array has ndim > = 2 [ self.total_loss ] + self.metrics_tensors , ' a time dimension . ' ) for i in indices_for_conversion_to_dense : def set_of_lengths ( x ) : `` pay more attention '' to samples from an under-represented class . if do_validation : 'the same number of samples as target arrays . ' raise ValueError ( ' ` steps_per_epoch=None ` is only valid for a ' for i in range ( len ( outs ) ) : for i , out in enumerate ( outs ) : if str ( name ) in self.stateful_metric_names ] size : Integer , total size of the data to slice into batches . processed based on the size of the first dimension of the batches = _make_batches ( len ( x_test ) , batch_size ) if step == 0 : 'array per model output . ' ) % ( existing_classes - existing_class_weight ) ) validation_steps , training_utils.check_array_length_consistency ( progbar.update ( step + 1 ) if hasattr ( start , '__len__ ' ) : out = model.fit_generator ( RandomSequence ( 3 ) , callbacks.on_epoch_end ( epoch , epoch_logs ) '\n ' key_losses = { losses.mean_squared_error , A list ( one entry per model output ) of lists of metric functions . return weights the targets ( i.e . we are weighting timesteps , not samples ) . averages.append ( np.average ( [ out [ i ] for out in outs_per_batch ] , score_array * = mask 'Alternatively , you can use the loss function ' into a sample-weighted , cost-masked objective function verbose=verbose , steps=steps ) from .. import callbacks as cbks nested_weighted_metrics = _collect_metrics ( weighted_metrics , self.output_names ) batch_size=None , exception_prefix= '' ) : index_array = np.arange ( num_samples ) def handle_metrics ( metrics , weights=None ) : passed to the callbacks . They should be the enqueuer = OrderedEnqueuer ( finally : 'y_binary = to_categorical ( y_int ) \n ' and just `` binary_accuracy '' for the second output , 'targets to be binary matrices ( 1s and 0s ) ' metric_name = self.output_names [ i ] + ' _ ' + metric_name for batch_out in enumerate ( batch_outs ) : if issparse ( ins [ i ] ) and not K.is_sparse ( feed [ i ] ) : if callback_model.stop_training : if sample_weight is not None and len ( sample_weight.shape ) ! = 1 : if hasattr ( start , 'shape ' ) : j = 1 val_outs = [ val_outs ] x = standardize_input_data ( x , if enqueuer is not None : if len ( x_weight ) ! = len ( output_names ) : if callback_model.stop_training : batch_ids = index_array [ batch_start : batch_end ] 'class_weight ' ) elif len ( generator_output ) == 3 : val_outs = test_loop ( model , val_f , val_ins , str ( data ) [ :200 ] ) metric_name = metric_name_prefix + metric_name return self.history if self._uses_dynamic_learning_phase ( ) : A numpy array of target weights , one entry per sample to weight . output_names , y , val_y = ( _slice_arrays ( y , 0 , split_at ) , _slice_arrays ( y , split_at ) ) val_outs = [ val_outs ] [ np.random.random ( ( self.batch_size , 4 ) ) , return outs [ 0 ] if val_f and val_ins : Everything gets normalized to a single sample-wise ( or timestep-wise ) loss_fns : list of loss functions . if num_train_samples is not None : raise TypeError ( 'The model has multiple outputs , so ` ' if y.shape [ -1 ] == 1 : all_outs = [ ] from keras.utils.generic_utils import slice_arrays stateful_metrics=model.stateful_metric_names ) ) if out_dim is not None and target_dim ! = out_dim : is_sequence = isinstance ( generator , Sequence ) cbk.validation_data = val_data callbacks.on_epoch_begin ( epoch ) the targets ( i.e . we are weighting timesteps , not samples ) . # To prevent a slowdown , raise ValueError ( callbacks.on_train_begin ( ) raise ValueError ( ' '' sample_weight_mode ' class_weights , generator_output = next ( output_generator ) def handle_metrics ( metrics , weights=None ) : elif hasattr ( start , '__getitem__ ' ) : stateful_metric_indices = [ ] 'Expected sample_weight with rank ' with open ( path ) as f : ValueError : In case of invalid user-provided argument . metric_result = weighted_metric_fn ( y_true , y_pred , raise ValueError ( ' a defined shape , or ' + steps_name 'batch_size ' : batch_size , if not isinstance ( batch_outs , list ) : do_validation = bool ( validation_data ) _check_loss_and_target_compatibility ( [ a ] , [ losses.categorical_crossentropy ] , [ ( 2 , 3 , 6 ) ] ) } ) if workers > 0 : out = model.fit_generator ( RandomSequence ( 3 ) , steps_per_epoch=4 , use_multiprocessing=True , workers=2 ) averages.append ( float ( outs_per_batch [ -1 ] [ i ] ) ) ' Numpy arrays instead . The list you passed was : ' model.compile ( optimizer , loss='mse ' , loss_weights= { 'dense_1 ' : 0.2 , if y.shape [ : sample_weight.ndim ] ! = sample_weight.shape : weights = np.asarray ( [ class_weight [ cls ] for cls in y_classes # case : binary accuracy/crossentropy return np.append ( index_array , last_batch ) str ( sample_weight.shape ) + ' . ' batch_size=batch_size , for i in range ( num_batches ) ] _slice_arrays ( None ) key_losses = { losses.mean_squared_error , if i in stateful_metric_indices : output_names , elif len ( names ) == 1 : metric_name = metric_fn.__name__ y = _standardize_input_data ( y , self._feed_output_names , if steps_done == 1 : try : return [ None for _ in output_names ] steps=None , # Prepare inputs , delegate logic to ` _predict_loop ` . shapes : Optional list of expected array shapes . metric_name_prefix = 'weighted_ ' if weights is not None else `` # Handle data tensors support when no input given if batch_index == 0 : for i in range ( len ( outs ) ) : enqueuer.stop ( ) if ( output_shape [ -1 ] == 1 or indices_for_conversion_to_dense.append ( i ) ValueError : when ` steps ` is ` None ` and the attribute ` ins.shape ` stateful_metrics=self.stateful_metric_names ) ) validation_steps=None ) : if metric in ( 'accuracy ' , 'acc ' ) : generator_output = next ( output_generator ) ' ` ( val_x , val_y , val_sample_weight ) ` ' A function with signature ` fn ( y_true , y_pred , weights , mask ) ` . `` `` '' Wrapper function . return weighted if batch_index == 0 : outs = [ ] for y , loss , shape in zip ( targets , loss_fns , output_shapes ) : 'steps ' ) validation_steps=3 , loss_fns : list of loss functions . self.loss_functions [ i ] == losses.binary_crossentropy ) : y_classes = np.reshape ( y , y.shape [ 0 ] ) metric_fn = metrics_module.get ( metric ) [ output_a_np , output_b_np ] , elif isinstance ( x , list ) : for m in model.stateful_metric_functions : ins : list of tensors to be fed to ` f ` . self._feed_input_shapes ) if y.shape [ : sample_weight.ndim ] ! = sample_weight.shape : from keras.engine.training import _make_batches out = model.evaluate ( [ input_a_np , input_b_np ] , [ output_a_np , output_b_np ] , batch_size=4 ) sample_weight= [ sample_weight [ 1 ] , validation_data , val_f : Keras function to call for validation if weights is not None : def _uses_dynamic_learning_phase ( self ) : 'you can convert them to the expected format via : \n ' val_x , val_y , val_sample_weights = self._standardize_user_data ( str ( len ( x_weight ) ) metric_name = metric_fn.name str ( [ w.shape for w in weights ] ) ) mask = K.cast ( mask , K.floatx ( ) ) batch_ids = index_array [ batch_start : batch_end ] ' ` keras.utils.Sequence ` class . ' ) if batch_index == len ( batches ) - 1 : # Last batch . for name in output_names : # fit_generator will throw an exception if steps is unspecified for regular generator while True : ' Please specify ` steps ` or use the ' callbacks : List of callbacks to be called during training 'Provided ` ' + weight_type def _standardize_weights ( y , sample_weight=None , class_weight=None , steps_done = 0 for l , o in zip ( out_labels , outs ) : if verbose == 1 : out = model.fit_generator ( gen_data ( 4 ) , from .. import backend as K count_mode = 'samples ' * * kwargs ) : raise ValueError ( 'Found a sample_weight array for ' 'Error when checking ' + exception_prefix # Append to self.metrics_names , self.metric_tensors , ins : List of tensors to be fed to ` f ` raise TypeError ( 'Type of ` metrics ` argument not understood . ' ValueError : in case of improperly formatted user-provided data . self._make_predict_function ( ) workers=workers , TypeError : if an incorrect type is passed for the ` metrics ` argument . if x is None or len ( x ) == 0 : steps_per_epoch , if x is None or len ( x ) == 0 : This takes an array-like , or a list of def weighted ( y_true , y_pred , weights , mask=None ) : from .training_utils import check_loss_and_target_compatibility 'sample_weight_mode= '' temporal '' ' _callbacks = [ cbks.BaseLogger ( if sample_weight_mode is not None : return sample_weight self.metrics_tensors.append ( metric_result ) return weights set_w = set_of_lengths ( weights ) return np.append ( index_array , last_batch ) batch_size : integer batch size . Note that because this implementation val_x , val_y , val_sample_weights = model._standardize_user_data ( steps_done = 0 inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights if hasattr ( self , 'callback_model ' ) and self.callback_model : outs = f ( ins_batch ) val_outs = [ val_outs ] 'Timestep-wise sample weighting ( use of ' 'No data provided for `` ' + e.args [ 0 ] + ' '' . Need data ' j += 1 callbacks.on_epoch_end ( epoch , epoch_logs ) weighted_losses = [ ' ` type not understood : ' outs = [ ] np.random.random ( ( self.batch_size , 3 ) ) ] if workers > 0 : for l , o in zip ( out_labels , val_outs ) : targets : list of Numpy arrays of targets . y_classes = y ' : expected ' + names [ i ] + ' to have shape ' axis=list ( range ( weight_ndim , ndim ) ) ) output_generator = generator arrays have shapes that match the network 's expectations . if metric in ( 'accuracy ' , 'acc ' ) : if not hasattr ( generator_output , '__len__ ' ) : max_queue_size=10 , return [ out [ 0 ] for out in all_outs ] metric_name = metric_fn.name # stateful metrics ( i.e . metrics layers ) . 'metrics ' : callback_metrics or [ ] , if len ( output_names ) == 1 : outs [ 0 ] += batch_outs * len ( batch_ids ) if data and hasattr ( data [ 0 ] , 'shape ' ) : do_validation = bool ( validation_data ) # Handle data tensors support when no input given 'targets to have the same shape ' callback_model = self # ( because of class mode duality ) while epoch < epochs : is incompatible with an output . 'This loss expects ' sample_weight=sample_weight , 'should be either a list or a dict . ' raise ValueError ( 'All input arrays ( x ) should have ' return standardize_sample_or_class_weights ( sample_weight , elif isinstance ( x , list ) : val_f=None , slice_arrays ( y , split_at ) ) if i not in stateful_metric_indices : data = [ data [ x ] .values if data [ x ] .__class__.__name__ == 'DataFrame ' # avoid any explicit version checks # Returns before declaring predictions finished . 'sample_weight_mode= '' temporal '' ) is restricted to ' progbar.update ( steps_done ) 'Error when checking ' + exception_prefix elif ins and hasattr ( ins [ 0 ] , 'shape ' ) : ' class . Please specify ` steps_per_epoch ` or use ' # Returns val_sample_weight = None data = [ data ] str ( len ( data ) ) + ' arrays : ' + str ( data ) [ :200 ] + ' ... ' ) if steps_per_epoch is not None : for i in range ( len ( feed ) ) : 'steps ' : steps_per_epoch , out = model.fit_generator ( generator=RandomSequence ( 3 ) , steps_per_epoch=3 , epochs=5 , 'or ( x , y ) . Found : ' data : User-provided input data ( polymorphic ) . return _standardize_sample_or_class_weights ( sample_weight , output_generator = iter ( generator ) # Append to self.metrics_names , self.metric_tensors , epochs=1 , batch_size=2 , validation_split=0.5 ) data = [ np.asarray ( data ) ] _check_loss_and_target_compatibility ( [ a ] , [ losses.categorical_crossentropy ] , [ a.shape ] ) steps=None , from scipy.sparse import issparse if not isinstance ( val_outs , list ) : num_batches = ( size + batch_size - 1 ) // batch_size # round up # hdf5 datasets only support list objects as indices set_y = set_of_lengths ( targets ) outs [ 0 ] += batch_outs _slice_arrays ( sample_weights , split_at ) ) `` `` '' See docstring for ` Model.fit_generator ` . '' '' '' elif self.loss_functions [ i ] == losses.sparse_categorical_crossentropy : Scalar tensor . return [ None ] for l , o in zip ( out_labels , val_outs ) : verbose=verbose , callbacks=callbacks , ins : List of tensors to be fed to the Keras function . # step-size = 1 for data tensors val_enqueuer.stop ( ) try : raise ValueError ( and ` batch_size ` is not ` None ` because they are mutually self.output_names ) steps_per_epoch=steps_per_epoch , shape = ( num_samples , ) + batch_out.shape [ 1 : ] for _ in enumerate ( batch_outs ) : return training_arrays.predict_loop ( self , f , ins , steps=steps ) weights : list of Numpy arrays of sample weights . for step_index in range ( steps_per_epoch ) : str ( len ( output_names ) ) + ' outputs . ' `` `` '' Adds support for masking and sample-weighting to an objective function . from .training_utils import standardize_class_weights outs = self.train_on_batch ( x , y , if cls in class_weight ] ) name=name + '_target ' , return outs [ 0 ] ' should be specified . ' ) if set_x and set_y and list ( set_x ) [ 0 ] ! = list ( set_y ) [ 0 ] : weighted_masked_objective ( fn ) for fn in loss_functions ] nested_metrics = [ ] if batch_index == len ( batches ) - 1 : # Last batch . self._feed_targets 'If your targets are integer classes , ' val_outs = self._test_loop ( val_f , val_ins , outs = [ outs ] output_names : List of output names ( strings ) in the model . callbacks.on_batch_begin ( batch_index , batch_logs ) return [ x_weight [ output_names [ 0 ] ] ] for target_dim , out_dim in zip ( y.shape [ 1 : ] , shape [ 1 : ] ) : outs = f ( ins ) if not hasattr ( generator_output , '__len__ ' ) : ` `` temporal '' ` indicated that we expect 2D weight data if isinstance ( ins [ -1 ] , float ) : callback_metrics : List of strings , the display names of the metrics metric_name = metric_fn.__name__ When steps is ` None ` , returns the number of samples to be `` `` '' shape = shape [ 1 : ] str ( metrics ) ) # Same labels assumed . batch_size = list ( x.values ( ) ) [ 0 ] .shape [ 0 ] def _slice_arrays ( arrays , start=None , stop=None ) : exception_prefix : String prefix used for exception formatting . inputs : list of Numpy arrays of inputs . val_outs = self.evaluate ( return arrays [ start ] outs [ i ] += batch_out * len ( batch_ids ) if steps_done > = steps_per_epoch and do_validation : ' while using as loss ` ' + loss.__name__ + ' ` . ' out = model.evaluate ( [ input_a_np , input_b_np ] , check_batch_axis=False , val_outs = [ val_outs ] raise ValueError ( 'Sample_weight arrays should have ' str ( generator_output ) ) batch_size = 1 x , _ , _ = generator_output training_updates self._feed_output_names ) str ( [ x.shape for x in inputs ] ) ) batch_size = x [ 0 ] .shape [ 0 ] elif isinstance ( x , list ) : for i , batch_out in enumerate ( batch_outs ) : arrays [ start : stop ] if ` arrays ` is an array-like nested_weighted_metrics = collect_metrics ( weighted_metrics , updates = ( self.updates return averages [ 0 ] # we will see , we can not pre-allocate return np.concatenate ( unconcatenated_outs [ 0 ] , axis=0 ) callback_metrics : List of strings , the display names of the metrics batch_logs [ l ] = o for i in range ( len ( names ) ) : return [ ] weighted_loss = weighted_masked_objective ( losses.get ( 'mae ' ) ) slice_arrays ( x , split_at ) ) if sample_weight_mode ! = 'temporal ' : # to the number of unmasked samples . elif len ( names ) == 1 : return [ np.concatenate ( unconcatenated_outs [ i ] , axis=0 ) for line in f : raise TypeError ( ' ` categorical_crossentropy ` expects ' if isinstance ( data , dict ) : mask=masks [ i ] ) ' Please consider using the ` keras.utils.Sequence ' if issparse ( ins [ i ] ) and not K.is_sparse ( feed [ i ] ) : nested_metrics = _collect_metrics ( metrics , self.output_names ) elif isinstance ( class_weight , dict ) : def _check_num_samples ( self , ins , batch_size=None , steps=None , steps_name='steps ' ) : updates=updates , do_validation = True 'at least one item . ' ) yield ( [ np.random.random ( ( batch_sz , 3 ) ) , np.random.random ( ( batch_sz , 3 ) ) ] , model.compile ( optimizer , loss='mse ' , sample_weight_mode= { 'dense_1 ' : 'temporal ' } ) 'class_weight ' ) shuffle=shuffle ) if not isinstance ( output_metrics , list ) : # apply sample weighting # to the number of unmasked samples . progbar.update ( batch_end ) if batch_size is not None : def _make_batches ( size , batch_size ) : str ( len ( shape ) ) + ' dimensions , but got array ' shuffle=shuffle , y_classes = np.argmax ( y , axis=1 ) the outputs of ` f ` 'when doing step-wise ' self._feed_input_shapes , return None val_sample_weight = None self._feed_loss_fns ) : x , _ = generator_output while epoch < epochs : raise ValueError ( 'Received an empty batch . ' steps = len ( generator ) epoch_logs [ 'val_ ' + l ] = o for out in outs : 'No data provided for `` ' + e.args [ 0 ] + ' '' . Need data ' if isinstance ( ins [ -1 ] , float ) : arrays : Single array or list of arrays . self._feed_output_shapes ) batch_index += 1 if len ( set_w ) > 1 : out = model.predict_on_batch ( { 'input_a ' : input_a_np , 'input_b ' : input_b_np } ) # Arguments return ( [ np.random.random ( ( self.batch_size , 3 ) ) , steps : Total number of steps ( batches of samples ) out = model.fit_generator ( generator=RandomSequence ( 3 ) , epochs=5 , # Sample-based predictions . 'you should pass a 2D sample_weight array . ' ) ValueError : In case of invalid user-provided argument . # No need for try/except because single_output_model.compile ( optimizer , loss , batch_size : integer . for y , loss , shape in zip ( targets , loss_fns , output_shapes ) : # python 2 has 'next ' , 3 has '__next__ ' # create numpy arrays of input data concatenation of list the display names of the outputs of 'This loss expects ' callbacks.set_params ( { % ( existing_classes - existing_class_weight ) ) steps_per_epoch=10000 , epochs=10 ) outs [ i ] = batch_out ins_batch = slice_arrays ( raise ValueError ( ' ` class_weight ` must contain all classes in the data . ' index_array = index_array.reshape ( ( batch_count , batch_size ) ) if metric in ( 'accuracy ' , 'acc ' , 'crossentropy ' , 'ce ' ) : str ( data_shape ) ) validation_data= ( [ input_a_np , input_b_np ] , [ output_a_np , output_b_np ] ) ) metric_fn = metrics_module.categorical_crossentropy # Since we do not know how many samples generator , training_utils.check_array_length_consistency ( a_np , a_np , None ) class_weight=class_weight ) m.reset_states ( ) x_weight : User-provided ` sample_weight ` or ` class_weight ` argument . from .training_utils import check_num_samples sample_weight_mode=None , weighted_metrics=None , batches = make_batches ( len ( x_test ) , batch_size ) score_array = fn ( y_true , y_pred ) shape = ( num_samples , ) + batch_out.shape [ 1 : ] if str ( name ) in model.stateful_metric_names ] out = model.predict_generator ( verbose=0 ) [ a ] , [ losses.categorical_crossentropy ] , [ ( 2 , 3 , 6 ) ] ) with signature ` fn ( y_true , y_pred ) ` . if len ( self.output_names ) > 1 : ' The classes % s exist in the data but not in ' str ( sample_weight.shape ) + ' . ' `` `` '' Checks the number of samples provided for training and evaluation . elif len ( names ) > 1 : verbose=verbose , steps = len ( generator ) ' The classes % s exist in the data but not in ' UserWarning ( 'Using a generator with ` use_multiprocessing=True ` ' else x for x in data ] ' ` sparse_categorical_crossentropy ` instead , ' ' for an input with shape ' x , y , sample_weight = generator_output if len ( x_weight ) ! = len ( output_names ) : size : Integer , total size of the data to slice into batches . name='test_function ' , val_f : Keras function to call for validation data = [ np.expand_dims ( x , 1 ) if x is not None and x.ndim == 1 # return a set with the variation between or a list/array of indices elif isinstance ( data , list ) : raise ValueError ( ' ` validation_data ` should be a tuple ' ' a tuple ` ( x , y , sample_weight ) ` ' self.metrics_names.append ( metric_name ) shape = shapes [ i ] ` `` temporal '' ` indicated that we expect 2D weight data `` `` '' Does validation on the compatibility of targets and loss functions . This helps prevent users from using loss functions incorrectly . # Same labels assumed . ' 3+ dimensional targets . ' ) ValueError : In case of invalid arguments . and ` batch_size ` is not ` None ` because they are mutually callbacks=callbacks , finally : enqueuer.stop ( ) def evaluate_generator ( self , generator , 'in compile ( ) . If you just mean to use ' batch_sizes = [ ] str ( len ( x_weight ) ) return [ None ] steps=steps , ' ` categorical_crossentropy ` expects ' except KeyError as e : weight_type : A string used purely for exception printing . metric_fn = metrics_module.binary_accuracy x_weights.append ( x_weight.get ( name ) ) validation_steps=validation_steps , outs.append ( np.zeros ( shape , dtype=batch_out.dtype ) ) i for i , name in enumerate ( model.metrics_names ) epochs=1 , if is_sequence : batches = make_batches ( num_samples , batch_size ) if mask is not None : if out_dim is not None and target_dim ! = out_dim : y : Numpy array of model targets to be weighted . ` [ [ binary_accuracy , binary_crossentropy ] , [ binary_accuracy ] ] ` 'less than or equal to ' + str ( len ( y.shape ) ) ) ` start ` was a list . When steps is ` None ` , returns the number of samples to be callbacks=callbacks , while metric_name in self.metrics_names : y_classes = np.argmax ( y , axis=1 ) if do_validation and not val_gen : workers=1 , batch_size = 1 return self._test_loop ( f , ins , verbose=0 ) out_labels = out_labels or [ ] 'pass shuffle= '' batch '' . ' ) outs = self.test_on_batch ( x , y , sample_weight=sample_weight ) or list of scalars ( if the model has multiple outputs not validation_steps ) : before declaring ` _predict_loop ` finished . model._make_test_function ( ) losses.categorical_crossentropy } if loss is losses.categorical_crossentropy : if not isinstance ( val_outs , list ) : if callback_model.stop_training : # Do not slice the training phase flag . ( ins [ 0 ] .shape [ 0 ] , val_ins [ 0 ] .shape [ 0 ] ) ) if isinstance ( data , dict ) : ` f ` and the list of display names of the outputs of ` f_val ` . exception_prefix + ' : ' A list of ` sample_weight ` or ` class_weight ` where there are exactly elif metric in ( 'crossentropy ' , 'ce ' ) : def generate_arrays_from_file ( path ) : 'val_ ' + n for n in out_labels ] val_outs = self._test_loop ( val_f , val_ins , return arrays [ start : stop ] epoch_logs [ 'val_ ' + l ] = o 'as the output . ' ) if steps_per_epoch is None : use_multiprocessing=False , epoch_logs = { } ' : expected ' + names [ i ] + ' to have ' 'metrics ' : callback_metrics , sample_weight : User-provided ` sample_weight ` argument . ' while using as loss ` categorical_crossentropy ` . ' `` `` '' Trains the model on data generated batch-by-batch by a Python generator or an instance of ` Sequence ` . data = [ data ] sample_weight = None self._make_test_function ( ) # custom handling of accuracy/crossentropy callbacks.on_train_end ( ) self._feed_input_shapes ) for step in range ( steps ) : if not isinstance ( val_outs , list ) : ' generator based on the ` keras.utils.Sequence ` ' outs [ 0 ] += batch_outs metric_fn = metrics_module.sparse_categorical_crossentropy output_shape = self._internal_output_shapes [ i ] return model.history batch_logs [ 'size ' ] = len ( batch_ids ) return K.mean ( score_array ) model.fit_generator ( generate_arrays_from_file ( '/my_file.txt ' ) , def test_loop ( model , f , ins , batch_size=None , verbose=0 , steps=None ) : # Handle data tensors support when no input given from keras.engine.training import _check_loss_and_target_compatibility break hasattr ( validation_data , '__next__ ' ) or steps=validation_steps , initial_epoch=initial_epoch , 'verbose ' : verbose , def test_check_array_lengths ( ) : if validation_steps : 'Found : ' + str ( sample_weight_mode ) ) model.compile ( optimizer , loss='mse ' , loss_weights= { 'dense_1 ' : 0.2 , 'dropout ' : 0.8 } ) batch_index += 1 if not isinstance ( val_outs , list ) : callback_model = self.callback_model batch_size = list ( x.values ( ) ) [ 0 ] .shape [ 0 ] generator , # create numpy arrays of input data batch_size = x [ 0 ] .shape [ 0 ] enqueuer.start ( workers=workers , max_queue_size=max_queue_size ) steps_per_epoch = len ( generator ) and/or metrics ) . The attribute ` model.metrics_names ` will give you raise ValueError ( 'Can only use ` validation_steps ` ' if shuffle == 'batch ' : if not isinstance ( val_outs , list ) : from .. import callbacks as cbks if sample_weight is not None : return self._predict_loop ( f , ins , batch_size=batch_size , callbacks=None , callbacks : List of callbacks to be called during training [ a ] , [ losses.categorical_crossentropy ] , [ a.shape ] ) '\n ' sample_weight_mode= { 'dense_1 ' : 'temporal ' } ) nested_metrics = collect_metrics ( metrics , self.output_names ) if not metrics : raise ValueError ( 'All target arrays ( y ) should have ' def fit_generator ( model , if len ( outs ) == 1 : 'sample-wise weights , make sure your ' if isinstance ( metric_fn , Layer ) and metric_fn.stateful : from .. utils.data_utils import OrderedEnqueuer sample_weight_mode : One of ` None ` or ` `` temporal '' ` . target = K.placeholder ( target = K.placeholder ( ndim=len ( shape ) , It transforms an objective function ` fn ( y_true , y_pred ) ` use_multiprocessing=use_multiprocessing , ' generator based on the ` keras.utils.Sequence ` ' 'Error when checking model ' + exception_prefix y : Numpy array of model targets to be weighted . `` `` '' Part of the training engine related to plain array data ( e.g . Numpy ) . ' elements , but the model has ' from keras.engine.training import _check_array_lengths for batch_out in batch_outs : single_output_model.compile ( optimizer , loss , metrics= [ ] , sample_weight_mode=None ) sample_weight=val_sample_weights , ' while using as loss ` ' + loss.__name__ + ' ` . ' from .training_utils import standardize_sample_weights num_samples = None Note that because this implementation relies on multiprocessing , data = [ np.expand_dims ( x , 1 ) if x is not None and x.ndim == 1 else x for x in data ] `` `` '' Maps metric functions to model outputs . callbacks.set_model ( callback_model ) callback_metrics = copy.copy ( out_labels ) + [ # avoid any explicit version checks import warnings batch_size = list ( x.values ( ) ) [ 0 ] .shape [ 0 ] batch_size=batch_size , # to reshape we need to be cleanly divisible by batch size exception_prefix='target ' ) return num_samples val_f=val_f , val_ins=val_ins , shuffle=shuffle , ins_batch = _slice_arrays ( ins [ : -1 ] , batch_ids ) + [ ins [ -1 ] ] epoch_logs [ 'val_ ' + l ] = o outs.append ( np.zeros ( shape , dtype=batch_out.dtype ) ) training_utils.check_array_length_consistency ( [ None ] , [ None ] , [ None ] ) return [ np.concatenate ( out ) for out in all_outs ] batch_size : Integer , batch size . if not is_sequence and use_multiprocessing and workers > 1 : outs [ i ] [ batch_start : batch_end ] = batch_out val_x , val_y = validation_data exception_prefix= '' ) : batch_size=batch_size , # if steps is unspecified for regular generator enqueuer.start ( workers=workers , max_queue_size=max_queue_size ) or as a single array . We normalize this to an ordered list of metric_fn = metrics_module.categorical_accuracy callback_metrics = out_labels + [ 'val_ ' + n for n in out_labels ] nested_metrics.append ( output_metrics ) class_weight=class_weight , x_weights.append ( x_weight.get ( name ) ) if callback_model.stop_training : if len ( all_outs ) == 1 : last_batch = index_array [ batch_count * batch_size : ] def fit_loop ( model , f , ins , slice_arrays ( input_a , 0 , 1 ) index_array = _batch_shuffle ( index_array , batch_size ) str ( validation_data ) ) for m in model.stateful_metric_functions : # self.stateful_metric_names progbar.update ( step + 1 ) callback_metrics=callback_metrics , 'Found ' + str ( list ( set_x ) [ 0 ] ) + ' input samples ' batch_logs [ 'batch ' ] = batch_index metric_name = metric_name_prefix + suffix weighted_loss = _weighted_masked_objective ( losses.get ( 'mae ' ) ) 'of shape ( samples , classes ) . ' for batch_index , ( batch_start , batch_end ) in enumerate ( batches ) : wait_time = 0.01 return data steps_per_epoch=10 , the display labels for the scalar outputs . if enqueuer is not None : output_generator = iter ( generator ) The ` index_array ` array , shuffled in a batch-wise fashion . metric_name = metric_name_prefix + metric_name x , y = generator_output # Sample-based predictions . metrics : a list or dict of metric functions . if ref_dim ! = dim and ref_dim : output_metrics = [ output_metrics ] max_queue_size=max_queue_size ) workers=workers , ValueError : in case of incorrectly formatted data . updates=self.state_updates + self.metrics_updates , shuffle=shuffle ) if hasattr ( metric_fn , 'name ' ) : verbose=0 ) sample_weight : User-provided ` sample_weight ` argument . if len ( generator_output ) == 2 : raise ValueError ( 'Provided ` ' + weight_type + ' ` was a list of ' ' elements , but the model has ' `` `` '' Performs sample weight validation and standardization . stateful_metric_indices = [ ] `` `` '' Shuffles an array in a batch-wise fashion . steps_per_epoch : Total number of steps ( batches of samples ) nested_metrics.append ( output_metrics ) verbose=verbose , batch_size=batch_size , epochs=epochs , ' ( x , y , sample_weight ) ' epoch_logs = { } return training_generator.predict_generator ( if len ( weights ) ! = len ( y_classes ) : ( useful for resuming a previous training run ) A list of tuples of array indices . # Construct epoch logs . outs [ 0 ] += batch_outs * len ( batch_ids ) if hasattr ( start , '__len__ ' ) : 'sample_weight_mode= '' temporal '' ' loss=None , self.metrics_updates += metric_fn.updates outs = model.train_on_batch ( x , y , use_multiprocessing=False , in zip ( y , ValueError : when ` steps ` is ` None ` and the attribute ` ins.shape ` ' class . ' ) ) `` `` '' Does validation on the compatibility of targets and loss functions . if steps is None : return x_weight from . import training_arrays break validation_data=validation_data , for i in range ( len ( outs ) ) : verbose=0 ) : # we stash extra items and reappend them after shuffling batch_sizes = [ ] if len ( averages ) == 1 : `` `` '' sample_weights = [ _standardize_weights ( ref , sw , cw , mode ) if isinstance ( generator_output , tuple ) : from .training_utils import collect_metrics 'Provided ` ' + weight_type from .training_utils import make_batches arrays : Single array or list of arrays . str ( len ( shape ) ) + ' dimensions , but got array ' return outs if steps_per_epoch is None : raise ValueError ( 'Found a sample_weight array with shape ' x , y = generator_output 'expected no data , but got : ' , data ) return training_generator.evaluate_generator ( steps=steps , validation_steps=None , arrays ( same order as ` names ` ) , while checking that the provided 'the same number of samples as target arrays . ' if shapes [ i ] is not None : training_utils.check_loss_and_target_compatibility ( 'sample_weight can not be broadcast . ' ) outs_per_batch.append ( outs ) callbacks.on_epoch_begin ( epoch ) if steps is not None : np.random.shuffle ( index_array ) epochs=epochs , steps_per_epoch=None , outs = f ( ins ) def predict_generator ( self , generator , data_shape = data [ i ] .shape initial_epoch=0 , A function with signature ` fn ( y_true , y_pred , weights , mask ) ` . val_ins : List of tensors to be fed to ` val_f ` raise ValueError ( ' ` class_weight ` not supported for ' if not isinstance ( val_outs , list ) : str ( list ( set_w ) [ 0 ] ) + ' target samples . ' ) and/or metrics ) . The attribute ` model.metrics_names ` will give you self._feed_input_names , finally : def generate_arrays_from_file ( path ) : 'You should provide one ` ' + weight_type + ' ` ' callback_metrics = out_labels + [ 'val_ ' + n for n in out_labels ] return np.ones ( ( y.shape [ 0 ] , ) , dtype=K.floatx ( ) ) index_array = np.arange ( num_train_samples ) initial_epoch=initial_epoch , with signature ` fn ( y_true , y_pred ) ` . except KeyError as e : if not names : check_batch_axis=True , 'as the output . ' ) return nested_metrics epochs=100 , verbose=1 , callbacks=None , exception_prefix='input ' ) check_batch_axis=True , `` `` '' Part of the training engine related to Python generators of array data . 'Expected a list or dictionary , found : ' # step-size = 1 for data tensors self._feed_sample_weight_modes.append ( self.sample_weight_modes [ i ] ) initial_epoch=initial_epoch ) outs = f ( ins_batch ) # - * - coding : utf-8 - * if not names : # prepare callbacks return averages index_array : array of indices to be shuffled . inputs , elif len ( generator_output ) == 3 : array-likes , and outputs : val_x , val_y , val_sample_weight ) ' based on the ` keras.utils.Sequence ` class . ' cbk.validation_data = val_data def test_check_array_length_consistency ( ) : def _standardize_input_data ( data , names , shapes=None , one element per model output . _slice_arrays ( input_a , 0 ) _check_array_lengths ( x , y , sample_weights ) def _predict_loop ( self , f , ins , batch_size=32 , verbose=0 , steps=None ) : model._make_test_function ( ) output_shapes : list of shapes of model outputs . batch_logs [ 'size ' ] = batch_size epoch = initial_epoch for i , batch_out in enumerate ( batch_outs ) : 'and ' + str ( list ( set_y ) [ 0 ] ) + ' target samples . ' ) str ( len ( output_names ) ) + ' outputs . ' return [ x_weight ] return outs [ 0 ] all_outs [ i ] .append ( out ) # apply sample weighting return _standardize_sample_or_class_weights ( class_weight , for batch_out in enumerate ( batch_outs ) : y_pred : ` y_pred ` argument of ` fn ` . hasattr ( ins [ 0 ] , 'shape ' ) and hasattr ( val_ins [ 0 ] , 'shape ' ) ) : outs [ i ] /= num_samples callback_metrics=callback_metrics , self._feed_input_shapes , # Compatibility with the generators sample_weight= [ sample_weight [ 1 ] , sample_weight [ 1 ] [ :2 ] ] ) 'outputs that are at least 3D , i.e . that have ' # Reset stateful metrics return [ np.concatenate ( unconcatenated_outs [ i ] , axis=0 ) for l , o in zip ( out_labels , val_outs ) : A numpy array of target weights , one entry per sample to weight . batch_size=None , arrays [ start : stop ] if ` arrays ` is an array-like ins : List of tensors to be fed to the Keras function . raise ValueError ( elif len ( names ) > 1 : 'training , i.e . ` steps_per_epoch ` ' we want to compute `` binary_accuracy '' and `` binary_crossentropy '' , m.reset_states ( ) return data 'You are passing a target array of shape ' + str ( y.shape ) validation_steps : Number of steps to run validation for if not isinstance ( outs , list ) : str ( generator_output ) ) batches = make_batches ( num_train_samples , batch_size ) 'the same number of samples . Got array shapes : ' return np.ones ( ( y.shape [ 0 ] , ) , dtype=K.floatx ( ) ) print ( 'Train on % d samples , validate on % d samples ' % Ignored with the default value of ` None ` . inputs : list of Numpy arrays of inputs . model.compile ( optimizer , loss='mse ' , sample_weight_mode= { 'lstm ' : 'temporal ' } ) # Prepare inputs , delegate logic to ` test_loop ` . steps_name : The public API 's parameter name for ` steps ` . self._feed_sample_weight_modes ) ] count_mode , weights : list of Numpy arrays of sample weights . count_mode = 'steps ' elif self.loss_functions [ i ] == losses.sparse_categorical_crossentropy : return outs raise ValueError ( 'Found a sample_weight array for ' ' a tuple ` ( x , y , sample_weight ) ` ' data_shape = data [ i ] .shape def standardize_class_weights ( class_weight , output_names ) : weights : Weights tensor . weighted_losses = [ _weighted_masked_objective ( fn ) for fn in loss_functions ] `` `` '' Abstract fit function for ` f ( ins ) ` . # Delegate logic to ` _fit_loop ` . outs [ i ] /= num_samples # stateful metrics ( i.e . metrics layers ) . before declaring one epoch finished and starting the generator_output = next ( output_generator ) val_f=val_f , batches = _make_batches ( num_train_samples , batch_size ) `` `` '' Normalizes inputs and targets provided by users . for l , o in zip ( out_labels , val_outs ) : self , generator , return [ copy.copy ( metrics ) for _ in output_names ] ins : list of tensors to be fed to ` f ` . self.metrics_updates ) isinstance ( validation_data , Sequence ) ) return x_weights * * self._function_kwargs ) for l , o in zip ( out_labels , val_outs ) : use_multiprocessing=use_multiprocessing ) x_weight : User-provided ` sample_weight ` or ` class_weight ` argument . `` `` '' Training-related utilities . y = standardize_input_data ( y , callbacks.on_epoch_begin ( epoch ) yield ( [ np.random.random ( ( batch_sz , 3 ) ) , verbose : verbosity mode . # Instead , we store one array per batch seen def fit_generator ( self , generator , # Prepare display labels . # we stash extra items and reappend them after shuffling * * self._function_kwargs ) batch_size : integer batch size . 'Error when checking ' + exception_prefix batch_size : Integer batch size or ` None ` if not defined . slice_arrays ( input_a , stop=2 ) elif shuffle : raise ValueError ( 'In order to use timestep-wise sample weights , ' data = [ data [ x ] .values if data [ x ] .__class__.__name__ == 'DataFrame ' else data [ x ] for x in names ] passed to the callbacks . They should be the progbar = Progbar ( target=steps ) batch_logs [ 'size ' ] = 1 output_names : a list of the names ( strings ) of model outputs . batch_size = x.shape [ 0 ] arrays ( same order as ` names ` ) , while checking that the provided return [ ] enqueuer = GeneratorEnqueuer ( generator , Useful for shuffling HDF5 arrays arrays have shapes that match the network 's expectations . if do_validation : if verbose : # Pre-allocate the results arrays . metric_fn = metrics_module.sparse_categorical_accuracy if fn is None : fn : The objective function to wrap , def _fit_loop ( self , f , ins , out_labels=None , batch_size=None , elif isinstance ( arrays , list ) : 'or use the ` keras.utils.Sequence ` class . ' ) if batch_index == 0 : targets : list of Numpy arrays of targets . if do_validation : except TypeError : for metric in metrics : raise ValueError ( 'If ' + steps_name if x is None : sample_weight=None , return standardize_sample_or_class_weights ( class_weight , callbacks.on_batch_end ( step_index , batch_logs ) if sample_weight is not None and len ( sample_weight.shape ) ! = 1 : if isinstance ( ins [ -1 ] , float ) : f : Keras function returning a list of tensors . # Reset stateful metrics for step in range ( steps ) : metric_fn = metrics_module.binary_crossentropy existing_classes = set ( y_classes ) for epoch in range ( initial_epoch , epochs ) : break try : # Same labels assumed . if sample_weight_mode ! = 'temporal ' : workers=1 , if enqueuer is not None : if not is_sequence and use_multiprocessing and workers > 1 : if verbose and ins and hasattr ( ins [ 0 ] , 'shape ' ) and hasattr ( val_ins [ 0 ] , 'shape ' ) : batch_outs = f ( ins ) while True : return [ [ ] for _ in output_names ] `` `` '' See docstring for ` Model.evaluate_generator ` . '' '' '' return x_weight epoch += 1 y_pred : ` y_pred ` argument of ` fn ` . 'your model is not the size the model expected . ' mask : Mask tensor . num_samples = check_num_samples ( ins , x , y = generator_output updates = self.updates + training_updates + self.metrics_updates callbacks.on_batch_begin ( step_index , batch_logs ) steps : Total number of steps ( batches of samples ) generator , # Keep track of state updates created by ' a defined shape , or ' + steps_name if shuffle == 'batch ' : if val_gen : validation_data=RandomSequence ( 4 ) , def batch_shuffle ( index_array , batch_size ) : if not check_batch_axis : def standardize_sample_or_class_weights ( x_weight , outs = self.predict_on_batch ( x ) if shapes [ i ] is not None : if issparse ( ins [ i ] ) and not K.is_sparse ( self._feed_inputs [ i ] ) : str ( list ( set_y ) [ 0 ] ) + ' input samples and ' for i in indices_for_conversion_to_dense : weights : Weights tensor . ` batch_size ` is ` None ` , returns ` None ` . check_batch_axis=False ) shuffle=True , name='train_function ' , x , y , sample_weight = generator_output List of standardized input arrays ( one array per model input ) . val_outs = self.evaluate_generator ( x , _ = generator_output class_weight : User-provided ` class_weight ` argument . elif isinstance ( class_weight , dict ) : elif len ( generator_output ) == 3 : target_tensors=None , weight_type ) : shuffle : Whether to shuffle the data at the beginning of each epoch enqueuer = None `` `` '' Maps ` sample_weight ` or ` class_weight ` to model outputs . `` `` '' See docstring for ` Model.predict_generator ` . '' '' '' output_metrics = metrics.get ( name , [ ] ) from .. utils.data_utils import Sequence cbk.validation_data = val_ins class_weights = _standardize_class_weights ( class_weight , str ( len ( data ) ) + ' arrays : ' + str ( data ) [ :200 ] + ' ... ' ) ins_batch [ i ] = ins_batch [ i ] .toarray ( ) batch_size = 1 batch_logs [ 'batch ' ] = step_index if val_enqueuer is not None : # mask should have the same shape as score_array return nested_metrics elif isinstance ( x , dict ) : weighted_function = training_utils.weighted_masked_objective ( ValueError : In case of invalid user-provided arguments . _callbacks.append ( # to reshape we need to be cleanly divisible by batch size # Assumes a generator that only # Arguments from keras.engine.training import _weighted_masked_objective if val_enqueuer is not None : score_array /= K.mean ( K.cast ( K.not_equal ( weights , 0 ) , K.floatx ( ) ) ) metric_fn = metrics_module.binary_crossentropy if cls in class_weight ] ) if batch_index == 0 : ' class . ' ) ) List of standardized input arrays ( one array per model input ) . `` `` '' Abstract method to loop over some data in batches . # and concatenate them upon returning . ValueError : In case of invalid user-provided arguments . str ( sample_weight.shape ) + ' for an input with shape ' the batch axis of the arrays matches the expected for dim , ref_dim in zip ( data_shape , shape ) : if step == 0 : _check_array_lengths ( a_np , a_np , a_np ) ` start ` was a list . ` [ [ binary_accuracy , binary_crossentropy ] , [ binary_accuracy ] ] ` wait_time=wait_time ) raise ValueError ( ' ` steps=None ` is only valid for a generator ' self._feed_output_names ) if do_validation : return K.mean ( score_array ) ' ` ( val_x , val_y , val_sample_weight ) ` ' initial_epoch : Epoch at which to start training ' class . Please specify ` validation_steps ` or use ' 'an input with shape ' _callbacks += ( callbacks or [ ] ) + [ self.history ] from keras.engine.training_utils import weighted_masked_objective steps_done += 1 yield ( { 'input_1 ' : x1 , 'input_2 ' : x2 } , { 'output ' : y } ) 'y_binary = to_categorical ( y_int ) \n ' `` `` '' Maps metric functions to model outputs . _check_array_lengths ( [ a_np , a_np ] , [ a_np , a_np ] , [ a_np , a_np ] ) callback_model = model first input numpy array . When steps is not ` None ` and if len ( sample_weight.shape ) > len ( y.shape ) : if i not in stateful_metric_indices : 'outputs that are at least 3D , i.e . that have ' # Do not slice the training phase flag . from keras.engine.training_utils import make_batches data = [ x.values if x.__class__.__name__ == 'DataFrame ' else x for x in data ] `` `` '' Performs sample weight validation and standardization . elif isinstance ( x , dict ) : The number of samples is not defined when running with ` steps ` , ValueError : in case of incorrectly formatted data . str ( [ y.shape for y in targets ] ) ) # and in general ` x [ 0 ] .shape [ 0 ] ! = self._feed_input_shapes [ 0 ] [ 0 ] ` all_outs.append ( [ ] ) 'when doing step-wise ' weight_ndim = K.ndim ( weights ) for epoch in range ( initial_epoch , epochs ) : verbose=verbose ) unconcatenated_outs.append ( [ ] ) the list would look like : if steps_done == 1 : 'Error when checking model ' + exception_prefix existing_class_weight = set ( class_weight.keys ( ) ) return training_arrays.test_loop ( self , f , ins , def compile ( self , optimizer , _check_array_lengths ( a_np , None , None ) `` `` '' Normalizes inputs and targets provided by users . callbacks.on_train_begin ( ) training_utils.check_array_length_consistency ( [ a_np ] , [ None ] , None ) else : # the loss per batch should be proportional do_validation = False # we find beforehand the arrays that need conversion . if loss in key_losses : model.train_on_batch ( [ input_a_np , input_b_np ] , return all_outs [ 0 ] [ 0 ] # To prevent a slowdown , we find beforehand the arrays that need conversion . for output_shape , loss_fn in zip ( self._feed_output_shapes , self._feed_loss_fns ) : losses.categorical_crossentropy } return [ x_weight ] if set_x and set_y and list ( set_x ) [ 0 ] ! = list ( set_y ) [ 0 ] : This helps prevent users from using loss functions incorrectly . return np.concatenate ( unconcatenated_outs [ 0 ] , axis=0 ) 'Numpy arrays . Found : ' + str ( data ) [ :200 ] + ' ... ' ) sample_weight_mode=None , all_outs [ i ] .append ( out ) # custom handling of accuracy/crossentropy if steps_done > = steps_per_epoch and do_validation : # the loss per batch should be proportional ' ` class_weight ` . ' model.history = cbks.History ( ) # Need range check here as filling of the queue depends on sleep in the enqueuers all_outs.append ( [ ] ) steps_name='steps_per_epoch ' ) `` `` '' the display labels for the scalar outputs . score_array * = mask elif metric in ( 'crossentropy ' , 'ce ' ) : while steps_done < steps : val_enqueuer = None if data is not None and hasattr ( data , '__len__ ' ) and len ( data ) : if steps is not None : raise TypeError ( val_x , val_y , val_sample_weight = validation_data class_weight : User-provided ` class_weight ` argument . val_f=None , val_ins=None , shuffle=True , shuffle=shuffle , str ( data ) [ :200 ] ) 'Found ' + str ( list ( set_x ) [ 0 ] ) + ' input samples ' batch_index = 0 def _collect_metrics ( metrics , output_names ) : ' a time dimension . ' ) # reduce score_array to same ndim as weight array batch_size = x.shape [ 0 ] max_queue_size=max_queue_size , outs = [ outs ] steps_name='steps ' ) : def testslice_arrays ( ) : ndim = K.ndim ( score_array ) 'but the model expects a list of ' + str ( len ( names ) ) index_array = index_array [ : batch_count * batch_size ] validation_steps=validation_steps ) for i in range ( len ( unconcatenated_outs ) ) ] self._feed_sample_weights ) value found in ` shapes ` . This takes an array-like , or a list of `` `` '' Wrapper function . 'sample-wise weights , make sure your ' sample_weight=sample_weight , validation_steps=3 , callbacks= [ tracker_cb ] ) and just `` binary_accuracy '' for the second output , return weighted val_ins : List of tensors to be fed to ` val_f ` set_x = set_of_lengths ( inputs ) # Same labels assumed . _check_array_lengths ( [ a_np ] , [ None ] , None ) try : m.reset_states ( ) output_generator = enqueuer.get ( ) self.metrics_updates += metric_fn.updates raise ValueError ( 'All sample_weight arrays should have ' x = _standardize_input_data ( x , self._feed_input_names , val_ins=None , 'which does expect integer targets . ' ) # Returns return [ None for _ in range ( len ( names ) ) ] self.test_function = K.function ( if len ( generator_output ) == 2 : callbacks.set_model ( callback_model ) for dim , ref_dim in zip ( data_shape , shape ) : val_outs = [ val_outs ] stop : integer ( stop index ) ; should be None if A slice of the array ( s ) . metric_fn = metrics_module.binary_accuracy enqueuer = OrderedEnqueuer ( generator , training_utils.check_array_length_consistency ( a_np , a_np , a_np ) if ( output_shape [ -1 ] == 1 or cbk.validation_data = val_ins ' was passed for an output of shape ' + str ( shape ) unconcatenated_outs = [ ] if callback_model.stop_training : 'sample_weight_mode= '' temporal '' ) is restricted to ' ' A target array with shape ' + str ( y.shape ) # Prepare data for validation # case : categorical accuracy/crossentropy 'but the model expects a list of ' + str ( len ( names ) ) output_shapes , 'dropout ' : 0.8 } ) out = single_output_model.predict_generator ( # it 's possible to callback a different model than itself 'do_validation ' : do_validation , batch_sizes.append ( batch_size ) 'array per model output . ' ) out_labels=None , # with sparse targets 'steps_per_epoch ' ) # prepare callbacks import copy steps_per_epoch=None , steps=None , outs [ i ] = float ( batch_out ) for _ in enumerate ( batch_outs ) : out_labels = self.metrics_names batch_logs = { } batch_size : Integer batch size or ` None ` if not defined . feed = self._feed_inputs + self._feed_targets + self._feed_sample_weights return [ None ] or as a single array . We normalize this to an ordered list of else : with K.name_scope ( metric_name ) : str ( y.shape ) + ' . ' elif len ( validation_data ) == 3 : weights = np.asarray ( [ class_weight [ cls ] for cls in y_classes ins_batch [ i ] = ins_batch [ i ] .toarray ( ) if len ( generator_output ) == 2 : return set ( [ 0 if y is None else y.shape [ 0 ] for y in x ] ) if len ( set_y ) > 1 : _check_array_lengths ( [ None ] , [ None ] , [ None ] ) ' ( x , y , sample_weight ) ' if isinstance ( metrics , list ) : if verbose == 1 : break # Raises return None if ( val_gen and not isinstance ( validation_data , Sequence ) and batch_size=batch_size , ' the ` keras.utils.Sequence ` class . ' ) 'verbose ' : verbose , epochs=100 , Scalar tensor . if hasattr ( model , 'callback_model ' ) and model.callback_model : if steps_per_epoch is None : epochs : Number of times to iterate over the data # data has already been validated . callbacks = cbks.CallbackList ( _callbacks ) warnings.warn ( if data is None : _slice_arrays ( input_a , stop=2 ) # Prepare inputs , delegate logic to ` _test_loop ` . str ( [ y.shape for y in targets ] ) ) raise TypeError ( 'The model has multiple outputs , so ` ' if data is not None and hasattr ( data , '__len__ ' ) and len ( data ) : A list of tuples of array indices . if is_sequence : batch_sizes.append ( batch_size ) # Arguments is incompatible with an output . return [ None for _ in range ( len ( names ) ) ] The ` index_array ` array , shuffled in a batch-wise fashion . 'Timestep-wise sample weighting ( use of ' return ( self.uses_learning_phase and index_array = index_array [ : batch_count * batch_size ] existing_classes = set ( y_classes ) for i in range ( len ( unconcatenated_outs ) ) ] output_names , if metric in ( 'accuracy ' , 'acc ' ) : from .. utils.data_utils import OrderedEnqueuer from __future__ import absolute_import ins_batch = _slice_arrays ( ins [ : -1 ] , batch_ids ) + [ ins [ -1 ] ] # the returned Numpy arrays . return { 0 } [ self.total_loss ] + self.metrics_tensors , score_array = fn ( y_true , y_pred ) ( if the model has multiple outputs ) . # ( because of class mode duality ) def standardize_weights ( y , for i in range ( len ( outs ) ) : continue # Arguments outs [ i ] = batch_out progbar = Progbar ( target=num_samples ) training_utils.check_array_length_consistency ( [ a_np ] , [ b_np ] , None ) callbacks.on_batch_begin ( batch_index , batch_logs ) np.random.shuffle ( index_array ) unconcatenated_outs.append ( [ ] ) if sample_weight_mode is None : output_metrics = [ output_metrics ] raise ValueError ( 'Can only use ` validation_steps ` ' verbose=verbose , ValueError : in case of improperly formatted user-provided data . y , val_y = ( slice_arrays ( y , 0 , split_at ) , def weighted ( y_true , y_pred , weights , mask=None ) : str ( validation_data ) ) isinstance ( validation_data , Sequence ) ) class_weight=None , val_x , val_y , val_sample_weight ) steps_per_epoch=steps_per_epoch , steps_done += 1 'sample_weight ' ) if not all_outs : return [ ( i * batch_size , min ( size , ( i + 1 ) * batch_size ) ) # ( used by Sequential models ) 'must be set . ' ) outs.append ( 0 . ) batch_size = x [ 0 ] .shape [ 0 ] return np.ones ( ( y.shape [ 0 ] , y.shape [ 1 ] ) , dtype=K.floatx ( ) ) base_metric_name = metric_name self.metrics_tensors.append ( metric_result ) batch_size : Integer , batch size . # subtract the sets to pick all missing classes for output_shape , loss_fn in zip ( self._feed_output_shapes , self._make_train_function ( ) if sample_weight_mode is None : does not exist . Also raises ValueError when ` steps ` is not ` None ` def predict_loop ( model , f , ins , batch_size=32 , verbose=0 , steps=None ) : sample_weight_mode=None ) : raise ValueError ( 'All input arrays ( x ) should have ' self._feed_sample_weight_modes.append ( str ( y.shape ) + ' . ' outs = [ outs ] For instance , if the model has 2 outputs , and for the first output out = model.predict_generator ( RandomSequence ( batch_size , int ) : check_batch_axis : Boolean ; whether to check that if loss in key_losses : ValueError : if a loss function or target array self.stateful_metric_functions.append ( metric_fn ) averages = [ ] if metric in ( 'accuracy ' , 'acc ' ) : return outs [ 0 ] verbose=1 , base_metric_name = metric_name if len ( unconcatenated_outs ) == 1 : return [ np.random.random ( ( self.batch_size , 3 ) ) , np.random.random ( ( self.batch_size , 3 ) ) ] , [ # and labels , from each line in the file if callback_model.stop_training : Scalar loss ( if the model has a single output and no metrics ) ' ` keras.utils.Sequence ` class . ' ) 'epochs ' : epochs , def fit_generator ( self , ( if the model has multiple outputs ) . does not exist . Also raises ValueError when ` steps ` is not ` None ` return { 0 } elif isinstance ( x , dict ) : val_outs = model.evaluate ( # Same labels assumed . if len ( outs ) == 1 : `` `` '' Slice an array or list of arrays . outs [ i ] /= steps def check_loss_and_target_compatibility ( targets , loss_fns , output_shapes ) : # Epoch finished . initial_epoch=0 , callbacks.on_train_end ( ) A list of ` sample_weight ` or ` class_weight ` where there are exactly sequence_length=sequence_length ) ) from .training_utils import standardize_input_data def _test_loop ( self , f , ins , batch_size=None , verbose=0 , steps=None ) : if len ( self.output_names ) > 1 : score_array /= K.mean ( mask ) 'an input with shape ' steps_done += 1 def _check_loss_and_target_compatibility ( targets , loss_fns , output_shapes ) : m.reset_states ( ) if isinstance ( x_weight , dict ) and output_names [ 0 ] in x_weight : 'you should specify ' ins : List of tensors to be fed to ` f ` 'the same number of samples as target arrays . Got ' x , val_x = ( _slice_arrays ( x , 0 , split_at ) , _slice_arrays ( x , split_at ) ) or list of arrays of predictions x = generator_output 'Expected sample_weight with rank ' if batch_size is not None : ' : expected ' + names [ i ] + ' to have ' Scalar loss ( if the model has a single output and no metrics ) exception_prefix='input ' ) outs = model.test_on_batch ( x , y , sample_weight=sample_weight ) score_array * = weights self._feed_output_names ) val_enqueuer.stop ( ) sample_weight [ 1 ] [ :2 ] ] ) num_batches = ( size + batch_size - 1 ) // batch_size # round up raise ValueError ( 'Provided ` ' + weight_type + ' ` was a list of ' 'You should provide one ` ' + weight_type + ' ` ' or list of arrays of predictions for m in self.stateful_metric_functions : 'or ( x , y ) . Found : ' index_array = index_array.reshape ( ( batch_count , batch_size ) ) next epoch . Ignored with the default value of ` None ` . if do_validation and not val_gen : weight_ndim = K.ndim ( weights ) else : callbacks=None , ins_batch = slice_arrays ( ins [ : -1 ] , batch_ids ) + [ ins [ -1 ] ] if ( verbose and ins and 'in compile ( ) . If you just mean to use ' elif isinstance ( data , list ) : str ( list ( set_w ) [ 0 ] ) + ' target samples . ' ) str ( generator_output ) ) averages.append ( np.average ( [ out [ i ] for out in outs_per_batch ] , if len ( set_w ) > 1 : steps_per_epoch : Total number of steps ( batches of samples ) 'or ` ( val_x , val_y ) ` . Found : ' continue # Cast the mask to floatX to avoid float64 upcasting in Theano model.fit_generator ( generate_arrays_from_file ( '/my_file.txt ' ) , val_data += [ 0 . ] set_w = set_of_lengths ( weights ) epoch += 1 def predict_generator ( self , generator , steps=None , # Since we do not know how many samples `` `` '' # it 's possible to callback a different model than self # ( used by Sequential models ) outs_per_batch = [ ] if set_y and set_w and list ( set_y ) [ 0 ] ! = list ( set_w ) [ 0 ] : suffix = 'ce ' for i in range ( len ( names ) ) : if validation_steps : batch_size : Integer batch size or None if unknown . ' A target array with shape ' + str ( y.shape ) 'Error when checking ' + exception_prefix workers=workers , 'sample_weight can not be broadcast . ' ) if steps is None : def standardize_input_data ( data , [ self.total_loss ] + self.metrics_tensors , ` batch_size ` is ` None ` , returns ` None ` . for cbk in callbacks : # Cast the mask to floatX to avoid float64 upcasting in Theano batch_size=batch_size , if isinstance ( x_weight , dict ) : # Assumes a generator that only wait_time=wait_time ) steps_name : The public API 's parameter name for ` steps ` . if verbose == 1 : if sample_weight is not None and len ( sample_weight.shape ) ! = 2 : if metric in ( 'accuracy ' , 'acc ' , 'crossentropy ' , 'ce ' ) : batch_outs = [ batch_outs ] `` `` '' Slices an array or list of arrays . existing_class_weight = set ( class_weight.keys ( ) ) if sample_weight is not None : str ( sample_weight.shape ) model._make_predict_function ( ) ins_batch = slice_arrays ( ins , batch_ids ) f : Keras function returning a list of tensors if verbose == 1 : return training_arrays.fit_loop ( self , f , ins , weight_type + ' ` ' else data [ x ] for x in names ] ins_batch = _slice_arrays ( ins , batch_ids ) steps_per_epoch = len ( generator ) np.random.random ( ( self.batch_size , 4 ) ) , raise ValueError ( 'Error when checking model ' from .. utils.data_utils import GeneratorEnqueuer outs = model.predict_on_batch ( x ) sample_weight_mode= { 'dense_1 ' : None , 'dropout ' : 'temporal ' } ) from scipy.sparse import issparse for cbk in callbacks : return [ None if x is None else x [ start ] for x in arrays ] for name in output_names : shuffle : Whether to shuffle the data at the beginning of each epoch if not isinstance ( outs , list ) : j += 1 For instance , if the model has 2 outputs , and for the first output verbose : verbosity mode . raise ValueError ( 'All sample_weight arrays should have ' num_samples = ins [ 0 ] .shape [ 0 ] use_multiprocessing=use_multiprocessing , # Instead , we store one array per batch seen batch_size=4 ) outs [ i ] = float ( batch_out ) self.stateful_metric_functions.append ( metric_fn ) 'the same number of samples . Got array shapes : ' batch_size : integer . num_samples = self._check_num_samples ( ins , batch_size , from .. utils.generic_utils import slice_arrays sample_weights = standardize_sample_weights ( sample_weight , validation_data , index_array = batch_shuffle ( index_array , batch_size ) batch_index = 0 stateful_metric_indices = [ def _check_array_lengths ( inputs , targets , weights=None ) : enqueuer.stop ( ) epochs=5 , stateful_metrics=self.stateful_metric_names ) ] if verbose == 1 : if i in stateful_metric_indices : ` f ` and the list of display names of the outputs of ` f_val ` . elif y.shape [ 1 ] == 1 : steps_per_epoch=3 , out_labels = model.metrics_names sample_weights = _standardize_sample_weights ( sample_weight , elif y.shape [ 1 ] == 1 : start = start.tolist ( ) ' ` sparse_categorical_crossentropy ` instead , ' 'If using HDF5 input data , ' ins_batch = _slice_arrays ( ins , batch_ids ) ' : data should be a Numpy array , or list/dict of ' output_generator = generator raise ValueError ( check_loss_and_target_compatibility ( y , outs [ i ] /= steps from .training_utils import weighted_masked_objective def predict_generator ( model , generator , for i in range ( num_batches ) ] import numpy as np # hdf5 datasets only support list objects as indices metric_name_prefix = 'weighted_ ' if weights is not None else `` # step-size = 1 for data tensors elif ins and hasattr ( ins [ 0 ] , 'shape ' ) : ' generator based on the ' _check_array_lengths ( None , None , None ) 'sample_weight ' ) Everything gets normalized to a single sample-wise ( or timestep-wise ) if step == 0 : batch_size=batch_size , # return a set with the variation between cbks.ProgbarLogger ( ' is set , the ` batch_size ` must be None . ' ) stateful_metric_indices = [ ] for line in f : ' class . Please specify ` steps_per_epoch ` ' count_mode='steps ' , if sample_weight is not None and len ( sample_weight.shape ) ! = 2 : if data and hasattr ( data [ 0 ] , 'shape ' ) : if len ( set_y ) > 1 : callbacks = cbks.CallbackList ( _callbacks ) elif len ( generator_output ) == 3 : we want to compute `` binary_accuracy '' and `` binary_crossentropy '' , else : wait_time = 0.01 # in seconds processed based on the size of the first dimension of the return [ out [ 0 ] for out in all_outs ] callbacks.on_epoch_end ( epoch , epoch_logs ) outs_per_batch.append ( outs ) # self.stateful_metric_names else : self.train_function = K.function ( inputs , exception_prefix='target ' ) `` pay more attention '' to samples ( useful for resuming a previous training run ) num_samples = ins [ 0 ] .shape [ 0 ] ' while using as loss ` categorical_crossentropy ` . ' def _standardize_class_weights ( class_weight , output_names ) : from .. utils.generic_utils import Progbar data_shape = data_shape [ 1 : ] name='train_function ' , x , val_x = ( slice_arrays ( x , 0 , split_at ) , raise ValueError ( 'Input arrays should have ' ValueError : In case of invalid arguments . ndim = K.ndim ( score_array ) verbose=verbose , # we then apply all metrics to all outputs . _callbacks.append ( metrics= [ ] , sample_weight_mode=None ) Useful for shuffling HDF5 arrays score_array /= K.mean ( K.cast ( K.not_equal ( weights , 0 ) , K.floatx ( ) ) ) in zip ( y , sample_weights , class_weights , self._feed_sample_weight_modes ) ] # of the queue depends on sleep in the enqueuers from keras.engine import training_utils callbacks.on_epoch_end ( epoch , epoch_logs ) for i in indices_for_conversion_to_dense : val_gen = ( hasattr ( validation_data , 'next ' ) or str ( sample_weight.shape ) + ' . ' return [ ( i * batch_size , min ( size , ( i + 1 ) * batch_size ) ) sample_weight_mode : One of ` None ` or ` `` temporal '' ` . data_shape = data_shape [ 1 : ] if batch_size == 0 : # different shapes , with None = > 0 y_true : ` y_true ` argument of ` fn ` . # Raises def check_num_samples ( ins , suffix = 'acc ' [ output_a_np , output_b_np ] , ( only if doing validation from data tensors ) . finally : last_batch = index_array [ batch_count * batch_size : ] else x for x in data ] if not hasattr ( generator_output , '__len__ ' ) : shape = shape [ 1 : ] str ( shape ) + ' but got array with shape ' try : if hasattr ( metric_fn , 'name ' ) : exception_prefix : String prefix used for exception formatting . count_mode='steps ' , if len ( validation_data ) == 2 : one element per model output . batch_logs [ 'batch ' ] = batch_index output_metrics = metrics.get ( name , [ ] ) sample_weight=val_sample_weights , if fn is None : if isinstance ( x_weight , list ) and len ( x_weight ) == 1 : validation_steps : Number of steps to run validation for ( ins [ 0 ] .shape [ 0 ] , val_ins [ 0 ] .shape [ 0 ] ) ) else : averages = [ ] @ pytest.mark.skipif ( sys.version_info < ( 3 , ) , reason=' Can not catch warnings in python 2 ' ) str ( generator_output ) ) val_data = val_x + val_y + val_sample_weights if val_f and val_ins : sparse=K.is_sparse ( self.outputs [ i ] ) , loss_weights=None , else : epoch_logs [ 'val_ ' + l ] = o batch_size=4 , epochs=1 ) ' the ` keras.utils.Sequence ` class . ' ) break ' Please specify ` steps ` or use the ' [ np.random.random ( ( batch_sz , 4 ) ) , if not all_outs : 'batch_size ' : batch_size , 'for each key in : ' + str ( names ) ) [ output_a_np , output_b_np ] ) batch_outs = [ batch_outs ] if ( val_gen and not isinstance ( validation_data , Sequence ) and from .. import losses before declaring predictions finished . raise ValueError ( ' ` steps_per_epoch=None ` is only valid for a ' else : num_train_samples = check_num_samples ( ins , # ` check_batch_axis ` is set to False since ` x ` may contain multiple batches callbacks= [ tracker_cb ] ) start = start.tolist ( ) while metric_name in self.metrics_names : 'targets to be binary matrices ( 1s and 0s ) ' if y.shape [ 1 ] > 1 : # data has already been validated . weighted_metric_fn = weighted_masked_objective ( metric_fn ) if ins and isinstance ( ins [ -1 ] , float ) : 'must be set . ' ) if len ( output_names ) == 1 : # python 2 has 'next ' , 3 has '__next__ ' raise ValueError ( 'Error when checking model ' verbose : Verbosity mode , 0 , 1 or 2 for i , batch_out in enumerate ( batch_outs ) : or list of scalars ( if the model has multiple outputs `` `` '' Returns a list of batch indices ( tuples of indices ) . 'input_b ' : input_b_np } , data : User-provided input data ( polymorphic ) . np.random.random ( ( self.batch_size , 3 ) ) ] , ' : expected ' + names [ i ] + ' to have shape ' if len ( generator_output ) == 2 : if len ( weights ) ! = len ( y_classes ) : y_classes = np.reshape ( y , y.shape [ 0 ] ) index_array : array of indices to be shuffled . batch_size : integer batch size or ` None ` . stop : integer ( stop index ) ; should be None if 'In order to use timestep-wise sample weighting , ' # the returned Numpy arrays . _callbacks += ( callbacks or [ ] ) + [ model.history ] np.random.random ( ( batch_sz , 3 ) ) ] , if sample_weight_mode is not None : # Same labels assumed . updates=updates , validation_data= ( { 'input_a ' : input_a_np , slice_arrays ( sample_weights , split_at ) ) if not hasattr ( generator_output , '__len__ ' ) : except TypeError : y_true : ` y_true ` argument of ` fn ` . raise ValueError ( 'Output of generator should be a tuple ' } ) target_tensors=None , * * kwargs ) : validation_data=None , relies on multiprocessing , ' : you are passing a list as input to your model , ' enqueuer = GeneratorEnqueuer ( steps=steps ) Can also work on list/array of indices : ` _slice_arrays ( x , indices ) ` steps_done += 1 if not isinstance ( output_metrics , list ) : from __future__ import print_function 'should be None or `` temporal '' . ' callbacks.on_batch_begin ( step_index , batch_logs ) names , [ output_a_np , output_b_np ] ) ) It transforms an objective function ` fn ( y_true , y_pred ) ` 'but instead got the following list of ' ' generator based on the ` keras.utils.Sequence ` ' def _weighted_masked_objective ( fn ) : self._feed_output_names ) 'of shape ( samples , classes ) . ' initial_epoch=0 , validation_data=RandomSequence ( 4 ) , callback_metrics=None , 'Batches should contain ' wait_time = 0.01 np.random.shuffle ( index_array ) batch_logs = { } def evaluate_generator ( model , generator , [ a ] , [ losses.categorical_crossentropy ] , [ ( 2 , None , 3 ) ] ) steps_done = 0 names : List of expected array names . if len ( outs ) == 1 : ' ` class_weight ` . ' outs [ i ] += batch_out steps_per_epoch=steps_per_epoch , batch_size = 1 str ( sample_weight.shape ) + ' . ' `` `` '' Checks if batch axes are the same for numpy arrays . val_enqueuer = None name='test_function ' , slice_arrays ( sample_weights , 0 , split_at ) , # Prepare data for validation `` `` '' Abstract method to loop over some data in batches . out = model.fit_generator ( generator=RandomSequence ( 3 ) , return set ( [ 0 if y is None else y.shape [ 0 ] for y in x ] ) sparse=K.is_sparse ( self.outputs [ i ] ) , if not isinstance ( outs , list ) : if steps is not None : 'In order to use timestep-wise sample weights , ' verbose : Verbosity mode , 0 , 1 or 2 return self._fit_loop ( f , ins , out_labels=out_labels , for m in self.stateful_metric_functions : batch_logs [ 'size ' ] = len ( batch_ids ) return [ None ] averages.append ( float ( outs_per_batch [ -1 ] [ i ] ) ) ` fn ( y_true , y_pred , weights , mask ) ` . 'epochs ' : epochs , x_weights = [ ] unconcatenated_outs [ i ] .append ( batch_out ) batches = _make_batches ( num_samples , batch_size ) elif len ( data ) == 1 and not hasattr ( data [ 0 ] , 'shape ' ) : stateful_metrics=model.stateful_metric_names ) ] 'If using HDF5 input data , ' ValueError : if a loss function or target array # Dedupe name num_train_samples = self._check_num_samples ( ins , batch_size , # Delegate logic to ` fit_loop ` . 'targets to have the same shape ' # mask should have the same shape as score_array batch_size=batch_size , # used for training . if len ( data ) ! = len ( names ) : set_y = set_of_lengths ( targets ) f : Keras function returning a list of tensors `` `` '' Checks if batch axes are the same for numpy arrays . callbacks.on_batch_end ( batch_index , batch_logs ) from .. utils.data_utils import Sequence 'Numpy arrays . Found : ' + str ( data ) [ :200 ] + ' ... ' ) index_array = index_array.flatten ( ) ' and multiple workers may duplicate your data . ' if is_sequence : str ( x_weight ) ) ins [ : -1 ] , batch_ids ) + [ ins [ -1 ] ] 'with shape ' + str ( data_shape ) ) sample_weight = None out = model.predict_on_batch ( { 'input_a ' : input_a_np , from .training_utils import check_array_length_consistency return np.ones ( ( y.shape [ 0 ] , y.shape [ 1 ] ) , dtype=K.floatx ( ) ) if isinstance ( x_weight , list ) : TypeError : if an incorrect type is passed for the ` metrics ` argument . 'sample_weight array is 1D . ' ) # it 's possible to callback a different model than self : 'Found : ' + str ( sample_weight_mode ) ) `` `` '' Trains the model on data generated batch-by-batch by a Python generator ( or an instance of ` Sequence ` ) . model.fit ( test_inputs , test_outputs , # case : categorical accuracy/crossentropy with K.name_scope ( metric_name ) : elif isinstance ( metrics , dict ) : workers=2 ) out = model.fit_generator ( gen_data ( 4 ) , steps_per_epoch=10 , use_multiprocessing=True , workers=2 ) verbose=0 ) : callbacks.on_batch_end ( step_index , batch_logs ) return x_weight self._make_test_function ( ) if y is None or loss is None : for step_index in range ( steps_per_epoch ) : inputs , def _standardize_sample_weights ( sample_weight , output_names ) : from .training_utils import standardize_weights ` fn ( y_true , y_pred , weights , mask ) ` . score_array /= K.mean ( mask ) else : if steps_per_epoch is None : is_sequence = isinstance ( generator , Sequence ) # ` check_batch_axis ` is set to False since ` x ` # may contain multiple batches `` `` '' Shuffles an array in a batch-wise fashion . return outs val_x , val_y , if isinstance ( x_weight , list ) and len ( x_weight ) == 1 : raise ValueError ( 'Found a sample_weight array with shape ' if y.shape [ -1 ] == 1 : 'but instead got the following list of ' raise ValueError ( 'Either the input data should have ' # Construct epoch logs . ' is set , the ` batch_size ` must be None . ' ) 'training , i.e . ` steps_per_epoch ` ' out_labels : List of strings , display names of x , y , sample_weight = generator_output if not metrics : in which case the number of samples is set to ` None ` . outs.append ( 0 . ) x , _ , _ = generator_output initial_epoch : Epoch at which to start training return averages [ 0 ] metrics : a list or dict of metric functions . indices_for_conversion_to_dense = [ ] # and labels , from each line in the file validation_steps=validation_steps ) try : The number of samples is not defined when running with ` steps ` , Can also work on list/array of indices : ` _slice_arrays ( x , indices ) ` if data [ i ] .ndim ! = len ( shape ) : # Step-based predictions . if isinstance ( x_weight , dict ) and output_names [ 0 ] in x_weight : epoch = initial_epoch weight array . steps_per_epoch=None , validation_steps=None ) : ' : you are passing a list as input to your model , ' # Get metric name as string raise ValueError ( 'Sample_weight arrays should have ' batch_size=batch_size , in which case the number of samples is set to ` None ` . if x is None or len ( x ) == 0 : for i in indices_for_conversion_to_dense : initial_epoch=0 ) : np.random.random ( ( self.batch_size , 3 ) ) ] ) outs_per_batch = [ ] if isinstance ( generator_output , tuple ) : [ a_np , a_np ] , [ a_np , a_np ] , [ a_np , a_np ] ) 'You are passing a target array of shape ' + str ( y.shape ) return [ None if x is None else x [ start : stop ] for x in arrays ] concatenation of list the display names of the outputs of from . import training_generator # and in general ` x [ 0 ] .shape [ 0 ] ! = self._feed_input_shapes [ 0 ] [ 0 ] ` self._feed_output_names , weights=batch_sizes ) ) if self.uses_learning_phase and not isinstance ( K.learning_phase ( ) , int ) : out = model.fit ( [ input_a_np , input_b_np ] , callbacks.on_epoch_begin ( epoch ) 'for each key in : ' + str ( names ) ) # we then apply all metrics to all outputs . names : List of expected array names . shapes : Optional list of expected array shapes . check_batch_axis : Boolean ; whether to check that batch_size = x [ 0 ] .shape [ 0 ] # Check shapes compatibility . raise ValueError ( ' '' sample_weight_mode ' before declaring one epoch finished and starting the warnings.warn ( 'Batches should at least contain one item . ' ) if isinstance ( ins [ -1 ] , float ) : max_queue_size=10 , do_validation = False # yields inputs ( not targets and sample weights ) . ` History ` object . feed = ( model._feed_inputs batch_logs [ 'size ' ] = batch_size hasattr ( validation_data , '__next__ ' ) or [ np.random.random ( ( batch_sz , 4 ) ) , np.random.random ( ( batch_sz , 3 ) ) ] ) metric_name = self.output_names [ i ] + ' _ ' + metric_name metric_result = weighted_metric_fn ( y_true , y_pred , wait_time = 0.01 # in seconds raise ValueError ( suffix = 'ce ' metric_name = base_metric_name + ' _ ' + str ( j ) val_outs = model.evaluate_generator ( shapes=None , model : Keras model instance . raise ValueError ( ' ` class_weight ` must contain ' import numpy as np use_multiprocessing=use_multiprocessing , _check_array_lengths ( a_np , a_np , None ) class_weights = standardize_class_weights ( class_weight , for batch_index , ( batch_start , batch_end ) in enumerate ( batches ) : if shapes : verbose=0 ) str ( [ w.shape for w in weights ] ) ) 'In order to use timestep-wise sample weighting , ' mask = K.cast ( mask , K.floatx ( ) ) for out in outs : data = data.values if data.__class__.__name__ == 'DataFrame ' else data def check_array_length_consistency ( inputs , targets , weights=None ) : start : can be an integer index ( start index ) if len ( names ) == 1 and data and isinstance ( data [ 0 ] , ( float , int ) ) : training_utils.check_array_length_consistency ( a_np , None , None ) indices_for_conversion_to_dense = [ ] f : Keras function returning a list of tensors . x , y = generator_output shuffle=True , 'samples ' : num_train_samples , batch_ids = index_array [ batch_start : batch_end ] raise ValueError ( 'Output of generator should be ' self.test_function = K.function ( inputs , # yields inputs ( not targets and sample weights ) . 'with shape ' + str ( data_shape ) ) return x_weights suffix = 'acc ' count_mode , str ( shape ) + ' but got array with shape ' return np.concatenate ( all_outs [ 0 ] ) if self._uses_dynamic_learning_phase ( ) : for l , o in zip ( out_labels , val_outs ) : self.metrics_names.append ( metric_name ) ins_batch = slice_arrays ( ins , batch_ids ) output_generator = enqueuer.get ( ) x , y , sample_weight = generator_output # and concatenate them upon returning . 'less than or equal to ' + str ( len ( y.shape ) ) ) batch_size : integer batch size or ` None ` . ' : data should be a Numpy array , or list/dict of ' metrics=None , name=name + '_target ' , `` `` '' Maps ` sample_weight ` or ` class_weight ` to model outputs . if not isinstance ( batch_outs , list ) : for cbk in callbacks : raise ValueError ( 'If ' + steps_name # Keep track of state updates created by sample_weights , ' ` type not understood : ' return all_outs [ 0 ] [ 0 ] Assumes that f returns a list , labeled by out_labels . indices_for_conversion_to_dense.append ( i ) steps=validation_steps , while steps_done < steps_per_epoch : batch_size=batch_size , check_batch_axis=False ) sample_weight = None val_ins=val_ins , 'should be either a list or a dict . ' raise ValueError ( ' ` class_weight ` not supported for ' mask : Mask tensor . if isinstance ( batch_outs , list ) : batch_size=batch_size , model.compile ( optimizer , loss='mse ' , batch_logs [ l ] = o callback_metrics = copy.copy ( out_labels ) + [ 'val_ ' + n for n in out_labels ] def collect_metrics ( metrics , output_names ) : def standardize_sample_weights ( sample_weight , output_names ) : that will be applied to the last 2 dimensions of if loss is losses.categorical_crossentropy : # case : binary accuracy/crossentropy [ self.total_loss ] + self.metrics_tensors , updates=self.state_updates + self.metrics_updates , the outputs of ` f ` generator_output = next ( output_generator ) all_outs = [ ] if arrays is None : unconcatenated_outs = [ ] # Handle data tensors support when no input given class_weight=None , batch_logs [ 'size ' ] = 1 elif isinstance ( x , dict ) : * * self._function_kwargs ) if steps_done == 1 : steps , def evaluate_generator ( self , generator , steps=None , ' class . Please specify ` validation_steps ` or use ' check_batch_axis=False , 'sample_weight array is 1D . ' ) not isinstance ( K.learning_phase ( ) , int ) ) if mask is not None : 'your model is not the size the model expected . ' val_x , val_y = validation_data Array of predictions ( if the model has a single output ) sample_weight_mode=None ) : i for i , name in enumerate ( self.metrics_names ) nested_metrics = [ ] 'expected no data , but got : ' , data ) progbar = Progbar ( target=num_samples ) def _batch_shuffle ( index_array , batch_size ) : return [ np.concatenate ( out ) for out in all_outs ] return [ None if x is None else x [ start ] for x in arrays ] 'Alternatively , you can use the loss function ' sample_weights = [ standardize_weights ( ref , sw , cw , mode ) val_data = val_x + val_y + val_sample_weights def slice_arrays ( arrays , start=None , stop=None ) : if step == 0 : progbar.update ( batch_end ) max_queue_size=10 , # Get metric name as string return [ None for _ in output_names ] steps=None , outs = [ outs ] elif len ( data ) == 1 and not hasattr ( data [ 0 ] , 'shape ' ) : self._feed_loss_fns , sample_weight = None mask=masks [ i ] ) index_array = np.arange ( num_train_samples ) 'metrics ' : callback_metrics , if enqueuer is not None : str ( data_shape ) ) elif hasattr ( start , '__getitem__ ' ) : self._feed_output_shapes ) _check_array_lengths ( [ a_np ] , [ b_np ] , None ) model.train_on_batch ( [ input_a_np , input_b_np ] , [ output_a_np , output_b_np ] ) if num_train_samples is not None : unconcatenated_outs [ i ] .append ( batch_out ) weight array . if len ( all_outs ) == 1 : # Prepare inputs , delegate logic to ` predict_loop ` . ' `` ` \n ' raise ValueError ( val_x , val_y , for i , out in enumerate ( outs ) : return [ x_weight [ output_names [ 0 ] ] ] # build batch logs if len ( validation_data ) == 2 : validation_steps , np.random.shuffle ( index_array ) metric_fn = metrics_module.sparse_categorical_accuracy elif metric in ( 'crossentropy ' , 'ce ' ) : if isinstance ( metric_fn , Layer ) and metric_fn.stateful : return [ [ ] for _ in output_names ] if y is None or loss is None : self.stateful_metric_names.append ( metric_name ) elif isinstance ( x , list ) : progbar = Progbar ( target=steps ) [ x [ start : stop ] for x in arrays ] if ` arrays ` is a list `` `` '' Trains the model for a given number of epochs ( iterations on a dataset ) . dtype=K.dtype ( self.outputs [ i ] ) ) if not check_batch_axis : def make_batches ( size , batch_size ) : out_labels=out_labels , raise ValueError ( 'Found a sample_weight with shape ' # Do not slice the training phase flag . training_utils.check_loss_and_target_compatibility ( if len ( averages ) == 1 : ( only if doing validation from data tensors ) . epochs : Number of times to iterate over the data slice_arrays ( input_a , 0 ) ( where one can not access arbitrary indices ) . model.compile ( optimizer , loss='mse ' , sample_weight_mode= { 'dense_1 ' : None , 'dropout ' : 'temporal ' } ) if len ( names ) == 1 and data and isinstance ( data [ 0 ] , ( float , int ) ) : while steps_done < steps_per_epoch : the list would look like : epochs=epochs , 'or ` ( val_x , val_y ) ` . Found : ' # Epoch finished . raise ValueError ( 'Input arrays should have ' array-likes , and outputs : `` `` '' Returns a list of batch indices ( tuples of indices ) . 'you should specify ' if weights is not None : count_mode = 'samples ' batch_size=batch_size , elif isinstance ( arrays , list ) : 'samples ' : num_train_samples , A list ( one entry per model output ) of lists of metric functions . batch_size = x.shape [ 0 ] steps_name='steps ' ) weighted_function = _weighted_masked_objective ( losses.categorical_crossentropy ) RandomSequence ( batch_size , sequence_length=sequence_length ) ) training_utils.check_array_length_consistency ( None , None , None ) 'steps ' : steps_per_epoch , # subtract the sets to pick all missing classes reason=' Can not catch warnings in python 2 ' ) val_outs = test_loop ( model , val_f , val_ins , use_multiprocessing=True , slice_arrays ( None ) metric_name = metric_name_prefix + suffix from .. utils.data_utils import GeneratorEnqueuer if issparse ( ins [ i ] ) and not K.is_sparse ( model._feed_inputs [ i ] ) : dtype=K.dtype ( self.outputs [ i ] ) ) sample_weight_mode= { 'lstm ' : 'temporal ' } ) if len ( y.shape ) < 3 : out = model.fit ( [ input_a_np , input_b_np ] , [ output_a_np , output_b_np ] , batch_size=4 , epochs=1 ) break # step-size = 1 for data tensors 'you should pass a 2D sample_weight array . ' ) score_array = K.mean ( score_array , outs [ i ] [ batch_start : batch_end ] = batch_out str ( metrics ) )","['keras/engine/training.py', 'keras/engine/training_arrays.py', 'keras/engine/training_generator.py', 'keras/engine/training_utils.py', 'keras/utils/generic_utils.py', 'tests/keras/engine/test_training.py', 'tests/keras/test_sequential_model.py', 'tests/test_loss_masking.py']",Refactor training part of ` engine ` module . ( # 10029 )
472,b076e227da6beaf87d6c84eff1a92285e4662acf,2018-04-24 13:36:25-07:00,"forward_updates + backward_updates ) assert len ( layer.updates ) == 4 layers.SimpleRNN ( 3 , kernel_regularizer='l1 ' , bias_regularizer='l1 ' ) ) layer = wrappers.Bidirectional ( backward_losses = self.backward_layer.get_losses_for ( inputs ) assert len ( layer.get_updates_for ( None ) ) == 2 layer.backward_layer.add_loss ( 1 , inputs=None ) return ( super ( Wrapper , self ) .get_losses_for ( inputs ) assert len ( layer.get_losses_for ( x ) ) == 2 assert len ( layer.losses ) == 8 assert len ( layer.losses ) == 4 def test_Bidirectional_updates ( ) : assert len ( layer.updates ) == 0 assert len ( layer.get_updates_for ( None ) ) == 0 def get_losses_for ( self , inputs=None ) : layer.backward_layer.add_loss ( 0 , inputs=x ) layer = wrappers.Bidirectional ( layers.SimpleRNN ( 3 ) ) assert len ( layer.get_losses_for ( None ) ) == 6 forward_losses + backward_losses ) layer.forward_layer.add_update ( 0 , inputs=x ) layer.forward_layer.add_loss ( 0 , inputs=x ) assert len ( layer.get_updates_for ( x ) ) == 0 x = Input ( shape= ( 3 , 2 ) ) layer.forward_layer.add_loss ( 1 , inputs=None ) backward_updates = self.backward_layer.get_updates_for ( inputs ) layer.forward_layer.add_update ( 1 , inputs=None ) layer.backward_layer.add_update ( 0 , inputs=x ) assert len ( layer.get_losses_for ( None ) ) == 4 forward_updates = self.forward_layer.get_updates_for ( inputs ) assert len ( layer.get_losses_for ( x ) ) == 0 _ = layer ( x ) def test_Bidirectional_losses ( ) : def get_updates_for ( self , inputs=None ) : return ( super ( Wrapper , self ) .get_updates_for ( inputs ) layer.backward_layer.add_update ( 1 , inputs=None ) forward_losses = self.forward_layer.get_losses_for ( inputs ) assert len ( layer.get_updates_for ( x ) ) == 2","['keras/layers/wrappers.py', 'tests/keras/layers/wrappers_test.py']",Fix Bidirectional Regularization ( # 10012 )
473,49f5b931410bc2e56378f20a15e8ac919e0efb88,2018-04-24 12:34:58-07:00,"tensor_index , def generate_arrays_from_file ( path ) : from __future__ import division some of the layers have changed . ( one array per model weight ) . # Collect input shapes to build layer . on this data at the end of each epoch . 'config ' : obj.get_config ( ) } # based on layer names , because names can potentially return insecure 'input_shapes ' , for layer in merge.layers : model._make_train_function ( ) from .. utils.generic_utils import is_all_none 'and thus can not be built . ' node_index = node.node_indices [ j ] for axis , value in spec.axes.items ( ) : if element is not None : f : A pointer to a HDF5 group . self._built = False as a string . of ` Sequence ` ( ` keras.utils.Sequence ` ) . string , path where to save the model , or of the layer uses ` K.in_training_phase ( ) ` trainable_weights = [ ] all_attrs = [ ] input_masks= [ None ] , original_backend , layer : Target layer instance . def save ( self , filepath , overwrite=True , include_optimizer=True ) : output_shapes=output_shapes , if len ( output_shapes ) == 1 : layers_by_depth = { } } A ` History ` object . Its ` History.history ` attribute is with h5py.File ( filepath , mode= ' r ' ) as f : config [ 'input_layers ' ] = model_inputs epochs : Integer . Number of epochs to train the model . raise ValueError ( 'Asked to get ' + attr_name name : string , name of layer . def add ( self , layer ) : from .engine.saving import load_model from .legacy import interfaces batch . Therefore , all arrays in this tuple must have the same num_chunks = 1 model._make_train_function ( ) loss = convert_custom_objects ( training_config [ 'loss ' ] ) ( useful for resuming a previous training run ) . # Layer instances created during if hasattr ( self , 'dtype ' ) : reshape=reshape ) 'class_name ' : model.__class__.__name__ , f.attrs [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) # convert the weights between CuDNNLSTM and LSTM class Model ( Network ) : if not val.shape : # We were passed a model as first layer . spec = [ ( 'dtype= ' + str ( self.dtype ) ) if self.dtype else `` , return shapes # This is always a single layer , never a list . A model instance . So the channel axis needs to be flipped when we 're loading TF weights onto a TH model , for layer in layers_for_depth : if layer.name == name : ' ' + str ( len ( output_masks ) ) + ' output masks . ' ) output_shapes = output_shapes [ 0 ] ' '' in the current model ) was found to ' Note that because this implementation relies on multiprocessing , str ( len ( params ) ) 'Conv2DTranspose ' , layer = node.outbound_layer ' ( missing Keras metadata ) . ' ) layer_names = load_attributes_from_hdf5_group ( f , 'layer_names ' ) elif hasattr ( K , 'int_shape ' ) : # If the class is private the name starts with `` _ '' which is not secure An epoch is an iteration over the entire ` x ` and ` y ` you should not pass non-picklable arguments to the generator intermediate = re.sub ( ' ( . ) ( [ A-Z ] [ a-z0-9 ] + ) ' , r'\1_\2 ' , name ) raise AttributeError ( 'The layer `` ' + str ( self.name ) # Gather layer inputs . masks = [ ] return specs [ 0 ] 'Found input_spec = ' # of the deepest model to infer input shape and dtype . output_tensors=self.outputs , for name in layer_names : `` `` '' Returns the current weights of the layer . name = config.get ( 'name ' ) config = { self.outputs = [ layer._inbound_nodes [ -1 ] .output_tensors [ 0 ] ] Assumes that the layer will be built 'the model was * not * compiled . Compile it manually . ' ) model_outputs = [ ] a tuple ` ( inputs , targets , sample_weights ) ` . weights : The concatenation of the lists trainable_weights and layers : list of layers to add to the model . output_shapes = None inbound_layer_name = input_data [ 0 ] 'output_masks ' , if isinstance ( output_shapes , list ) and len ( output_shapes ) == 1 : return self.model.get_losses_for ( inputs ) # List of tensors , created by outbound_layer.compute_mask ( ) . layer_data : layer config dict . # The node is relevant to the model : return input_shape if len ( computed_data ) == len ( reference_input_tensors ) : name = str ( w.name ) # First , we create all layers and enqueue nodes to be processed # Nodes that can not yet be processed ( if the inbound node to a weight ( float ) value , used for weighting the loss function output_shapes.append ( shape ) if issubclass ( layer.__class__ , Container ) : in each line . If not provided , weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 2 , 3 , 4 , 1 , 0 ) ) limitations of HDF5 data ; it shuffles in batch-sized chunks . when using process-based threading . should be the same as when the weights were saved . else : self.build ( ) node_indices=node_indices , # actually create the model that the layer itself is also trainable ) . self._trainable_weights.append ( weight ) self.sparse = sparse if node in nodes_in_progress : opened_new_file = False RuntimeError : if the model was never compiled . raise TypeError ( 'Not JSON Serializable : ' , obj ) and will raise an exception . In such cases , use if spec_dim is not None and dim is not None : its new shape ( obtained via self.compute_output_shape ) . # First layer in model : check that it is an input layer . if len ( shapes ) == 1 : /getting-started/faq/ layer : Origin layer of the tensor . Will be self.node_indices = node_indices if kwarg not in allowed_kwargs : if masks is None : self._feed_inputs = [ ] return masks ` None ` defaults to sample-wise weights ( 1D ) . terminal window sizes ) . get_weights ( ) if len ( all_output_shapes ) == 1 : config [ 'dtype ' ] = self.dtype tensor=None ) : Python dictionary . self.add_loss ( layer.get_losses_for ( None ) , None ) # all layers in order of horizontal graph traversal . raise ValueError ( 'All layers in a Sequential model ' some of the layers have changed . the _keras_shape of the input ( s ) . compute_output_shape # This is necessary for shared layers that have inputs at different JSON-serializable structure representing ` obj ` . ( only valid when ` by_name ` =True ) . # It would be unreliable to build a dictionary num_weights = len ( sublayer.trainable_weights ) for layer in self.layers : # From the earliest layers on . inbound_names.append ( None ) return raise ValueError ( 'An Input layer should be passed either ' TypeError : if there are no layers in the model . if index is not None : A layer with ` n ` input tensors must have batch_shape : A shape tuple ( integer ) , including the batch size . from .. engine import Layer , InputSpec optimizer_weights_group = f [ 'optimizer_weights ' ] y_expected = to_list ( merge_func ( y_forward [ 0 ] , y_backward [ 0 ] ) ) return self._get_node_attribute_at_index ( node_index , regularizer : An optional Regularizer instance . 'value ' : obj.tolist ( ) } the provided inputs and the expectations of the layer . input_shapes.append ( x_elem._keras_shape ) with open ( path ) as f : output_shapes= [ batch_input_shape ] ) layer.__class__.__name__ ) ) config = [ ] workers=workers , if spec_dim is not None and dim is not None : if len ( input_shapes ) == 1 : if len ( output_ls_copy ) == 1 : the updates as conditional on these inputs . The model is not trained for a number of iterations 'before being used . ' ) for instance activity losses are conditional on the layer 's inputs . name the ` len ( validation_data ) ` as a number of steps . for layer in self.output_layers : * * kwargs : Additional keyword arguments . from .. engine.base_layer import InputSpec , Layer validation_steps=None , from .input_layer import InputLayer ' does not support masking , ' max_queue_size=10 , workers=1 , kwargs = { } 'hence the notion of `` layer output mask '' ' backward_weights = preprocess_weights_for_loading ( layer.backward_layer , is simply a Network with added training routines . weights = preprocess_weights_for_loading ( layer.layer , to be passed to ` yaml.dump ( ) ` . # we compute the output tensors , return custom_objects [ obj ] # Update _keras_shape . `` pay more attention '' to samples from an under-represented class . str ( K.ndim ( x ) ) ) filepath : String , path to the file to save the weights to . nodes_in_progress.add ( node ) initializer = initializers.get ( initializer ) return layer_cache [ name ] constraint : An optional Constraint instance . return any ( [ x._uses_learning_phase for x in self.outputs ] ) raise RuntimeError ( 'The layer has never been called ' tensor : Some tensor in a graph . merge_inputs.append ( merge_input ) if node_key in self._container_nodes : If ` True ` , use process-based threading . return K.batch_get_value ( weights ) # Arguments weights [ 9 ] ] , axis=-1 ) from .. import __version__ as keras_version # Update the depth of inbound nodes . weight_values = K.batch_get_value ( symbolic_weights ) batch_size = kwargs [ 'batch_size ' ] self.output_names.append ( layer.name ) weights [ 0 ] = weights [ 0 ] [ : , 0 , : , : ] self._feed_input_shapes.append ( self.inputs [ i ] ._keras_shape ) 'and thus can not be built . ' 'backend ' : K.backend ( ) if name in layer_cache : or ` K.in_test_phase ( ) ` . ' ( see keras.io/optimizers ) . ' ) raise ValueError ( 'Input ' + str ( input_index ) return layer for i , ( w , val ) in enumerate ( zip ( symbolic_weights , weight_values ) ) : regularization_losses = [ layer.activity_regularizer ( x ) for x in output_tensors ] verbose=1 , for node_data in unprocessed_nodes.pop ( layer ) : 'Received inputs : { } . ' name inbound_layers.append ( inbound_layer ) # Collect updates that are dependent on inputs initial_epoch=initial_epoch , output_shape = None if not val.shape : saving.load_weights_from_hdf5_group ( weight_values ) ) : build_map_of_graph ( x , finished_nodes , nodes_in_progress ) return updates if node.arguments : If a Keras tensor is passed : output_shapes= [ x._keras_shape for x in self.outputs ] ) weights [ 0 ] = conv_utils.convert_kernel ( weights [ 0 ] ) a tuple ` ( inputs , targets ) ` layer_names = _load_attributes_from_hdf5_group ( f , 'layer_names ' ) if not name : callbacks=callbacks , # Instantiate layer . inputs_ls = _to_list ( inputs ) output_shapes= [ batch_input_shape ] ) self._outbound_nodes = [ ] return output_shapes [ 0 ] The model will set apart this fraction of the training data , not trained for n steps given by epochs , but until the # to insert before the current layer if not layer or node_index is None or tensor_index is None : # for its inputs , and no outbound nodes . def save_weights ( self , filepath , overwrite=True ) : backward_weights = preprocess_weights_for_loading ( layer.backward_layer , def is_all_none ( iterable_or_element ) : 'cntk ' : True } self.loss = self.model.loss return values sample_weight=sample_weight , input_tensors = to_list ( input_tensors ) The input samples are processed batch by batch . 'and thus has no defined ' + attr_name + ' . ' ) while True : layers.append ( self.layers [ 0 ] ) input_shape : Shape tuple ( tuple of integers ) weight_values [ i ] .shape ) ) 'Use ` get_output_at ( node_index ) ` instead . ' ) validation_steps : Only relevant if ` steps_per_epoch ` inbound_nodes_data = layer_data [ 'inbound_nodes ' ] The loaded Model . epochs : Integer , total number of iterations on the data . will not train on it , and will evaluate for merge_input_config in first_layer_config.pop ( 'layers ' ) : 'Input layers to a ` Model ` must be ` InputLayer ` objects . ' # ( filters , input_dim , filter_length , 1 ) for its ` build ` call . # If all previous input tensors are available in tensor_map , for i in range ( len ( self.output_layers ) ) : A tensor ( or list of tensors if the layer has multiple inputs ) . layer_class_name = layer.__class__.__name__ a list of strings return self.model.train_on_batch ( x , y , # Add to the model any layers passed to the constructor . # the graph reconstruction process h5py.File object from which to load the model if node.arguments : ' ` batch_input_shape ` argument . ' ) the model 's optimizer 's state ( if any ) self.inputs = [ ] # List of input tensors compute_mask ( x , mask ) class_weight=None , model_config = f.attrs.get ( 'model_config ' ) # we should override the default mask . ( ` keras.utils.Sequence ` ) object in order to avoid duplicate data 'Sequential model must ' layer ( input_tensors [ 0 ] , * * kwargs ) tensor_index = node.tensor_indices [ i ] use_multiprocessing=use_multiprocessing ) self._nodes_by_depth = self.model._nodes_by_depth outbound_layer = None import h5py The model weights . node_index ) A tensor if there is a single output , or layer = get_or_create_layer ( conf ) raise ValueError ( 'Layer weight shape ' AttributeError : if the layer is connected to * * kwargs ) : ValueError : In case of mismatch between the provided input data # Early return if compilation is not required . inbound_layers : a list of layers , the same length as ` input_tensors ` , if layer.data_format == 'channels_first ' : def __init__ ( self , layers=None , name=None ) : node_key = self._node_key ( layer , node_index ) if layer.is_placeholder : if layer.__class__.__name__ == 'LSTM ' : return x metrics=metrics , uses_learning_phase : Whether any operation ' due to mismatch in number of weights ' get_output_mask_at ( node_index ) as part of the saved model , the model is already The updates may potentially be conditional on some inputs tensors , if layer not in unprocessed_nodes : 'config ' : model.optimizer.get_config ( ) if hasattr ( self , 'batch_input_shape ' ) : if layer not in layers : # determine if we 're loading a CuDNNLSTM layer from the number of bias weights : if hasattr ( x , '_keras_history ' ) : tensor , mask = tensor_map [ str ( id ( x ) ) ] f = h5py.File ( filepath , mode= ' r ' ) # which means any batch size will be accepted by the model . get_input_at ( node_index ) * * kwargs ) str ( len ( self._inbound_nodes ) ) + ' inbound nodes . ' ) return output_shapes A JSON string . and weights file and skip_mismatch=False . self.is_placeholder = True weights [ : num_weights_per_layer ] , if not isinstance ( layer , ( InputLayer , legacy_layers.Merge ) ) : # Build self.input_names and self.output_names . ` True ` if conversion on kernel matrices is required , otherwise ` False ` . save_attributes_to_hdf5_group ( g , 'weight_names ' , weight_names ) The generator is run in parallel to the model , for efficiency . depth_keys.sort ( reverse=True ) layer = deserialize_layer ( layer_data , A Keras tensor is a tensor object from the underlying backend cache_key = ' , '.join ( [ str ( id ( x ) ) for x in inputs ] ) original_keras_version = f.attrs [ 'keras_version ' ] .decode ( 'utf8 ' ) `` `` '' for input_index , ( x , spec ) in enumerate ( zip ( inputs , input_spec ) ) : if layer.stateful : 0 = silent , 1 = progress bar , 2 = one line per epoch . depth_keys.sort ( reverse=True ) # with the input_spec set at build time . # Returns name , if weights : computable_tensors.append ( x ) custom_objects=custom_objects ) def transpose_input ( from_cudnn ) : `` `` '' Parses a JSON model configuration file and returns a model instance . self.output_layers_tensor_indices.append ( tensor_index ) self.supports_masking = self.model.supports_masking `` `` '' Specifies the ndim , dtype and shape of every input to a layer . to apply a different weight to every timestep of every sample . if hasattr ( self , 'dtype ' ) : # Load weights that were specified at layer instantiation . from .. engine.base_layer import InputSpec for layer in getattr ( self , 'input_layers ' , [ ] ) : elif not self._inbound_nodes : If the input layer in the model is named , you can also pass a This allows you to save the entirety of the state of a model 'input shape ' ) h5py = None self.metrics_names = self.model.metrics_names def load_weights_from_hdf5_group ( f , layers , reshape=False ) : * * kwargs ) * * kwargs ) : # Collect losses that are dependent on inputs for layer in self.output_layers : print_fn : Print function to use . try : verbose : Integer . 0 , 1 , or 2 . Verbosity mode . if layer.__class__.__name__ == 'TimeDistributed ' : from keras.engine.topology import Input self.inputs = list ( inputs ) # Tensor or list of tensors . tensor_map = { } if len ( weights ) == 12 : epochs=1 , weights [ 3 ] , A Numpy array of predictions . config.append ( { 'class_name ' : self.layers [ 0 ] .__class__.__name__ , # Then we process nodes in order of layer depth . If unspecified , ` use_multiprocessing ` will default to ` False ` . ValueError : In case of invalid layer name or index . self.batch_input_shape = batch_input_shape self.name elif 'input_shape ' in kwargs : This method deals with an inherent problem of HDF5 file which is not line_length : Total length of printed lines or a single instance if the model has only one input . the training samples , used for weighting the loss function def _add_inbound_node ( self , input_tensors , output_tensors , prefix = 'input ' computed_data.append ( tensor_map [ str ( id ( x ) ) ] ) # hence the same layer might appear twice ) def __init__ ( self , input_shape=None , batch_size=None , validation_data=None , output_shape = self.compute_output_shape ( input_shape ) for key , value in obj.items ( ) : return self.model.uses_learning_phase if source ! = target : 'theano ' : False , node_indices.append ( None ) `` `` '' Calls the model on new inputs . Connect current layer with last layer from tensor : ( 2 , 3 , 1 , 0 ) ) if len ( layer_names ) ! = len ( filtered_layers ) : ' ( { } vs { } ) . '.format ( self._trainable = value weights [ 0 ] = weights [ 0 ] [ : , 0 , : , : ] outputs = input_layer._inbound_nodes [ 0 ] .output_tensors ' ' + str ( len ( output_masks ) ) + ' output masks . ' ) def __init__ ( self , outbound_layer , with multiple input shapes ) , in which case Note : Please also see 'batch_size ' , get_input_mask_at ( node_index ) def assert_input_compatibility ( self , inputs ) : initializer=None , val.shape , # Read final output shapes from layers_to_output_shapes . target_class = layer.__class__.__name__ outputs str ( weights [ 0 ] .shape ) + ' and size ' ` layer.get_input_at ( node_index ) ` . first_layer_config = first_layer [ 'config ' ] import json a generator for the validation data determined via tensor._keras_history if not provided . ` ( x_val , y_val , val_sample_weights ) ` on which to evaluate number of the dimensions of the weights return [ ] on images on CPU in parallel to training your model on GPU . weight_values [ i ] ) ) layers_to_output_shapes = { } return [ kernels , recurrent_kernels , biases ] layer = created_layers [ layer_name ] if cache_key in self._output_mask_cache : model.optimizer.set_weights ( optimizer_weight_values ) 'keras_version ' : keras_version , config = yaml.load ( yaml_string ) steps=None ) : `` `` '' Retrieves an attribute ( e.g . input_tensors ) from a node . 'hence the notion of `` layer input mask '' ' self._per_input_updates [ inputs_hash ] = [ ] outbound_layer._inbound_nodes.append ( self ) workers=workers , if isinstance ( inputs , list ) and inputs == [ ] : output_ls_copy.append ( x ) return deserialized for i in range ( len ( input_shapes ) ) : The same layer can be reinstantiated later with the current layer . # Create the node linking internal inputs to internal outputs . prefix = 'sequential_ ' RuntimeError : if the layer is n't yet built ` node_indices ` and ` tensor_indices ` are basically fine-grained coordinates # to the index of the nodes that are saved in the config . 'Layer ' + layer.name model_config = self._updated_config ( ) if has_arg ( layer.call , 'mask ' ) : # Apply activity regularizer if any : output_tensors : list of output tensors . ( useful for resuming a previous training run ) . # set weights def input_shape ( self ) : sample_weight_mode : If you need to do timestep-wise return self._per_input_losses [ inputs_hash ] 'batch ' is a special option for dealing with the def trainable_weights ( self , weights ) : name=name , dtype=dtype , mask : Tensor or list of tensors . str ( input_shape ) + ' : model has ' y : labels , as a Numpy array . 'should have a single output tensor . ' 'sample_weight_mode ' : model.sample_weight_mode , `` `` '' Retrieves the input mask tensor ( s ) of a layer . n_gates = 4 ( or list of tensors if the layer has multiple inputs ) . if node_key in self._network_nodes : first time the layer was called . } , default=get_json_type ) .encode ( 'utf8 ' ) `` `` '' Trains the model for a fixed number of epochs ( iterations on a dataset ) . used for model definition or training . A tensor . group.attrs [ ' % s % d ' % ( name , chunk_id ) ] = chunk_data inbound_layer = created_layers [ inbound_layer_name ] containing the configuration of a layer . # carry over the input mask x._keras_shape = s node = inbound_layer._inbound_nodes [ node_index ] from .network import Network from_config ( config ) dtype=None , while isinstance ( first_layer , ( Model , Sequential ) ) : It keeps the shape , but changes between the layout ( Fortran/C ) . Eg . : weights += layer.non_trainable_weights if output_masks is None : tensor_index = node.tensor_indices [ i ] self.inputs = [ inputs ] ( if the model has multiple inputs ) . # We create an input node , which we will keep updated self.outputs = [ ] self.layers [ -1 ] ._outbound_nodes = [ ] if layer.data_format == 'channels_first ' : `` `` '' Retrieves an attribute ( e.g . input_tensors ) from a node . weights : List of weights values ( Numpy arrays ) . if not trainable : # Ensure name unicity , which will be crucial for serialization # and there is only one node and one tensor output . ' '' in the current model ) was found to ' def model_from_config ( config , custom_objects=None ) : if input_tensors : if inputs_hash in self._per_input_losses : if spec.min_ndim is not None : if len ( self._inbound_nodes ) > 1 : layers_by_depth [ depth ] .append ( layer ) JSON-serializable structure representing ` obj ` . x = K.identity ( x ) kwargs = { } model_layers = model.layers return [ tensor ] batch_shape = first_layer.batch_input_shape if isinstance ( config , list ) : input_layers self.is_placeholder = True save_model ( self , filepath , overwrite , include_optimizer ) weights , return output_tensors outbound_nodes : List of nodes . if layer.data_format == 'channels_last ' : str ( layers_with_complete_input ) ) We update the _keras_shape of every input tensor with name=name , dtype=dtype , callbacks=callbacks , # input shape and dtype . input_uid = _object_list_uid ( inputs ) tensor : The tensor to start from . if x in inputs_ls : The model returned by ` load_model ` layer = filtered_layers [ k ] from .. utils.layer_utils import count_params storing the weight value , named after the weight tensor . The same structure , where occurrences insecure = re.sub ( ' ( [ a-z ] ) ( [ A-Z ] ) ' , r'\1_\2 ' , intermediate ) .lower ( ) requesting ` input_shape ` will raise an Exception . output_tensors : list of output tensors . # Update cache ; previous_depth = layers_depths.get ( node.outbound_layer , 0 ) self._updates += updates # new : ( kernel_rows , kernel_cols , stack_size , filters ) uses_learning_phase = computed_tensors [ 0 ] ._uses_learning_phase from .engine import topology if preds.min ( ) < 0. or preds.max ( ) > 1. : source = 'CuDNNLSTM ' if weight_names : or ( inputs , targets , sample_weights ) self._outbound_nodes = [ ] It will be called on each line of the summary . This method is the reverse of ` get_config ` , layer=sublayer , finished_nodes : Set of nodes whose subgraphs have been traversed from keras.models import load_model if shape is not None and not batch_shape : dtype=None , # List of tensors , 1:1 mapping with input_tensor . if layer.name == name : somewhere else ( forbidden in ` Sequential ` models ) . multi-output model , you could also pass a dictionary , process_node ( layer , node_data ) 'name ' , callbacks=None , limitations of HDF5 data ; it shuffles in batch-sized chunks . self.model.trainable = self.trainable weights [ 1 ] = conv_utils.convert_kernel ( weights [ 1 ] ) name = str ( w.name ) + ' _ ' + str ( i ) 'keras_version ' : keras_version , weights [ 9 ] ] , axis=-1 ) ' ( and thus will be missing ' `` `` '' Collects the output shape ( s ) of a list of Keras tensors . uses_correlation = { 'tensorflow ' : True , input_masks = _to_list ( input_masks ) 'You can build it manually via : ` ' input_tensors : list of input tensors ( or single input tensor ) . trainable=True , raise TypeError ( 'Sequential model can not be built : model is empty . ' self._internal_input_shapes = [ x._keras_shape for x in self.inputs ] inputs : Tensor or list of tensors . # self.trainable_weights 'All layer names should be unique . ' self.input_layers_node_indices = self.model.input_layers_node_indices ' weights . Provided weights : ' input_shapes= [ batch_input_shape ] , ndim=None , str ( spec.dtype ) + ' , found dtype= ' and C layout , recurrent kernels are transposed . For LSTM biases are summed/ Note that layers that do n't have weights are not taken def updates ( self ) : original_backend=None , if layer.__class__.__name__ == 'TimeDistributed ' : updates = [ ] 'loss ' : model.loss , model.fit_generator ( generate_arrays_from_file ( '/my_file.txt ' ) , ` call ` method of the layer at the call that created the node . weights : a list of Numpy arrays . The number def predict_classes ( self , x , batch_size=None , verbose=0 , steps=None ) : 'file because they are larger than % d bytes : % s ' the ` len ( validation_data ) ` as a number of steps . # of the deepest model to infer input shape and dtype . for n in group.attrs [ ' % s % d ' % ( name , chunk_id ) ] ] ) a None shape is compatible with any shape . Note : Please also see 'in the serialized model ( and thus will be missing ' reshape : Reshape weights to fit the layer when the correct number have different sizes . For example , the last batch of the epoch computed_masks = [ x [ 1 ] for x in computed_data ] If unspecified , ` use_multiprocessing ` will default to ` False ` . # Handle ` name ` argument . if callable ( obj ) : The config of a layer does not include connectivity n_gates ) model = cls ( ) will be batches of 32-dimensional vectors . initial_epoch=0 ) : # Class Methods # Build a dict { depth : list of nodes with this depth } return trainable_weights + weights source_tensors = [ ] def load_model ( filepath , custom_objects=None , compile=True ) : continue if 'backend ' in f.attrs : This is useful for separating training updates and model_config = json.loads ( model_config.decode ( 'utf-8 ' ) ) inbound_nodes : List of nodes . positions=positions , before declaring the evaluation round finished . ( during training only ) . This can be useful to tell the model to cache_key += ' _ ' + ' , '.join ( [ str ( id ( x ) ) for x in masks ] ) 'inbound_nodes ' : filtered_inbound_nodes , import yaml weight_value_tuples += zip ( symbolic_weights , weight_values ) `` `` '' Retrieves the output shape ( s ) of a layer at a given node . # The following are implemented as property functions : input_tensor._keras_shape = batch_input_shape for k , name in enumerate ( layer_names ) : from keras.engine import Input , Layer , saving , get_source_inputs a node is added to ` layer._outbound_nodes ` . self._trainable_weights = weights if len ( bad_attributes ) > 0 : outbound_layer = self.outbound_layer.name get_input_at return K.batch_get_value ( params ) y2 = _to_list ( model.predict ( X ) ) # here the batch dimension is None , chunk_id = 0 'input shape ' ) use_multiprocessing : if True , use process based threading . return self.model.fit ( x , y , input_spec : List of InputSpec class instances def regularizers ( self ) : regularization_losses = [ self.activity_regularizer ( x ) for x in _to_list ( output ) ] `` `` '' Returns the ` updates ` from all layers that are stateful . `` `` '' Computes the output shape of the layer . layer : layer instance . return any ( [ x._uses_learning_phase for x in self.outputs ] ) if layer : For missing biases in ` LSTM ` / ` GRU ` ( ` use_bias=False ` ) no conversion is made . source_tensors = [ ] # the model we will return node_indices=node_indices , return self._get_node_attribute_at_index ( 0 , 'output_masks ' , of ` Sequence ` ( ` keras.utils.Sequence ` ) . use_multiprocessing=use_multiprocessing , `` `` '' Retrieves the model 's losses . def input ( self ) : weight_value_tuples.append ( ( symbolic_weights [ i ] , assert node_index == 0 weight_values = [ np.asarray ( g [ weight_name ] ) for weight_name in weight_names ] 'should have a single output tensor . ' layer_config = { 'class_name ' : layer.__class__.__name__ , layer_config = layer.get_config ( ) dtype=val.dtype ) if hasattr ( layer , '_flattened_layers ' ) : 'get an ` input_shape ` or ' for layer in self.layers [ 1 : ] : if not proceed : # Check specific shape axes . from keras.models import load_model if not params : # that are part of the model . for input_data in node_data : 'output_shapes ' , # Prevent cycles . verbose=verbose , ndim dtype=dtype , layers : a list of target layers . while ( ' % s % d ' % ( name , chunk_id ) ) in group.attrs : for x in input_tensors : for layer in self._flattened_layers : or vice verca . However , there 's no conversion required between TF and CNTK . 'but was passed an input_mask : ' if ndim is not None and ndim > spec.max_ndim : config : Model config dictionary . warning . str ( weights [ 0 ] .size ) + ' . ' ) these arguments are passed into ` tf.Session.run ` . include_optimizer : If True , save optimizer 's state together . ' can not obtain value for tensor ' str ( layer.input_spec ) ) The output of the generator must be either for x in input_tensors : node_key = self._node_key ( layer , node_index ) # if there 's no bias weight in the file , skip this conversion 'automatically inferred . ' Output mask tensor ( potentially None ) or list of output if len ( layer._inbound_nodes [ -1 ] .output_tensors ) ! = 1 : See [ optimizers ] ( /optimizers ) . weight_names.append ( name.encode ( 'utf8 ' ) ) 'Use ` get_output_shape_at ( node_index ) ` ' `` `` '' Loads a model saved via ` save_model ` . raise AttributeError ( 'Layer ' + self.name node_index = layer._inbound_nodes.index ( node ) 'output_tensors ' , self.node_indices = node_indices self._nodes_by_depth = nodes_by_depth a node is added to ` layer._inbound_nodes ` . `` `` '' A Network is a directed acyclic graph of layers . # on the fly because the inbound node may not yet exist , if model.__class__.__name__ == 'Sequential ' : if isinstance ( layer , topology.InputLayer ) : self.build ( ) for layer in merge.layers : self._output_shape_cache [ cache_key ] = output_shapes class_name = conf [ 'name ' ] can specify them via the ` target_tensors ` argument . 'file because they are larger than % d bytes : % s ' return [ x ] g = f [ name ] if skip_mismatch : # if masking is explicitly supported , by default or create its a placeholder tensor ( pass arguments ` input_shape ` ' about its expected input shape , ' use_multiprocessing=False , original_keras_version , def reset_states ( self ) : output_tensors = to_list ( The loss may potentially be conditional on some inputs tensors , inbound_names.append ( None ) str ( np.prod ( layer_weights_shape ) ) + ' . ' nodes = self._nodes_by_depth [ depth ] model.model._make_train_function ( ) # that can be computed from the inputs provided . return count_params ( self.weights ) weights = forward_weights + backward_weights layers.append ( sublayer ) # - * - coding : utf-8 - * custom_objects=custom_objects ) x = Input ( shape= ( 32 , ) ) node_indices = [ ] class_weight=None , weights [ 11 ] ] , axis=-1 ) Scalar test loss ( if the model has no metrics ) layer_config = layer.get_config ( ) # This will build the current layer If x is a Keras tensor : # every time the Container is called on a set on input tensors , for chunk_id , chunk_data in enumerate ( chunked_data ) : epochs=1 , base_layer.Node ( outbound_layer=self , steps_per_epoch=1000 , epochs=10 ) metrics=None , if legacy_models.needs_legacy_support ( self ) : 'dtype ' : self.dtype , model = Model ( x , y ) param_dset [ ( ) ] = val chunked_data = np.array_split ( data_npy , num_chunks ) def transpose_input ( from_cudnn ) : object_list = _to_list ( object_list ) if isinstance ( outputs , ( list , tuple ) ) : output_shapes = output_shapes [ 0 ] 'they can not be the output of ' if ndim is not None and ndim < spec.min_ndim : return self._output_mask_cache [ cache_key ] steps_per_epoch=None , shape_key = layer.name + '_0_0 ' from which to retrieve the attribute . if spec_dim ! = dim : 'but was passed an input_mask : ' `` `` '' Dumps all layer weights to a HDF5 file . depth_keys = list ( layers_by_depth.keys ( ) ) target_tensors=target_tensors , skip_mismatch=False , reshape=False ) : epochs , name = conf.get ( 'custom_name ' ) # In this case we will later create an input layer indefinitely . An epoch finishes when ` steps_per_epoch ` weights = convert_weights ( weights , from_cudnn=False ) weights=weights [ : num_weights ] , # because in that case even chunking the array would not make the saving return data the same length as ` inbound_layers ` . cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) For instance , this allows you to do real-time data augmentation if isinstance ( obj , list ) : symbolic_weights = layer.weights self.built = False max_queue_size=max_queue_size , `` `` '' Adds a weight variable to the layer . if inbound_layer_name not in created_layers : # Networks start with a pre-existing node self._feed_input_names = [ ] f = h5py.File ( filepath , mode= ' w ' ) for x in previous_sources : node_key = self._node_key ( layer , node_index ) `` pay more attention '' to samples from self._nodes_by_depth = self.model._nodes_by_depth arguments : dictionary of keyword arguments that were passed to the model.model._make_train_function ( ) inbound_node = inbound_layer._inbound_nodes [ node_index ] shuffle : Boolean ( whether to shuffle the training data # instantiate optimizer if not dtype : original_keras_version=None , # ( e.g . a model such as A ( B ( A ( B ( x ) ) ) ) ) if hasattr ( layer , '_flattened_layers ' ) : output_masks = to_list ( def reset_states ( self ) : nodes = self._nodes_by_depth [ depth ] `` `` '' Creates a layer from its config . trainable ( boolean ) if len ( input_data ) == 3 : use_multiprocessing : Boolean . self.tensor_indices = tensor_indices output_tensors [ i ] ._uses_learning_phase = getattr ( from .. legacy import interfaces all ops in the graph to the new inputs the list ` nodes_in_decreasing_depth ` and the set ` container_nodes ` . # tensor output of each node . ` _keras_history ` : Last layer applied to the tensor . x : Numpy array of training data . load_weights_from_hdf5_group_by_name ( import re is meant to be sparse . weights : A list of Numpy arrays with shapes and types matching if symbolic_weights : def save_weights_to_hdf5_group ( f , layers ) : return [ ] return all_attrs from .topology import get_source_inputs A Keras model instance ( uncompiled ) . # computable_tensors : all tensors in the graph ( or list of shape tuples if the layer has multiple inputs ) . count_params ( ) output_tensors [ i ] ._keras_history = ( self , if layer not in layers : 'Received inputs : { } . ' inputs : input tensor or list of input tensors . raise ImportError ( ' ` load_weights ` requires h5py . ' ) will be obtained from ` tensor._keras_history ` . inputs = inputs [ : ] `` `` '' Linear stack of layers . A mask tensor or list of mask tensors . mask : Tensor or list of tensors . def normalize_legacy_config ( conf ) : config.append ( { 'class_name ' : self.layers [ 0 ] .__class__.__name__ , self.shape = shape def build_map_of_graph ( tensor , finished_nodes , nodes_in_progress , get_input_shape_at ( node_index ) epochs=1 , # scalar Assumes that the layer will be built self._initial_weights = None Function that converts input kernel to the other format . def __init__ ( self , * * kwargs ) : class Layer ( object ) : losses += layer.get_losses_for ( inputs ) class_weight=class_weight , to match that input shape provided . def _updated_config ( self ) : model.add ( Dense ( 32 , input_shape= ( 500 , ) ) ) add_unprocessed_node ( layer , node_data ) return masks ` keras.models.model_from_yaml ( yaml_string , custom_objects= { } ) ` . if layer.__class__.__name__ == 'GRU ' : A node from layer A to layer B is added to : if not self.supports_masking : # List of integers , 1:1 mapping with inbound_layers . from .network import get_source_inputs if ( hasattr ( self , 'activity_regularizer ' ) and if layer in unprocessed_nodes : 'batch_input_shape argument . ' ) ' '' ) expects ' def trainable ( self , value ) : or its index in the graph . Indices are based on 'Keras tensors . Found : ' + str ( x ) ) from . import __version__ as keras_version str ( len ( self.layers ) ) + ' layers . ' ) A flat list of Numpy arrays self.output_layers_tensor_indices = [ ] of weight arrays is present but their shape does not match . if w.name.split ( '/ ' ) [ -1 ] == 'variable ' : To load a network from a JSON save file , use raise ValueError ( 'The list of inputs passed to the model ' sample_weight=None , weight_values = K.batch_get_value ( symbolic_weights ) For instance , this allows you to do real-time data augmentation for x , y , mask in zip ( reference_output_tensors , output_tensors , output_masks ) : from keras.utils.generic_utils import object_list_uid , to_list Input shape tuple Scalar training loss ( if the model has no metrics ) model 's target , which will be fed with the target data during 'Found : ' + str ( layer ) ) if len ( computed_tensors ) == 1 : f.attrs [ 'training_config ' ] = json.dumps ( { Numpy array with the same length as the input samples original_backend = f.attrs [ 'backend ' ] .decode ( 'utf8 ' ) target_tensors : By default , Keras will create a placeholder for the uses_learning_phase = any ( [ x._uses_learning_phase for x in computed_tensors ] ) self._output_tensor_cache = { } batch_input_shape = tuple ( kwargs [ 'batch_input_shape ' ] ) 'loss_weights ' : model.loss_weights , class_weight : dictionary mapping classes to a weight value , return self._updates kwargs [ 'mask ' ] = computed_masks # List of layer instances . # Keep track of unconditional losses ( in which case its weights are n't yet defined ) . `` ` python self.input_names = self.model.input_names f = f [ 'model_weights ' ] constraint=None ) : specs = [ ] # It 's supposed to be an input layer , so only one node 'in the serialized model ' # masking not explicitly supported : return None as mask for x in self.inputs : layer_cache [ name ] = layer batches have been seen by the model . sparse : A boolean specifying whether the placeholder model = Model ( x , y ) either a tensor or None ( no mask ) . x , y = process_line ( line ) and guarantees the single use of every input per epoch when layer_weights_shape = K.int_shape ( layer.weights [ 0 ] ) for x in input_tensors : max_queue_size=10 , workers=1 , if not _is_all_none ( previous_mask ) : self._per_input_losses [ inputs_hash ] += losses 'dtype ' : self.dtype , 'apply a reshape operation . ' # call layer self.input_masks = input_masks self.name + ' : expected axis ' return self.model.call ( inputs , mask ) target = 'GRU ( reset_after=False ) ' if spec_dim ! = dim : weights = weights [ nb_param : ] if layer.__class__.__name__ == 'Conv2D ' : layer = self.output_layers [ i ] `` `` '' Implements topological ( order-based ) weight loading . merge_inputs.append ( merge_input ) epochs=epochs , inbound_node.output_tensors [ inbound_tensor_index ] ) batch_input_shape : Shape tuple , including the batch axis . self.model.callback_model = self verbose=verbose , input_shape = layers_to_output_shapes [ shape_key ] # Apply activity regularizer if any : self.outputs = [ output_tensor ] 'and thus has no defined input shape . ' ) model_weights_group = f.create_group ( 'model_weights ' ) output_mask = [ output_mask ] * len ( output_ls ) `` `` '' Load a model from a legacy configuration . def non_trainable_weights ( self ) : ValueError : In case the generator yields data in an invalid format . # and there is only one node and one tensor output . # Get sorted list of node depths . weights [ 1 ] = np.transpose ( weights [ 1 ] , ( 3 , 2 , 0 , 1 ) ) 'Found : ' + str ( layer ) ) 'name ' : self.name , params = self.weights inbound_layers= [ ] , for sublayer in layer._flattened_layers : for i , ( w , val ) in enumerate ( zip ( symbolic_weights , layer=None , node_index=None , tensor_index=None ) : `` `` '' Parses a yaml model configuration file and returns a model instance . sample_weight=sample_weight , # and building the layer if needed . `` `` '' # Update cache ; self.loss = self.model.loss if spec is None : `` `` '' Internal method to create an inbound node for the layer . 'config ' : obj.get_config ( ) } if depth not in nodes_by_depth : `` `` '' Linear stack of layers . order = ' F ' if from_cudnn else ' C ' # trainable weights 'state . As a result , your model is ' of 32-dimensional vectors . if str ( id ( x ) ) in tensor_map : K.is_keras_tensor ( x ) self._output_shape_cache = { } # collecting output ( s ) , mask ( s ) , and shape ( s ) . for instance activity losses are conditional on the layer 's inputs . def trainable ( self ) : This is done as part of _add_inbound_node ( ) . shape=None , sample_weight=sample_weight , self._trainable_weights = [ ] validation_steps=validation_steps ) if inputs_hash not in self._per_input_updates : while any ( map ( lambda x : x.nbytes > HDF5_OBJECT_HEADER_LIMIT , chunked_data ) ) : raise RuntimeError ( 'The following attributes can not be saved to HDF5 ' output_tensors , _ , _ = self.run_internal_graph ( inputs , masks ) return layer of HDF5 file which is not able to store mask : A mask or list of masks . A mask can be # will call the parent Sequential model . # List of tensors , created by outbound_layer.call ( ) . if layer.__class__.__name__ in [ 'Model ' , 'Sequential ' ] : # Update tensor history , _keras_shape and _uses_learning_phase . str ( len ( input_spec ) ) + ' inputs , ' return unique_tensors + non_tensors # without the container being notified of it . raise ValueError ( 'Invalid input_shape argument ' str ( weights ) [ :50 ] + ' ... ' ) class Model ( Container ) : `` `` '' Returns a yaml string containing the network configuration . # old : ( filters , stack_size , kernel_rows , kernel_cols ) for x in self.outputs : raise ValueError ( after loading . layer_indices [ layer ] = len ( layer_indices ) for name , val in zip ( weight_names , weight_values ) : output_shape weights [ 5 ] , # Properties source = 'CuDNNGRU ' warnings.warn ( 'Skipping loading of weights for layer { } '.format ( layer.name ) return input_shape 'automatically inferred . ' if shape is not None and not batch_shape : else : from .. engine.base_layer import Layer return trainable_weights ( list of variables ) chunked_data = np.array_split ( data_npy , num_chunks ) before each epoch ) or str ( for 'batch ' ) . weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 0 , 1 , 3 , 2 ) ) for fine-tuning or transfer-learning models where 'optimizer attributes or optimizer state ' ndim = K.ndim ( x ) self.layers = [ ] # Stack of layers . input_shapes= [ x._keras_shape for x in self.inputs ] , if isinstance ( input_shape , list ) : for i , layer in enumerate ( self.input_layers ) : layers_with_complete_input.append ( layer.name ) custom_objects = { } call ( x , mask=None ) : Where the layer 's logic lives . # same size of node.input_tensors . if len ( set ( self.inputs ) ) ! = len ( self.inputs ) : layers = self.layers # old : ( kernel_rows , kernel_cols , stack_size , filters ) in the FAQ for instructions on how to install ` h5py ` . assert tensor_index == 0 during prediction . instead of an integer . include_optimizer : If True , save optimizer 's state together . callbacks=None , model.add ( Dense ( 10 , activation='softmax ' ) ) A tensor or list/tuple of tensors . class Node ( object ) : List of input tensors . filtered_layers.append ( layer ) # Check that x is a Keras tensor . 'InputLayer , not both at the same time . ' ) layer_configs.append ( { filepath : one of the following : inbound_layers , node_indices , tensor_indices , B._inbound_nodes # List of integers , 1:1 mapping with inbound_layers . input_tensor=tensor ) # ( e.g . activity regularizers ) . trainable : A boolean , whether the weight should 'config ' : layer.get_config ( ) } ( necessary since each inbound layer might have several nodes , `` `` '' Retrieves the input mask tensor ( s ) of a layer . try : is meant to be sparse . inbound_layer , node_index , tensor_index = x._keras_history depth_keys = list ( self._nodes_by_depth.keys ( ) ) if cache_key in self._output_tensor_cache : model : Keras model instance to be saved . # Arguments self._built = False mask = masks return deserialize ( config , custom_objects=custom_objects ) for x in node.output_tensors : str ( x.name ) ) weights = weights [ nb_param : ] 'output mask ' ) the layers from where ` input_tensors ` originate . inputs : A tensor or list of tensors . 'config ' : layer.get_config ( ) } ) @ interfaces.legacy_add_weight_support from .engine.saving import model_from_config a list of size 1 containing the tensor . for conf in config [ 1 : ] : `` `` '' Generates output predictions for the input samples . custom_objects=custom_objects ) shape = weights [ 0 ] .shape as they ca n't be passed easily to children processes . Build from x._keras_shape # Default values of symbolic_weights is /variable # a few lines above . updates = to_list ( updates ) If the output layer in the model is named , you can also pass a config : dictionary with configuration . self._non_trainable_weights = weights # mirror model attributes trainable_weights = self._gather_list_attr ( 'trainable_weights ' ) self.assert_input_compatibility ( inputs ) 'name ' : self.name } if type ( obj ) .__name__ == type.__name__ : ' ` batch_input_shape ` argument . ' ) self.build ( input_shapes [ 0 ] ) kernels , [ biases ] ) ( Numpy arrays ) . The updates may potentially be conditional on some inputs tensors , verify the input assumptions of the layer one per output tensor of the layer ) . dtype=None , input_tensor=None , sparse=False , name=None ) : depth = max ( depth , previous_depth ) # Legacy support return json.dumps ( model_config , default=get_json_type , * * kwargs ) # first layer must have a defined input shape ` custom_objects ` should be a dictionary mapping None or a tensor ( or list of tensors , weights = new_weights max_queue_size=max_queue_size , warnings.warn ( msg.format ( type ( self ) .__module__ , type ( self ) .__name__ ) , stacklevel=2 ) # to non-input layers . tensor_map [ str ( id ( x ) ) ] = ( y , mask ) output_tensors = output_tensors [ 0 ] layer , node_index , _ = tensor._keras_history # splitting does n't matter as long as the two sets sum is kept . original_backend ) if input_tensor is not None and batch_input_shape is None : reshape : Reshape weights to fit the layer when the correct number We update the _keras_history of the output tensor ( s ) original_keras_version , # and labels , from each line in the file node_index = self.output_layers_node_indices [ i ] ' weight ( s ) , but the saved weights ' for depth in depth_keys : For instance , ` batch_shape= ( 10 , 32 ) ` indicates that self.is_placeholder = False if 'weights ' in kwargs : length ( equal to the size of this batch ) . Different batches may if layer : if self.layers [ 0 ] not in layers : Will only include updates that are either self._output_shape_cache = { } `` `` '' Wrapper around self.call ( ) , for handling internal references . str ( node.arguments ) + ' . They will not be included ' raise Exception ( 'Layers should have equal number of output tensors ' node_index = self.output_layers_node_indices [ i ] 'tensor_indices ' : self.tensor_indices } # Raises def __init__ ( self , * * kwargs ) : # convert the weights between CuDNNGRU and GRU ( reset_after=True ) It is the topological form of a `` model '' . A Model raise RuntimeError ( 'The name `` ' + name + ' '' is used ' output_tensors=self.outputs , model_layers = legacy_models.legacy_sequential_layers ( model ) num_chunks += 1 finished_nodes = set ( ) if not isinstance ( self.input_spec , ( list , tuple ) ) : str ( inputs ) + ' . All inputs to the layer ' node_index = self.input_layers_node_indices [ i ] created_layers = { } weights = weights [ num_weights : ] input_shapes = self._inbound_nodes [ 0 ] .input_shapes a list of strings if 'batch_size ' in kwargs : ( if any ) . If not , exceptions are raised . def stateful ( self ) : kwargs = input_data [ 3 ] # are relevant to the current graph ) . f , self.layers , reshape=reshape ) if layer is not None : trainable : Boolean , whether the layer weights try : masks : List of masks ( tensors or None ) . or ` layer.get_input_shape_at ( node_index ) ` . # splitting does n't matter as long as the two sets sum is kept . input_tensor._uses_learning_phase = False def get_or_create_layer ( layer_data ) : ' due to mismatch in shape ' x : input data , as a Numpy array or list of Numpy arrays ( 'axes= ' + str ( self.axes ) ) if self.axes else `` ] if shape [ :2 ] ! = ( layer.kernel_size [ 0 ] , 1 ) or shape [ 3 ] ! = layer.filters : elif not self._inbound_nodes : deserialized [ key ] = convert_custom_objects ( value ) if node_key in model._network_nodes : self._feed_inputs = self.model._feed_inputs for i in range ( len ( input_shapes ) ) : self.output_layers_node_indices = self.model.output_layers_node_indices A list of dicts ( each dict is a layer config ) . layer.name + ' '' is part of a cycle . ' ) warnings.warn ( ` custom_objects ` should be a dictionary mapping config.append ( { 'class_name ' : layer.__class__.__name__ , tensor_index = self.output_layers_tensor_indices [ i ] from .. import __version__ as keras_version # old : ( filters , stack_size , ... ) def predict_generator ( self , generator , steps=None , `` `` '' layers.append ( layer ) ValueError : In case of invalid arguments for types = ( source , target ) Has no effect when ` steps_per_epoch ` is not ` None ` . min_ndim=None , def _node_key ( layer , node_index ) : ( Theano , TensorFlow or CNTK ) , which we augment with certain user_kwargs = copy.copy ( kwargs ) ' ` Sequential.from_config ( config ) ` ? ' ) batch_size , ) + tuple ( kwargs [ 'input_shape ' ] ) # old : i , c , f , o h5py = None add_unprocessed_node ( layer , node_data ) raise ImportError ( ' ` save_model ` requires h5py . ' ) mask_cache_key += ' _ ' + ' , '.join ( [ str ( id ( x ) ) for x in masks ] ) def build ( self , input_shape ) : # Set self.layers and self.layers_by_depth . self._initial_weights = None Saved models can be reinstantiated via ` keras.models.load_model ` . # depth levels in the graph . input_shapes , output_shapes , for layer in self.layers [ 1 : ] : if len ( set ( self.outputs ) ) ! = len ( self.outputs ) : `` `` '' Retrieves the input shape ( s ) of a layer at a given node . weights += layer.non_trainable_weights if hasattr ( w , 'name ' ) and w.name : class InputSpec ( object ) : layer : layer instance . self._outbound_nodes = [ ] # Appended to by calls to ` __call__ ` . inputs : List of tensors workers=workers , if self.built : if hasattr ( x , '_keras_history ' ) : return { 'class_name ' : class_name , config = yaml.load ( yaml_string ) output_masks = [ None for _ in output_tensors ] if not custom_objects : # old : i , c , f , o See [ losses ] ( /losses ) . finished_nodes : Set of nodes whose subgraphs ' ( named `` ' + layer.name def call ( self , inputs , * * kwargs ) : 'Conv2D ' , self.optimizer = self.model.optimizer max_queue_size : Integer . Maximum size for the generator queue . def predict_classes ( self , x , batch_size=None , verbose=0 , steps=None ) : raise ValueError ( ' Can not add an empty model ' return self.legacy_get_config ( ) output_masks = layer.compute_mask ( computed_tensor , is commonly smaller than the others , if the size of the dataset f , self.layers , skip_mismatch=skip_mismatch , class_weight : dictionary mapping classes to a weight value , weights : Should be a list input_tensor._uses_learning_phase = False model.compile ( optimizer='rmsprop ' , # we assume a 1:1 mapping from tensor to mask tensor_indices=tensor_indices , # mirror model attributes batch_size : Optional input batch size ( integer or None ) . inbound_names = [ ] from .. engine import InputSpec , Layer self.output_layers.append ( layer ) try : if w.name.split ( '/ ' ) [ -1 ] == 'variable ' : output_shape_keys.append ( shape_key ) def __init__ ( self , input_shape=None , batch_size=None , iterable = iterable_or_element original_backend=original_backend ) ) to be created is sparse . cache_key = ' , '.join ( [ str ( id ( x ) ) for x in inputs ] ) dictionary mapping the input name to a Numpy array . ' is incompatible with layer ' str ( pv.shape ) name : A name of the attributes to load . _add_inbound_node ( layer , index=0 ) output of ` get_weights ` ) . try : import numpy as np batch_input_shape = tuple ( batch_input_shape ) axes : Dictionary mapping integer axes to output_masks = _to_list ( output_masks ) input_shape : Shape tuple ( tuple of integers ) Every layer should expose ( if appropriate ) an ` input_spec ` attribute : self._outbound_nodes = [ ] node_index = self.input_layers_node_indices [ i ] nodes . self._trainable_weights = [ ] weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 2 , 3 , 1 , 0 ) ) for k , name in enumerate ( layer_names ) : # only convert between different types `` `` '' Retrieves the output tensor ( s ) of a layer . for layer in self.layers : optimizer_config = training_config [ 'optimizer_config ' ] raise ValueError ( 'Invalid bias shape : ' + str ( bias_shape ) ) # Ensure name unicity , which will be crucial for serialization return node.input_tensors verbose=verbose , return x raise ImportError ( ' ` save_model ` requires h5py . ' ) inputs_hash = None TypeError : if ` obj ` can not be serialized . name = prefix + ' _ ' + str ( K.get_uid ( prefix ) ) return self._gather_list_attr ( 'trainable_weights ' ) This allows you to save the entirety of the state of a model if type ( obj ) .__module__ == np.__name__ : for spec_dim , dim in zip ( spec.shape , x_shape ) : `` `` '' Checks if conversion on kernel matrices is required during weight loading . # Store in cache . # trainable weights 'dimension . ' ) self.input_tensors = input_tensors self.output_shapes = output_shapes # Do n't repeat work for shared subgraphs # as we add more layers # first layer must have a defined input shape `` `` '' Computes the loss on some input data , batch by batch . return cls ( inputs=input_tensors , outputs=output_tensors , name=name ) # The following 3 properties describe where nodes_in_progress = set ( ) nodes_by_depth = { } ImportError : if h5py is not available . 'You will have to compile your model again ' # If file exists and should not be overwritten : from .. layers import deserialize as deserialize_layer loss_weights = training_config [ 'loss_weights ' ] is commonly smaller than the others , if the size of the dataset weights += layer.weights if inputs_hash not in self._per_input_losses : 'You can build it manually via : ' layer_cache [ name ] = layer if isinstance ( obj , np.ndarray ) : for node , depth in nodes_depths.items ( ) : `` `` '' Computes the output shape of the layer . original_backend : Keras backend the weights were trained with , as a string . return custom_objects [ obj ] model_inputs.append ( [ layer.name , new_node_index , tensor_index ] ) else : for depth in depth_keys : recurrent_kernel = np.transpose ( recurrent_kernel , `` `` '' Normalizes a list/tensor into a list . output = self.call ( inputs , * * kwargs ) self.input_names.append ( layer.name ) def count_params ( self ) : else : return self.model.predict ( x , batch_size=batch_size , verbose=verbose , Each time a layer is connected to some new input , original_backend=None , # a few lines above . merge_input = layer_module.deserialize ( merge_input_config ) 'is not a list . We expect a list . ' Epoch at which to start training # Collect input tensor ( s ) coordinates . if issubclass ( layer.__class__ , Network ) : g = f.create_group ( layer.name ) config.append ( { 'class_name ' : 'Merge ' , 'config ' : merge_config } ) A Numpy array of probability predictions . # Set model name . computed_data.append ( tensor_map [ str ( id ( x ) ) ] ) # Set self.layers and self.layers_by_depth . _save_attributes_to_hdf5_group ( g , 'weight_names ' , weight_names ) # because in that case even chunking the array would not make the saving # new : ( ... , stack_size , filters ) f = h5py.File ( filepath , mode= ' r ' ) f.flush ( ) `` `` '' Generates output predictions for the input samples . will then be the sum of all individual losses . original_backend ) if len ( all_input_shapes ) == 1 : 'into probabilities ' # the input tensors come from : which layers , # Update self.losses # Add any potential unconditional model-level loss . raise AttributeError ( 'The layer `` ' + str ( self.name ) weights = [ kernel , recurrent_kernel , bias ] except TypeError : non_trainable_weights : List of variables . the model 's configuration ( topology ) `` `` '' Retrieves the input shape ( s ) of a layer at a given node . # Keep track of updates that depend on the inputs x_shape = None ` ( x_val , y_val , val_sample_weights ) ` on which to evaluate `` `` '' Fits the model on data generated batch-by-batch by a Python generator . the beginning of each epoch . Only used with instances RuntimeError : if a cycle is detected . node_index : The layer 's position ( e.g . via enumerate ) in a list of ' ( named `` ' + layer.name return self.model.fit ( x , y , A Keras model instance ( uncompiled ) . `` `` '' Retrieves the weights of the model . return input_shapes [ 0 ] def get_config ( self ) : inputs : List of tensors # which provides a speedup in TensorFlow . str ( mask ) ) `` `` '' Saves attributes ( data ) of the specified name into the HDF5 group . @ interfaces.legacy_generator_methods_support layer , node_index , _ = tensor._keras_history for layer_data in config [ 'output_layers ' ] : name = str ( w.name ) # ( not all nodes included in the layers continue A flat list of Numpy arrays target location , or provide the user with a manual prompt . steps_per_epoch=steps_per_epoch , training_config = f.attrs.get ( 'training_config ' ) # Update the depth of inbound nodes . validation_data=None , each entry describes one required input : if target_class == 'CuDNNGRU ' : layers_by_depth [ depth ] = [ ] used for scaling the loss function ( during training only ) . input_tensors=self.inputs , name = prefix + str ( K.get_uid ( prefix ) ) max_queue_size=max_queue_size , conf = normalize_legacy_config ( conf ) all_attrs += getattr ( layer , attr , [ ] ) for i in range ( len ( node.inbound_layers ) ) : 'before being used . ' ) # create the underlying model if value is not None and x_shape [ int ( axis ) ] not in { value , None } : epochs : Integer . Number of epochs to train the model . reshape=False ) : output_layers by sample_weight or class_weight during training and testing . str ( layers_with_complete_input ) ) The added Keras attributes are : kept_nodes = 0 if first_layer [ 'class_name ' ] == 'Merge ' : original_keras_version , source = 'GRU ( reset_after=False ) ' finally : the exact same state , without any of the code if losses is None or losses == [ ] : if spec.dtype is not None : layer_output_tensors = layer._inbound_nodes [ node_index ] .output_tensors if layer.__class__.__name__ == 'Conv3D ' : The convolution operation is implemented differently in different backends . raise ImportError ( ' ` load_model ` requires h5py . ' ) if len ( values ) == 1 : defaults to ` [ .33 , .55 , .67 , 1 . ] ` . g = f.create_group ( layer.name ) `` `` '' Wrapper around self.call ( ) , for handling internal references . `` `` '' Retrieves the output tensor ( s ) of a layer at a given node . If unspecified , ` max_queue_size ` will default to 10 . tensor_indices=tensor_indices , ` call ` method of the layer at the call that created the node . weight_tensor_bi_convlstm_new = topology.preprocess_weights_for_loading ( A ` History ` object . # Network.layers needs to have a deterministic order : return uses_correlation [ original_backend ] ! = uses_correlation [ K.backend ( ) ] of arrays and their shape must match layers.append ( self.layers [ 0 ] ) } , default=get_json_type ) .encode ( 'utf8 ' ) ' Add some layers first . ' ) input_tensor = K.placeholder ( shape=batch_input_shape , `` If this is a Keras 1 layer , please implement ` compute_output_shape ` to support Keras 2 . '' 'class_name ' : self.__class__.__name__ , if isinstance ( self.layers [ 0 ] , legacy_layers.Merge ) : the names of custom losses / layers / etc to the corresponding regularization_losses = [ A flat list of Numpy arrays . `` `` '' Parses a JSON model configuration file and returns a model instance . y=None , def _convert_rnn_weights ( layer , weights ) : y : labels , as a Numpy array . from . import saving model = load_model ( 'my_model.h5 ' ) layers = [ ] x : Numpy array of training data . filtered_inbound_nodes = [ ] ( e.g . L2 weight regularization , which only depends A mask tensor if not name : return output_shapes [ 0 ] if opened_new_file : If None is passed , the updates are assumed unconditional . def save_model ( model , filepath , overwrite=True , include_optimizer=True ) : # This will build the current layer input , output : Input/output tensor ( s ) . Note that if the layer is used self.max_ndim = max_ndim weights = convert_weights ( weights , from_cudnn=source == 'CuDNNLSTM ' ) ValueError : for incompatible GRU layer/weights or incompatible biases # Get sorted list of node depths . self.input_layers_node_indices.append ( node_index ) layer_output_tensors = layer._inbound_nodes [ node_index ] .output_tensors ' a ` batch_input_shape ` or an ` input_shape ` . ' ) if not isinstance ( self.input_spec , ( list , tuple ) ) : verbose : verbosity mode , 0 or 1 . `` `` '' Computes the loss on some input data , batch by batch . dtype = K.dtype ( input_tensor ) if layer.__class__.__name__ in conv_layers : if cache_key in self._output_shape_cache : reshape=False ) : if hasattr ( w , 'name ' ) and w.name : of 32-dimensional vectors . kwargs = node.arguments should have a defined input shape . What that input_shapes.append ( x_elem._keras_shape ) if hasattr ( w , 'name ' ) and w.name : def pop ( self ) : * * kwargs ) : # We do n't process nodes ( i.e . make layer calls ) self.add_loss ( layer.get_losses_for ( computed_tensors ) , inputs ) raise ValueError ( 'Was asked to retrieve layer at index ' validation_steps : Only relevant if ` steps_per_epoch ` if len ( shapes ) == 1 : # Inferring the output shape is only relevant for Theano . ( 'ndim= ' + str ( self.ndim ) ) if self.ndim else `` , ask the user with a manual prompt . def add_update ( self , updates , inputs=None ) : ValueError : In case of invalid layer name or index . `` ` node_index = node.node_indices [ i ] def trainable ( self , value ) : self._inbound_nodes [ 0 ] .output_tensors = self.outputs self.inputs = list ( inputs ) # Tensor or list of tensors . # This is for performance optimization self.model = None # Internal Model instance . a tuple ` ( inputs , targets , sample_weights ) ` . x : target object to be normalized . steps , inbound_names.append ( layer.name ) for i in range ( len ( node.inbound_layers ) ) : In this case you should make sure to specify steps_per_epoch , raise ValueError ( 'Layer # ' + str ( k ) `` `` '' Creates a layer from its config . ( e.g . build a new computational graph from the provided inputs ) . import numpy as np a specific dimension value . Scalar test loss ( if the model has no metrics ) # Support for legacy models attr_name : Human-readable attribute name , for error messages . if node in finished_nodes : if dtype is None : ( handled by Network ) , nor weights ( handled by ` set_weights ` ) . # List of tensors . 1:1 mapping with inbound_layers . if len ( values ) == 1 : if hasattr ( self , 'batch_input_shape ' ) : build_map_of_graph ( x , finished_nodes , nodes_in_progress , return False n.decode ( 'utf8 ' ) for n in for element in iterable : chunked_data = np.array_split ( data_npy , num_chunks ) def legacy_get_config ( self ) : # List of shape tuples , shapes of input_tensors . output_masks = _to_list ( output_masks ) self.targets = self.model.targets # self.trainable_weights name , dtype=None , input_tensor=None , sparse=False , name=None ) : dtype=dtype , if hasattr ( layer , 'activity_regularizer ' ) and layer.activity_regularizer is not None : or if all inbound nodes have the same output shape . 'loss ' : model.loss , # old : ( filters , stack_size , kernel_rows , kernel_cols ) raise TypeError ( 'The added layer must be ' in the FAQ for instructions on how to install ` h5py ` . dtype=val.dtype ) if type ( obj ) .__module__ == np.__name__ : output_shapes = to_list ( output_shapes ) return self.model.test_on_batch ( x , y , raise RuntimeError ( 'The name `` ' + name + ' '' is used ' from .engine.topology import Layer ` x ` can be ` None ` ( default ) if feeding from self.assert_input_compatibility ( inputs ) weights [ 0 ] = np.reshape ( weights [ 0 ] , layer_weights_shape ) 'The last layer might not normalize predictions ' str ( weights [ 0 ] .shape ) + ' and size ' ( e.g . will not include losses that depend on tensors if 'class_name ' not in config [ 0 ] or config [ 0 ] [ 'class_name ' ] == 'Merge ' : return cls.from_config ( losses += self.get_losses_for ( None ) f.attrs [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) layer , 'the notion of `` input shape '' is ' [ How can I install HDF5 or h5py to save my models in Keras ? ] ( reshape=False ) : `` `` '' Retrieve a layer that is part of the model . ` x ` can be ` None ` ( default ) if feeding from dictionary . It does not handle layer connectivity layers = [ ] source = 'GRU ( reset_after=True ) ' return cls.legacy_from_config ( config ) def get_json_type ( obj ) : and between ` CuDNNGRU ` and ` GRU ( reset_after=True ) ` . Default ` GRU ` is not output_masks = to_list ( output_masks ) class InputLayer ( Layer ) : min_ndim : Integer , minimum rank of the input . # Store the traversal order for layer sorting . `` `` '' Creates the layer weights . its new shape ( obtained via self.compute_output_shape ) . attr_name : Human-readable attribute name , for error messages . metrics=metrics , yaml_string : YAML string encoding a model configuration . 'optimizer_config ' : { layer , node_index , tensor_index ) self._trainable = True return self.model.evaluate ( x , y , for node in reversed ( nodes_in_decreasing_depth ) : for i , ( w , val ) in enumerate ( zip ( symbolic_weights , describing the origin of the ` input_tensors ` , verifying the following : iterable = [ iterable_or_element ] weights = layer.weights @ interfaces.legacy_model_constructor_support A layer config is a Python dictionary ( serializable ) that are n't inputs to this model ) . string , path where to save the model , or # Check shape . # Entries are unique . Includes input and output layers . if self.__class__.__name__ == 'Sequential ' : steps=steps ) units = weights [ 1 ] .shape [ 0 ] def load_weights ( self , filepath , by_name=False , # and one tensor output . self._inbound_nodes = [ ] # Appended to by calls to ` __call__ ` . new_node_index = node_conversion_map.get ( else : for value in obj : 'output shape ' ) group.attrs [ name ] = data node_index = node.node_indices [ i ] if hasattr ( x_elem , '_keras_shape ' ) : self.metrics_tensors = self.model.metrics_tensors use_multiprocessing=use_multiprocessing , input_tensors = [ ] def process_layer ( layer_data ) : optimizer_weight_values = [ optimizer_weights_group [ n ] for n in if name in layer_cache : epoch epochs is reached . `` `` '' Generates class probability predictions for the input samples . # for its inputs , and no outbound nodes . be equal to the number of samples of your output_tensors.append ( tensor ) 'optimizer . ' ) x=None , # on the fly because the inbound node may not yet exist , # ( computed tensor , compute mask ) ` optimizer ` , ` loss ` , ` metrics ` or ` sample_weight_mode ` . ( potentially with 1 element ) . # CuDNNLSTM has ( units * 8 ) weights ; while LSTM has ( units * 4 ) ` input_tensors ` and turns them into ` output_tensors ` self.build ( input_shapes ) config [ 'layers ' ] = layer_configs ) str ( node.arguments ) ' ( see keras.io/optimizers ) . ' ) # create Numpy arrays of input data from keras.engine.topology import InputLayer f.attrs [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) ( or list of tensors if the layer has multiple inputs ) . or if all inbound nodes have the same output shape . from .. utils.generic_utils import to_list `` `` '' Retrieves the output tensor ( s ) of a layer at a given node . If a tensor is passed , we return For instance , if a , b and c are Keras tensors , When using the TensorFlow backend , # does not return a list the same size as ` call ` # Following 2 properties : input and output shapes . for sublayer in layer.layers : if num_chunks > 1 : unique_tensors = list ( set ( x for x in losses if not isinstance ( x , ( float , int ) ) ) ) ' has multiple inbound nodes , ' if not name : 'output_masks ' , where there is a mismatch in the number of weights , ' '' was not an Input tensor , ' A ` History ` object . # Bad luck , we have to run the graph manually . trainable = getattr ( self , 'trainable ' , True ) compile : Boolean , whether to compile the model # note : ` Node ` is an internal class , regularizer=None , for name in layer_names : assert_input_compatibility ( ) `` `` '' Instantiates a Keras model from its config . if not input_shape and not batch_input_shape : input_shapes=input_shape , output_shapes=output_shape , input_shape = input_shapes [ i ] uses_lp = getattr ( self , 'uses_learning_phase ' , False ) or uses_lp class_weight=None , ' ` layer.build ( batch_input_shape ) ` ' ) or for some type of layers ( recurrent , Dense ... ) self.assert_input_compatibility ( inputs ) # The new container starts with a single inbound node inbound_nodes : list of nodes ` predict_on_batch ` . input_shapes= [ x._keras_shape for x in self.inputs ] , batch_size=None , workers=1 , weight_values = K.batch_get_value ( symbolic_weights ) Returns a layer based on either its name ( unique ) `` `` '' Retrieves a layer based on either its name ( unique ) or index . inputs : Input tensor , or list/tuple of input tensors . generator : generator yielding batches of input samples . Useful to prevent duplicated work . the expected input will be batches of 10 32-dimensional vectors . return 'private ' + insecure for layer in inbound_layers : spec = [ ( 'dtype= ' + str ( self.dtype ) ) if self.dtype else `` , node_key = self._node_key ( layer , original_node_index ) If set , the layer will not create a placeholder tensor . metrics : List of metrics to be evaluated by the model of a custom object name have been replaced initial_epoch : Epoch at which to start training return self.model.evaluate ( x , y , computable_tensors = [ ] if layer.name : model.add ( Dense ( 32 ) ) model_config = self._updated_config ( ) # Collect unconditional losses . self.outputs = [ outputs ] deserialized = { } 'hence the notion of `` layer input '' ' `` `` '' Calls the model on new inputs . self.model = Model ( self.inputs , self.outputs [ 0 ] , return self._per_input_updates [ inputs_hash ] `` `` '' Retrieves the input tensor ( s ) of a layer . opened_new_file = not isinstance ( filepath , h5py.File ) but note that there may be cases in which this max_ndim : Integer , maximum rank of the input . optimizer_weights_group = f.create_group ( 'optimizer_weights ' ) 'For multi-output layers , ' # of the call and of all new variables created during the call . input_shape = layers_to_output_shapes [ shape_key ] # Instantiate the input layer . inbound_layer = node.inbound_layers [ i ] ' does not support masking , ' layer : Target layer instance . def _get_node_attribute_at_index ( self , node_index , attr , attr_name ) : self._internal_output_shapes = [ x._keras_shape for x in self.outputs ] self.model.compile ( optimizer , loss , json_string : JSON string encoding a model configuration . for chunk_id , chunk_data in enumerate ( chunked_data ) : AttributeError : if the layer is connected to * * kwargs : Additional keyword arguments to be passed to ` call ( ) ` . dtype = kwargs.get ( 'input_dtype ' ) output_shapes = _to_list ( output_shapes ) The model will not be trained on this data . model.add ( layer ) self.outbound_layer = outbound_layer initial_epoch=initial_epoch ) from_config reshape : Reshape weights to fit the layer when the correct number return weights of HDF5 file which is not able to store tensor=None ) : # Returns [ getattr ( x , '_uses_learning_phase ' , False ) if layer in self.input_layers : weights2 = topology.preprocess_weights_for_loading ( layer , weights1 ) ValueError : In case of improperly formatted ` layer_data ` dict . self._trainable_weights.append ( weight ) input_masks=input_masks , while isinstance ( first_layer , ( Model , Sequential ) ) : # Dictionary mapping layer instances to arguments : dictionary of keyword arguments that were passed to the # Gather info about inputs and outputs . exactly where you left off . custom_objects=dict ( list ( _GLOBAL_CUSTOM_OBJECTS.items ( ) ) ` model = Model ( input= [ a , b ] , output=c ) ` 'theano ' : False , # and turns them into a list of output tensors . f.flush ( ) if spec.axes : constraint=None ) : `` `` '' Adds losses to the layer . self._feed_input_shapes = [ ] order of horizontal graph traversal ( bottom-up ) . the training samples , used for weighting the loss function # These properties will be set upon call of self.build ( ) warnings.warn ( 'Network returning invalid probability values . ' original_keras_version=original_keras_version , Should be unique in a model ( do not reuse the same name twice ) . `` `` '' Returns the config of the layer . def model_from_yaml ( yaml_string , custom_objects=None ) : weights : a list of Numpy arrays . The number name = layer_data [ 'config ' ] .get ( 'name ' ) f_merged = K.function ( [ inputs ] , _to_list ( layer ( inputs ) ) ) The savefile includes : layer.activity_regularizer ( x ) 'config ' : layer.get_config ( ) } filepath : one of the following : original_keras_version = f.attrs [ 'keras_version ' ] .decode ( 'utf8 ' ) of a custom object name have been replaced saving.load_weights_from_hdf5_group_by_name ( f , layers , state_updates = [ ] return model.add ( Dense ( 32 , input_shape= ( 500 , ) ) ) batch_size = None ( Theano , TensorFlow or CNTK ) , which we augment with certain return False def get_input_mask_at ( self , node_index ) : node_indices = [ ] You can set it to a custom function raise TypeError ( 'Layer ' + self.name warnings.warn ( cls_name + ' inputs must come from ' class Network ( Layer ) : layers_depths [ node.outbound_layer ] = depth def get_output_mask_at ( self , node_index ) : 'name ' : layer.name , # Note : # ( e.g . weight regularizers ) . ValueError : in case of mismatch between only if they share the same name . This is useful def get_weights ( self ) : uses_lp = any ( [ getattr ( x , '_uses_learning_phase ' , False ) for x in input_tensors ] ) in the ` x ` and ` y ` data provided , before shuffling . 'input to `` ' + self.name def _collect_previous_mask ( input_tensors ) : except TypeError : loss_weights = training_config [ 'loss_weights ' ] if len ( output_tensors ) == 1 : if 'batch_input_shape ' in kwargs : from . import optimizers output_shapes = self._output_shape_cache [ cache_key ] `` ` tensor_map = { } 'The tensor ' + str ( tensor ) + ' at layer `` ' steps : Total number of steps ( batches of samples ) ( e.g . set this to adapt the display to different ' about its expected input shape , ' A flat list of Numpy arrays . if len ( weights ) == 12 : if proba.shape [ -1 ] > 1 : steps , if hasattr ( layer , 'losses ' ) : # layer call until it becomes possible to process it raise TypeError ( 'Keyword argument not understood : ' , kwarg ) Model config with Keras version information added . ValueError : in case of mismatch between workers : maximum number of processes to spin up if not self.input_spec : ' is incompatible with layer ' A list of loss tensors . return unique_tensors + non_tensors filtered_inbound_nodes.append ( node_data ) For every such layer group , a group attribute ` weight_names ` , to False , the compilation is omitted without any # Following 2 properties : load_weights_from_hdf5_group ( f [ 'model_weights ' ] , model.layers ) # old : ( filters , stack_size , kernel_rows , kernel_cols ) output_shapes=output_shape , 'TensorFlow optimizers do not ' input_shapes = _to_list ( input_shape ) functions / classes . name=name , % ( HDF5_OBJECT_HEADER_LIMIT , if hasattr ( x , '_keras_history ' ) : To specify different metrics for different outputs of a self.output_layers_node_indices = [ ] 'they can not be the output of ' # Handle laying building ( weight creating , input spec locking ) . `` `` '' Sequential model class . for node_data in inbound_nodes_data : y = Dense ( 16 , activation='softmax ' ) ( x ) weights += layer.weights from .. utils.generic_utils import has_arg def losses ( self ) : if 'GRU ( reset_after=False ) ' in types : from .. engine.topology import get_source_inputs self.supports_masking = self.model.supports_masking nodes_depths [ node ] = depth # update self._inbound_nodes # Internal methods : if len ( computed_data ) == 1 : If the output layer in the model is named , you can also pass a from .. engine import Layer , InputSpec bias_shape = weights [ 2 ] .shape `` `` '' for instance batch norm updates are conditional on the layer 's inputs . topology.load_weights_from_hdf5_group_by_name ( f , layers , def from_config ( cls , config , custom_objects=None ) : for layer in inbound_layers : # and one tensor output . 'is ill-defined . ' mask = masks [ 0 ] input_tensor : Optional tensor to use as layer input `` `` '' Handles custom object lookup . if self._initial_weights is not None : get_config for spec_dim , dim in zip ( spec.shape , x_shape ) : 'weight_names ' ] = weight_names str ( x ) + ' at layer `` ' + layer.name + ' '' . ' if hasattr ( layer , 'updates ' ) : trainable_weights = [ ] uid = object_list_uid ( inputs ) ' or a ` batch_shape ` argument . Note that ' masks.append ( None ) 'with different output shapes . Hence ' input_layers self.layers.pop ( ) batch_size=batch_size , raise ValueError ( 'Only provide the input_shape OR ' raise ValueError ( 'Input ' + str ( input_index ) node_indices= [ ] , elif layer.reset_after : To load a network from a yaml save file , use shapes.append ( K.int_shape ( x ) ) from __future__ import print_function `` `` '' Retrieves the output mask tensor ( s ) of a layer . get_config ( ) is a generator . Total number of steps ( batches of samples ) from .engine.topology import Input `` `` '' Abstract base layer class . 'batch_input_shape argument to ' return weights know its input shape . 'make it possible to access ' batch_input_shape = ( batch_size , ) + tuple ( input_shape ) return 'InputSpec ( % s ) ' % ' , '.join ( x for x in spec if x ) if len ( input_shapes ) == 1 : 'config ' : self.layers [ 0 ] .get_config ( ) } ) ( ` keras.utils.Sequence ` ) object in order to avoid duplicate data output_tensors = [ ] overwrite : Whether we should overwrite any existing The config of a layer does not include connectivity # Retrieve losses for all internal layers . str ( inputs ) + ' . All inputs to the layer ' reshape=reshape ) model_layers = legacy_models.legacy_sequential_layers ( model ) inbound_tensor_index = input_data [ 2 ] 'were accessed without issue : ' previous_depth = nodes_depths.get ( inbound_node , 0 ) or for some type of layers ( recurrent , Dense ... ) first_layer_config = first_layer [ 'config ' ] output = output_ls_copy This tuple ( a single output of the generator ) makes a single model = cls ( ) means is that it should have received an ` input_shape ` if layer.__class__.__name__ == 'ConvLSTM2D ' : str ( mask ) ) # Build train function ( to get weight updates ) . if len ( output_shapes ) == 1 : return input_shapes [ 0 ] 'optimizer_weights ' ) def _collect_previous_mask ( input_tensors ) : raise ValueError ( 'Layer weight shape ' `` `` '' Load a model from a legacy configuration . if self.__class__.__name__ == 'Sequential ' : raise TypeError ( 'Layer ' + self.name ' at node ' + str ( node_index ) except ValueError : 'Use ` get_input_at ( node_index ) ` instead . ' ) from .legacy import models as legacy_models return self.model.call ( inputs , mask ) config [ 'config ' ] , weight_values = K.batch_get_value ( symbolic_weights ) assert_input_compatibility ( ) weight_tensor_bi_convlstm_new = saving.preprocess_weights_for_loading ( have different sizes . For example , the last batch of the epoch if len ( input_shapes ) ! = len ( self.input_layers ) : optimizer_weights_group.attrs [ 'weight_names ' ] ] 'is not a list . We expect a list . ' for layer in layers_for_depth : raise ValueError ( 'Input ' + str ( input_index ) for x in input_tensors : `` `` '' Parses a yaml model configuration file and returns a model instance . for i in range ( len ( self.output_layers ) ) : sample_weight=None , # know about its input shape . Otherwise , that 's an error . layer = node.outbound_layer original_backend , # TODO : raise exception when a ` .compute_mask ( ) ` call 'input_dtype ' , # legacy return print_layer_summary ( self , f.close ( ) filtered_layers = [ ] will be batches of 32-dimensional vectors . # Check that all tensors required are computable . ' has multiple inbound nodes , ' a specific dimension value . a warning will be displayed . When ` compile ` is set It will be called on each line of the summary . dtype = K.floatx ( ) to be passed to ` json.dumps ( ) ` . def output_mask ( self ) : loss = convert_custom_objects ( training_config [ 'loss ' ] ) 'ConvLSTM2D ' ] This checks that the tensor ( s ) ` input ` output_ls_copy = [ ] the loss and any model metrics # Build self.input_layers : values = getattr ( self._inbound_nodes [ node_index ] , attr ) with h5py.File ( filepath , ' w ' ) as f : self.build ( input_shapes [ 0 ] ) split in half , for GRU biases are reshaped . When using the TensorFlow backend , if legacy_models.needs_legacy_support ( self ) : weights : List of source weights values ( input kernels , recurrent # output masks and output shapes in one pass , # The model owns this layer node . If ` by_name ` is True , weights are loaded into layers def load_weights ( self , filepath , by_name=False , skip_mismatch=False , reshape=False ) : epochs = kwargs.pop ( 'nb_epoch ' ) return layer_module.deserialize ( config , custom_objects=custom_objects ) from .. layers import deserialize inputs : A tensor or list of tensors . weights [ 7 ] ] , axis=-1 ) # We were passed a regular layer , and it should for node_data in unprocessed_nodes.pop ( layer ) : self.sample_weight_mode = self.model.sample_weight_mode return output if len ( self._inbound_nodes ) ! = 1 : 'correspond to layer ' + name `` `` '' Returns a JSON string containing the network configuration . weights = _convert_rnn_weights ( layer , weights ) `` `` '' Removes the last layer in the model . return self._built # Check for redundancy in outputs . # only convert between different types Every layer should expose ( if appropriate ) an ` input_spec ` attribute : The validation data is selected from the last samples from .. import backend as K # Build a map from a layer unique name ( self._node_key ) input_tensors : list of input tensors . for x_elem in to_list ( inputs ) : for layer_data in config [ 'layers ' ] : original_backend = f.attrs [ 'backend ' ] .decode ( 'utf8 ' ) x = node.input_tensors [ i ] def built ( self , value ) : layer , node_index , tensor_index = x._keras_history ImportError : if h5py is not available . 'provided weight shape ' + str ( w.shape ) ) `` `` '' Returns the current weights of the layer . weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 2 , 3 , 0 , 1 ) ) return { 'class_name ' : obj.__class__.__name__ , `` `` '' Handles custom object lookup . f : A pointer to a HDF5 group . process_layer ( layer_data ) return weights if updates is None or updates == [ ] : nodes_by_depth [ depth ] .append ( node ) for axis , value in spec.axes.items ( ) : self.model.compile ( optimizer , loss , weights [ num_weights_per_layer : ] , input_tensors : A tensor or list of tensors . Note that because this implementation return specs [ 0 ] weight_values [ i ] .shape ) ) # Class Methods via Keras-side shape inference . arguments=user_kwargs ) # If we 've seen this layer before at a higher depth , def call ( self , inputs , * * kwargs ) : if hasattr ( tensor , '_keras_shape ' ) and output_shapes is not None : first_layer = layer.layers [ 0 ] # Container_nodes : set of nodes included in the graph ` tensor_indices [ i ] ` is the index of ` input_tensors [ i ] ` within the outbound_nodes : list of nodes training_config = json.loads ( training_config.decode ( 'utf-8 ' ) ) return self._per_input_updates [ inputs_hash ] recurrent_kernels = transform_kernels ( weights [ 1 ] , lambda k : k.T , n_gates ) if spec.min_ndim is not None : return weights and validation metrics values ( if applicable ) . weight_values , A list . def input_shape ( self ) : warnings.warn ( 'Skipping loading of weights for layer { } '.format ( layer.name ) self.max_ndim = max_ndim raise ValueError ( 'No model found in config file . ' ) trainable : A boolean , whether the weight should data_npy = np.asarray ( data ) config [ 'layers ' ] = layer_configs def compute_mask ( self , inputs , mask=None ) : if training_config is None : def state_updates ( self ) : # e.g . optimizer , layer # Returns TypeError : if input tensors are not Keras tensors from InputLayer objects ' a previous non-Input layer . ' of index ` epochs ` is reached . bias_shape = weights [ 2 ] .shape elif layer.reset_after : `` `` '' Counts the total number of scalars composing the weights . output_masks : list of output masks ( a mask can be a tensor , or None ) . For instance , if a , b and c are Keras tensors , continue input_shapes.append ( input_shape ) ( if the model has multiple inputs ) . def set_weights ( self , weights ) : An input shape tuple . the losses as conditional on these inputs . before declaring one epoch finished and starting the add_unprocessed_node ( layer , node_data ) the beginning of each epoch . Only used with instances return cls ( inputs=input_tensors , outputs=output_tensors , name=name ) `` `` '' Returns predictions for a single batch of samples . inputs : Tensor or list of tensors . layer_configs = [ ] self.layers = [ ] ` input_tensors [ i ] == inbound_layers [ i ] ._inbound_nodes [ node_indices [ i ] ] .output_tensors [ tensor_indices [ i ] ] ` 'Note that input tensors are ' return tensor output_shape_keys = [ ] layers : list of layers to add to the model . self.set_weights ( self._initial_weights ) get_config ( ) ' in the save file . ' target location , or provide the user with a manual prompt . # this is the layer that takes a list of input tensors from .. utils import conv_utils validation_data=validation_data , self._inbound_nodes = [ ] return layer.name + '_ib- ' + str ( node_index ) if all_names.count ( name ) ! = 1 : if regularizer is not None : The model will set apart this fraction of the training data , f = filepath trainable_weights += layer.trainable_weights self.dtype = dtype using ` use_multiprocessing=True ` . For missing biases in ` LSTM ` / ` GRU ` ( ` use_bias=False ` ) , A mask tensor Numpy data for these targets at training time ) , you return copy.deepcopy ( config ) somewhere else ( forbidden in ` Sequential ` models ) . Build from x._keras_shape in the FAQ for instructions on how to install ` h5py ` . `` `` '' Loads attributes of the specified name from the HDF5 group . # Instantiate layer . on the layer 's weights variables , not on any inputs tensors ) . proba = self.predict ( x , batch_size=batch_size , verbose=verbose , from keras.layers import Layer output_masks=output_masks , for layer_data in config [ 'output_layers ' ] : self.targets = self.model.targets for node in nodes_by_depth [ depth ] : if not self._inbound_nodes : return shapes cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) node_key = self._node_key ( layer , node_index ) If unspecified , ` workers ` will default to 1 . If 0 , will Can be run on non-Keras tensors . when using multiprocessing . ( e.g . will not include updates that depend on tensors non_trainable_weights ( list of variables ) # and labels , from each line in the file if l not in sublayer.trainable_weights ] ) output_tensors : list of output tensors . data : Attributes data . the output of ` model.get_weights ( ) ` . name=self.name + '_model ' ) return output steps_per_epoch=1000 , epochs=10 ) self.axes = axes or { } # new : ( kernel_rows , kernel_cols , filters , stack_size ) from .. engine.topology import Layer self.name + ' : expected dtype= ' def _load_attributes_from_hdf5_group ( group , name ) : # Update self.updates ' was passed non-serializable keyword arguments : ' param_dset = optimizer_weights_group.create_dataset ( opened_new_file = not isinstance ( filepath , h5py.File ) # new : ( kernel_rows , kernel_cols , stack_size , filters ) return self.model.state_updates ( for a single-output ` Sequential ` model ) . 'Input ' + str ( input_index ) from . import layers as layer_module 'output mask ' ) self.inbound_layers = inbound_layers if not self.trainable and not self.stateful : weights [ 8 ] ] , axis=-1 ) the number of samples in your dataset divided by i ) transpose_input ( from_cudnn ) , output_shapes = [ ] param_values = K.batch_get_value ( params ) for i in range ( len ( node.inbound_layers ) ) : if not isinstance ( filepath , h5py.File ) : self.output_layers = self.model.output_layers or create its a placeholder tensor ( pass arguments ` input_shape ` if name in group.attrs : Will only include losses that are either original_keras_version : Keras version for the weights , as a string . weights = keras.engine.topology.preprocess_weights_for_loading ( target_layer , weights ) verify the input assumptions of the layer output_tensors [ i ] ._keras_shape = output_shapes [ i ] if layer not in layer_indices : return obj.__name__ if not batch_shape and tensor is None : optimizer_weight_names ] if all ( [ hasattr ( x , '_keras_shape ' ) for x in computed_tensors ] ) : sample_weight=sample_weight ) self.output_layers = [ ] if layer.__class__.__name__ == 'LSTM ' : self.name + ' : expected axis ' input_spec = _to_list ( self.input_spec ) for i in range ( len ( weight_values ) ) : and between ` CuDNNGRU ` and ` GRU ( reset_after=True ) ` . Default ` GRU ` is not return self._trainable_weights + self._non_trainable_weights `` `` '' Retrieves the output mask ( s ) of the previous node . from .. import initializers output_tensors , _ , _ = self.run_internal_graph ( inputs , masks ) kernels : Stacked array of kernels for individual gates . inbound_layers = [ ] def save ( self , filepath , overwrite=True , include_optimizer=True ) : Output tensor or list of output tensors . if not self.layers : return self._trainable_weights for i in range ( len ( node.inbound_layers ) ) : output of the inbound layer if type ( obj ) .__name__ == type.__name__ : 'All layer names should be unique . ' inputs_hash = object_list_uid ( inputs ) or ` batch_input_shape ` argument , # Raises is a compiled model ready to be used ( unless the saved model This is used to implement the methods : arguments : dictionary of keyword arguments that were passed to the original_keras_version , node_indices : a list of integers , the same length as ` inbound_layers ` . assert layer_name in created_layers 'input to `` ' + self.name for pv , p , w in zip ( param_values , params , weights ) : if h5py is None : batch_shape = first_layer.batch_input_shape ( ` float32 ` , ` float64 ` , ` int32 ` ... ) max_queue_size : maximum size for the generator queue verbose=verbose ) A layer config is a Python dictionary ( serializable ) input_tensors.append ( layer_output_tensors [ tensor_index ] ) config = self.get_config ( ) if ndim is not None and ndim > spec.max_ndim : inbound_layers : a list of layers , the same length as ` input_tensors ` , str ( len ( input_spec ) ) + ' inputs , ' from .engine.saving import save_model from .. utils import conv_utils # Following 2 properties : input and output shapes . attr : Exact node attribute name . self._add_inbound_node ( input_tensors=inputs , This will override ` validation_split ` . independently manipulable ) . if pv.shape ! = w.shape : # Iterate over nodes , by depth level . # First , we need to infer its expected input shape and dtype . config [ 'batch_input_shape ' ] = self.batch_input_shape input_tensors = to_list ( input_tensors ) A list of update ops . return obj.item ( ) raise ValueError ( ' % s is not compatible with % s ' % types ) if node_key not in self._container_nodes : self._add_inbound_node ( input_tensors=inputs , output_tensors=output , for sw , w in zip ( layer.weights , layer_weights ) : validation dataset divided by the batch size . output_shapes = self._inbound_nodes [ 0 ] .output_shapes str ( len ( self._inbound_nodes ) ) + ' inbound nodes . ' ) weights [ 11 ] ] , axis=-1 ) raise ValueError ( 'No model found in config file . ' ) layers.append ( layer ) for layer in self.layers : # From the earliest layers on . ( during training only ) . You can either pass a flat ( 1D ) self.output_names.append ( layer.name ) self.outputs = [ output_tensor ] from .. legacy import models as legacy_models @ property # then cache them here . When one of these output is queried later , # Store in cache . for x , y , mask in zip ( self.inputs , inputs , masks ) : layers_with_complete_input.append ( layer.name ) while ( ' % s % d ' % ( name , chunk_id ) ) in group.attrs : # List of shape tuples , shapes of output_tensors . dtype=dtype , functions / classes . raise TypeError ( 'Sequential model can not be built : model is empty . ' output_masks = layer.compute_mask ( computed_tensors , Each time a layer is connected to some new input , raise ValueError ( 'You tried to call layer `` ' + self.name # Reached an Input layer , stop recursion . self.assert_input_compatibility ( inputs ) first_layer_config [ 'layers ' ] = merge_inputs if len ( layer_names ) ! = len ( filtered_layers ) : return self.legacy_get_config ( ) symbolic_weights = layer.weights # self.non_trainable_weights return [ kernels , recurrent_kernels , biases ] on the recursion stack . Useful to detect cycles . output_shapes = self._inbound_nodes [ 0 ] .output_shapes loss='categorical_crossentropy ' , if not self._inbound_nodes : If unspecified , it will default to 32 . input_masks=input_masks , self._inbound_nodes = [ ] can specify them via the ` target_tensors ` argument . at successive epochs , as well as validation loss values `` `` '' Retrieves a layer based on either its name ( unique ) or index . return copy.deepcopy ( config ) reshape=reshape ) ' ( like softmax or sigmoid would ) . ' ) terminal window sizes ) . A list of converted weights values ( Numpy arrays ) . index : Integer , index of layer . # We 've already covered the input layers # from the number of bias weights : node_index : Node index from which ` tensor ` comes from . if hasattr ( w , 'name ' ) : recurrent_kernels = transform_kernels ( weights [ 1 ] , lambda k : k.T , n_gates ) if input_tensor is None : for i in range ( len ( self.input_layers ) ) : you should not pass non-picklable arguments to the generator sample_weight=sample_weight ) def transform_kernels ( kernels , func , n_gates ) : def normalize_legacy_config ( conf ) : input_tensors = [ ] input_tensors [ i ] == origin_node.output_tensors [ tensor_indices [ i ] ] A YAML string . 'is ill-defined . ' initializer = initializers.get ( initializer ) x_shape [ int ( axis ) ] not in { value , None } ) : dtype : The data type expected by the input , as a string self.input_layers_node_indices = [ ] # Add an inbound node to the layer , so that it keeps track def _collect_input_shape ( input_tensors ) : unprocessed_nodes [ layer ] = [ node_data ] ( or list of tensors if the layer has multiple outputs ) . new_node_index , use_multiprocessing=False , verbose=0 ) : state_updates += layer.updates if inputs_hash in self._per_input_updates : return shapes [ 0 ] kept_nodes = 1 to yield from ` validation_data ` generator before stopping # Recover loss functions and metrics . Indices are based on order of horizontal graph traversal ( bottom-up ) . y = Dense ( 16 , activation='softmax ' ) ( x ) kwargs [ 'mask ' ] = computed_mask if h5py is None : self.model.callback_model = self # Layer parameters . where there is a mismatch in the number of weights , def __call__ ( self , inputs , * * kwargs ) : return { 'outbound_layer ' : outbound_layer , RuntimeError : If the layer has no inbound nodes . outputs = input_layer._inbound_nodes [ 0 ] .output_tensors new_weights.extend ( preprocess_weights_for_loading ( if layer not in layer_indices : steps_per_epoch=None , # returns a compiled model 'metrics ' : model.metrics , raise TypeError ( 'There are no layers in the model . ' ) output = output_ls_copy [ 0 ] container_nodes = set ( ) # ids of all nodes relevant to the Container # If the layer returns tensors from its inputs , unmodified , # Expecting this to never be true . axes=None ) : in order to capture the string summary . self.model = None # Internal Model instance . 'at deserialization time ) . ' ) except TypeError : 'is redundant . ' `` `` '' Retrieves the output mask tensor ( s ) of a layer . A list of weights values ( Numpy arrays ) . # non-trainable weights A model instance . input_uid = object_list_uid ( inputs ) layer = self.input_layers [ i ] for layer in index.get ( name , [ ] ) : output of get_config . input_shapes = [ x._keras_shape for x in inputs ] if not overwrite and os.path.isfile ( filepath ) : node_conversion_map = { } if input_tensors : ` tensor_indices [ i ] ` is the index of ` input_tensors [ i ] ` within the ValueError : In case of an invalid savefile . biases = np.sum ( np.split ( weights [ 2 ] , 2 , axis=0 ) , axis=0 ) except TypeError : reshape=reshape ) 'output_shapes ' , if cache_key in self._output_shape_cache : if len ( set ( self.outputs ) ) ! = len ( self.outputs ) : warnings.warn ( 'Error in loading the saved optimizer ' x = Input ( shape= ( 32 , ) ) return model weights : The concatenation of the lists trainable_weights and ( in which case its weights are n't yet defined ) . If layer is not built : weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 2 , 3 , 1 , 0 ) ) The model weights . return tensor # rather than input-dependent if not overwrite and os.path.isfile ( filepath ) : 'Use ` get_input_at ( node_index ) ` instead . ' ) except TypeError : group.attrs [ name ] = data updates += layer.get_updates_for ( None ) inbound_nodes : List of nodes . inputs=computed_tensors ) # we retrieve it from there instead of recomputing it . # are relevant to the current graph ) . unprocessed_nodes = { } dtype = K.floatx ( ) # Raises layer , node_index , tensor_index ) during training and testing . def evaluate ( self , x=None , y=None , TensorFlow data tensors , the default ` None ` is equal to # know about its input shape . Otherwise , that 's an error . the losses as conditional on these inputs . if symbolic_weights : assert hasattr ( self.layers [ 0 ] , 'layers ' ) if hasattr ( w , 'name ' ) : order = ' F ' if from_cudnn else ' C ' # Build a map from a layer unique name ( self._node_key ) layer.set_weights ( weights [ : nb_param ] ) Fraction of the training data to be used as validation data . raise TypeError ( 'Input tensors to a ' + cls_name + ' ' shape_key = inbound_layer.name + ' _ % s_ % s ' % ( node_index , tensor_index ) if dtype is None : epochs is to be understood as `` final epoch '' . The model is 'config ' : config , weights [ 4 ] , max_queue_size : maximum size for the generator queue the model 's weights ` _keras_history ` : Last layer applied to the tensor . h5py.File object from which to load the model weighted_metrics : List of metrics to be evaluated and weighted `` `` '' if layer.__class__.__name__ == 'Conv3D ' : `` `` '' Adds a layer instance on top of the layer stack . nb_param = len ( layer.weights ) kernels : Stacked array of kernels for individual gates . shapes = [ ] A tensor ( or list of tensors if the layer has multiple outputs ) . RuntimeError : if the layer is n't yet built output_shapes.append ( shape ) unprocessed_nodes [ layer ] .append ( node_data ) have multiple tensor outputs , with each one being for layer in layers : loss : String ( name of objective function ) or objective function . if len ( weight_values ) ! = len ( symbolic_weights ) : # convert the weights between CuDNNLSTM and LSTM num_weights_per_layer = len ( weights ) // 2 losses += layer.get_losses_for ( None ) return weight if not hasattr ( layer , 'batch_input_shape ' ) : model.compile ( optimizer=optimizer , # to insert before the current layer finally : batch_input_shape=None , group : A pointer to a HDF5 group . weights [ 10 ] ] , axis=-1 ) self.batch_input_shape = batch_input_shape } ) the number of samples in your dataset divided by This is done as part of _add_inbound_node ( ) . inbound_layer_name = input_data [ 0 ] self.inputs = [ inputs ] from .topology import Input from keras.engine.topology import Layer able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes . for x in reference_input_tensors : name : String , the name for the weight variable . 'config ' : layer_config , arguments=None ) : or list of scalars ( if the model computes other metrics ) . # and create the node connecting the current layer input_shapes= [ x._keras_shape for x in self.inputs ] , if len ( weight_values ) ! = len ( symbolic_weights ) : conf [ 'name ' ] = name Node ( outbound_layer=self , print_fn=print_fn ) if len ( inputs ) ! = len ( input_spec ) : 'hence the notion of `` layer output mask '' ' raise ValueError ( 'Layer ' + self.name + ' expects ' raise TypeError ( 'Unrecognized keyword arguments : ' + str ( kwargs ) ) unprocessed_nodes [ layer ] .append ( node_data ) initial_epoch : Integer . if isinstance ( output_shapes , list ) and len ( output_shapes ) == 1 : List of callbacks to apply during training . output_mask = [ output_mask ] * len ( output_ls ) if hasattr ( obj , 'get_config ' ) : def _get_node_attribute_at_index ( self , node_index , attr , attr_name ) : `` `` '' Transforms kernel for each gate separately using given function . str ( weights ) [ :50 ] + ' ... ' ) model at the target location , or instead if not self.inputs or not self.outputs : node_indices= [ ] , data = [ n.decode ( 'utf8 ' ) for n in group.attrs [ name ] ] weights = weights [ num_param : ] for input_data in node_data : model.add ( Dense ( 32 , input_shape= ( 500 , ) ) ) return json.dumps ( model_config , default=get_json_type , * * kwargs ) layers_to_output_shapes [ shape_key ] = input_shape See [ optimizers ] ( /optimizers ) . self.add ( layer ) `` `` '' Builds a map of the graph of layers . Number of samples per gradient update . RuntimeError : If the model was never compiled . tensor_indices = [ ] self._output_shape_cache = self.model._output_shape_cache class_weight=class_weight ) def regularizers ( self ) : # Arguments if spec is None : tensor_index : Tensor_index from which ` tensor ` comes from . the entire layer graph is retrievable from that layer , ( or list of shape tuples if the layer has multiple outputs ) . return K.batch_get_value ( weights ) new_node_index , self._feed_input_names.append ( layer.name ) input_shapes = [ ] def add_unprocessed_node ( layer , node_data ) : def get_input_mask_at ( self , node_index ) : 'ill-defined for the layer . ' layer.__class__.__name__ ) ) ( may include None for unchecked axes ) . filtered_layer_names = [ ] to a weight ( float ) value , used for weighting the loss function custom_objects = { } layer_indices = { } # dict { layer : index in traversal } layer_cache = { } Ignored with the default value of ` None ` . layer , node_index , tensor_index = tensor._keras_history self._feed_input_shapes.append ( self.inputs [ i ] ._keras_shape ) # Check dtype . from .. legacy import layers as legacy_layers ' ` layer.build ( batch_input_shape ) ` ' ) # Then we process nodes in order of layer depth . weights [ 7 ] ] , axis=-1 ) def _need_convert_kernel ( original_backend ) : if len ( masks ) == 1 : model_config = { `` `` '' Configures the model for training . data larger than HDF5_OBJECT_HEADER_LIMIT bytes . # Load weights that were specified at layer instantiation . ValueError : In case the ` layer ` argument does not def legacy_from_config ( cls , config , layer_cache=None ) : masks.append ( mask ) return mask `` ` output_masks = to_list ( output_masks ) batch_shape = layer.batch_input_shape inputs : Can be a tensor or list/tuple of tensors . return output_masks sample_weight=sample_weight , [ How can I install HDF5 or h5py to save my models in Keras ? ] ( an under-represented class . tensor_index = self.output_layers_tensor_indices [ i ] str ( len ( filtered_layers ) ) + ' layers . ' ) def build_map_of_graph ( tensor , finished_nodes , nodes_in_progress , # Keep track of unconditional losses validation_data : tuple ` ( x_val , y_val ) ` or tuple node_index = node.node_indices [ i ] name=self.name + '_model ' ) from .. legacy import interfaces using ` use_multiprocessing=True ` . config [ 'batch_input_shape ' ] = self.batch_input_shape for x in output_tensors ] max_queue_size=max_queue_size , if 'mask ' not in kwargs : weighted_metrics : List of metrics to be evaluated and weighted `` `` '' Returns the ` updates ` from all layers that are stateful . `` `` '' Instantiates a Model from its config ( output of ` get_config ( ) ` ) . outbound_layer : the layer that takes # here we order them by traversal order . def compute_mask ( self , inputs , mask ) : The model will not be trained on this data . self.activity_regularizer is not None ) : if 'input_shape ' in kwargs or 'batch_input_shape ' in kwargs : layer.name + ' '' is part of a cycle . ' ) to be created is sparse . nodes_by_depth [ depth ] = [ ] if layer not in unprocessed_nodes : # Create node , add it to inbound nodes . new_node_index = node_conversion_map [ node_key ] if layer.__class__.__name__ == 'Conv2DTranspose ' : def get_losses_for ( self , inputs ) : # the current node will be added to # List of tensors , created by outbound_layer.compute_mask ( ) . raise ValueError ( 'The list of inputs passed to the model ' completely . Useful to prevent duplicated work . from .base_layer import Node # Get sorted list of layer depths . return output_shapes if len ( inbound_layer._inbound_nodes ) < = inbound_node_index : layer , 'the model was * not * compiled . Compile it manually . ' ) each entry describes one required input : mask tensors . from_cudnn : ` True ` if source weights are in CuDNN format , ` False ` loss=loss , except ImportError : n_gates = 4 if any ( m is not None for m in mask ) : # User-provided arguments validation . if len ( bad_attributes ) > 0 : while True : input_shape : Shape tuple . Provided for convenience , input_shape : Shape tuple , not including the batch axis . from .. import optimizers weights = [ ] # Propagate to all previous tensors connected to this node . raise TypeError ( 'Unrecognized keyword arguments : ' + str ( kwargs ) ) inbound_layers.append ( None ) ' is incompatible with layer ' dtype=val.dtype ) all_attrs += getattr ( layer , attr , [ ] ) A model is callable on non-Keras tensors . batch_size : Integer or ` None ` . self.input_shapes = input_shapes kwargs [ 'mask ' ] = previous_mask if type ( obj ) .__name__ == type.__name__ : # list of layers ( 1 to 1 mapping with self.inputs , # Update tensor history , _keras_shape and _uses_learning_phase . if not isinstance ( layer , InputLayer ) : inputs = node.input_tensors target_tensors=None , if isinstance ( model.optimizer , optimizers.TFOptimizer ) : if kwargs : inputs = None def load_weights ( self , filepath , by_name=False , skip_mismatch=False , reshape=False ) : # Add to the model any layers passed to the constructor . raise ValueError ( 'Provide either a layer name or layer index . ' ) origin_node = inbound_layers [ i ] ._inbound_nodes [ node_indices [ i ] ] dtype=dtype , mask = masks if not hasattr ( layer , 'batch_input_shape ' ) : B._inbound_nodes an ` input_dim ` argument . `` `` '' This is where the layer 's logic lives . previous_depth = layers_depths.get ( node.outbound_layer , 0 ) weights = weights [ num_param : ] shuffle=True , model_inputs = [ ] optimizer = optimizers.deserialize ( optimizer_config , # This will never loop forever thanks to the test above . build ( input_shape ) ValueError : If the provided weights list does not match the # Arguments on this data at the end of each epoch . return self.model.get_weights ( ) if isinstance ( self.layers [ 0 ] , legacy_layers.Merge ) : the expected input will be batches of 10 32-dimensional vectors . input_shapes= [ batch_input_shape ] , Output of the layer 's ` call ` method . model_outputs = [ ] layers_by_depth = { } self._output_mask_cache = self.model._output_mask_cache reshape=False ) : mask = masks [ 0 ] model_weights_group = f.create_group ( 'model_weights ' ) This will override ` validation_split ` . # Propagate to all previous tensors connected to this node . # for creating scopes . We prefix the name with `` private '' in this case . name : String , name of layer . input_masks : list of input masks ( a mask can be a tensor , or None ) . relies on multiprocessing , you should not pass ' ( { } vs { } ) . '.format ( layer.name + '.\n ' if mask is None : mask = node.output_masks [ tensor_index ] for merge_input_config in first_layer_config.pop ( 'layers ' ) : units = weights [ 1 ] .shape [ 0 ] If set , the layer will not create a placeholder tensor . weight_value_tuples = [ ] if kwarg not in allowed_kwargs : weights = preprocess_weights_for_loading ( layer.layer , self.add_update ( layer.get_updates_for ( computed_tensors ) , inputs ) layer = self.output_layers [ i ] `` `` '' Makes a function that transforms input kernels from/to CuDNN format . @ trainable_weights.setter 'Sequential model must ' x_shape = K.int_shape ( x ) Should be unique in a model ( do not reuse the same name twice ) . 'input_masks ' , dtype = K.floatx ( ) node_data = [ ] # Legacy support target = 'GRU ( reset_after=True ) ' for x in output_ls : ValueError : If the index is does not match any node . If unspecified , ` workers ` will default to 1 . If 0 , will computable_tensors.append ( x ) before each epoch ) or str ( for 'batch ' ) . # Return tensor including _keras_shape and _keras_history . # We do n't process nodes ( i.e . make layer calls ) for node , depth in nodes_depths.items ( ) : warnings.warn ( # add to filtered_inbound_nodes . line_length : Total length of printed lines 'As a result , we can not save the optimizer ' max_queue_size=max_queue_size , custom_objects : Optional dictionary mapping names be equal to the number of samples of your # ( i.e . until the input tensors to the call all exist ) . ` y ` can be ` None ` ( default ) if feeding from layers.append ( layer ) elif layer_weights_shape ! = weights [ 0 ] .shape : information , nor the layer class name . These are handled Layers that have no matching name are skipped . the display labels for the scalar outputs . shape , # we should override the default mask . ` input_tensors ` and turns them into ` output_tensors ` ' or a ` batch_shape ` argument . Note that ' # update self._inbound_nodes output_masks : list of output masks ( a mask can be a tensor , or None ) . self.output_layers_node_indices.append ( node_index ) return config tensor_indices.append ( None ) sample_weight=sample_weight , Note that layers that do n't have weights are not taken if legacy_models.needs_legacy_support ( model ) : all_names = [ layer.name for layer in self.layers ] if sublayer not in layers : f.attrs [ 'training_config ' ] = json.dumps ( { # Support for legacy behavior class_weight : Optional dictionary mapping class indices ( integers ) ' weights , but the saved weights have ' return self._get_node_attribute_at_index ( 0 , 'input_tensors ' , shape : Shape tuple , expected shape of the input 'node_indices ' : self.node_indices , model.add ( Dense ( 32 , input_dim=500 ) ) node_index : Integer index of the node from which try : nodes_in_decreasing_depth = [ ] def get_config ( self ) : ( strings ) to custom classes or functions to be weights = layer.weights Note that because this implementation raise ValueError ( 'Weights must be of equal size to ' if not dtype : msg = `` Class ` { } . { } ` defines ` get_output_shape_for ` but does not override ` compute_output_shape ` . `` + \ # Avoid input redundancy . def state_updates ( self ) : target = 'CuDNNGRU ' workers=1 , class_weight=class_weight ) json.dumps ( node.arguments ) elif 'input_shape ' in kwargs : A Keras model instance . If an optimizer was found def compute_mask ( self , inputs , mask=None ) : for layer , depth in layers_depths.items ( ) : for sublayer in layer.layers : # Model attributes . output_masks= [ None ] , self.outputs = [ ] # List of length 1 : the output tensor ( unique ) . if original_keras_version == ' 1 ' : length ( equal to the size of this batch ) . Different batches may if hasattr ( self , '_updates ' ) : tensor_indices : a list of integers , ValueError : In case of mismatch between the provided input data return source_tensors if hasattr ( self , '_updates ' ) : self.arguments = arguments if layer.data_format == 'channels_last ' : batch_input_shape : Shape tuple , including the batch axis . if hasattr ( layer , 'reset_states ' ) and getattr ( layer , 'stateful ' , False ) : # serialize and save the layers in layer_configs While TH implements convolution , TF and CNTK implement the correlation operation . if 'class_name ' not in config [ 0 ] or config [ 0 ] [ 'class_name ' ] == 'Merge ' : output_masks=output_mask , tensor_indices.append ( tensor_index ) first_layer = normalize_legacy_config ( first_layer ) name = 'param_ ' + str ( i ) ` call ` method of the layer at the call that created the node . self.layers.append ( layer ) elif len ( input_data ) == 4 : weight_tensor_td_conv_new = saving.preprocess_weights_for_loading ( raise RuntimeError ( `` `` '' Removes the last layer in the model . x._uses_learning_phase = getattr ( x , '_uses_learning_phase ' , False ) or uses_learning_phase return self._gather_list_attr ( 'trainable_weights ' ) layers = self.layers # It would be unreliable to build a dictionary self._initial_weights = kwargs [ 'weights ' ] arguments : dictionary of keyword arguments that were passed to the * * kwargs : When using the Theano/CNTK backends , these arguments return self._updates for x in self.inputs : get_output_shape_at ( node_index ) be equal to the number of samples of your dataset 'class_name ' : layer_class_name , If the model has multiple outputs , you can use a different `` `` '' Sequential model class and model-related utilities . # actually create the model to match that input shape provided . # Raise exceptions in case the input is not compatible if isinstance ( obj , dict ) : self.metrics = self.model.metrics input_shapes : list of input shape tuples . 'is redundant . ' The unique name . str ( all_names.count ( name ) ) `` `` '' Retrieves the input shape tuple ( s ) of a layer . ' '' was not an Input tensor , ' ' a ` batch_input_shape ` or an ` input_shape ` . ' ) to validate before stopping . output_shapes= [ self.outputs [ 0 ] ._keras_shape ] ) A tensor . self._per_input_updates [ inputs_hash ] += updates # Arguments if layer : 'Graph disconnected : ' config : Configuration dictionary . This can be useful to tell the model to # Handle Keras 1.1 format @ staticmethod if len ( input_data ) == 3 : return output_tensors Note that in conjunction with ` initial_epoch ` , def predict_on_batch ( self , x ) : 'ill-defined for the layer . ' all_output_shapes = set ( output_tensors=self.outputs , workers=workers , kwargs = { } weight = K.variable ( initializer ( shape ) , layers_for_depth = layers_by_depth [ depth ] self._internal_output_shapes = [ x._keras_shape for x in self.outputs ] else : # We batch weight value assignments in a single backend call # instantiate model def predict_proba ( self , x , batch_size=None , verbose=0 , steps=None ) : # and turns them into a list of output tensors . layer.stateful ) for layer in self.layers ] ) self.build ( input_shapes ) data_npy = np.asarray ( data ) # this is the layer that takes a list of input tensors if inputs_hash in self._per_input_updates : tuples = [ ] sample_weight_mode : If you need to do timestep-wise self.model.set_weights ( weights ) metrics = convert_custom_objects ( training_config [ 'metrics ' ] ) # we compute the output tensors , 'name ' : layer.name , def to_yaml ( self , * * kwargs ) : # if there 's no bias weight in the file , skip this conversion from .. engine.topology import _object_list_uid raise ValueError ( 'Improperly formatted model config . ' ) for x in inputs : uses_learning_phase = computed_tensors [ 0 ] ._uses_learning_phase node_index = node.node_indices [ i ] output_shape = layer.compute_output_shape ( input_shapes [ 0 ] ) node_conversion_map [ node_key ] = kept_nodes if node_key in self._network_nodes : # ( filters , input_dim , filter_length , 1 ) model = model_from_config ( model_config , custom_objects=custom_objects ) return self.model.losses self.is_placeholder = False self.trainable = kwargs.get ( 'trainable ' , True ) input_masks= [ None for _ in self.inputs ] , 'keyword arguments : ' execute the generator on the main thread . 'name ' : self.name , input_shapes = to_list ( input_shape ) self.add_loss ( regularization_losses , _to_list ( inputs ) ) input , output : Input/output tensor ( s ) . Note that if the layer is used prefix = 'sequential_ ' def from_config ( cls , config ) : 'input_tensors ' , if not proceed : skip_mismatch=skip_mismatch , self.arguments = arguments # Example # this is a logistic regression in Keras sample_weight : Optional Numpy array of weights for # convert the weights between CuDNNGRU and GRU ( reset_after=True ) # Keep track of unconditional updates ( e.g . a counter ) . from .. utils.generic_utils import has_arg If ` True ` , use process-based threading . self.ndim = len ( shape ) input_shapes : list of input shape tuples . if node_key in model._container_nodes : raise ValueError ( ' % s is not compatible with % s ' % types ) input_spec ( list of class instances ) str ( spec.max_ndim ) + ' , found ndim= ' layer.set_weights ( weights [ : nb_param ] ) 'All outputs should only appear once . ' nodes_depths [ inbound_node ] = max ( depth + 1 , previous_depth ) ` batch_shape= ( None , 32 ) ` indicates batches of an arbitrary number str ( len ( self.input_layers ) ) + ' tensor inputs . ' ) requesting ` input_shape ` will raise an Exception . optimizer_weights_group.attrs [ 'weight_names ' ] = weight_names optimizer_weight_values = [ optimizer_weights_group [ n ] for n in RuntimeError : if a cycle is detected . y2 = to_list ( model.predict ( X ) ) nodes_in_decreasing_depth.append ( node ) weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 0 , 1 , 3 , 2 ) ) weight_names = [ ] or a mismatch in the shape of the weights . def input_spec ( self ) : Node ( `` `` '' Evaluates the model over a single batch of samples . if source == 'CuDNNGRU ' : # Support for legacy behavior 'Here , a tensor specified as ' 'You can build it manually via : ` ' # Create an input layer . a list of instances of InputSpec ( one per input tensor ) . values = getattr ( self._inbound_nodes [ node_index ] , attr ) 'should have a single output tensor . ' kept_nodes += 1 return transform layer_indices [ layer ] = len ( layer_indices ) if node.arguments : str ( x ) + ' at layer `` ' + layer.name + ' '' . ' with the custom object . # new : ( kernel_rows , kernel_cols , filters , stack_size ) if opened_new_file : if len ( input_shapes ) == 1 : if isinstance ( obj , list ) : epochs=epochs , ( e.g . L2 weight regularization , which only depends if isinstance ( inputs , list ) : data.extend ( [ n.decode ( 'utf8 ' ) self.add_update ( layer.get_updates_for ( computed_tensors ) , inputs ) as part of the saved model , the model is already # Check for redundancy in outputs . weights = weights [ num_weights : ] model.save ( 'my_model.h5 ' ) # creates a HDF5 file 'my_model.h5 ' '\ 's weights have shape ' for x in node.output_tensors : shape_key = inbound_layer.name + ' _ % s_ % s ' % ( node_index , tensor_index ) @ interfaces.legacy_input_support raise ImportError ( ' ` load_model ` requires h5py . ' ) return all_attrs you can pass a 2D array with shape output_tensors = _to_list ( output_tensors ) if include_optimizer and hasattr ( model , 'optimizer ' ) : # In case of nested models : recover the first layer 'Use ` get_input_mask_at ( node_index ) ` ' `` `` '' Loads attributes of the specified name from the HDF5 group . 'as part of the model save file . ' self.input_layers.append ( layer ) 'dtype ' , self.layers_by_depth = layers_by_depth batch_size : Integer or ` None ` . saving.save_weights_to_hdf5_group ( f , layers ) def from_config ( cls , config ) : str ( spec.min_ndim ) + ' , found ndim= ' tensor_index = node.tensor_indices [ j ] else : # Handle automatic shape inference ( only useful for Theano ) . if hasattr ( layer , 'reset_states ' ) and getattr ( layer , 'stateful ' , False ) : 'trainable ' : self.trainable } `` `` '' Generates predictions for the input samples from a data generator . if isinstance ( inputs , list ) and inputs == [ ] : self.model.trainable = value # misc functions ( e.g . loss function ) if len ( params ) ! = len ( weights ) : # possible . layers_by_depth [ depth ] .append ( layer ) if node.inbound_layers : considered during deserialization . weights [ 8 ] ] , axis=-1 ) `` `` '' Sets the weights of the model . # here we order them by traversal order . raise TypeError ( unprocessed_nodes [ layer ] = [ node_data ] If ` by_name ` is False ( default ) weights are loaded chunked_data = np.array_split ( data_npy , num_chunks ) # If the class is private the name starts with `` _ '' which is not secure self.output_masks = output_masks layer ( input_tensors [ 0 ] , * * kwargs ) output_shapes : list of output shape tuples . # Following 2 properties : return obj.__name__ f.attrs [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) if len ( input_tensors ) == 1 : epochs : Integer , total number of iterations on the data . # It acts as a queue that maintains any unprocessed ValueError : In case the ` layer ` argument has new_weights = [ ] # Reverse index of layer name to list of layers with name . return yaml.dump ( self._updated_config ( ) , * * kwargs ) validation_steps=None , if cache_key in self._output_mask_cache : # If all previous input tensors are available in tensor_map , self._trainable = True defaults to ` [ .33 , .55 , .67 , 1 . ] ` . set_weights ( weights ) reshape=reshape ) num_chunks = 1 either a tensor or None ( no mask ) . object_list = to_list ( object_list ) x=None , yield ( x , y ) def preprocess_weights_for_loading ( layer , weights , nodes_depths [ node ] = depth shuffle=shuffle , save_weights_to_hdf5_group ( f , self.layers ) return self.model.train_on_batch ( x , y , for line in f : For instance , ` shape= ( 32 , ) ` indicates that the expected input 'starting with a freshly initialized ' skip_mismatch : Boolean , whether to skip loading of layers self.sample_weight_mode = self.model.sample_weight_mode shape = tensor._keras_shape self.name + ' : expected dtype= ' else : validation_split=validation_split , topology.load_weights_from_hdf5_group ( f , layers , reshape=reshape ) if layer.input_spec is None : inbound_layer = node.inbound_layers [ i ] self.inputs = [ ] # List of input tensors with multiple input shapes ) , in which case batch_size=batch_size , if target_class == 'CuDNNGRU ' : tensor_indices = [ ] self.output_layers_tensor_indices = self.model.output_layers_tensor_indices obj : object , dict , or list . computed_tensors = [ computed_tensor ] for s in to_list ( input_shape ) ] ) : if spec.ndim is not None : if spec.max_ndim is not None : layer = created_layers [ layer_name ] layers_with_complete_input = [ ] # To provide a better error msg . K.batch_set_value ( weight_value_tuples ) # call compile method of Model class `` `` '' Computes an output mask tensor . h5py.File object where to save the model K.batch_set_value ( tuples ) output_tensors= [ input_tensor ] , for x in input_tensors ] ) node_index = layer._inbound_nodes.index ( node ) str ( np.prod ( layer_weights_shape ) ) + ' . ' index = { } ( 'ndim= ' + str ( self.ndim ) ) if self.ndim else `` , str ( K.dtype ( x ) ) ) or a mismatch in the shape of the weights . name = str ( w.name ) dtype = K.floatx ( ) # Model attributes . # Create an input layer . model.add ( Dense ( 32 , input_shape= ( 500 , ) ) ) def load_model ( filepath , custom_objects=None , compile=True ) : if ( not isinstance ( output_mask , ( list , tuple ) ) and # If file exists and should not be overwritten . dtype = first_layer.dtype batch . Therefore , all arrays in this tuple must have the same ' have ' + str ( len ( weight_values ) ) if isinstance ( layer , InputLayer ) : return self._get_node_attribute_at_index ( 0 , 'output_masks ' , try : # then call node.inbound_layer on them . verbose=verbose ) The generator should return the same kind of data as accepted by masks = to_list ( mask ) ( 'shape= ' + str ( self.shape ) ) if self.shape else `` , layer_name , node_index , tensor_index = layer_data # List of initial layers ( 1 to 1 mapping with self.inputs , initial_epoch : Integer . specs += layer.input_spec 'output ' ) multiple output tensors , or is already connected 'with different input shapes . Hence ' return state_updates Numpy array with the same length as the input samples # Build self.input_names and self.output_names . # Keep track of losses that depend on the inputs Only applicable if the layer has one inbound node , epochs = kwargs.pop ( 'nb_epoch ' ) 'As a result , we can not save the optimizer ' # identical to the previous one ' has multiple inbound nodes , ' # Collect unconditional updates . If a Keras tensor is passed : for i in range ( len ( output_tensors ) ) : list ( custom_objects.items ( ) ) ) ) and weights file . if num_weights > 0 : kwargs [ 'mask ' ] = computed_masks layers_to_output_shapes [ shape_key ] = output_shapes [ j ] self.input_layers.append ( layer ) ` True ` if conversion on kernel matrices is required , otherwise ` False ` . ( e.g . will not include losses that depend on tensors where there is a mismatch in the number of weights , if not self.built : before declaring the prediction round finished . from .. utils.io_utils import ask_to_proceed_with_overwrite node_index : Node index from which ` tensor ` comes from . ` optimizer ` , ` loss ` , ` metrics ` or ` sample_weight_mode ` . computed_tensors = [ x [ 0 ] for x in computed_data ] that the layer itself is also trainable ) . 'after instantiation . ' 'Prefer using a Keras optimizer instead ' output_layers # instantiate model Returns a layer based on either its name ( unique ) raise ValueError ( 'Layer ' + self.name + ' was called with ' for layer_data in config [ 'input_layers ' ] : # input shape and dtype . # If obj is a python 'type ' layers_depths [ node.outbound_layer ] = depth output_shapes = _to_list ( output_shape ) def uses_learning_phase ( self ) : It should be a single tensor compile : Boolean , whether to compile the model return self._output_tensor_cache [ cache_key ] param_values = K.batch_get_value ( params ) original_keras_version = ' 1 ' def call ( self , inputs , mask=None ) : Output shape tuple `` `` '' Saves the weights of a model . ndim = K.ndim ( x ) # Check that x is an input tensor . TypeError : If ` layer ` is not a layer instance . outputs = _to_list ( wrapped ( inputs , training=True ) ) 'value ' + str ( value ) if output_shapes is not None : self.trainable = False shuffle : Boolean ( whether to shuffle the training data ` node_indices ` and ` tensor_indices ` are basically fine-grained coordinates sample_weight=None , layers.append ( layer_config ) if layer.name : input_shapes.append ( input_shape ) def save_weights ( self , filepath , overwrite=True ) : if original_backend is None : from . import base_layer shape = tensor._keras_shape build_map_of_graph ( x , finished_nodes , nodes_in_progress , output_tensors [ i ] ._keras_history = ( self , self._output_mask_cache [ mask_cache_key ] = mask num_chunks += 1 layers = legacy_models.legacy_sequential_layers ( self ) return self.model.get_layer ( name , index ) return layer_cache [ name ] for input_index , ( x , spec ) in enumerate ( zip ( inputs , input_spec ) ) : inbound_layer = node.inbound_layers [ i ] cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) self.outputs [ 0 ] ._keras_shape ] # New file format . # Dictionary mapping reference tensors to tuples was never compiled in the first place ) . have been traversed completely . ' has no inbound nodes . ' ) max_queue_size : Integer . Maximum size for the generator queue . x , y = process_line ( line ) config = { # Update tensor_map . raise ValueError ( 'Invalid bias shape : ' + str ( bias_shape ) ) class Sequential ( Model ) : # Example return model used for model definition or training . # Get sorted list of layer depths . 'after loading it . ' 'Use ` get_input_mask_at ( node_index ) ` ' ' have ' + str ( len ( weight_values ) ) str ( layer_weights_shape ) + ' and size ' previous_depth = nodes_depths.get ( inbound_node , 0 ) from .utils.io_utils import ask_to_proceed_with_overwrite elif len ( input_data ) == 4 : # Augment the mask to match the length of the output . `` `` '' Checks compatibility between the layer and provided inputs . arguments=arguments ( 2 , 3 , 1 , 0 ) ) at the end of every epoch . It should typically str ( spec.max_ndim ) + ' , found ndim= ' return weights input_spec : List of InputSpec class instances output_shapes.append ( layers_to_output_shapes [ key ] ) to be passed to ` yaml.dump ( ) ` . from keras.layers.core import Masking output_masks= [ None ] , def save_attributes_to_hdf5_group ( group , name , data ) : if by_name : layers_depths = { } # dict { layer : depth value } in the FAQ for instructions on how to install ` h5py ` . self._updates = [ ] if spec.shape is not None : validation_steps : Only relevant if ` validation_data ` more than once ( shared layer ) , this is ill-defined from .. models import save_model # Attempt automatic input shape inference . weights.append ( layer.get_weights ( ) ) # the node has no outbound nodes ( depth 0 ) . # List of shape tuples , shapes of output_tensors . `` `` '' Checks if conversion on kernel matrices is required during weight loading . during training and testing . if len ( inbound_layer._inbound_nodes ) < = inbound_node_index : all_output_shapes = set ( [ str ( node.output_shapes ) for node in self._inbound_nodes ] ) original_backend=original_backend ) ) Weights values as a list of numpy arrays . `` `` '' Gets the model 's input specs . if isinstance ( origin_layer , InputLayer ) : inbound_layers = [ ] # ( computed tensor , compute mask ) return values [ 0 ] weight_names = _load_attributes_from_hdf5_group ( g , 'weight_names ' ) 'batch_input_shape ' , def _convert_rnn_weights ( layer , weights ) : batch_size=batch_size , } , default=get_json_type ) .encode ( 'utf8 ' ) name : String , must be unique within a model . of values are present but the shape does not match . model at the target location , or instead arguments=user_kwargs ) based on the network 's topology , meaning the architecture # Note f.attrs [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) filtered_layer_names.append ( name ) weights = new_weights to be passed to ` json.dumps ( ) ` . # Save optimizer weights . if not node.inbound_layers : inbound_names = [ ] def trainable ( self ) : To load a network from a JSON save file , use return self.model.get_layer ( name , index ) get_input_shape_at input_shape : Keras tensor ( future input to layer ) return cls ( * * config ) layer = node.inbound_layers [ i ] model.add ( Dense ( 32 , input_dim=500 ) ) from .engine.input_layer import InputLayer epoch epochs is reached . return outputs in order to capture the string summary . # this does nothing . raise AttributeError ( 'Layer ' + self.name # of the call and of all new variables created during the call . dtype = kwargs.get ( 'dtype ' ) min_ndim : Integer , minimum rank of the input . for depth in depth_keys : regularization_losses = [ shapes = _to_list ( layer.compute_output_shape ( computed_tensors [ 0 ] ._keras_shape ) ) def model_from_json ( json_string , custom_objects=None ) : self.input_names = self.model.input_names if K.ndim ( x ) ! = spec.ndim : `` `` '' This module is deprecated , but kept around for backwards compatibility . merge = legacy_layers.Merge.from_config ( first_layer_config ) index : integer , index of layer . y_expected = _to_list ( merge_func ( f_forward ( X ) [ 0 ] , f_backward ( X ) [ 0 ] ) ) conv_layers = [ 'Conv1D ' , 'but it received ' + str ( len ( inputs ) ) class_weight=class_weight , one per output tensor of the layer ) . # also possible ( equivalent to the above ) : shape_key = layer.name + ' _ % s_ % s ' % ( node_index , tensor_index ) if len ( depth_keys ) > 1 : ( necessary since each inbound layer might have several nodes , # Store the traversal order for layer sorting . ValueError : in case of mismatch between provided layers # If obj is a python 'type ' layer : Layer instance . layers = self.layers # Create node , add it to inbound nodes . self.build ( ) created_layers [ layer_name ] = layer For instance , ` shape= ( 32 , ) ` indicates that the expected input `` `` '' A Container is a directed acyclic graph of layers . ( strings ) to custom classes or functions to be if weight_names : `` `` '' Converts weights for RNN layers between native and CuDNN format . # and set output_tensors ' _keras_history . get_input_shape_at ( node_index ) created_layers = { } return obj.item ( ) layer._outbound_nodes.append ( self ) outputs nodes_in_decreasing_depth = [ ] if not layer_cache : self.output_layers_node_indices = [ ] source = 'GRU ( reset_after=True ) ' ValueError : In case the ` layer ` argument does not return transform def _add_inbound_node ( self , input_tensors , output_tensors , layer_name = layer_data [ 'name ' ] if len ( layer._inbound_nodes [ -1 ] .output_tensors ) ! = 1 : metrics=None , elif layer_weights_shape ! = weights [ 0 ] .shape : only if they share the same name . This is useful [ 3 , 4 , 5 ] ] [ 1 , 3 , 5 ] ] return output_tensors , output_masks , output_shapes 'containing ' + str ( len ( layer_names ) ) if not self.trainable : TypeError : if input tensors are not Keras tensors return insecure def _flattened_layers ( self ) : Node ( self , for x in inputs : model_layers = model.layers if not isinstance ( iterable_or_element , ( list , tuple ) ) : Only applicable if the layer has exactly one inbound node , if not len ( self._inbound_nodes ) > node_index : # self.input_spec f.attrs [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) 'Input ' + str ( input_index ) # Nodes that can not yet be processed ( if the inbound node for i in range ( len ( output_tensors ) ) : str ( type ( x ) ) + ' . Full input : ' if not self.outputs : Each time the output of a layer is used by another layer , sample_weight : Optional Numpy array of weights for output_ls = to_list ( output ) # These properties should be set by the user via keyword arguments . layers = legacy_models.legacy_sequential_layers ( self ) for layer in layers : kernels , [ biases ] ) ( Numpy arrays ) . from .. import __version__ as keras_version input_tensors = _to_list ( input_tensors ) 'InputLayer , not both at the same time . ' ) # if obj is any numpy type kernels = transform_kernels ( weights [ 0 ] , the loss and any model metrics at the end of each epoch . initializer : An Initializer instance ( callable ) . and weights file . except TypeError : if layer.data_format == 'channels_first ' : if target_class in [ 'GRU ' , 'CuDNNGRU ' ] and len ( weights ) == 3 : input_shape = _collect_input_shape ( inputs ) # Check dtype . if self.layers [ 0 ] not in layers : # The model owns this layer node . symbolic_weights [ i ] .shape , layer 's specifications . dictionary or a list of modes . validation_steps : Only relevant if ` validation_data ` for key , value in obj.items ( ) : for node in nodes : assert layer_name in created_layers weight_values [ i ] ) ) weights = forward_weights + backward_weights if source ! = target : layer ( input_tensors , * * kwargs ) # Build train function ( to get weight updates ) . recursion stack . Useful to detect cycles . tensor_indices.append ( tensor_index ) input_shape = input_shapes [ i ] self.input_names.append ( layer.name ) for sw , w in zip ( layer.weights , layer_weights ) : self.inputs = network.get_source_inputs ( self.outputs [ 0 ] ) return input_shapes Add layer to tensor history input_tensor._keras_history = ( self , 0 , 0 ) 'dtype ' , # without the network being notified of it . output_tensors= [ input_tensor ] , ( or list of input shape tuples , one tuple per input tensor ) . # Build self.output_layers : self.ndim = ndim If None is passed , the updates are assumed unconditional . first_layer = config [ 0 ] input_masks : list of input masks ( a mask can be a tensor , or None ) . if not layer.layers : # and create the node connecting the current layer information , nor the layer class name . These are handled layer = layer_module.deserialize ( layer_data ) ' '' ) expects ' generator : A generator or an instance of ` Sequence ` layer_cache = { } self._output_shape_cache = self.model._output_shape_cache to yield from ` generator ` before stopping . will not train on it , and will evaluate self._losses = [ ] # Set dtype . ` sample_weight_mode= '' temporal '' ` in ` compile ( ) ` . Input kernels for each gate are transposed and converted between Fortran str ( len ( symbolic_weights ) ) merge = self.layers [ 0 ] `` `` '' The ` Model ` class adds training & evaluation routines to a ` Network ` . if len ( self.layers ) < = index : set_weights # Compile model . # Fill in the output mask cache . super ( InputLayer , self ) .__init__ ( dtype=dtype , name=name ) 'Graph disconnected : ' output_masks : list of output masks ( a mask can be a tensor , or None ) . return self._trainable_weights + self._non_trainable_weights ' expects ' + str ( len ( symbolic_weights ) ) `` `` '' Loads all layer weights from a HDF5 save file . save_weights_to_hdf5_group ( model_weights_group , model_layers ) with K.name_scope ( self.name ) : if not batch_input_shape : chunk_id = 0 if isinstance ( output_tensor , list ) : raise ValueError ( 'An Input layer should be passed either ' order of horizontal graph traversal ( bottom-up ) . verbose=1 , # Legacy support raise Exception ( data provided . def get_output_shape_at ( self , node_index ) : depth_keys = list ( nodes_by_depth.keys ( ) ) The generator should return the same kind of data as accepted by data = [ ] `` `` '' Prints a string summary of the network . The loss may potentially be conditional on some inputs tensors , weights = keras.engine.saving.preprocess_weights_for_loading ( target_layer , weights ) for original_node_index , node in enumerate ( layer._inbound_nodes ) : losses : loss tensor or list of loss tensors trainable_weights = self._gather_list_attr ( 'trainable_weights ' ) self.input_layers_node_indices = self.model.input_layers_node_indices # Support for legacy models if not is_all_none ( previous_mask ) : 'must be Keras tensors . Found : ' + str ( x ) # node data that specifies a layer call . model_inputs.append ( [ layer.name , new_node_index , tensor_index ] ) sparse=self.sparse , `` ` python ValueError : In case of invalid arguments for if node.arguments : 'You should pass an input_shape or ' # does not return a list the same size as ` call ` def get_input_at ( self , node_index ) : # Legacy shape : class_weight=None , # does not yet exist ) are re-enqueued , and the process return weights output_shapes.append ( layers_to_output_shapes [ key ] ) a list of size 1 containing the tensor . get_input_mask_at ( node_index ) min_ndim=None , tensor_index = node.tensor_indices [ j ] inputs = None param_dset [ ( ) ] = val `` `` '' Saves the model to a single HDF5 file . of arrays and their shape must match else : return specs topology.save_weights_to_hdf5_group ( model_weights_group , model_layers ) # We can determine the source of the weights from the shape of the bias . `` `` '' Instantiates a Keras model from its config . return self._losses attributes that allow us to build a Keras model 'Use ` get_output_shape_at ( node_index ) ` ' the ` len ( generator ) ` as a number of steps . `` `` '' Returns predictions for a single batch of samples . # node data that specifies a layer call . # Collect unconditional losses . weights = convert_weights ( weights , from_cudnn=False ) inputs steps=None ) : input_spec = self.input_spec self._non_trainable_weights.append ( weight ) merge = self.layers [ 0 ] import warnings for n in group.attrs [ ' % s % d ' % ( name , chunk_id ) ] ] ) # Inferring the output shape is only relevant for Theano . input_shape validation_data=validation_data , `` `` '' Retrieves the output mask tensor ( s ) of a layer at a given node . if isinstance ( mask , list ) : The state of the optimizer , allowing to resume training 'and thus has no defined output shape . ' ) output_shape : Shape tuple . See above . if layer is None or node_index : input_shape # This is for performance optimization self._output_tensor_cache = { } generator : generator yielding batches of input samples . from .. import initializers and C layout , recurrent kernels are transposed . For LSTM biases are summed/ depth_keys = list ( self._nodes_by_depth.keys ( ) ) str ( spec.dtype ) + ' , found dtype= ' shapes = to_list ( layer.compute_output_shape ( [ x._keras_shape for x in computed_tensors ] ) ) `` `` '' Computes an output mask tensor . To load a network from a yaml save file , use 'state . As a result , your model is ' if str ( id ( x ) ) in tensor_map : return any ( [ ( hasattr ( layer , 'stateful ' ) and layer.stateful ) for layer in self.layers ] ) TypeError : If ` layer ` is not a layer instance . TypeError : if ` config ` is not a dictionary . when using multiprocessing . tensor , mask = tensor_map [ str ( id ( x ) ) ] ( necessary since each inbound layer might inputs_ls = to_list ( inputs ) more than one incoming layers . layer ( input_tensors , * * kwargs ) # Raises inbound_layer , node_index , tensor_index = x._keras_history input_tensors : list of input tensors . # No container-level masking for now . dtype=val.dtype ) self.outputs = [ self.layers [ -1 ] .output ] input_shapes.append ( K.int_shape ( x_elem ) ) sample_weight=sample_weight , model_outputs.append ( [ layer.name , new_node_index , tensor_index ] ) tensor_indices : a list of integers , layer 's specifications . if x not in source_tensors : if node_key in self._container_nodes : # Recover loss functions and metrics . if legacy_models.needs_legacy_support ( model ) : weight_values = preprocess_weights_for_loading ( computed_masks = [ computed_mask ] name = 'param_ ' + str ( i ) A mask tensor or list of mask tensors . 'The weights for loading have shape ' get_weights model.add ( merge ) is simply a Container with added training routines . batch_size : Optional input batch size ( integer or None ) . recursively . h5py.File object where to save the model to add to the layer . # for Theano and CNTK if hasattr ( layer , 'updates ' ) : from __future__ import absolute_import of Numpy arrays with shapes and types matching It can either wrap an existing tensor ( pass an ` input_tensor ` argument ) self.ndim = ndim finished_nodes = set ( ) for layer in self.layers [ 1 : ] : for layer_data in config [ 'layers ' ] : 'Layer names : ' , all_names ) self.output_tensors = output_tensors losses : loss tensor or list of loss tensors from .training import Model def run_internal_graph ( self , inputs , masks=None ) : return np.hstack ( [ func ( k ) for k in np.hsplit ( kernels , n_gates ) ] ) ' '' with a weight list of length ' Python dictionary . * * kwargs : Additional keyword arguments . Thus the saved model can be reinstantiated in @ classmethod def process_layer ( layer_data ) : inbound_layers.append ( inbound_layer ) # be changed at any point by the user # Updates indexed by None are unconditional batch_size=None , A Numpy array of predictions . self._built = value if not self.input_spec : self._output_tensor_cache [ cache_key ] = output_tensors constraint : An optional Constraint instance . if proba.shape [ -1 ] > 1 : def get_output_mask_at ( self , node_index ) : biases = weights [ 2 ] .reshape ( ( 2 , -1 ) if from_cudnn else -1 ) updates : update op or list of update ops for x in reference_input_tensors : The use of ` keras.utils.Sequence ` guarantees the ordering self.ndim = len ( shape ) weight_values , x._uses_learning_phase = getattr ( x , '_uses_learning_phase ' , False ) or uses_learning_phase # Check that no item in ` data ` is larger than ` HDF5_OBJECT_HEADER_LIMIT ` inputs = to_list ( inputs ) include_optimizer : If True , save optimizer 's state together . at successive epochs , as well as validation loss values sample_weight_mode=None , depth_keys.sort ( reverse=True ) from .. engine import Layer inbound_node_index = input_data [ 1 ] input_spec = self.input_spec def compile ( self , optimizer , loss , continue if layer.__class__.__name__ == 'ConvLSTM2D ' : config.append ( { 'class_name ' : layer.__class__.__name__ , for j in range ( len ( output_shapes ) ) : A layer instance . # this is a logistic regression in Keras layers = self.layers output_masks= [ None ] , nodes_in_progress.remove ( node ) original_keras_version = ' 1 ' if len ( all_input_shapes ) == 1 : return self._trainable for layer in self.inbound_layers : removing layers is fine as long as they do n't have weights . def transform ( kernel ) : Will only include updates that are either Keras CuDNN 'All inputs should only appear once . ' if all ( [ s is not None for s in _to_list ( input_shape ) ] ) : saving.load_weights_from_hdf5_group ( f , layers , reshape=reshape ) For every weight in the layer , a dataset ' Found : ' + str ( self.outputs ) ) if len ( params ) ! = len ( weights ) : def build ( self , input_shape ) : input_tensors = [ ] return def trainable_weights ( self ) : ` layer.get_input_at ( node_index ) ` . ( ordered names of model layers ) . original_backend : Keras backend the weights were trained with , # If there is no bias we skip the conversion since CuDNNGRU always has biases . def to_json ( self , * * kwargs ) : ( e.g . will not include updates that depend on tensors from .. utils.layer_utils import print_summary as print_layer_summary or a single instance if the model has only one input . `` `` '' Creates the layer weights . # tensor inputs and outputs of outbound_layer . depth = nodes_depths.setdefault ( node , 0 ) self.weighted_metrics = self.model.weighted_metrics given by ` epochs ` , but merely until the epoch if weights [ 0 ] .size ! = np.prod ( layer_weights_shape ) : input_shapes = to_list ( input_shapes ) A numpy array of class predictions . self._per_input_updates [ inputs_hash ] = [ ] will be obtained from ` tensor._keras_history ` . str ( all_names.count ( name ) ) self.output_layers_tensor_indices = self.model.output_layers_tensor_indices if skip_mismatch : else : if not self.inputs or not self.outputs : 'The tensor that caused the issue was : ' self._network_nodes = self.model._network_nodes prefix = self.__class__.__name__ name : Name of the layer ( string ) . name = _to_snake_case ( prefix ) + ' _ ' + str ( K.get_uid ( prefix ) ) output_masks=output_masks , if not input_shape and not batch_input_shape : layer._outbound_nodes.append ( self ) use_multiprocessing=use_multiprocessing , all_names = [ layer.name for layer in self.layers ] # Example } , A list of weights values ( Numpy arrays ) . return self.model.predict_generator ( generator , steps , config.append ( { 'class_name ' : 'Merge ' , 'config ' : merge_config } ) callbacks=callbacks , dtype : Expected datatype of the input . if not name : config = { 'batch_input_shape ' : self.batch_input_shape , set ( x for x in losses if not isinstance ( x , ( float , int ) ) ) ) ' layers into a model with ' if K.backend ( ) == 'theano ' or K.backend ( ) == 'cntk ' : use_multiprocessing=False , metrics= [ 'accuracy ' ] ) for sublayer in layer._flattened_layers : get_output_at ( node_index ) ' input tensors . Input received : ' to False , the compilation is omitted without any kwargs = node.arguments In this case ` call ` just reapplies ' '' . This layer has no information ' an ` input_dim ` argument . def built ( self , value ) : # Iterate over nodes , by depth level . return output_masks self.name + '.build ( batch_input_shape ) ` . ' ) by_name : Boolean , whether to load weights by name The created weight variable . layer = layer_module.deserialize ( conf , custom_objects=custom_objects ) Expects ` inputs ` to be a list ( potentially with 1 element ) . model = Sequential ( ) ( e.g . set this to adapt the display to different layer = layer_module.deserialize ( layer_data ) in each line . If not provided , if len ( computed_data ) == len ( reference_input_tensors ) : skip_mismatch : Boolean , whether to skip loading of layers layer.add_loss ( regularization_losses , # ( e.g . a model such as A ( B ( A ( B ( x ) ) ) ) ) if include_optimizer and hasattr ( model , 'optimizer ' ) : ( only valid when ` by_name ` =True ) . return uses_correlation [ original_backend ] ! = uses_correlation [ K.backend ( ) ] sample_weight : sample weights , as a Numpy array . for layer in self._flattened_layers : def compute_mask ( self , inputs , mask ) : `` `` '' Serialize any object to a JSON-serializable structure . ValueError : If the index is does not match any node . See [ callbacks ] ( /callbacks ) . model.add ( merge ) def uses_learning_phase ( self ) : layer.add_loss ( regularization_losses , computed_tensors ) for i , layer in enumerate ( self.input_layers ) : self.layers = [ ] reference_output_tensors = node.output_tensors ' not compatible with ' ' was passed non-serializable ' if hasattr ( self , 'activity_regularizer ' ) and self.activity_regularizer is not None : compute_output_shape raise ValueError ( 'The first layer in a ' # self.non_trainable_weights return inputs # If the depth is not set , # CuDNNLSTM has ( units * 8 ) weights ; while LSTM has ( units * 4 ) the ` len ( generator ) ` as a number of steps . # if masking is explicitly supported , by default if they 're in plain Keras format . # If there is no bias we skip the conversion since CuDNNGRU always has biases . ndim node_index : Origin node index of the tensor . self.input_layers_tensor_indices = self.model.input_layers_tensor_indices self._losses += losses str ( input_shape ) + ' : model has ' layer_weights = weights [ : num_param ] or ` layer.get_input_shape_at ( node_index ) ` . if not self.outputs : if len ( all_output_shapes ) == 1 : # for Theano and CNTK param_dset = g.create_dataset ( name , val.shape , name = str ( w.name ) + ' _ ' + str ( i ) } output_tensors : list of output tensors . symbolic_weights = layer.weights input_tensors=input_tensors , compiled . Otherwise , the model is uncompiled and if node in nodes_in_progress : 'cntk ' : True } split in half , for GRU biases are reshaped . def preprocess_weights_for_loading ( layer , weights , from . import network the layers from where ` input_tensors ` originate . self._per_input_losses = { } output_masks = layer.compute_mask ( computed_tensors , original_backend , 'batch_input_shape ' , # It 's supposed to be an input layer , so only one node if not isinstance ( layer , Layer ) : # in case of layer shared at different topological depths # Update self._per_input_updates self.loss_weights = self.model.loss_weights self.input_layers = self.model.input_layers 'sample_weight_mode ' : model.sample_weight_mode , kwargs [ 'mask ' ] = previous_mask # Gather info about inputs and outputs . eta_format = ( ' % d : % 02d : % 02d ' % ' , but the layer was expecting ' if not batch_input_shape : for x_elem in _to_list ( inputs ) : try : 'must be Keras tensors . Found : ' + str ( x ) output_mask = self.compute_mask ( inputs , previous_mask ) 'is redundant . ' layers = legacy_models.legacy_sequential_layers ( self ) class Node ( object ) : # If the layer returns tensors from its inputs , unmodified , # Default values of symbolic_weights is /variable raise ValueError ( 'All layers in a Sequential model ' or ( inputs , targets , sample_weights ) self.add_loss ( regularization_losses , def _flattened_layers ( self ) : if isinstance ( origin_layer , topology.InputLayer ) : weights = self._gather_list_attr ( 'non_trainable_weights ' ) def get_json_type ( obj ) : weight_value_tuples.append ( ( p , w ) ) batch_input_shape = tuple ( kwargs [ 'batch_input_shape ' ] ) nodes = self._nodes_by_depth [ depth ] source = 'CuDNNLSTM ' 'an input_tensor argument , ' # These properties will be set upon call of self.build ( ) shape_key = layer.name + '_0_0 ' method of the layer was called ) . # old : ( filters , stack_size , ... ) sample_weight=None , ' weight ( s ) , but the saved weights ' self.name + ' , but the layer isn\'t built . ' `` `` '' Computes output tensors for new inputs . ( ordered names of weights tensor of the layer ) . # self.input_spec use_multiprocessing=False , verbose=0 ) : initializer : An Initializer instance ( callable ) . weights [ 8 ] , deserialized.append ( convert_custom_objects ( value ) ) return input_shapes # Update container_nodes . # It 's an input layer : compute_output_shape is identity , updates += layer.get_updates_for ( None ) ( strings ) to custom classes or functions to be We update the _keras_shape of every input tensor with self.supports_masking = False return model len ( self._inbound_nodes ) - 1 , weight_values , if has_arg ( self.call , 'mask ' ) : The weight file has : raise ValueError ( 'Layer # ' + str ( k ) # Add any potential unconditional model-level loss . `` `` '' Retrieves the input tensor ( s ) of a layer at a given node . ' element ( s ) . ' ) assert shape [ 0 ] == layer.filters and shape [ 2 : ] == ( layer.kernel_size [ 0 ] , 1 ) str ( len ( params ) ) return masks [ 0 ] weight_names.append ( name.encode ( 'utf8 ' ) ) with open ( path ) as f : # new : ( kernel_rows , kernel_cols , stack_size , filters ) The created weight variable . It keeps the shape , but changes between the layout ( Fortran/C ) . Eg . : if legacy_models.needs_legacy_support ( self ) : return output_shapes for depth in depth_keys : weights = [ kernel , recurrent_kernel , bias ] # the graph reconstruction process for node_data in inbound_nodes_data : nb_param = len ( layer.weights ) 'class_name ' : self.__class__.__name__ , # Example data larger than HDF5_OBJECT_HEADER_LIMIT bytes . If None is passed , the loss is assumed unconditional input_tensors.append ( layer_output_tensors [ tensor_index ] ) ( ordered names of weights tensor of the layer ) . if len ( output_masks ) == 1 : validation dataset divided by the batch size . 'Layers should have equal number of output tensors ' a tuple ` ( inputs , targets ) ` validation_split=validation_split , conv_layers = [ 'Conv1D ' , import re if _need_convert_kernel ( original_backend ) : if self.layers : inbound_layer = created_layers [ inbound_layer_name ] if node_key not in self._network_nodes : A None entry in a shape is compatible with any dimension , if node.inbound_layers : 'Received type : ' def save_weights_to_hdf5_group ( f , layers ) : 'apply a reshape operation . ' ' but got shape ' + str ( x_shape ) ) continue 'and thus has no defined output shape . ' ) for x , y , mask in zip ( self.inputs , inputs , masks ) : ( may include None for unchecked axes ) . target_class = layer.__class__.__name__ # merge input and recurrent biases into a single set node_index : Integer , index of the node ( or list of shape tuples if the layer has multiple inputs ) . It will be autogenerated if it is n't provided . Model config with Keras version information added . validation_data=None , computable_tensors.append ( x ) # Check specific shape axes . if isinstance ( mask , list ) : model.save ( 'my_model.h5 ' ) # creates a HDF5 file 'my_model.h5 ' kernels = transform_kernels ( weights [ 0 ] , transpose_input ( from_cudnn ) , n_gates ) # First , we create all layers and enqueue nodes to be processed Output tensor or list of output tensors . ' a Keras Input layer , ' instead of creating a placeholder . layer = self.output_layers [ i ] if layers : computed_masks = [ computed_mask ] just by knowing the inputs and outputs of the model . input_tensors= [ input_tensor ] , if input_tensor is None : data.extend ( [ n.decode ( 'utf8 ' ) if bias_shape == ( 2 * units * n_gates , ) : The loaded Model . kernel = np.concatenate ( [ weights [ 0 ] , 'hence the notion of `` layer output '' ' from .legacy import layers as legacy_layers # Legacy shape : It can be passed to ` transform_kernels ( ) ` . for layer_data in config [ 'layers ' ] : def get_updates_for ( self , inputs ) : 'Here , a tensor specified as ' or list/tuple of Keras tensors to reference # TODO : raise exception when a ` .compute_mask ( ) ` call self.output_layers_tensor_indices = [ ] str ( len ( weight_values ) ) if training_config is None : unconditional , or conditional on inputs to this model def non_trainable_weights ( self ) : ` batch_shape= ( None , 32 ) ` indicates batches of an arbitrary number input_masks : list of input masks ( a mask can be a tensor , or None ) . original_keras_version , weights = [ ] def evaluate ( self , x=None , y=None , 'All outputs should only appear once . ' for layer in index.get ( name , [ ] ) : f = filepath # Check ndim . layer : Layer from which ` tensor ` comes from . If not provided , masks : List of masks ( tensors or None ) . ' is incompatible with layer ' outbound_nodes : List of nodes . # i.e . we mark it to be saved warnings.warn ( 'Network returning invalid probability values . ' elif bias_shape == ( 2 , units * n_gates ) : `` `` '' Counts the total number of scalars composing the weights . layers : A list of target layers . 'and output masks . Layer ' + str ( layer.name ) + ' has ' 'use the functional API . ' ) target = 'CuDNNGRU ' filepath : String , path to the file to save the weights to . elif bias_shape == ( units * n_gates , ) : # Collect updates that are dependent on inputs raise RuntimeError ( 'The model needs to be compiled ' capable of instantiating the same layer from the config on each output by passing a dictionary or a list of losses . raise ValueError ( 'No such layer : ' + name ) TypeError : if there are no layers in the model . class_weight : Optional dictionary mapping class indices ( integers ) nodes_by_depth [ depth ] = [ ] self.trainable = False self.input_masks = input_masks name : A name of the attributes to save . shapes = _to_list ( layer.compute_output_shape ( [ x._keras_shape for x in computed_tensors ] ) ) self.trainable = True def get_input_at ( self , node_index ) : if len ( masks ) == 1 : input_masks , output_masks , 'should be tensors . ' ) assert shape is not None , ( 'Please provide to Input either a ` shape ` ' trainable_weights : List of variables . if type ( obj ) .__name__ == type.__name__ : if layer.data_format == 'channels_first ' : batch_size : Integer . If unspecified , it will default to 32 . original_keras_version=original_keras_version , model_outputs.append ( [ layer.name , new_node_index , tensor_index ] ) return [ ] dictionary or a list of modes . Optional for ` Sequence ` : if unspecified , will use 'However the new layer ' + layer.name x = Input ( batch_shape=batch_shape , or in the case of temporal data , str ( len ( self.input_layers ) ) + ' tensor inputs . ' ) validation_data=None , ( 'max_ndim= ' + str ( self.max_ndim ) ) if self.max_ndim else `` , # create Numpy arrays of input data # new : ( ... , stack_size , filters ) tensor_map [ str ( id ( x ) ) ] = ( y , mask ) output_tensors = [ ] arguments=arguments initial_epoch=0 , x : the input data , as a Numpy array . if len ( output_masks ) == 1 : 'the notion of `` output shape '' is ' name = conf.get ( 'custom_name ' ) if len ( output_ls_copy ) == 1 : the loss and any model metrics If the model has multiple outputs , you can use a different loss finished and starting the next epoch . It should typically dtype = kwargs.get ( 'input_dtype ' ) are passed into ` K.function ` . of Numpy arrays with shapes and types matching if output_shapes is not None : ( instead of topological weight loading ) . output = output_ls_copy [ 0 ] be equal to the number of samples of your dataset def _node_key ( layer , node_index ) : weights += layer.trainable_weights layer_data : layer config dict . def get_output_at ( self , node_index ) : if not isinstance ( layer.input_spec , list ) : for node_index , node in enumerate ( layer._inbound_nodes ) : tensor : The tensor to start from . layer = get_or_create_layer ( first_layer ) output_tensors=output , If the model has multiple outputs , you can use a different loss 'For multi-output layers , ' # Arguments self.build ( ) return self._trainable_weights Note that in conjunction with ` initial_epoch ` , def get_json_type ( obj ) : str ( index ) + ' but model only has ' Must be implemented on all layers that have weights . return layers 'class_name ' : model.optimizer.__class__.__name__ , # List of shape tuples , shapes of input_tensors . # Read final output shapes from layers_to_output_shapes . # for creating scopes . We prefix the name with `` private '' in this case . ' Found : ' + str ( self.outputs ) ) warnings.warn ( computed_data = [ ] # List of tuples ( input , mask ) . def add_weight ( self , return self.trainable_weights + self.non_trainable_weights # Collect losses that are dependent on inputs self._output_mask_cache = { } if not compile : container_nodes.add ( self._node_key ( layer , node_index ) ) if len ( self._inbound_nodes ) > 1 : in the ` x ` and ` y ` data provided , before shuffling . filepath : String , path to the weights file to load . if layer is not None : if isinstance ( input_shape , list ) : self.input_layers = [ ] def input_mask ( self ) : name : Name of the layer ( string ) . is a generator . Total number of steps ( batches of samples ) input_shapes , output_shapes , arguments=None ) : 'config ' : conf } original_backend , sparse : Boolean , whether the placeholder created self._nodes_by_depth = nodes_by_depth created_layers [ layer_name ] = layer 'use the functional API . ' ) if not layer._inbound_nodes : or ` batch_input_shape ` as well as ` dtype ` ) . A Keras tensor is a tensor object from the underlying backend An epoch is an iteration over the entire ` x ` and ` y ` str ( type ( x ) ) + ' . Full input : ' self._feed_inputs.append ( layer.input ) input_shapes= [ x._keras_shape for x in self.inputs ] , name : An optional name string for the layer . if 'batch_size ' in kwargs : used for scaling the loss function ( during training only ) . if 'optimizer_weights ' in f : for kwarg in kwargs : # be changed at any point by the user if not self.supports_masking : return self.model.regularizers # Raise exceptions in case the input is not compatible 'The last layer might not normalize predictions ' 'sparse ' : self.sparse , # ( e.g . BN updates ) . outbound_nodes : list of nodes self.inbound_layers = inbound_layers input_shapes = [ ] raise TypeError ( 'Output tensors to a ' + cls_name + ' must be ' before declaring the evaluation round finished . node = layer._inbound_nodes [ node_index ] with K.name_scope ( layer.name ) : def input_mask ( self ) : return values input_tensors : list of input tensors . bad_attributes = [ x for x in data if len ( x ) > HDF5_OBJECT_HEADER_LIMIT ] first_layer_config [ 'layers ' ] = merge_inputs str ( axis ) + ' of input shape to have ' config [ 'output_layers ' ] = model_outputs is specified . Total number of steps ( batches of samples ) removing layers is fine as long as they do n't have weights . from keras.engine import Input , Layer , topology , get_source_inputs # Optional keyword arguments to layer 's ` call ` . original_keras_version , layers.append ( sublayer ) input_shapes : list of input shape tuples . depth_keys = list ( nodes_by_depth.keys ( ) ) input_shapes = _to_list ( input_shapes ) ' , but the layer was expecting ' weight_names.append ( name.encode ( 'utf8 ' ) ) if 'class_name ' not in conf : # to self._add_inbound_node ( ) . if len ( output_tensors ) == 1 : If ` by_name ` is False ( default ) weights are loaded steps : Integer or ` None ` . if type ( obj ) .__module__ == np.__name__ : return state_updates config : A Python dictionary , typically the if spec.shape is not None : custom_objects : Optional dictionary mapping names max_ndim : Integer , maximum rank of the input . if isinstance ( output_tensor , list ) : filtered_inbound_nodes = [ ] 'The tensor that caused the issue was : ' } , default=get_json_type ) .encode ( 'utf8 ' ) # Set self._network_nodes and self._nodes_by_depth . i ) return [ x ] dictionary mapping the input name to a Numpy array . 'However the new layer ' + layer.name self._updates += updates for weight shape computations . sample_weight=None ) : if 'class_name ' not in conf : merge_config = self.layers [ 0 ] .get_config ( ) if has_arg ( layer.call , 'mask ' ) : node = layer._inbound_nodes [ node_index ] # call compile method of Model class ' expects ' + str ( len ( symbolic_weights ) ) weights [ 3 ] , node_key = self._node_key ( layer , original_node_index ) target_tensors=target_tensors , if hasattr ( self , '_losses ' ) : output_masks= [ None ] , if layer.__class__.__name__ == 'GRU ' : layer = get_or_create_layer ( first_layer ) ValueError : for incompatible GRU layer/weights or incompatible biases self.layers = [ ] # Stack of layers . K.batch_set_value ( weight_value_tuples ) output_tensor = layer ( self.outputs [ 0 ] ) next epoch . When training with input tensors such as ( or list of input shape tuples , one tuple per output tensor ) . ' . They will not be included ' def input_spec ( self ) : raise ValueError ( 'Layer ' + self.name + ' was called with ' # Potentially redundant list , config = json.loads ( json_string ) return self.model.state_updates ' due to mismatch in number of weights ' while any ( map ( lambda x : x.nbytes > HDF5_OBJECT_HEADER_LIMIT , chunked_data ) ) : layer , node_index , tensor_index = tensor._keras_history if losses is None or losses == [ ] : Keras CuDNN warnings.warn ( 'No training configuration found in save file : ' This is used to implement the methods : `` `` '' Converts a layer and its index to a unique ( immutable type ) name . layer = deserialize_layer ( layer_data , def _updated_config ( self ) : if not compile : `` `` '' A ` Network ` is way to compose layers : the topological form of a ` Model ` . initial_epoch=0 ) : num_weights_per_layer = len ( weights ) // 2 input_tensors=input_tensors , return updates original_keras_version : Keras version for the weights , as a string . inbound_node = inbound_layer._inbound_nodes [ node_index ] The validation data is selected from the last samples 'input mask ' ) self.input_names = [ ] input_masks : list of input masks ( a mask can be a tensor , or None ) . ' Found : ' + str ( self.inputs ) ) 'to a ` Sequential ` model . ' ) ` epochs ` is to be understood as `` final epoch '' . raise RuntimeError ( 'The following attributes can not be saved to HDF5 ' if len ( output_masks ) ! = len ( output_tensors ) : RuntimeError : if the model was never compiled . merge_config [ 'layers ' ] = layers # Potentially redundant list , recurrent_kernel = np.concatenate ( [ weights [ 1 ] , name : A name of the attributes to load . `` `` '' Loads a model saved via ` save_model ` . from .engine.sequential import Sequential It defaults to ` print ` ( prints to stdout ) . batch_input_shape=None , get_weights ( ) # Actually call the layer , collecting output ( s ) , mask ( s ) , and shape ( s ) . sample_weight=None ) : verbose=verbose , ' , but the layer has only ' self.name + ' : expected ndim= ' weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 3 , 2 , 0 , 1 ) ) overwrite : Whether to silently overwrite any existing file at the raise ValueError ( 'Layer ' + self.name + ' expects ' ' ( named `` ' + layer.name name , uses_correlation = { 'tensorflow ' : True , in a single file . ( 'min_ndim= ' + str ( self.min_ndim ) ) if self.min_ndim else `` , import warnings # This is always a single layer , never a list . return cls.legacy_from_config ( config ) y_expected = to_list ( merge_func ( f_forward ( X ) [ 0 ] , f_backward ( X ) [ 0 ] ) ) The saved model contains : # Build a dict { depth : list of nodes with this depth } input_masks= [ None for _ in self.inputs ] , if not self.trainable and not self.stateful : This can be useful to tell the model to batch_size=None , from .. utils.io_utils import ask_to_proceed_with_overwrite input_spec = to_list ( self.input_spec ) n_gates = 3 `` `` '' Returns the config of the layer . to yield from ` generator ` before declaring one epoch weights [ 0 ] = conv_utils.convert_kernel ( weights [ 0 ] ) Number of samples per gradient update . training_config = f.attrs.get ( 'training_config ' ) except TypeError : the display labels for the scalar outputs . a node is added to ` layer._inbound_nodes ` . name = 'param_ ' + str ( i ) return self._per_input_losses [ inputs_hash ] get_output_at ( node_index ) return ' , '.join ( [ str ( abs ( id ( x ) ) ) for x in object_list ] ) # determine if we 're loading a CuDNNLSTM layer dtype = K.dtype ( input_tensor ) # Make sure child model callbacks self.built = True return data 'an input that isn\'t a symbolic tensor . ' nodes_in_progress = set ( ) def legacy_from_config ( cls , config , layer_cache=None ) : user_kwargs = copy.copy ( kwargs ) opened_new_file = True kernel = np.concatenate ( [ weights [ 0 ] , self.input_tensors = input_tensors # These properties should be set by the user via keyword arguments . input_shapes , output_shapes , output_shapes= [ x._keras_shape for x in self.outputs ] ) return { 'type ' : type ( obj ) , an under-represented class . for x in self.outputs : if spec.max_ndim is not None : name=self.name ) from which to retrieve the attribute . A list . It should be a single tensor `` `` '' Returns the list of input tensors necessary to compute ` tensor ` . layer_names = filtered_layer_names ( 'min_ndim= ' + str ( self.min_ndim ) ) if self.min_ndim else `` , input_masks=previous_mask , f.flush ( ) if layer.stateful : def train_on_batch ( self , x , y , class_weight=None , 'were accessed without issue : ' self.min_ndim = min_ndim raise ValueError ( 'InputLayer was provided ' after loading . get_input_at return self.model.uses_learning_phase x = K.identity ( x ) self.input_layers_tensor_indices.append ( tensor_index ) def stateful ( self ) : 'an input that isn\'t a symbolic tensor . ' ` sample_weight_mode ` on each output by passing a def compute_output_shape ( self , input_shape ) : `` `` '' Saves the weights of a model . class InputSpec ( object ) : inbound_node_index = input_data [ 1 ] 'the notion of `` output shape '' is ' first time the layer was called . output_tensors = to_list ( output_tensors ) # Methods get_input_at ( node_index ) ` sample_weight_mode= '' temporal '' ` in ` compile ( ) ` . layer._inbound_nodes [ 0 ] .inbound_layers ) ) : `` `` '' Normalizes a list/tensor into a list . return self._get_node_attribute_at_index ( node_index , self.output_layers = [ ] `` `` '' Saves attributes ( data ) of the specified name into the HDF5 group . self._losses = [ ] 'You will have to compile your model again ' Weights values as a list of numpy arrays . input_shapes = [ ] with h5py.File ( filepath , mode= ' r ' ) as f : if ( len ( layer._inbound_nodes ) > 1 or skip_mismatch=False , reshape=False ) : if not self.layers : [ 3 , 4 , 5 ] ] [ 1 , 3 , 5 ] ] ( 'axes= ' + str ( self.axes ) ) if self.axes else `` ] def get_layer ( self , name=None , index=None ) : output_shape = self.compute_output_shape ( input_shape ) # if obj is a python 'type ' from .. import layers as layer_module ' times in the model . ' `` `` '' Implements name-based weight loading . losses = [ ] for weight shape computations . # Update the depth of the corresponding layer import copy self.build ( ) 'The tensor ' + str ( tensor ) + ' at layer `` ' ' Add some layers first . ' ) max_queue_size=10 , workers=1 , # Layer instance ( NOT a list ) . sample weighting ( 2D weights ) , set this to ` `` temporal '' ` . data : Attributes data to store . for j in range ( len ( node.inbound_layers ) ) : custom_objects : Optional dictionary mapping names keras.engine.saving.preprocess_weights_for_loading ( /getting-started/faq/ elif hasattr ( K , 'int_shape ' ) : if K.ndim ( x ) ! = spec.ndim : index = { } # This is always a single layer , never a list . `` `` '' Transforms kernel for each gate separately using given function . 'and thus has no defined input shape . ' ) Must be implemented on all layers that have weights . saving.save_weights_to_hdf5_group ( f , self.layers ) raise ValueError ( 'Improperly formatted model config . ' ) intermediate = re.sub ( ' ( . ) ( [ A-Z ] [ a-z0-9 ] + ) ' , r'\1_\2 ' , name ) instead of creating a placeholder . validation_steps=validation_steps , self._network_nodes = network_nodes group.attrs [ ' % s % d ' % ( name , chunk_id ) ] = chunk_data prefix = self.__class__.__name__.lower ( ) # scalar Numpy data for these targets at training time ) , you 'instead . ' ) This method is the reverse of ` get_config ` , # Update self.losses self._feed_input_names = self.model._feed_input_names node_index : Integer , index of the node ValueError : in case of mismatch between provided layers and guarantees the single use of every input per epoch when name = kwargs.get ( 'name ' ) self.weighted_metrics = self.model.weighted_metrics HDF5_OBJECT_HEADER_LIMIT = 64512 model : Keras model instance to be saved . self._non_trainable_weights.append ( weight ) return kernel.T.reshape ( kernel.shape , order=order ) mask : A mask or list of masks . A mask can be of values are present but the shape does not match . `` `` '' Retrieves the weights of the model . The savefile includes : losses += layer.get_losses_for ( None ) # Check that layer is an InputLayer . # Properties if len ( output_masks ) ! = len ( output_tensors ) : # Raise exceptions in case the input is not compatible for line in f : config = [ ] def add_loss ( self , losses , inputs=None ) : def losses ( self ) : _save_attributes_to_hdf5_group ( The loss value that will be minimized by the model from .engine.saving import model_from_json if len ( input_shapes ) == 1 : self.shape = shape A list of ` InputSpec ` instances ( one per input to the model ) output_masks = [ ] def get_layer ( self , name=None , index=None ) : This is useful for separating training updates and The output of the generator must be either warnings.warn ( 'Error in loading the saved optimizer ' ) A None entry in a shape is compatible with any dimension , name = str ( w.name ) class_weight=class_weight , tensor_index = self.output_layers_tensor_indices [ i ] # Keep track of updates that depend on the inputs The first layer passed to a Sequential model raise TypeError ( 'Not JSON Serializable : ' , obj ) ' ' + str ( len ( output_tensors ) ) + ' output tensors and ' config [ 'input_layers ' ] = model_inputs def __init__ ( self , dtype=None , def get_input_shape_at ( self , node_index ) : if not input_shape : self.name shuffle=True , param_dset [ ( ) ] = val 'inbound_layers ' : inbound_names , ' has an input_spec attribute that ' if mask is None : shuffle : Boolean ( whether to shuffle the order of the batches at def add_unprocessed_node ( layer , node_data ) : if first_layer [ 'class_name ' ] == 'Merge ' : 0 = silent , 1 = progress bar , 2 = one line per epoch . print_fn : Print function to use . 'at deserialization time ) . ' ) raise ValueError ( 'Was asked to retrieve layer at index ' process_node ( layer , node_data ) It can be passed to ` transform_kernels ( ) ` . node_data.append ( [ inbound_layer.name , self._initial_weights = kwargs [ 'weights ' ] reference_input_tensors = node.input_tensors model.compile ( optimizer='rmsprop ' , # Following 2 properties : input and output masks . cls_name = self.__class__.__name__ input_shapes , output_shapes , arguments=None ) : self.batch_input_shape = batch_input_shape depth = max ( depth , previous_depth ) 'from layer type ` { } ` . '.format ( inputs , if original_keras_version == ' 1 ' : weights [ 8 ] , 'an instance of class Layer . ' and will raise an exception . In such cases , use via Keras-side shape inference . `` `` '' Util hared between different serialization methods . A shape tuple 'the notion of `` input shape '' is ' if inputs_hash not in self._per_input_losses : json.dumps ( node.arguments ) print_fn=print_fn ) weighted_metrics=weighted_metrics , List of callbacks to apply during training . if not isinstance ( output_mask , ( list , tuple ) ) and len ( output_ls ) > 1 : loss='categorical_crossentropy ' , A list of update ops . 'hence the notion of `` layer input mask '' ' losses = _to_list ( losses ) steps_per_epoch , reshape=reshape ) raise TypeError ( 'Layer ' + self.name class_name = conf [ 'name ' ] if len ( outputs ) == 1 : str ( K.dtype ( x ) ) ) return mask validation_steps=validation_steps ) tuples.append ( ( sw , w ) ) self.optimizer = self.model.optimizer eta_format = ' % d : % 02d : % 02d ' % ( eta // 3600 , ( eta % 3600 ) // 60 , eta % 60 ) self._output_tensor_cache = self.model._output_tensor_cache # Reverse index of layer name to list of layers with name . multi-output model , you could also pass a dictionary , # linking their input to output . is a compiled model ready to be used ( unless the saved model try : weight_value_tuples.append ( ( p , w ) ) ' elements . ' ) dtype = first_layer.dtype self._per_input_losses [ inputs_hash ] = [ ] sparse : Boolean , whether the placeholder created return model The model is not trained for a number of iterations a warning will be displayed . When ` compile ` is set use_multiprocessing=use_multiprocessing ) A layer instance . self.layers [ -1 ] ._outbound_nodes = [ ] shape : The shape tuple of the weight . unprocessed_nodes = { } if not hasattr ( x , '_keras_history ' ) : previous_sources = get_source_inputs ( x , if hasattr ( obj , 'get_config ' ) : depth_keys.sort ( reverse=True ) layer_cache : cache to draw pre-existing layer . Layers that have no matching name are skipped . # Update network_nodes . # then call node.inbound_layer on them . `` `` '' Converts layers weights from Keras 1 format to Keras 2 . return self.model.fit_generator ( generator , tuples.append ( ( sw , w ) ) if hasattr ( layer , 'activity_regularizer ' ) and layer.activity_regularizer is not None : from .topology import Container 'optimizer_config ' : { layer ( x ) self._feed_inputs = self.model._feed_inputs raise ValueError ( 'You tried to call layer `` ' max_queue_size=10 , workers=1 , or list of shape tuples ( one per output tensor of the layer ) . If x is a Keras tensor : build_map_of_graph ( x , finished_nodes , nodes_in_progress ) # that can be computed from the inputs provided . self._output_mask_cache [ mask_cache_key ] = mask num_weights = len ( sublayer.trainable_weights ) # computable_tensors : all tensors in the graph a list of tensors if there are more than one outputs . # are only applicable to input layers : do not pass these keywords An integer count . if h5py is None : # Update self.updates if inbound_layer_name not in created_layers : `` `` '' Collects the output shape ( s ) of a list of Keras tensors . max_ndim=None , node_index = self.output_layers_node_indices [ i ] shapes = to_list ( layer.compute_output_shape ( computed_tensors [ 0 ] ._keras_shape ) ) def __repr__ ( self ) : metrics = convert_custom_objects ( training_config [ 'metrics ' ] ) to yield from ` validation_data ` generator before stopping model_config = json.loads ( model_config.decode ( 'utf-8 ' ) ) 'Conv2DTranspose ' , ` _keras_shape ` : Integer shape tuple propagated if not self.built : 'and output masks . Layer ' + str ( layer.name ) + ' has ' initial_epoch : Epoch at which to start training return layer A JSON string . self._per_input_updates [ inputs_hash ] += updates f.attrs [ 'backend ' ] = K.backend ( ) .encode ( 'utf8 ' ) `` ` python ` model = Model ( input= [ a , b ] , output=c ) ` for name , val in zip ( weight_names , weight_values ) : return None E.g . ` node_index=0 ` will correspond to the ( ordered names of model layers ) . computed_masks ) # Container.layers needs to have a deterministic order : ` ( samples , sequence_length ) ` , So the channel axis needs to be flipped when we 're loading TF weights onto a TH model , self.sparse = sparse ' element ( s ) . ' ) `` `` '' Configures the model for training . determined via tensor._keras_history if not provided . weights [ 4 ] , y=None , nodes = self._nodes_by_depth [ depth ] self.model.trainable = value original_backend = None # call layer from .topology import InputLayer config = { 'name ' : self.name , # Set optimizer weights . output_masks = [ ] # keys are based on ids on input tensors and inputs masks . else : ' has multiple inbound nodes , ' batch_size : Integer . If unspecified , it will default to 32 . str ( x_shape ) ) return [ tensor ] filtered_layer_names = [ ] tensor_index : Tensor_index from which ` tensor ` comes from . if hasattr ( w , 'name ' ) and w.name : if isinstance ( inputs , ( list , tuple ) ) : # here the batch dimension is None , An input shape tuple . to retrieve the attribute . Scalar training loss ( if the model has no metrics ) if layer.is_placeholder : weights [ 1 ] = conv_utils.convert_kernel ( weights [ 1 ] ) mask_cache_key = ' , '.join ( [ str ( id ( x ) ) for x in self.inputs ] ) elif source == 'GRU ( reset_after=True ) ' : input_shapes=input_shapes , self.name = name # We were passed a model as first layer . with h5py.File ( filepath , ' w ' ) as f : self.outbound_layer = outbound_layer str ( spec.ndim ) + ' , found ndim= ' # Reached an Input layer , stop recursion . # the input tensors come from : which layers , keras.engine.topology.preprocess_weights_for_loading ( # is repeated until all nodes are processed . shuffle=shuffle , if layer.__class__.__name__ in conv_layers : If ` by_name ` is True , weights are loaded into layers workers=workers , trainable_weights += layer.trainable_weights and what the model expects . layers.append ( layer ) def convert_custom_objects ( obj ) : biases = np.tile ( 0.5 * weights [ 2 ] , 2 ) # Bad luck , we have to run the graph manually . source_tensors.append ( x ) if layer.__class__.__name__ == 'Conv1D ' : self._outbound_nodes = [ ] uid = object_list_uid ( model.inputs ) `` `` '' Sets the weights of the layer , from Numpy arrays . 'an input_tensor argument , ' # List of initial layers ( 1 to 1 mapping with self.inputs , # and building the layer if needed . # Build a dict { depth : list of layers with this depth } 'input_masks ' , nodes . return obj if not isinstance ( layer , InputLayer ) : layer = self.input_layers [ i ] return output_tensors , output_masks , output_shapes if len ( inputs ) ! = len ( input_spec ) : str ( spec.shape ) + ' , found shape= ' # linking their input to output . layer = layer_module.deserialize ( conf , custom_objects=custom_objects ) layer , 'after instantiation . ' # Returns def _is_all_none ( iterable_or_element ) : def fit ( self , on images on CPU in parallel to training your model on GPU . a record of training loss values and metrics values for layer_data in config [ 'layers ' ] : previous_mask = _collect_previous_mask ( inputs ) include_optimizer : If True , save optimizer 's state together . positions : Relative or absolute positions of log elements `` `` '' A ` Node ` describes the connectivity between two layers . output_shape_keys.append ( shape_key ) validation_data=validation_data , if input_tensor is None : instead of an integer . layer , 'Layer ' + layer.name return self.model.fit_generator ( generator , 'input_shapes ' , 'Prefer using a Keras optimizer instead ' 'it was generated by layer ' A tensor or list/tuple of tensors . def load_weights ( self , filepath , by_name=False , return conf # This will never loop forever thanks to the test above . if pv.shape ! = w.shape : return self.layers [ index ] non_trainable_weights : List of variables . the model 's weights # note that 'dtype ' , 'input_shape ' and 'batch_input_shape ' `` `` '' Generate class predictions for the input samples . # Handle mask propagation . masks = [ ] # Gather layer inputs . if all ( [ hasattr ( x , '_keras_shape ' ) for x in computed_tensors ] ) : else : # If file exists and should not be overwritten . if inputs_hash not in self._per_input_updates : if layers : previous_mask = _collect_previous_mask ( inputs ) raise TypeError ( 'Keyword argument not understood : ' , kwarg ) # It acts as a queue that maintains any unprocessed for node in nodes : filtered_inbound_nodes.append ( node_data ) # which means any batch size will be accepted by the model . def non_trainable_weights ( self , weights ) : ndim : Integer , expected rank of the input . shapes.append ( None ) 'starting with a freshly initialized ' output_masks : list of output masks # We can determine the source of the weights from the shape of the bias . `` `` '' Returns a JSON string containing the network configuration . # Raises def get_output_at ( self , node_index ) : if len ( layer._inbound_nodes ) > 1 or ( layer._inbound_nodes and layer._inbound_nodes [ 0 ] .inbound_layers ) : get_layer exactly where you left off . input_layer = InputLayer ( batch_input_shape=batch_shape , model = load_model ( 'my_model.h5 ' ) dtype : The data type expected by the input , as a string self.name = name elif source == 'GRU ( reset_after=True ) ' : return trainable_weights + weights # Return tensor including _keras_shape and _keras_history . ' due to mismatch in shape ' tensor_index = self.input_layers_tensor_indices [ i ] prefix = self.__class__.__name__.lower ( ) if isinstance ( self.layers [ 0 ] , legacy_layers.Merge ) : kwargs = { } from .engine.input_layer import Input self.output_names = self.model.output_names inputs : input tensor or list of input tensors . def object_list_uid ( object_list ) : 'after loading it . ' self.name + ' , but the layer isn\'t built . ' tensor : Some tensor in a graph . Has no effect when ` steps_per_epoch ` is not ` None ` . original_backend ) recurrent_kernel = np.transpose ( recurrent_kernel , if layer.__class__.__name__ == 'Bidirectional ' : Node ( x_shape = K.int_shape ( x ) self.metrics_names = self.model.metrics_names layers_to_output_shapes [ shape_key ] = output_shapes [ j ] `` pay more attention '' to samples from an under-represented class . self.outputs = list ( outputs ) deserialized [ key ] = convert_custom_objects ( value ) name , uses_learning_phase : Whether any operation output_ls = _to_list ( output ) layer = node.outbound_layer while unprocessed_nodes : if x_shape is not None : def get_source_inputs ( tensor , layer=None , node_index=None ) : for layer in self.layers [ 0 ] .layers : outbound_layer : the layer that takes layer=None , node_index=None , tensor_index=None ) : [ How can I install HDF5 or h5py to save my models in Keras ? ] ( self._output_mask_cache [ cache_key ] = output_masks return conf or list of scalars ( if the model computes other metrics ) . # scalar 'as part of the model save file . ' weights.append ( layer.get_weights ( ) ) inbound_nodes_data = layer_data [ 'inbound_nodes ' ] layers.append ( layer_config ) weights [ num_weights_per_layer : ] , computed_data = [ ] # List of tuples ( input , mask ) . optimizer_weights_group = f [ 'optimizer_weights ' ] def to_json ( self , * * kwargs ) : weights += layer.trainable_weights weights : List of source weights values ( input kernels , recurrent return True index.setdefault ( layer.name , [ ] ) .append ( layer ) # Methods # Internal methods : regularizer : An optional Regularizer instance . } , for i , ( w , val ) in enumerate ( zip ( symbolic_weights , weight_values ) ) : considered during deserialization . to yield from ` generator ` before stopping . self.model.set_weights ( weights ) for conf in config : # Containers start with a pre-existing node continue sample_weight_mode=sample_weight_mode ) input_masks= [ None for _ in self.inputs ] , # Apply activity regularizer if any : axes : Dictionary mapping integer axes to weights2 = saving.preprocess_weights_for_loading ( layer , weights1 ) # Fill in the output mask cache . # Dictionary mapping layer instances to self.output_names = self.model.output_names # Create an input node to add to self.outbound_node layers = legacy_models.legacy_sequential_layers ( self ) input_shapes = [ x._keras_shape for x in inputs ] input_tensors = topology._to_list ( input_tensors ) layer ( x ) def get_weights ( self ) : return outputs [ 0 ] RuntimeError : If the model was never compiled . def count_params ( self ) : know its input shape . shape_key = layer.name + ' _ % s_ % s ' % ( node_index , j ) 'ConvLSTM2D ' ] See [ losses ] ( /losses ) . for its ` build ` call . output_shape = layer.compute_output_shape ( input_shapes [ 0 ] ) return K.batch_get_value ( params ) inputs=to_list ( inputs ) ) the same length as ` inbound_layers ` . ValueError : In case the generator yields data in an invalid format . param_dset [ : ] = val return yaml.dump ( self._updated_config ( ) , * * kwargs ) # this does nothing . input_tensors= [ input_tensor ] , input_tensors.append ( inbound_node.output_tensors [ inbound_tensor_index ] ) use_multiprocessing=False ) : Epoch at which to start training depth = nodes_depths.setdefault ( node , 0 ) class Container ( Layer ) : ' elements . ' ) previous_sources = get_source_inputs ( x , inbound_node = inbound_layer._inbound_nodes [ inbound_node_index ] self.built = True weight_names = load_attributes_from_hdf5_group ( g , 'weight_names ' ) super ( InputLayer , self ) .__init__ ( dtype=dtype , name=name ) else : if any ( m is not None for m in mask ) : # Following 2 properties : input and output masks . if len ( input_shapes ) == 1 : `` `` '' Sets the weights of the layer , from Numpy arrays . output_shape = None ( if any ) . If not , exceptions are raised . Thus the saved model can be reinstantiated in ( or list of shape tuples if the layer has multiple outputs ) . ValueError : In case of improperly formatted config dict . if len ( self._inbound_nodes ) ! = 1 : Prefer using ` layer.get_input_shape_for ( input_shape ) ` , return ( proba > 0.5 ) .astype ( 'int32 ' ) weights [ 6 ] ] , axis=-1 ) sparse : A boolean specifying whether the placeholder first_layer = layer.layers [ 0 ] 'make it possible to access ' with K.name_scope ( self.name ) : arguments=None ) : all ops in the graph to the new inputs param_dset = optimizer_weights_group.create_dataset ( 'containing ' + str ( len ( layer_names ) ) from .. import __version__ as keras_version # If mask is explicitly passed to __call__ , input_tensors = _to_list ( input_tensors ) `` `` '' Abstract base layer class . ` call ` method of the layer at the call that created the node . with the current layer . self._trainable = value def Input ( shape=None , batch_shape=None , considered during deserialization . config : A Python dictionary , typically the for node in nodes : weights2 = saving.preprocess_weights_for_loading ( 'metrics ' : model.metrics , # ( not all nodes included in the layers str ( len ( symbolic_weights ) ) 'to a ` Sequential ` model . ' ) x : target object to be normalized . # Note : number of the dimensions of the weights # Apply activity regularizer if any : symbolic_weights = getattr ( model.optimizer , 'weights ' ) A YAML string . but note that there may be cases in which this name : string , name of layer . generator : Generator yielding tuples ( inputs , targets ) raise AttributeError ( 'The layer has never been called ' # Container-specific properties . by_name : Boolean , whether to load weights by name weights : List of weights values ( Numpy arrays ) . if depth not in layers_by_depth : optimizer_weights_group.attrs [ 'weight_names ' ] ] output_shapes = None if isinstance ( inputs , list ) : Only applicable if the layer has one inbound node , if not hasattr ( x , '_keras_history ' ) : node_index : Integer index of the node from which 'trainable ' : self.trainable } steps=steps ) K.batch_set_value ( weight_value_tuples ) from .base_layer import Layer ( or list of input shape tuples , one tuple per output tensor ) . `` `` '' Input layer code ( ` Input ` and ` InputLayer ` ) . self.batch_input_shape = batch_input_shape masks = [ ] iterable = [ iterable_or_element ] `` `` '' Retrieves the model configuration as a Python list . except ImportError : 'is redundant . ' finished and starting the next epoch . It should typically # Check that x is a Keras tensor . shape , except TypeError : self.layers.append ( layer ) A._outbound_nodes dtype : Datatype of the input . # These lists will be filled via successive calls A tensor if there is a single output , or ` node_indices [ i ] ` is the origin node of ` input_tensors [ i ] ` layer.call ( computed_tensors , * * kwargs ) ) if weights : return self._non_trainable_weights str ( K.ndim ( x ) ) ) def predict_generator ( self , generator , steps=None , workers : Integer . Maximum number of processes to spin up proceed = ask_to_proceed_with_overwrite ( filepath ) inbound_layer , node_index , tensor_index = x._keras_history nodes_in_progress.remove ( node ) class InputLayer ( Layer ) : self._non_trainable_weights = [ ] self._output_shape_cache [ cache_key ] = output_shapes 'from layer type ` { } ` . '.format ( inputs , 'name ' : self.name } # Collect input tensor ( s ) coordinates . str ( spec.min_ndim ) + ' , found ndim= ' # afterwards , Keras does automatic shape inference from .. models import save_model ( 1:1 mapping between weights and samples ) , computed_mask ) if not isinstance ( iterable_or_element , ( list , tuple ) ) : # if obj is a serializable Keras class instance if not layer or node_index is None or tensor_index is None : return self.model.predict_generator ( generator , steps , It defaults to ` print ` ( prints to stdout ) . weight_tensor_td_conv_new = topology.preprocess_weights_for_loading ( ' , but the layer has only ' ` _keras_shape ` : Integer shape tuple propagated `` `` '' Retrieves the model 's updates . # The previous layer generated a mask . output_shape = layer.compute_output_shape ( input_shapes ) raise TypeError ( for i in range ( len ( self.input_layers ) ) : self.total_loss = self.model.total_loss K.batch_set_value ( tuples ) get_layer 'config ' : model.get_config ( ) a list of instances of InputSpec ( one per input tensor ) . ` keras.models.model_from_yaml ( yaml_string , custom_objects= { } ) ` . if mask is not None : layer_configs = [ ] 'but it received ' + str ( len ( inputs ) ) # ( since serialized nodes refer to layers by their name ) . the exact same state , without any of the code reshape : Reshape weights to fit the layer when the correct number The generator should return the same kind of data capable of instantiating the same layer from the config except ValueError : specs += layer.input_spec return self.model.predict_on_batch ( x ) ' layers into a model with ' if trainable : kept_nodes = 1 callbacks=None , # we should use that depth instead of the node depth . `` `` '' The ` Model ` class adds training & evaluation routines to a ` Container ` . `` `` '' Saves the model to a single HDF5 file . `` `` '' Prints a string summary of the network . raise RuntimeError ( 'You tried to call ` count_params ` on ' ` layer_names ` ( attribute ) , a list of strings self.layers = layers self.name + '.build ( batch_input_shape ) ` . ' ) non picklable arguments to the generator ( or list of tensors if the layer has multiple outputs ) . This function is used internally with ` self._network_nodes ` . ImportError : If h5py is not available . def predict ( self , x , batch_size=None , verbose=0 , steps=None ) : node_indices.append ( None ) for kwarg in kwargs : def assert_input_compatibility ( self , inputs ) : trainable_weights ( list of variables ) if inputs_hash in self._per_input_losses : raise ValueError ( the _keras_shape of the input ( s ) . target tensor ( in turn , Keras will not expect external # If mask is explicitly passed to __call__ , batch_shape : A shape tuple ( integer ) , including the batch size . Input mask tensor ( potentially None ) or list of input from_cudnn : ` True ` if source weights are in CuDNN format , ` False ` raise ValueError ( 'InputLayer was provided ' def _need_convert_kernel ( original_backend ) : A tensor ( or list of tensors if the layer has multiple outputs ) . `` `` '' Fits the model on data generated batch-by-batch by a Python generator . # new : i , f , c , o output_shapes = [ ] str ( spec.ndim ) + ' , found ndim= ' 'hence the notion of `` layer output '' ' if not len ( self._inbound_nodes ) > node_index : if not self.trainable : return preds dtype=dtype , these arguments are passed into ` tf.Session.run ` . non_trainable_weights ( in this order ) . if 'batch_input_shape ' in kwargs : name : A name of the attributes to save . shuffle=shuffle , `` `` '' Model-related utilities . opened_new_file = False self._node_key ( inbound_layer , node_index ) , 0 ) return self._built if callable ( obj ) : 'optimizer attributes or optimizer state ' if shape is not None : inbound_layer = node.inbound_layers [ i ] An integer count . self.input_layers_tensor_indices = [ ] dtype = K.floatx ( ) if not hasattr ( tensor , '_keras_history ' ) : self._initial_weights = None overwrite : Whether to silently overwrite any existing file at the if weights [ 0 ] .size ! = np.prod ( layer_weights_shape ) : def _collect_input_shape ( input_tensors ) : The layer 's attribute ` attr ` at the node of index ` node_index ` . or vice verca . However , there 's no conversion required between TF and CNTK . The convolution operation is implemented differently in different backends . if trainable : topology.save_weights_to_hdf5_group ( f , layers ) # Raise exceptions in case the input is not compatible source = 'GRU ( reset_after=False ) ' updates : update op or list of update ops weight_values , custom_objects : Optional dictionary mapping names original_keras_version , % ( HDF5_OBJECT_HEADER_LIMIT , if not trainable : # No network-level masking for now . weight = K.variable ( initializer ( shape ) , raise RuntimeError ( layer = node.inbound_layers [ i ] if not params : compatible with ` CuDNNGRU ` . inputs = node.input_tensors # ( e.g . BN updates ) . self.metrics_tensors = self.model.metrics_tensors This recursively updates the map ` layer_indices ` , @ trainable.setter def get_json_type ( obj ) : `` `` '' Dumps all layer weights to a HDF5 file . load_weights_from_hdf5_group ( sample_weight_mode=sample_weight_mode , return proba.argmax ( axis=-1 ) batch_size=batch_size , * * kwargs : When using the Theano/CNTK backends , these arguments return obj.__name__ # transpose ( and reshape ) input and recurrent kernels self._feed_input_names = [ ] For every layer , a ` group ` named ` layer.name ` model.add ( layer ) to add to the layer . output of ` get_weights ` ) . forward_weights = preprocess_weights_for_loading ( layer.forward_layer , to validate before stopping . get_output_shape_at if len ( input_shapes ) == 1 : ( handled by Container ) , nor weights ( handled by ` set_weights ` ) . Total number of steps ( batches of samples ) # the model we will return weighted_metrics=weighted_metrics , with the custom object . non_tensors = [ x for x in losses if isinstance ( x , ( float , int ) ) ] ValueError : In case of improperly formatted config dict . # This requires a specific way to figure out the [ [ 0 , 1 , 2 ] , < -- - > [ [ 0 , 2 , 4 ] , verbose : Integer . 0 , 1 , or 2 . Verbosity mode . The model architecture , allowing to re-instantiate the model . return [ ] biases = np.tile ( 0.5 * weights [ 2 ] , 2 ) self._per_input_updates = { } `` `` '' Retrieves the output mask ( s ) of the previous node . the loss and any model metrics at the end of each epoch . def add_update ( self , updates , inputs=None ) : process_layer ( layer_data ) # Early return if compilation is not required . not trained for n steps given by epochs , but until the # the current node will be added to str ( len ( weight_values ) ) layer_cache : cache to draw pre-existing layer . # This also updates the layer history of the output tensor ( s ) . for node in nodes : or its index in the graph . Indices are based on from .. engine.base_layer import Layer , InputSpec weight_value_tuples = [ ] for sublayer in layer.layers : input_masks= [ None ] , Only applicable if the layer has exactly one inbound node , # Handle ` name ` argument . self.stateful = False ( strings ) to custom classes or functions to be layer = created_layers [ layer_data [ 'name ' ] ] input_masks , output_masks , Output will always be a list of tensors tuples = [ ] 'but its input shape can not be ' The loss value that will be minimized by the model return cls ( * * config ) func : Function applied to kernel of each gate . ValueError : In case of an invalid savefile . layers_for_depth.sort ( key=lambda x : layer_indices [ x ] ) # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) else : # tensor inputs and outputs of outbound_layer . if hasattr ( tensor , '_keras_shape ' ) and output_shapes is not None : if type ( obj ) .__module__ == np.__name__ : self.inputs = topology.get_source_inputs ( self.outputs [ 0 ] ) We update the _keras_history of the output tensor ( s ) validation_data : tuple ` ( x_val , y_val ) ` or tuple # Make sure child model callbacks reshape=reshape ) self.loss_weights = self.model.loss_weights If necessary , we ` build ` the layer to match relies on multiprocessing , you should not pass name = prefix + str ( K.get_uid ( prefix ) ) return values [ 0 ] layer_weights = weights [ : num_param ] deserialized = { } nodes_in_progress : Set of nodes that are currently active on the bad_attributes = [ x for x in data if len ( x ) > HDF5_OBJECT_HEADER_LIMIT ] self._per_input_losses [ inputs_hash ] += losses input_shapes=input_shape , bias = np.concatenate ( [ weights [ 2 ] , y : Numpy array of target ( label ) data . # The following are implemented as property functions : if len ( self.layers ) < = index : # merge input and recurrent biases into a single set config = json.loads ( json_string ) y : Numpy array of target ( label ) data . specs = [ ] batch_size = None if isinstance ( model.optimizer , optimizers.TFOptimizer ) : self.name + ' : expected ndim= ' self.sample_weights = self.model.sample_weights Optional for ` Sequence ` : if unspecified , will use trainable ( boolean ) ` predict_on_batch ` . data = [ n.decode ( 'utf8 ' ) for n in group.attrs [ name ] ] raise ValueError ( 'Weights must be of equal size to ' input_masks = to_list ( input_masks ) output_mask = self.compute_mask ( inputs , previous_mask ) for node in reversed ( nodes_in_decreasing_depth ) : # tensor output of each node . Input shape tuple # It 's an input layer : compute_output_shape is identity , json_string : JSON string encoding a model configuration . raise ValueError ( 'You are trying to load a weight file ' Fraction of the training data to be used as validation data . # Keep track of unconditional updates ( e.g . a counter ) . # Split single set of biases evenly to two sets . The way of 'Input { } ( 0-based ) originates ' ' ( { } vs { } ) . '.format ( len ( symbolic_weights ) , len ( weight_values ) ) ) raise ValueError ( 'Layer # ' + str ( k ) if dtype is None : inputs = inputs [ : ] If the model has multiple outputs , you can use a different str ( len ( self.layers ) ) + ' layers . ' ) 'backend ' : K.backend ( ) layer = filtered_layers [ k ] self.activity_regularizer ( x ) return { 'type ' : type ( obj ) , This method deals with an inherent problem of HDF5 file which is not validation_split=0. , deserialized = [ ] cache_key += ' _ ' + ' , '.join ( [ str ( id ( x ) ) for x in masks ] ) param_dset [ : ] = val warnings.warn ( 'Skipping loading of weights for layer { } '.format ( layer.name ) import os output_masks = [ None for _ in output_tensors ] def predict ( self , x , batch_size=None , verbose=0 , steps=None ) : reference_output_tensors = node.output_tensors RuntimeError : If the layer has no inbound nodes . # rather than input-dependent containing the configuration of a layer . `` `` '' Checks compatibility between the layer and provided inputs . output_shapes = [ ] `` `` '' Returns a yaml string containing the network configuration . layer : The layer . # Collect unconditional updates . output_shape = [ None for _ in input_shape ] # in case of layer shared at different topological depths output_shapes=output_shapes , self._node_key ( inbound_layer , node_index ) , 0 ) ` keras.models.model_from_json ( json_string , custom_objects= { } ) ` . obj : object , dict , or list . to yield from ` generator ` before declaring one epoch if not self.built : if K.dtype ( x ) ! = spec.dtype : f , self.layers , reshape=reshape ) `` `` '' Evaluates the model over a single batch of samples . self.name + ' : expected max_ndim= ' 'Maybe you meant to use ' raise ValueError ( 'The first layer in a ' kept_nodes = 0 for instance batch norm updates are conditional on the layer 's inputs . # Update the depth of the corresponding layer f = f [ 'model_weights ' ] callbacks : List of ` keras.callbacks.Callback ` instances . e.g . if the layer is being shared with a different data stream ) . return any ( [ ( hasattr ( layer , 'stateful ' ) and tensor : Optional existing tensor to wrap into the ` Input ` layer . nodes_depths = { } # dict { node : depth value } for original_node_index , node in enumerate ( layer._inbound_nodes ) : if insecure [ 0 ] ! = ' _ ' : def weights ( self ) : self.stateful = False If a tensor is passed , we return layer = get_or_create_layer ( conf ) for conf in config : raise ValueError ( ' Can not add an empty model ' # Set optimizer weights . ( without its trained weights ) from this configuration . self.input_layers = [ ] Node ( outbound_layer=self , a node is added to ` layer._outbound_nodes ` . config = { 'batch_input_shape ' : self.batch_input_shape , Note that in conjunction with initial_epoch , the parameter assert str ( id ( x ) ) in tensor_map , 'Could not compute output ' + str ( x ) storing the weight value , named after the weight tensor . Note : Please also see It can either wrap an existing tensor ( pass an ` input_tensor ` argument ) # Update model updates and losses : # no model-level masking for now raise ImportError ( ' ` load_weights ` requires h5py . ' ) allowed_kwargs = { 'input_shape ' , for i in range ( len ( self.output_layers ) ) : outputs = to_list ( wrapped ( inputs ) ) # If the depth is not set , the node has no outbound nodes ( depth 0 ) . def __init__ ( self , outbound_layer , self.outputs = [ ] # List of length 1 : the output tensor ( unique ) . def output_shape ( self ) : `` `` '' Converts weights for RNN layers between native and CuDNN format . from .. engine import get_source_inputs # Check shape . # to the input layer we just created . # The previous layer generated a mask . if isinstance ( config , list ) : def evaluate_generator ( self , generator , steps=None , if self.built : return cls.from_config ( config [ 'config ' ] , self.outputs = [ layer._inbound_nodes [ -1 ] .output_tensors [ 0 ] ] `` `` '' Specifies the ndim , dtype and shape of every input to a layer . self.metrics = self.model.metrics if K.dtype ( x ) ! = spec.dtype : inputs masks.append ( None ) return obj if insecure [ 0 ] ! = ' _ ' : Will only include losses that are either self.outputs = [ self.layers [ -1 ] .output ] return self.model.evaluate_generator ( generator , for name , val in zip ( weight_names , weight_values ) : Note that in conjunction with initial_epoch , the parameter generator : A generator or an instance of ` Sequence ` save_model ( self , filepath , overwrite , include_optimizer ) inbound_layers=inbound_layers , if layer is None or node_index : self._inbound_nodes = [ ] def __init__ ( self , layers=None , name=None ) : `` `` '' Retrieve a layer that is part of the model . return kernel.T.reshape ( kernel.shape , order=order ) nodes_by_depth = { } merge_inputs = [ ] metrics=metrics , batch_shape = ( None , ) + tuple ( shape ) by ` Network ` ( one layer of abstraction above ) . if len ( masks ) == 1 : `` `` '' Converts layers weights from Keras 1 format to Keras 2 . masks.append ( mask ) ' ( like softmax or sigmoid would ) . ' ) return { 'class_name ' : class_name , assert shape [ 0 ] == layer.filters and shape [ 2 : ] == ( layer.kernel_size [ 0 ] , 1 ) weight_names = [ ] # non-trainable weights param_dset [ ( ) ] = val str ( inputs ) ) weights = self._gather_list_attr ( 'non_trainable_weights ' ) self.input_layers_node_indices = [ ] self._output_mask_cache = { } # every time the Network is called on a set on input tensors , layer_class_name = layer.__class__.__name__ return False self.name + ' : expected shape= ' etc ... sparse=sparse , return { 'outbound_layer ' : self.outbound_layer.name if self.outbound_layer else None , ( eta // 3600 , ( eta % 3600 ) // 60 , eta % 60 ) ) if layer.__class__.__name__ in [ 'Model ' , 'Sequential ' ] : 'trainable ' , input_shapes.append ( K.int_shape ( x_elem ) ) cache_key = ' , '.join ( [ str ( x ) for x in input_shapes ] ) ( 'shape= ' + str ( self.shape ) ) if self.shape else `` , f , 'layer_names ' , [ layer.name.encode ( 'utf8 ' ) for layer in layers ] ) input_tensor = K.placeholder ( shape=batch_input_shape , weights [ 0 ] = np.reshape ( weights [ 0 ] , layer_weights_shape ) # that are part of the model . sparse=sparse , 'instantiated via ` tensor = Input ( shape ) ` .\n ' if model_config is None : if x in inputs_ls : self.input_spec = None source_tensors.append ( x ) weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 3 , 2 , 0 , 1 ) ) layer_indices = { } # dict { layer : index in traversal } 'optimizer . ' ) index.setdefault ( layer.name , [ ] ) .append ( layer ) iterable = iterable_or_element inbound_layer , node_index , tensor_index = x._keras_history Input mask tensor ( potentially None ) or list of input assert key in layers_to_output_shapes x = node.input_tensors [ i ] raise TypeError ( 'All layers in a Sequential model ' str ( len ( weights ) ) @ non_trainable_weights.setter compiled . Otherwise , the model is uncompiled and output_shapes = [ ] layer = node.outbound_layer K.batch_set_value ( weight_value_tuples ) # Dictionary mapping reference tensors to tuples def from_config ( cls , config , custom_objects=None ) : x = topology._to_list ( input_tensors ) [ 0 ] def get_output_shape_at ( self , node_index ) : dtype : The dtype of the weight . computed_masks ) if isinstance ( layer , ( Model , Sequential ) ) : cls_name = self.__class__.__name__ if not input_shape : This checks that the tensor ( s ) ` input ` ( during training only ) . trainable : Boolean , whether the layer weights for value in obj : output_ls_copy = [ ] # Note that in this case train_output and test_output are the same pointer . max_ndim=None , of the layer ( i.e . it should match the return self.model.get_weights ( ) name : String , must be unique within a model . and what the model expects . self._output_tensor_cache = self.model._output_tensor_cache weighted_metrics=None , `` `` '' Retrieves the output shape tuple ( s ) of a layer . return True callbacks=None , raise TypeError ( 'Not JSON Serializable : ' , obj ) state_updates = [ ] # we copy them to avoid loss of tensor metadata . f.attrs [ 'keras_version ' ] = str ( keras_version ) .encode ( 'utf8 ' ) `` `` '' Single gradient update over one batch of samples . import yaml skip_mismatch : Boolean , whether to skip loading of layers if len ( outputs ) == 1 : input_masks , output_masks , if not isinstance ( layer , ( InputLayer , legacy_layers.Merge ) ) : output_shapes = to_list ( output_shape ) ' has an input_spec attribute that ' steps_per_epoch=None , warnings.warn ( 'The ` nb_epoch ` argument in ` fit ` ' A numpy array of class predictions . into account in the topological ordering , so adding or indefinitely . An epoch finishes when ` steps_per_epoch ` or list of shape tuples ( one per output tensor of the layer ) . # Check that x is an input tensor . return self._get_node_attribute_at_index ( 0 , 'input_masks ' , self._per_input_losses = { } 'input_tensors ' , metrics=metrics , layers : a list of target layers . optimizer : String ( name of optimizer ) or optimizer object . 'but was passed an input_mask : ' layers_for_depth = layers_by_depth [ depth ] self._inbound_nodes = [ ] # Will be appended to below , and by future calls to __call__ warnings.warn ( cls_name + ' inputs must come from ' def get_or_create_layer ( layer_data ) : num_weights = len ( [ l for l in sublayer.weights if l not in sublayer.trainable_weights ] ) insecure = re.sub ( ' ( [ a-z ] ) ( [ A-Z ] ) ' , r'\1_\2 ' , intermediate ) .lower ( ) merge_config = self.layers [ 0 ] .get_config ( ) `` `` '' Evaluates the model on a data generator . f = h5py.File ( filepath , mode= ' w ' ) # Network-specific properties . the output of ` model.get_weights ( ) ` . inbound_layers= [ ] , If ` name ` and ` index ` are both provided , ` index ` will take precedence . reshape=reshape ) x_shape = None steps_per_epoch=steps_per_epoch , Typically you will use ` metrics= [ 'accuracy ' ] ` . `` `` '' Retrieves the input mask tensor ( s ) of a layer at a given node . if h5py is None : if isinstance ( layer , ( Model , Sequential ) ) : def build ( self , input_shape=None ) : 'config ' : config , # Entries are unique . Includes input and output layers . if 'mask ' not in kwargs : # depth levels in the graph . ( ` float32 ` , ` float64 ` , ` int32 ` ... ) 'Input ' + str ( input_index ) if reshape and layer_weights_shape ! = weights [ 0 ] .shape : such as ` metrics= { 'output_a ' : 'accuracy ' } ` . 'config ' : layer.get_config ( ) } ) self.axes = axes or { } return np.hstack ( [ func ( k ) for k in np.hsplit ( kernels , n_gates ) ] ) input_shapes = self._inbound_nodes [ 0 ] .input_shapes For every such layer group , a group attribute ` weight_names ` , if spec.ndim is not None : name = config.get ( 'name ' ) self.output_layers_tensor_indices.append ( tensor_index ) # with the input_spec set at build time . # ( e.g . activity regularizers ) . layer.name + '.\n ' # all layers in order of horizontal graph traversal . self.output_names = [ ] self.input_layers = self.model.input_layers # If input_tensor is set , and batch_input_shape is not set : [ How can I install HDF5 or h5py to save my models in Keras ? ] ( framework-native tensors ( e.g . TensorFlow data tensors ) . self.model = Model ( self.inputs , self.outputs [ 0 ] , 'Use ` get_input_shape_at ( node_index ) ` ' config [ 'dtype ' ] = self.dtype sample_weight=None ) : steps=steps ) def convert_weights ( weights , from_cudnn=True ) : def train_on_batch ( self , x , y , class_weight=None , `` `` '' Makes a function that transforms input kernels from/to CuDNN format . if input_shape and batch_input_shape : weights [ : num_weights_per_layer ] , mask = node.output_masks [ tensor_index ] config : Configuration dictionary . divided by the batch size . non_tensors = [ x for x in losses if isinstance ( x , ( float , int ) ) ] name = str ( w.name ) trainable_weights : List of variables . from .. import backend as K if opened_new_file : verbose : 0 , 1 , or 2 . Verbosity mode . shapes = [ ] ' times in the model . ' output of the inbound layer if spec.dtype is not None : inbound_layer = node.inbound_layers [ j ] 'config ' : model.optimizer.get_config ( ) # old : ( kernel_rows , kernel_cols , stack_size , filters ) if 'layer_names ' not in f.attrs and 'model_weights ' in f : if x not in source_tensors : if source == 'CuDNNGRU ' : def fit ( self , Indices are based on order of horizontal graph traversal ( bottom-up ) . `` `` '' Adds updates to the layer . `` ` python self.input_layers_tensor_indices = [ ] def model_from_config ( config , custom_objects=None ) : for i , key in enumerate ( output_shape_keys ) : string , path to the saved model , or raise TypeError ( 'Layer ' + layer.name inbound_names.append ( layer.name ) if layer in self.input_layers : return layers # Set self._container_nodes and self._nodes_by_depth . self , # afterwards , Keras does automatic shape inference Add layer to tensor history # serialize and save the layers in layer_configs # This requires a specific way to figure out the summary warnings.warn ( 'No training configuration found in save file : ' if layer in unprocessed_nodes : raise TypeError ( 'Not JSON Serializable : ' , obj ) weight_names = [ ] i.e . if it is connected to one incoming layer . kwargs ] ) it becomes possible to do : if sublayer not in layers : use_multiprocessing : Boolean . original_keras_version , # also possible ( equivalent to the above ) : masks = [ None for _ in range ( len ( inputs ) ) ] if self.layers : to retrieve the attribute . ' ` Sequential.from_config ( config ) ` ? ' ) tensor_index = self.input_layers_tensor_indices [ i ] self.add ( layer ) # Actually call the layer , get_config for layer in layers : def pop ( self ) : # output masks and output shapes in one pass , weights2 = topology.preprocess_weights_for_loading ( if obj in custom_objects : losses = [ ] layer_name = layer_data [ 'name ' ] weights = _convert_rnn_weights ( layer , weights ) inbound_layers , node_indices , tensor_indices , ( during training only ) . return def weights ( self ) : input_spec ( list of class instances ) def _to_snake_case ( name ) : positions : Relative or absolute positions of log elements layer : Layer from which ` tensor ` comes from . If not provided , Input tensor or list of input tensors . metrics : List of metrics to be evaluated by the model output = output_ls_copy updates += layer.get_updates_for ( inputs ) # This is always a single layer , never a list . symbolic_weights [ i ] .shape , This method deals with an inherent problem ( one array per model weight ) . `` `` '' Trains the model for a fixed number of epochs ( iterations on a dataset ) . output_masks = topology._to_list ( # In this case we will later create an input layer 'The following previous layers ' A list of loss tensors . from .input_layer import Input , InputLayer 'it was generated by layer ' For every layer , a ` group ` named ` layer.name ` # when calling the Network on new inputs . # Network_nodes : set of nodes included in the graph def convert_weights ( weights , from_cudnn=True ) : self._non_trainable_weights = [ ] sample_weight_mode = training_config [ 'sample_weight_mode ' ] weights : Should be a list for name in all_names : # Create the node linking internal inputs to internal outputs . ( the node gets created when the ` call ` dtype = layer.dtype This method deals with an inherent problem weights [ 1 ] = np.transpose ( weights [ 1 ] , ( 3 , 2 , 0 , 1 ) ) sample_weight_mode = training_config [ 'sample_weight_mode ' ] return model_config num_param = len ( layer.weights ) chunk_id += 1 # Check for redundancy in inputs . framework-native tensors ( e.g . TensorFlow data tensors ) . str ( layer.input_spec ) ) `` `` '' Single gradient update over one batch of samples . if bias_shape == ( 2 * units * n_gates , ) : ( layer._inbound_nodes and def run_internal_graph ( self , inputs , masks=None ) : `` `` '' Adds updates to the layer . should be the same as when the weights were saved . 'node_indices ' : self.node_indices , source = 'LSTM ' dtype = K.floatx ( ) if 'nb_epoch ' in kwargs : kwargs = input_data [ 3 ] optimizer_weights_group.attrs [ axes=None ) : output_shapes : list of output shape tuples . of weight arrays is present but their shape does not match . if kwargs : for fine-tuning or transfer-learning models where `` `` '' Adds a layer instance on top of the layer stack . self._container_nodes = container_nodes # Note for node_index , node in enumerate ( layer._inbound_nodes ) : optimizer_config = training_config [ 'optimizer_config ' ] `` `` '' Instantiates a Model from its config ( output of ` get_config ( ) ` ) . Ignored with the default value of ` None ` . outbound_layer._inbound_nodes.append ( self ) if self.outbound_layer : ndim : Integer , expected rank of the input . name : String , the name for the weight variable . List of shape tuples ( or single tuple ) , one tuple per input . if hasattr ( x , '_keras_history ' ) : if layer.input_spec is None : batch_input_shape = K.int_shape ( input_tensor ) x : the input data , as a Numpy array . shuffle : Boolean ( whether to shuffle the order of the batches at return weight for x , s in zip ( output_tensors , shapes ) : ValueError : In case the ` layer ` argument has 'class_name ' : layer_class_name , `` `` '' Returns the list of input tensors necessary to compute ` tensor ` . if 'keras_version ' in f.attrs : layer_names = filtered_layer_names node_indices.append ( node_index ) shapes.append ( K.int_shape ( x ) ) weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 2 , 3 , 4 , 1 , 0 ) ) initial_epoch=initial_epoch ) output_tensors [ i ] , '_uses_learning_phase ' , False ) or uses_lp or by topological order . # hence the same layer might appear twice ) This recursively updates the map ` layer_indices ` , # Only nodes in container_nodes are saved . output_shape : Shape tuple . See above . overwrite : Whether we should overwrite any existing # First layer in model : check that it is an input layer . from .utils.generic_utils import to_list for i , key in enumerate ( output_shape_keys ) : 'and thus has no defined ' + attr_name + ' . ' ) symbolic_weights = getattr ( model.optimizer , 'weights ' ) if isinstance ( model , Sequential ) : from numpy.testing import assert_allclose i , continue input_shapes : list of input shape tuples . ( potentially with 1 element ) . from six.moves import zip This function is used internally with ` self._container_nodes ` . raise ValueError ( 'Invalid input_shape argument ' if len ( weight_values ) ! = len ( symbolic_weights ) : `` `` '' A ` Node ` describes the connectivity between two layers . `` `` '' Retrieves the output mask tensor ( s ) of a layer at a given node . from .. engine import InputSpec add_unprocessed_node ( layer , node_data ) For instance , ` batch_shape= ( 10 , 32 ) ` indicates that Weights can be converted in both directions between ` LSTM ` and ` CuDNNSLTM ` the list ` nodes_in_decreasing_depth ` and the set ` network_nodes ` . self.output_masks = output_masks input_tensors=self.inputs , first_layer = normalize_legacy_config ( first_layer ) config [ 'output_layers ' ] = model_outputs if _need_convert_kernel ( original_backend ) : input_tensors : list of input tensors ( or single input tensor ) . if layer not in layers : self.tensor_indices = tensor_indices uses_learning_phase = any ( [ x._uses_learning_phase for x in computed_tensors ] ) def add ( self , layer ) : output_tensors=output_tensors , shape = weights [ 0 ] .shape if input_tensor is not None and batch_input_shape is None : unique_tensors = list ( x : input data , as a Numpy array or list of Numpy arrays assert hasattr ( self.layers [ 0 ] , 'layers ' ) kwargs ] ) raise ImportError ( ' ` save_weights ` requires h5py . ' ) A shape tuple ` ( samples , sequence_length ) ` , attr : Exact node attribute name . weight_names = [ ] # will call the parent Sequential model . `` `` '' Retrieves the model 's losses . masks = [ None for _ in range ( len ( inputs ) ) ] # If we 've seen this layer before at a higher depth , we should use that depth instead shape_key = layer.name + ' _ % s_ % s ' % ( node_index , j ) TensorFlow data tensors , the default ` None ` is equal to The model architecture , allowing to re-instantiate the model . return ' , '.join ( [ str ( abs ( id ( x ) ) ) for x in object_list ] ) # Expecting this to never be true . such as ` metrics= { 'output_a ' : 'accuracy ' } ` . ( tensors returned by ` Input ` ) . ' ( named `` ' + layer.name model = Sequential ( ) name : String , name of layer . dtype : The dtype of the weight . def load_weights_from_hdf5_group_by_name ( f , layers , skip_mismatch=False , * * kwargs : Additional keyword arguments output_tensors = to_list ( layer.call ( computed_tensor , * * kwargs ) ) self.trainable = kwargs.get ( 'trainable ' , True ) self.input_layers_tensor_indices.append ( tensor_index ) from keras.layers import Masking shuffle=True , compute_output_shape ( input_shape ) output_masks.append ( mask ) node_index : The layer 's position ( e.g . via enumerate ) in a list of dtype epochs=1 , output_tensors.append ( tensor ) Expects ` inputs ` to be a list ( potentially with 1 element ) . first_layer = first_layer.layers [ 0 ] input_tensors , output_tensors , # Layer instance ( NOT a list ) . model = Sequential ( ) def convert_custom_objects ( obj ) : ' does not support masking , ' Shape tuples can include None for free dimensions , validation_data : This can be either name = str ( w.name ) self.name for sublayer in layer.layers : self.set_weights ( self._initial_weights ) count_params ( ) return self.trainable_weights + self.non_trainable_weights # i.e . we mark it to be saved 'Input { } ( 0-based ) originates ' computed_tensors = [ computed_tensor ] def __call__ ( self , inputs , * * kwargs ) : the names of custom losses / layers / etc to the corresponding are passed into ` K.function ` . 'Layer ' + layer.name # Legacy support if len ( topology._to_list ( input_tensors ) ) ! = 1 : # Call layer on its inputs , thus creating the node Function that converts input kernel to the other format . [ [ 0 , 1 , 2 ] , < -- - > [ [ 0 , 2 , 4 ] , return 'private ' + insecure inputs = _to_list ( inputs ) self.sample_weights = self.model.sample_weights # are only applicable to input layers : do not pass these keywords # the inbound_nodes of outbound_layer . original_backend : Keras backend the weights were trained with , # transpose ( and reshape ) input and recurrent kernels for layer in self.layers : if isinstance ( self.layers [ 0 ] , legacy_layers.Merge ) : if layer.__class__.__name__ == 'Conv2D ' : if hasattr ( self , '_losses ' ) : output_shape = layer.compute_output_shape ( input_shapes ) if node in finished_nodes : 'output_tensors ' , biases = np.sum ( np.split ( weights [ 2 ] , 2 , axis=0 ) , axis=0 ) if isinstance ( obj , dict ) : # Check ndim . ` epochs ` is to be understood as `` final epoch '' . # The new network starts with a single inbound node Typically you will use ` metrics= [ 'accuracy ' ] ` . if hasattr ( self , 'get_output_shape_for ' ) : def compile ( self , optimizer , loss , The unique name . weights [ 0 ] = np.transpose ( weights [ 0 ] , ( 2 , 3 , 0 , 1 ) ) 'but was passed an input_mask : ' # Do n't repeat work for shared subgraphs ( 1:1 mapping between weights and samples ) , assert key in layers_to_output_shapes Three lists : output_tensors , output_masks , output_shapes # layer call until it becomes possible to process it _add_inbound_node ( layer , index=0 ) output_tensors = _to_list ( layer.call ( computed_tensor , * * kwargs ) ) # Set dtype . dtype : Expected datatype of the input . weights = convert_weights ( weights , from_cudnn=True ) 'inbound_nodes ' : filtered_inbound_nodes , for name in all_names : return self.model.updates workers : maximum number of processes to spin up self._per_input_losses [ inputs_hash ] = [ ] multiple output tensors , or is already connected 'batch_size ' , def _to_snake_case ( name ) : 'sparse ' : self.sparse , if 'keras_version ' in f.attrs : output_tensors.append ( layer_output_tensors [ tensor_index ] ) # of the node depth . This is necessary for shared layers that have inputs at different state updates , e.g . when we need to update a layer 's internal state # new : ( kernel_rows , kernel_cols , stack_size , filters ) data = [ ] inbound_layer = node.inbound_layers [ j ] The state of the optimizer , allowing to resume training original_backend ) self.input_spec = None x._keras_shape = s if from_cudnn : batch_input_shape = ( from .engine.saving import model_from_yaml def _gather_list_attr ( self , attr ) : f.attrs [ 'model_config ' ] = json.dumps ( { ' but got shape ' + str ( x_shape ) ) str ( len ( weights ) ) self._initial_weights = None training_config = json.loads ( training_config.decode ( 'utf-8 ' ) ) if not hasattr ( tensor , '_keras_history ' ) : if layer.__class__.__name__ == 'ConvLSTM2D ' : `` `` '' input_shape = _collect_input_shape ( inputs ) 'provided weight shape ' + str ( w.shape ) ) considered during deserialization . def transform ( kernel ) : save_attributes_to_hdf5_group ( if K.int_shape ( symbolic_weights [ i ] ) ! = weight_values [ i ] .shape : validation_steps=None , self.output_shapes = output_shapes # then cache them here . When one of these output is queried later , 'hence the notion of `` layer input '' ' x = Input ( batch_shape=batch_shape , if target_class in [ 'LSTM ' , 'CuDNNLSTM ' ] and len ( weights ) == 3 : masks = [ ] `` `` '' Retrieves the model 's updates . # Update tensor_map . # is repeated until all nodes are processed . layers.append ( layer ) output_tensors = to_list ( name=self.name ) computed_tensor , computed_mask = computed_data [ 0 ] self._output_mask_cache = self.model._output_mask_cache return self.model.predict_on_batch ( x ) 'For multi-output layers , ' # If obj is any numpy type `` ` # keys are based on ids on input tensors and inputs masks . `` `` '' Implements name-based weight loading . filtered_layers = [ ] if depth not in layers_by_depth : from .topology import InputSpec return self.model.get_losses_for ( inputs ) param_dset = g.create_dataset ( name , val.shape , return self.model.predict ( x , batch_size=batch_size , verbose=verbose , if layer.__class__.__name__ == 'Conv1D ' : # old : ( filters , stack_size , kernel_rows , kernel_cols ) verbose=verbose , node_data.append ( [ inbound_layer.name , ' is incompatible with layer ' ask the user with a manual prompt . for layer_data in config [ 'input_layers ' ] : # carry over the input mask # identical to the previous one warnings.warn ( 'The ` nb_epoch ` argument in ` fit ` ' Output of the layer 's ` call ` method . compute_output_shape ( input_shape ) def load_attributes_from_hdf5_group ( group , name ) : if not node.inbound_layers : `` `` '' Layer to be used as an entry point into a graph . shape=None , workers=workers , # Augment the mask to match the length of the output . Three lists : output_tensors , output_masks , output_shapes original_backend = None # Add an inbound node to the layer , so that it keeps track assert str ( id ( x ) ) in tensor_map , 'Could not compute output ' + str ( x ) self.dtype = dtype max_queue_size=10 , if ndim is not None and ndim < spec.min_ndim : 'input ' ) will then be the sum of all individual losses . 'should be tensors . ' ) preds = self.predict ( x , batch_size , verbose , steps=steps ) `` `` '' Deserializes a layer , then call it on appropriate inputs . if len ( weight_values ) ! = len ( symbolic_weights ) : shuffle=True , raise AttributeError ( 'The layer has never been called ' input_masks , output_masks , of the layer uses ` K.in_training_phase ( ) ` input_shapes=input_shapes , # List of layer instances . ' Found : ' + str ( self.inputs ) ) warnings.warn ( 'The list of outputs passed to the model ' def legacy_get_config ( self ) : 'but its input shape can not be ' weights = convert_weights ( weights , from_cudnn=True ) it becomes possible to do : verbose=1 , # The following 3 properties describe where self.name + ' : expected max_ndim= ' self._feed_inputs.append ( layer.input ) data : Attributes data . # we copy them to avoid loss of tensor metadata . `` `` '' Generates predictions for the input samples from a data generator . # These lists will be filled via successive calls 'Use ` get_output_at ( node_index ) ` instead . ' ) output_tensors=output_tensors , dictionary . It does not handle layer connectivity epochs is to be understood as `` final epoch '' . The model is uid = _object_list_uid ( inputs ) call ( x , mask=None ) : Where the layer 's logic lives . input_tensors = [ ] Connect current layer with last layer from tensor : name = 'param_ ' + str ( i ) next epoch . When training with input tensors such as dictionary mapping the output name to a Numpy array . kernels = transform_kernels ( weights [ 0 ] , transpose_input ( from_cudnn ) , n_gates ) # does not yet exist ) are re-enqueued , and the process __call__ ( x , mask=None ) : Wrapper around the layer logic ( ` call ` ) . # create the underlying model if from_cudnn : merge_inputs = [ ] Output shape tuple finished_nodes.add ( node ) loss : String ( name of objective function ) or objective function . kept_nodes += 1 return self._get_node_attribute_at_index ( 0 , 'output_tensors ' , new_weights = [ ] output_masks = output_masks [ 0 ] no conversion is made . ' , '.join ( [ x for x in bad_attributes ] ) ) ) # and set output_tensors ' _keras_history . 'Use ` get_output_mask_at ( node_index ) ` ' ' '' . This layer has no information ' The generator should return the same kind of data data : Attributes data to store . use_multiprocessing : if True , use process based threading . # Note that in this case train_output and test_output are the same pointer . `` `` '' ` Input ( ) ` is used to instantiate a Keras tensor . del model # deletes the existing model # ( i.e . until the input tensors to the call all exist ) . layer=sublayer , output_masks= [ None for _ in self.outputs ] , str ( x_shape ) ) return ( proba > 0.5 ) .astype ( 'int32 ' ) from .base_layer import InputSpec if updates is None or updates == [ ] : config = { 'name ' : self.name , self._output_tensor_cache [ cache_key ] = output_tensors `` `` '' Model saving utilities . inbound_tensor_index = input_data [ 2 ] if hasattr ( layer , 'losses ' ) : updates = _to_list ( updates ) class Layer ( object ) : nodes_in_progress.add ( node ) def updates ( self ) : output_tensors [ i ] ._uses_learning_phase = getattr ( output_tensors [ i ] , '_uses_learning_phase ' , False ) or uses_lp 'Use ` get_input_shape_at ( node_index ) ` ' if 'mask ' not in kwargs : 'weights ' , build ( input_shape ) # the inbound_nodes of outbound_layer . the entire layer graph is retrievable from that layer , * * kwargs ) : index : Integer , index of layer . output_tensors = topology._to_list ( 'output ' ) def process_node ( layer , node_data ) : if len ( set ( self.inputs ) ) ! = len ( self.inputs ) : original_keras_version , uid = _object_list_uid ( model.inputs ) index : integer , index of layer . node_indices.append ( node_index ) ' is incompatible with layer ' self.build ( ) from .base_layer import Layer , Node , InputSpec deserialized = [ ] from keras.engine import Input describing the origin of the ` input_tensors ` , verifying the following : def summary ( self , line_length=None , positions=None , print_fn=None ) : batch_input_shape = ( batch_size , ) + tuple ( input_shape ) if not layer._inbound_nodes : from .input_layer import Input param_dset [ : ] = val tensor_indices.append ( None ) # If input_tensor is set , and batch_input_shape is not set : return self._get_node_attribute_at_index ( 0 , 'input_masks ' , The attribute ` model.metrics_names ` will give you computed_tensors = [ x [ 0 ] for x in computed_data ] 'weights ' , f.attrs [ 'model_config ' ] = json.dumps ( { sample weighting ( 2D weights ) , set this to ` `` temporal '' ` . if isinstance ( outputs , ( list , tuple ) ) : def output_mask ( self ) : ( without its trained weights ) from this configuration . name : An optional name string for the layer . def get_input_shape_at ( self , node_index ) : if layer.__class__.__name__ == 'Bidirectional ' : weights [ 10 ] ] , axis=-1 ) 'config ' : conf } ' a Keras Input layer , ' # Set values . `` `` '' Topology-related part of the Keras engine . `` pay more attention '' to samples from Stacked array of transformed kernels . Input kernels for each gate are transposed and converted between Fortran # set weights shape : A shape tuple ( integer ) , not including the batch size . if isinstance ( x , list ) : generator , `` `` '' Retrieves the output shape ( s ) of a layer at a given node . return inputs source = 'CuDNNGRU ' for element in iterable : str ( inputs ) ) # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) the provided inputs and the expectations of the layer . e.g . if the layer is being shared with a different data stream ) . computable_tensors.append ( x ) a record of training loss values and metrics values def transform_kernels ( kernels , func , n_gates ) : 'loss_weights ' : model.loss_weights , # Retrieve losses for all internal layers . weights=weights [ : num_weights ] , If unspecified , it will default to 32 . layers_with_complete_input = [ ] # To provide a better error msg . for x in to_list ( output ) ] `` `` '' list ( custom_objects.items ( ) ) ) ) self.add_loss ( regularizer ( weight ) ) 'input_dtype ' , # legacy # List of tensors , 1:1 mapping with input_tensor . Note that because this implementation relies on multiprocessing , # Handle mask propagation . raise RuntimeError ( 'You tried to call ` count_params ` on ' attribute is ill-defined ( e.g . a shared layer None or a tensor ( or list of tensors , if target_class in [ 'LSTM ' , 'CuDNNLSTM ' ] and len ( weights ) == 3 : output_masks = layer.compute_mask ( computed_tensor , weights [ 6 ] ] , axis=-1 ) filtered_layer_names.append ( name ) # We batch weight value assignments in a single backend call is specified . Total number of steps ( batches of samples ) `` `` '' Evaluates the model on a data generator . return layer.name + '_ib- ' + str ( node_index ) prefix = self.__class__.__name__ steps_per_epoch : Total number of steps ( batches of samples ) if reshape and layer_weights_shape ! = weights [ 0 ] .shape : return obj.__name__ shape : Shape tuple , expected shape of the input validation_data : This can be either inputs : Input tensor , or list/tuple of input tensors . biases = weights [ 2 ] .reshape ( ( 2 , -1 ) if from_cudnn else -1 ) if ( value is not None and computable_tensors = [ ] The use of ` keras.utils.Sequence ` guarantees the ordering if not dtype : of index ` epochs ` is reached . ( during training only ) . You can either pass a flat ( 1D ) nodes_depths [ inbound_node ] = max ( depth + 1 , previous_depth ) steps_per_epoch=None , the model 's optimizer 's state ( if any ) the updates as conditional on these inputs . ' at node ' + str ( node_index ) raise TypeError ( ' ` model_from_config ` expects a dictionary , ' # Instantiate the input layer . if 'nb_epoch ' in kwargs : str ( index ) + ' but model only has ' _ , output_masks , _ = self.run_internal_graph ( inputs , masks ) return { 'class_name ' : obj.__class__.__name__ , before declaring the prediction round finished . proceed = ask_to_proceed_with_overwrite ( filepath ) if masks is None : def call ( self , inputs , mask=None ) : self.built = True 'input mask ' ) steps_per_epoch : Total number of steps ( batches of samples ) losses += layer.get_losses_for ( inputs ) for layer in layers : The first layer passed to a Sequential model if not layer_cache : if name in group.attrs : return self._output_tensor_cache [ cache_key ] ' , '.join ( [ x for x in bad_attributes ] ) ) ) with K.name_scope ( layer.name ) : ' in the save file . ' len ( output_ls ) > 1 ) : topology.Node ( outbound_layer=self , raise ValueError ( 'No such layer : ' + name ) 'tensor_indices ' : self.tensor_indices } def process_node ( layer , node_data ) : the batch size , or 1 if that can not be determined . return node.input_tensors inbound_layers= [ ] , validation_split : Float between 0 and 1 . self.layers_by_depth = layers_by_depth shape : A shape tuple ( integer ) , not including the batch size . If layer is not built : # Update self._per_input_updates if x not in computable_tensors : # list of layers ( 1 to 1 mapping with self.inputs , return masks [ 0 ] verbose=1 , layer = self.output_layers [ i ] if source ! = target_class : is not divisible by the batch size . 'You can build it manually via : ' ' can not obtain value for tensor ' return node = layer._inbound_nodes [ node_index ] if K.backend ( ) == 'theano ' or K.backend ( ) == 'cntk ' : # This also updates the layer history of the output tensor ( s ) . if by_name : as accepted by ` test_on_batch ` . config : dictionary with configuration . # Handle Keras 1.1 format 'Layer ' + layer.name or list/tuple of Keras tensors to reference i.e . if it is connected to one incoming layer . A._outbound_nodes K.is_keras_tensor ( x ) yaml_string : YAML string encoding a model configuration . conf [ 'name ' ] = name ` layer_names ` ( attribute ) , a list of strings def _gather_list_attr ( self , attr ) : * * kwargs : Additional keyword arguments to be passed to ` call ( ) ` . depth_keys = list ( self._nodes_by_depth.keys ( ) ) # Handle laying building ( weight creating , input spec locking ) . input_tensor._keras_history = ( self , 0 , 0 ) output_tensors=self.outputs , # Split single set of biases evenly to two sets . The way of a list of tensors if there are more than one outputs . n_gates : Number of gates ( 4 for LSTM , 3 for GRU ) . If unspecified , ` max_queue_size ` will default to 10 . for layer , depth in layers_depths.items ( ) : weight_values = [ np.asarray ( g [ weight_name ] ) for weight_name in weight_names ] if len ( input_tensors ) == 1 : an ` input_spec ` of length ` n ` . nodes_by_depth [ depth ] .append ( node ) def generate_arrays_from_file ( path ) : new_node_index = node_conversion_map [ node_key ] input_masks= [ None for _ in self.inputs ] , # Build self.output_layers : finished_nodes.add ( node ) initial_epoch=initial_epoch , layers_depths = { } # dict { layer : depth value } return self.model.get_updates_for ( inputs ) `` `` '' Contains the base Layer class , from which all layers inherit . sample_weight : sample weights , as a Numpy array . ' '' with a weight list of length ' A Numpy array of probability predictions . i , # The node is relevant to the model : node_indices : a list of integers , the same length as ` inbound_layers ` . # and for each layer , which node and which shapes.append ( None ) input_shapes = [ ] ndim=None , ` y ` can be ` None ` ( default ) if feeding from first_layer = config [ 0 ] get_output_shape_at ( node_index ) `` `` '' Retrieves the model configuration as a Python list . raise TypeError ( 'Output tensors to a ' + cls_name + ' must be ' # backend information not available get_output_shape_at and validation metrics values ( if applicable ) . if mask is not None : The same structure , where occurrences self.add_update ( layer.get_updates_for ( None ) , None ) `` `` '' Layer to be used as an entry point into a model . config : Model config dictionary . def output ( self ) : if len ( specs ) == 1 : ( for a single-output ` Sequential ` model ) . input_tensors : A tensor or list of tensors . 'value ' : obj.tolist ( ) } 'name ' , 'config ' : self.layers [ 0 ] .get_config ( ) } ) # Build a dict { depth : list of layers with this depth } return self._trainable # Updates indexed by None are unconditional return self.model.updates # which provides a speedup in TensorFlow . assert shape is not None , ( 'Please provide to Input either a ` shape ` ' name=None , dtype=None , sparse=False , return specs 'output shape ' ) weight_values ) ) : model.add ( Dense ( 10 , activation='softmax ' ) ) layers_for_depth.sort ( key=lambda x : layer_indices [ x ] ) # Avoid input redundancy . optimizer_weight_names ] custom_objects=dict ( list ( _GLOBAL_CUSTOM_OBJECTS.items ( ) ) 'not a list . Maybe you meant to use ' output_shape = [ None for _ in input_shape ] types = ( source , target ) or ` batch_input_shape ` argument , divided by the batch size . # Update model updates and losses : `` `` '' Implements topological ( order-based ) weight loading . # if obj is a serializable Keras class instance Total number of steps ( batches of samples ) `` `` '' Retrieves the input mask tensor ( s ) of a layer at a given node . raise TypeError ( 'There are no layers in the model . ' ) self._feed_input_names = self.model._feed_input_names initial_epoch=0 , metrics= [ 'accuracy ' ] ) original_keras_version=None , # List of tensors . 1:1 mapping with inbound_layers . elif hasattr ( layer , 'layers ' ) : 'All inputs should only appear once . ' f , self.layers , skip_mismatch=skip_mismatch , trainable=True , name = kwargs.get ( 'name ' ) self._built = value before declaring one epoch finished and starting the weights = [ ] ' has no inbound nodes . ' ) line_length=line_length , if hasattr ( x_elem , '_keras_shape ' ) : self.name + ' : expected min_ndim= ' specs.append ( None ) # Check that layer is an InputLayer . Node ( self , A layer with ` n ` input tensors must have if not custom_objects : # returns a compiled model if num_weights > 0 : if all_names.count ( name ) ! = 1 : for node in nodes_by_depth [ depth ] : for i in range ( len ( weight_values ) ) : if input_shape and batch_input_shape : } ) return self.model.losses proba = self.predict ( x , batch_size=batch_size , verbose=verbose , del model # deletes the existing model return preds if layer.__class__.__name__ == 'ConvLSTM2D ' : self.outputs = [ outputs ] It will be autogenerated if it is n't provided . return False for layer in getattr ( self , 'input_layers ' , [ ] ) : # if obj is a python 'type ' 'config ' : layer_config , unconditional , or conditional on inputs to this model # note : topology.Node is an internal class , self.input_shapes = input_shapes inputs : input tensor or list of inputs tensors to mark # Set model name . that are n't inputs to this model ) . self.total_loss = self.model.total_loss inputs : input tensor or list of inputs tensors to mark The attribute ` model.metrics_names ` will give you 'batch_input_shape argument to ' the batch size , or 1 if that can not be determined . if not batch_shape and tensor is None : x = to_list ( input_tensors ) [ 0 ] merge_config [ 'layers ' ] = layers self.name + ' : expected min_ndim= ' training . If instead you would like to use your own ImportError : If h5py is not available . inbound_node = inbound_layer._inbound_nodes [ inbound_node_index ] ' not compatible with ' trainable = getattr ( self , 'trainable ' , True ) 'batch ' is a special option for dealing with the self.add_loss ( layer.get_losses_for ( computed_tensors ) , inputs ) continue for x in node.input_tensors : as accepted by ` test_on_batch ` . tensor_index , use_multiprocessing=use_multiprocessing , callbacks=callbacks , `` `` '' Gets the model 's input specs . raise TypeError ( ' ` model_from_config ` expects a dictionary , not a list . ' input_layer = InputLayer ( batch_input_shape=batch_shape , ' a previous non-Input layer . ' Input tensor or list of input tensors . tensor_indices= [ ] , # scalar The generator is run in parallel to the model , for efficiency . output = self.call ( inputs , * * kwargs ) return outputs [ 0 ] # Build self.input_layers : from .. utils.generic_utils import object_list_uid weights [ 5 ] , try : return proba.argmax ( axis=-1 ) output_tensors [ i ] ._keras_shape = output_shapes [ i ] self.layers.pop ( ) dtype = kwargs.get ( 'dtype ' ) mask tensors . warnings.warn ( 'The list of outputs passed to the model ' Each time the output of a layer is used by another layer , sample_weight_mode=sample_weight_mode ) mask_cache_key += ' _ ' + ' , '.join ( [ str ( id ( x ) ) for x in masks ] ) return print_layer_summary ( self , state_updates += layer.updates A model is callable on non-Keras tensors . self.output_layers.append ( layer ) List of input tensors . `` ` python input_tensors=self.inputs , dictionary mapping the output name to a Numpy array . # Call layer on its inputs , thus creating the node steps : Total number of steps ( batches of samples ) computed_tensor , computed_mask = computed_data [ 0 ] all_input_shapes = set ( [ str ( node.input_shapes ) for node in self._inbound_nodes ] ) 'class_name ' : model.optimizer.__class__.__name__ , have multiple tensor outputs , with each one being kwargs [ 'mask ' ] = computed_mask class Sequential ( Model ) : if len ( weights ) == 9 : 'Conv3D ' , The layer 's attribute ` attr ` at the node of index ` node_index ` . verbose=1 , skip_mismatch=skip_mismatch , steps : Integer or ` None ` . warnings.warn ( 'trainable ' , obj : the object to serialize optimizer_weight_names = [ n.decode ( 'utf8 ' ) for n in from .. utils.layer_utils import print_summary as print_layer_summary sample_weight_mode=sample_weight_mode , computed_mask ) model_inputs = [ ] if not layer.layers : constraint=constraint ) from_config loss_weights=loss_weights , output_shape num_param = len ( layer.weights ) # new : i , f , c , o weight_values = preprocess_weights_for_loading ( For every weight in the layer , a dataset # instantiate optimizer self._feed_input_shapes = [ ] set_weights ( weights ) target_tensors=None , 'should have a single output tensor . ' as a string . losses = to_list ( losses ) specs.append ( None ) group : A pointer to a HDF5 group . `` `` '' Generate class predictions for the input samples . batch_size=None , given by ` epochs ` , but merely until the epoch def trainable_weights ( self ) : bias = np.concatenate ( [ weights [ 2 ] , raise ValueError ( 'You are trying to load a weight file ' if self._initial_weights is not None : raise TypeError ( 'Layer ' + layer.name def __init__ ( self , inputs , outputs , name=None ) : layer_configs.append ( { layers.append ( layer ) node_indices= [ ] , weight_values = preprocess_weights_for_loading ( layer , self.output_layers = self.model.output_layers line_length=line_length , raise ValueError ( 'Layer # ' + str ( k ) just by knowing the inputs and outputs of the model . 'batch_input_shape argument . ' ) for i in range ( len ( self.output_layers ) ) : If the input layer in the model is named , you can also pass a masks = _to_list ( mask ) input_masks=previous_mask , output_masks=output_mask , 'The following previous layers ' __call__ ( x , mask=None ) : Wrapper around the layer logic ( ` call ` ) . weights [ 6 ] , if 'optimizer_weights ' in f : weights : A list of Numpy arrays with shapes and types matching # Check that all tensors required are computable . 'TensorFlow optimizers do not ' self._per_input_updates = { } return self.model.get_updates_for ( inputs ) sample_weight_mode=None , forward_weights = preprocess_weights_for_loading ( layer.forward_layer , `` `` '' Deserializes a layer , then call it on appropriate inputs . def __init__ ( self , dtype=None , if num_chunks > 1 : if not isinstance ( filepath , h5py.File ) : filtered_layers.append ( layer ) 'Found input_spec = ' if 'input_shape ' in kwargs or 'batch_input_shape ' in kwargs : layer , node_index , tensor_index = x._keras_history obj : the object to serialize be trained via backprop or not ( assuming verbose : 0 , 1 , or 2 . Verbosity mode . for x in output_ls : The model returned by ` load_model ` # if obj is any numpy type kernel = np.transpose ( kernel , ( 2 , 3 , 1 , 0 ) ) # Save optimizer weights . self._inbound_nodes [ 0 ] .output_shapes = [ self.outputs [ 0 ] ._keras_shape ] if len ( computed_data ) == 1 : when using process-based threading . layers : A list of target layers . output_masks= [ None for _ in self.outputs ] , str ( x.name ) ) def get_updates_for ( self , inputs ) : def non_trainable_weights ( self , weights ) : See [ callbacks ] ( /callbacks ) . import os A Keras model instance . If an optimizer was found self.layers = layers node = layer._inbound_nodes [ node_index ] constraint=constraint ) return layer if isinstance ( x , list ) : # Attempt automatic input shape inference . Output will always be a list of tensors def _object_list_uid ( object_list ) : 'Received type : ' model = model_from_config ( model_config , custom_objects=custom_objects ) self.input_layers_node_indices.append ( node_index ) layer = created_layers [ layer_data [ 'name ' ] ] outputs = to_list ( wrapped ( inputs , training=True ) ) self.add_loss ( regularizer ( weight ) ) layer.reset_states ( ) from keras.engine.topology import _object_list_uid , _to_list based on the network 's topology , meaning the architecture from .. layers import deserialize as deserialize_layer raise TypeError ( 'Input tensors to a ' + cls_name + ' ' input_shape : Keras tensor ( future input to layer ) node_conversion_map [ node_key ] = kept_nodes import copy network_nodes = set ( ) # ids of all nodes relevant to the Network if depth not in nodes_by_depth : # when calling the Container on new inputs . output_tensors = output_tensors [ 0 ] weight_values = preprocess_weights_for_loading ( layer , validation_split=0. , reference_input_tensors = node.input_tensors if opened_new_file : input_mask , output_mask : Same as above , for masks . def load_weights_from_hdf5_group ( f , layers , reshape=False ) : prefix = 'input ' if isinstance ( obj , np.ndarray ) : weight_value_tuples.append ( ( symbolic_weights [ i ] , opened_new_file = True node_index : Origin node index of the tensor . from __future__ import division ' does not support masking , ' # backend information not available if 'GRU ( reset_after=False ) ' in types : nodes_in_decreasing_depth.append ( node ) optimizer : String ( name of optimizer ) or optimizer object . all_attrs = [ ] if original_backend is None : recurrent_kernel = np.concatenate ( [ weights [ 1 ] , # First , we need to infer its expected input shape and dtype . ValueError : In case of improperly formatted ` layer_data ` dict . if not val.shape : self._internal_input_shapes = [ x._keras_shape for x in self.inputs ] return source_tensors 'The weights for loading have shape ' in a single file . assert node_index == 0 for layer in self.inbound_layers : self._outbound_nodes = [ ] # Will be appended to by future calls to __call__ if K.int_shape ( symbolic_weights [ i ] ) ! = weight_values [ i ] .shape : raise RuntimeError ( The saved model contains : from __future__ import absolute_import 'get an ` input_shape ` or ' '\ 's weights have shape ' target = 'GRU ( reset_after=True ) ' batch_size = kwargs [ 'batch_size ' ] raise RuntimeError ( 'The model needs to be compiled ' f.close ( ) if layer.__class__.__name__ == 'Conv2DTranspose ' : self._feed_inputs = [ ] # and for each layer , which node and which params = self.weights shuffle=shuffle , # with the input_spec specified in the layer constructor . return config target tensor ( in turn , Keras will not expect external loss=loss , batch_input_shape = tuple ( batch_input_shape ) # If file exists and should not be overwritten : y1 = _to_list ( model.predict ( X ) ) if not isinstance ( layer , Layer ) : # as we add more layers 'instantiated via ` tensor = Input ( shape ) ` .\n ' weights = convert_weights ( weights , from_cudnn=source == 'CuDNNLSTM ' ) # We were passed a regular layer , and it should name=name , `` `` '' ` Input ( ) ` is used to instantiate a Keras tensor . If ` name ` and ` index ` are both provided , ` index ` will take precedence . weights [ 7 ] , [ str ( node.output_shapes ) for node in self._inbound_nodes ] ) # Optional keyword arguments to layer 's ` call ` . The input samples are processed batch by batch . # Prevent cycles . node_indices= [ ] , y_expected = _to_list ( merge_func ( y_forward [ 0 ] , y_backward [ 0 ] ) ) of the layer ( i.e . it should match the return self._output_mask_cache [ cache_key ] self._inbound_nodes [ 0 ] .output_tensors = self.outputs if len ( computed_tensors ) == 1 : def add_loss ( self , losses , inputs=None ) : The same layer can be reinstantiated later if layer : input_tensors.append ( raise ValueError ( tensor_index = self.output_layers_tensor_indices [ i ] f_merged = K.function ( [ inputs ] , to_list ( layer ( inputs ) ) ) self._output_shape_cache [ cache_key ] = output_shapes string , path to the saved model , or layer : Layer instance . if they 're in plain Keras format . self._non_trainable_weights = weights Shape tuples can include None for free dimensions , if len ( weights ) == 9 : or ` K.in_test_phase ( ) ` . batch_input_shape = K.int_shape ( input_tensor ) updates = [ ] # possible . tensor : Optional existing tensor to wrap into the ` Input ` layer . if legacy_models.needs_legacy_support ( self ) : etc ... raise TypeError ( 'Layer ' + self.name weight_value_tuples = [ ] attribute is ill-defined ( e.g . a shared layer def input ( self ) : str ( pv.shape ) def evaluate_generator ( self , generator , steps=None , inbound_layers.append ( None ) def fit_generator ( self , layer = self.input_layers [ i ] raise RuntimeError ( ' ` shape ` does not include the batch ' layer_config = { 'class_name ' : layer.__class__.__name__ , steps=steps ) nodes_in_progress : Set of nodes that are currently active for layer in self.layers : depth_keys = list ( self._nodes_by_depth.keys ( ) ) or a mismatch in the shape of the weight model.optimizer.set_weights ( optimizer_weight_values ) at the end of every epoch . It should typically self.outputs = list ( outputs ) return self.model.test_on_batch ( x , y , positions=positions , self.trainable = True 'Layer names : ' , all_names ) input_tensor=tensor ) ' is not connected , no input to return . ' ) return shapes [ 0 ] We call self._add_inbound_node ( ) . return self._non_trainable_weights A node from layer A to layer B is added to : def get_source_inputs ( tensor , layer=None , node_index=None ) : should have a defined input shape . What that # to the input layer we just created . self._feed_input_names.append ( layer.name ) if inputs is not None : self.built = False steps=steps ) n_gates = 3 a None shape is compatible with any shape . for x , s in zip ( output_tensors , shapes ) : batch_input_shape = ( batch_size , ) + tuple ( kwargs [ 'input_shape ' ] ) max_queue_size=max_queue_size , output_ls_copy.append ( x ) weight_value_tuples = [ ] generator , all_input_shapes = set ( state updates , e.g . when we need to update a layer 's internal state weighted_metrics=None , target_tensors : By default , Keras will create a placeholder for the verbose=1 , `` `` '' Retrieves the input tensor ( s ) of a layer . def build ( self , input_shape=None ) : callbacks : List of ` keras.callbacks.Callback ` instances . str ( weights [ 0 ] .size ) + ' . ' ) # Set values . self.built = True # If the input tensor ( s ) had not previous Keras history , # Only nodes in network_nodes are saved . losses += self.get_losses_for ( None ) @ built.setter model.fit_generator ( generate_arrays_from_file ( '/my_file.txt ' ) , self.outputs = [ ] return output_shapes if layer not in layers : def model_from_json ( json_string , custom_objects=None ) : is not divisible by the batch size . # New file format . def __init__ ( self , inputs , outputs , name=None ) : 'correspond to layer ' + name uses_lp = any ( layers_to_output_shapes = { } def to_list ( x ) : will be updated during training . ValueError : If the provided weights list does not match the self.supports_masking = False tensor_map [ str ( id ( x ) ) ] = ( y , mask ) input_tensor._keras_shape = batch_input_shape return self.layers [ index ] A list of dicts ( each dict is a layer config ) . ValueError : in case the layer is missing shape information 'You should pass an input_shape or ' * * kwargs : Additional keyword arguments verbose=verbose , 'class_name ' : model.__class__.__name__ , elif hasattr ( layer , 'layers ' ) : def to_yaml ( self , * * kwargs ) : elif bias_shape == ( units * n_gates , ) : # with the input_spec specified in the layer constructor . self._updates = [ ] func : Function applied to kernel of each gate . import json topology.load_weights_from_hdf5_group ( f [ 'model_weights ' ] , model.layers ) E.g . ` node_index=0 ` will correspond to the 'into probabilities ' recursively . node_data = [ ] # Layer parameters . if len ( input_shapes ) ! = len ( self.input_layers ) : return model_config batches have been seen by the model . `` `` '' Generates class probability predictions for the input samples . layer_name , node_index , tensor_index = layer_data shape_key = layer.name + ' _ % s_ % s ' % ( node_index , tensor_index ) assert tensor_index == 0 get_weights In this case ` call ` just reapplies output_shapes : list of output shape tuples . # we assume a 1:1 mapping from tensor to mask `` `` '' Sets the weights of the model . shape : The shape tuple of the weight . ` self._add_inbound_node ( last_layer ) ` } layer.reset_states ( ) layers_by_depth [ depth ] = [ ] 'use the functional API . ' ) if input_tensor is None : warning . optimizer_weights_group = f.create_group ( name=layer.name + '_input ' ) weights [ 7 ] , return count_params ( self.weights ) model = Sequential ( ) name = _to_snake_case ( prefix ) + ' _ ' + str ( K.get_uid ( prefix ) ) Output mask tensor ( potentially None ) or list of output dtype=dtype , validation_steps=validation_steps , self , batch_shape = layer.batch_input_shape filepath : String , path to the weights file to load . output_shapes= [ self.outputs [ 0 ] ._keras_shape ] ) more than once ( shared layer ) , this is ill-defined def get_losses_for ( self , inputs ) : def test_on_batch ( self , x , y , kwargs = { } return self._get_node_attribute_at_index ( 0 , 'input_tensors ' , summary tensor_indices= [ ] , input_tensors=self.inputs , for layer in self.layers : `` `` '' Loads all layer weights from a HDF5 save file . execute the generator on the main thread . `` `` '' Builds a map of the graph of layers . depth_keys = list ( layers_by_depth.keys ( ) ) be trained via backprop or not ( assuming else : if inputs is not None : for pv , p , w in zip ( param_values , params , weights ) : merge = legacy_layers.Merge.from_config ( first_layer_config ) model_config = { merge_input = layer_module.deserialize ( merge_input_config ) input_tensor : Optional tensor to use as layer input node = inbound_layer._inbound_nodes [ node_index ] epochs , def predict_on_batch ( self , x ) : or by topological order . output_shapes = self._output_shape_cache [ cache_key ] was never compiled in the first place ) . if all ( [ s is not None sparse=self.sparse , name = layer_data [ 'config ' ] .get ( 'name ' ) workers : Integer . Maximum number of processes to spin up non_trainable_weights ( list of variables ) `` `` '' Retrieves the output tensor ( s ) of a layer . dtype = layer.dtype computed_masks = [ x [ 1 ] for x in computed_data ] 'has been renamed ` epochs ` . ' ) def compute_output_shape ( self , input_shape ) : The generator is expected to loop over its data # we retrieve it from there instead of recomputing it . from six.moves import zip weight_names.append ( name.encode ( 'utf8 ' ) ) weights [ 6 ] , if source ! = target_class : # Raises If None is passed , the loss is assumed unconditional # In case of nested models : recover the first layer if len ( masks ) == 1 : skip_mismatch : Boolean , whether to skip loading of layers outputs = _to_list ( wrapped ( inputs ) ) def add_weight ( self , def output ( self ) : from .. utils.layer_utils import count_params from .. engine.topology import Layer , InputSpec return self.model.evaluate_generator ( generator , `` `` '' Retrieves the input tensor ( s ) of a layer at a given node . kwargs = { } mask_cache_key = ' , '.join ( [ str ( id ( x ) ) for x in self.inputs ] ) ( a mask can be a tensor , or None ) . if isinstance ( inputs , ( list , tuple ) ) : name=None , dtype=None , sparse=False , # ( e.g . weight regularizers ) . ` sample_weight_mode ` on each output by passing a # If the input tensor ( s ) had not previous Keras history , if x_shape is not None : def built ( self ) : loss_weights=loss_weights , # to self._add_inbound_node ( ) . model_config = f.attrs.get ( 'model_config ' ) if 'layer_names ' not in f.attrs and 'model_weights ' in f : def built ( self ) : self._trainable_weights = weights self.add_update ( layer.get_updates_for ( None ) , None ) non picklable arguments to the generator def _to_list ( x ) : for layer in self.layers [ 1 : ] : ' weights . Provided weights : ' `` `` '' Converts a layer and its index to a unique ( immutable type ) name . TypeError : if ` config ` is not a dictionary . batch_shape = ( None , ) + tuple ( shape ) Saved models can be reinstantiated via ` keras.models.load_model ` . raise ValueError ( 'Provide either a layer name or layer index . ' ) layer : Origin layer of the tensor . Will be To specify different metrics for different outputs of a else : return self._get_node_attribute_at_index ( 0 , 'output_tensors ' , 'an instance of class Layer . ' self.add_loss ( layer.get_losses_for ( None ) , None ) # based on layer names , because names can potentially ' ( missing Keras metadata ) . ' ) 'use the functional API . ' ) for j in range ( len ( node.inbound_layers ) ) : This tuple ( a single output of the generator ) makes a single layer_weights_shape = K.int_shape ( layer.weights [ 0 ] ) model.add ( Dense ( 32 , batch_input_shape= ( None , 500 ) ) ) If necessary , we ` build ` the layer to match model 's target , which will be fed with the target data during input_shape : Shape tuple , not including the batch axis . raise TypeError ( 'All layers in a Sequential model ' g = f [ name ] tensor_indices= [ ] , optimizer_weight_names = [ `` `` '' Serialize any object to a JSON-serializable structure . 'Conv2D ' , [ str ( node.input_shapes ) for node in self._inbound_nodes ] ) preds = self.predict ( x , batch_size , verbose , steps=steps ) ` self._add_inbound_node ( last_layer ) ` if len ( to_list ( input_tensors ) ) ! = 1 : original_backend : Keras backend the weights were trained with , as a string . return 'InputSpec ( % s ) ' % ' , '.join ( x for x in spec if x ) raise ValueError ( 'You called ` set_weights ( weights ) ` on layer `` ' return self.model.regularizers model.add ( Dense ( 32 , batch_input_shape= ( None , 500 ) ) ) ' ( { } vs { } ) . '.format ( len ( symbolic_weights ) , len ( weight_values ) ) ) A ` History ` object . Its ` History.history ` attribute is inbound_layers= [ ] , param_dset [ : ] = val def output_shape ( self ) : node_index = self.output_layers_node_indices [ i ] inputs : Can be a tensor or list/tuple of tensors . While TH implements convolution , TF and CNTK implement the correlation operation . self._output_mask_cache [ cache_key ] = output_masks non_trainable_weights ( in this order ) . from __future__ import print_function source = 'LSTM ' ( or list of input shape tuples , one tuple per input tensor ) . for x in previous_sources : The added Keras attributes are : ( instead of topological weight loading ) . def set_weights ( self , weights ) : by sample_weight or class_weight during training and testing . allowed_kwargs = { 'input_shape ' , output_masks.append ( mask ) The weight file has : validation_steps=None , # Compile model . str ( layer_weights_shape ) + ' and size ' first_layer = first_layer.layers [ 0 ] A tensor ( or list of tensors if the layer has multiple inputs ) . `` `` '' Save a model to a HDF5 file . self.output_layers_node_indices = self.model.output_layers_node_indices if 'backend ' in f.attrs : if preds.min ( ) < 0. or preds.max ( ) > 1. : tensor_map [ str ( id ( x ) ) ] = ( y , mask ) 'inbound_layers ' : inbound_names , for name , val in zip ( weight_names , weight_values ) : from .engine.topology import InputLayer It is the topological form of a `` model '' . A Model if not self.built : 'dimension . ' ) # Add nodes to all layers involved . ` node_indices [ i ] ` is the origin node of ` input_tensors [ i ] ` as they ca n't be passed easily to children processes . y1 = to_list ( model.predict ( X ) ) 'value ' + str ( value ) # If obj is any numpy type conf = normalize_legacy_config ( conf ) from_config ( config ) if dtype is None : # e.g . optimizer , layer validation_data=validation_data , by ` Container ` ( one layer of abstraction above ) . optimizer = optimizers.deserialize ( optimizer_config , val.shape , on the layer 's weights variables , not on any inputs tensors ) . an ` input_spec ` of length ` n ` . input_tensors : list of input tensors . Weights can be converted in both directions between ` LSTM ` and ` CuDNNSLTM ` `` `` '' Internal method to create an inbound node for the layer . ( the node gets created when the ` call ` def trainable_weights ( self , weights ) : def model_from_yaml ( yaml_string , custom_objects=None ) : def load_weights_from_hdf5_group_by_name ( f , layers , skip_mismatch=False , network_nodes.add ( self._node_key ( layer , node_index ) ) 'input ' ) if x not in computable_tensors : 'has been renamed ` epochs ` . ' ) nodes_depths = { } # dict { node : depth value } deserialized.append ( convert_custom_objects ( value ) ) from .topology import Layer # no model-level masking for now chunk_id += 1 'Input layers to a ` Model ` must be ` InputLayer ` objects . ' tensor_indices= [ ] , if obj in custom_objects : if index is not None : on each output by passing a dictionary or a list of losses . # Layer instances created during def summary ( self , line_length=None , positions=None , print_fn=None ) : more than one incoming layers . return deserialized self._inbound_nodes [ 0 ] .output_shapes = [ compute_mask ( x , mask ) # misc functions ( e.g . loss function ) f.flush ( ) The generator is expected to loop over its data # ( since serialized nodes refer to layers by their name ) . for x in node.input_tensors : HDF5_OBJECT_HEADER_LIMIT = 64512 raise RuntimeError ( 'The layer has never been called ' target = 'GRU ( reset_after=False ) ' len ( self._inbound_nodes ) - 1 , self.input_layers_tensor_indices = self.model.input_layers_tensor_indices class_weight=class_weight , if cache_key in self._output_tensor_cache : } def _save_attributes_to_hdf5_group ( group , name , data ) : original_keras_version , elif bias_shape == ( 2 , units * n_gates ) : 'config ' : model.get_config ( ) input_tensors , output_tensors , # Handle automatic shape inference ( only useful for Theano ) . use_multiprocessing=False ) : str ( mask ) ) during prediction . weights , layers_to_output_shapes [ shape_key ] = input_shape inbound_nodes : list of nodes output_tensor = layer ( self.outputs [ 0 ] ) self._container_nodes = self.model._container_nodes if shape is not None : `` `` '' Adds a weight variable to the layer . self.model.trainable = self.trainable node_conversion_map = { } # Add nodes to all layers involved . if len ( specs ) == 1 : if element is not None : get_output_mask_at ( node_index ) Can be run on non-Keras tensors . 'instead . ' ) yield ( x , y ) if spec.axes : warnings.warn ( 'Skipping loading of weights for layer { } '.format ( layer.name ) self._inbound_nodes = [ ] 'with different output shapes . Hence ' will be updated during training . Note : Please also see if has_arg ( self.call , 'mask ' ) : or a mismatch in the shape of the weight layers = [ ] input_mask , output_mask : Same as above , for masks . 'Keras tensors . Found : ' + str ( x ) ) # same size of node.input_tensors . kernel = np.transpose ( kernel , ( 2 , 3 , 1 , 0 ) ) def fit_generator ( self , ( 'max_ndim= ' + str ( self.max_ndim ) ) if self.max_ndim else `` , A list of ` InputSpec ` instances ( one per input to the model ) name=layer.name + '_input ' ) self.input_names = [ ] raise TypeError ( 'The added layer must be ' while unprocessed_nodes : get_input_shape_at layer = self.input_layers [ i ] node_index ) into account in the topological ordering , so adding or self._output_shape_cache [ cache_key ] = output_shapes model.compile ( optimizer=optimizer , # add to filtered_inbound_nodes . data provided . 'Note that input tensors are ' `` `` '' Retrieves the output shape tuple ( s ) of a layer . tensor_index = node.tensor_indices [ i ] # masking not explicitly supported : return None as mask 'with different input shapes . Hence ' str ( len ( filtered_layers ) ) + ' layers . ' ) return None A list of converted weights values ( Numpy arrays ) . set_weights verbose : verbosity mode , 0 or 1 . new_node_index = node_conversion_map.get ( output_masks = output_masks [ 0 ] input_shape : Shape tuple . Provided for convenience , or in the case of temporal data , if regularizer is not None : generator : Generator yielding tuples ( inputs , targets ) ( necessary since each inbound layer might inbound_layers=inbound_layers , self.output_names = [ ] `` ` 'Use ` get_output_mask_at ( node_index ) ` ' raise ValueError ( 'Only provide the input_shape OR ' output_shapes : list of output shape tuples . self.dtype = dtype # Check for redundancy in inputs . training . If instead you would like to use your own Stacked array of transformed kernels . def save_model ( model , filepath , overwrite=True , include_optimizer=True ) : compatible with ` CuDNNGRU ` . def test_on_batch ( self , x , y , validation_split : Float between 0 and 1 . # List of tensors , created by outbound_layer.call ( ) . if len ( depth_keys ) > 1 : raise ValueError ( 'Asked to get ' + attr_name ' ` shape ` does not include the batch ' node_index = node.node_indices [ j ] name = prefix + ' _ ' + str ( K.get_uid ( prefix ) ) return outputs # We 've already covered the input layers # We create an input node , which we will keep updated a generator for the validation data List of shape tuples ( or single tuple ) , one tuple per input . custom_objects=custom_objects ) import h5py self._losses += losses weights = [ ] str ( spec.shape ) + ' , found shape= ' output_tensors.append ( layer_output_tensors [ tensor_index ] ) independently manipulable ) . for x , y , mask in zip ( reference_output_tensors , output_tensors , output_masks ) : self.min_ndim = min_ndim # Update _keras_shape . if shape [ :2 ] ! = ( layer.kernel_size [ 0 ] , 1 ) or shape [ 3 ] ! = layer.filters : # to the index of the nodes that are saved in the config . def predict_proba ( self , x , batch_size=None , verbose=0 , steps=None ) : output of get_config . try : ValueError : in case the layer is missing shape information initializer=None , original_backend ) `` `` '' Util hared between different serialization methods . ` None ` defaults to sample-wise weights ( 1D ) . for conf in config [ 1 : ] : # Keep track of losses that depend on the inputs ' is not connected , no input to return . ' ) raise ValueError ( 'You called ` set_weights ( weights ) ` on layer `` ' raise ImportError ( ' ` save_weights ` requires h5py . ' ) symbolic_weights = layer.weights layer : The layer . or ` batch_input_shape ` as well as ` dtype ` ) . 'For multi-output layers , ' output_shape_keys = [ ] output_tensors = _to_list ( layer.call ( computed_tensors , * * kwargs ) ) # Collect input shapes to build layer . str ( axis ) + ' of input shape to have ' # to non-input layers . weight_value_tuples += zip ( symbolic_weights , weight_values ) and weights file and skip_mismatch=False . you can pass a 2D array with shape if model_config is None : max_queue_size=10 , n_gates : Number of gates ( 4 for LSTM , 3 for GRU ) . means is that it should have received an ` input_shape ` if target_class in [ 'GRU ' , 'CuDNNGRU ' ] and len ( weights ) == 3 : ' input tensors . Input received : ' ( e.g . build a new computational graph from the provided inputs ) . layers = [ ] try : `` `` '' Computes output tensors for new inputs . # User-provided arguments validation . if not dtype : from .network import Network , get_source_inputs dtype # note that 'dtype ' , 'input_shape ' and 'batch_input_shape ' def Input ( shape=None , batch_shape=None , self.dtype = dtype In this case you should make sure to specify num_weights = len ( [ l for l in sublayer.weights dtype : Datatype of the input . self.output_layers_node_indices.append ( node_index ) regularizer=None , if 'weights ' in kwargs : if output_masks is None : tensor_index = node.tensor_indices [ i ] method of the layer was called ) . `` `` '' Adds losses to the layer . if 'mask ' not in kwargs : saving.load_weights_from_hdf5_group_by_name ( model.add ( Dense ( 32 ) ) attributes that allow us to build a Keras model ` keras.models.model_from_json ( json_string , custom_objects= { } ) ` . You can set it to a custom function return self._losses f , 'layer_names ' , [ layer.name.encode ( 'utf8 ' ) for layer in layers ] ) if not val.shape : the model 's configuration ( topology ) inputs_hash = _object_list_uid ( inputs ) if not isinstance ( layer.input_spec , list ) : for layer in self.layers [ 0 ] .layers : to apply a different weight to every timestep of every sample . # Create an input node to add to self.outbound_node uses_lp = getattr ( self , 'uses_learning_phase ' , False ) or uses_lp def __repr__ ( self ) : ( during training only ) . This can be useful to tell the model to str ( mask ) ) config = self.get_config ( ) `` `` '' Save a model to a HDF5 file . updates += layer.get_updates_for ( inputs ) We call self._add_inbound_node ( ) . able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes . inputs_hash = None 'Conv3D ' , `` `` '' This is where the layer 's logic lives . ' ' + str ( len ( output_tensors ) ) + ' output tensors and ' where there is a mismatch in the number of weights , sample_weight=None ) : for j in range ( len ( output_shapes ) ) : new_weights.extend ( preprocess_weights_for_loading ( self.name + ' : expected shape= ' # Check that no item in ` data ` is larger than ` HDF5_OBJECT_HEADER_LIMIT ` self.output_tensors = output_tensors original_backend ) `` `` '' Retrieves the input shape tuple ( s ) of a layer . Prefer using ` layer.get_input_shape_for ( input_shape ) ` , _ , output_masks , _ = self.run_internal_graph ( inputs , masks ) TypeError : if ` obj ` can not be serialized . steps=steps ) ' weights , but the saved weights have '","['examples/cifar10_cnn_capsule.py', 'keras/applications/densenet.py', 'keras/applications/inception_resnet_v2.py', 'keras/applications/inception_v3.py', 'keras/applications/mobilenet.py', 'keras/applications/nasnet.py', 'keras/applications/resnet50.py', 'keras/applications/vgg16.py', 'keras/applications/vgg19.py', 'keras/applications/xception.py', 'keras/callbacks.py', 'keras/engine/__init__.py', 'keras/engine/base_layer.py', 'keras/engine/input_layer.py', 'keras/engine/network.py', 'keras/engine/saving.py', 'keras/engine/sequential.py', 'keras/engine/topology.py', 'keras/engine/training.py', 'keras/layers/__init__.py', 'keras/layers/advanced_activations.py', 'keras/layers/convolutional.py', 'keras/layers/convolutional_recurrent.py', 'keras/layers/core.py', 'keras/layers/embeddings.py', 'keras/layers/local.py', 'keras/layers/merge.py', 'keras/layers/noise.py', 'keras/layers/normalization.py', 'keras/layers/pooling.py', 'keras/layers/recurrent.py', 'keras/layers/wrappers.py', 'keras/legacy/layers.py', 'keras/models.py', 'keras/utils/generic_utils.py', 'keras/utils/vis_utils.py', 'tests/keras/engine/test_topology.py', 'tests/keras/engine/test_training.py', 'tests/keras/layers/convolutional_test.py', 'tests/keras/layers/cudnn_recurrent_test.py', 'tests/keras/layers/recurrent_test.py', 'tests/keras/layers/wrappers_test.py']",Refactor topological part of ` engine ` module ( # 10023 )
474,73c90d014f024bfa13360c34fa99fe6220cc8e98,2018-04-23 21:57:57-07:00,"filter_shape=pointwise_kernel_shape , depthwise_kernel = depthwise_kernel [ : , : , : :-1 , : :-1 ] x : input tensor depthwise_kernel : convolution kernel for the depthwise convolution . conv_out = _postprocess_conv2d_output ( conv_out , x , padding , pointwise_kernel_shape = pointwise_kernel.eval ( ) .shape pointwise_kernel_shape , data_format : string , ` `` channels_last '' ` or ` `` channels_first '' ` . pointwise_kernel = _preprocess_conv2d_kernel ( pointwise_kernel , data_format ) depthwise_kernel = reshape ( depthwise_kernel , depthwise_kernel_shape ) # Will only work if ` pointwise_kernel ` is a shared variable . depthwise_kernel_shape = ( input_depth * output_depth , 1 ) + depthwise_kernel_shape [ 2 : ] raise ValueError ( 'Unknown data_format ' , data_format ) depthwise_kernel = depthwise_kernel.dimshuffle ( ( 1 , 0 , 2 , 3 ) ) raise NotImplementedError num_groups=input_depth ) image_shape = _preprocess_conv2d_image_shape ( int_shape ( x ) , data_format ) border_mode=th_padding , subsample= ( 1 , 1 ) , filter_dilation=dilation_rate ) Output tensor . @ pytest.mark.skipif ( K.backend ( ) == 'theano ' , reason='Theano does not support it yet ' ) data_format = image_data_format ( ) pointwise_kernel : kernel for the 1x1 convolution . @ pytest.mark.skipif ( K.backend ( ) == 'theano ' , reason='Not supported . ' ) # Arguments ValueError : if ` data_format ` is neither ` `` channels_last '' ` or ` `` channels_first '' ` . padding : string , ` `` same '' ` or ` `` valid '' ` . depthwise_kernel_shape = depthwise_kernel.eval ( ) .shape depthwise_kernel_shape = _preprocess_conv2d_filter_shape ( depthwise_kernel_shape , data_format ) pointwise_kernel_shape = _preprocess_conv2d_filter_shape ( pointwise_kernel_shape , data_format ) if data_format not in { 'channels_first ' , 'channels_last ' } : `` `` '' dilation rates for the separable convolution . if hasattr ( depthwise_kernel , '_keras_shape ' ) : if data_format is None : depthwise_kernel = _preprocess_conv2d_kernel ( depthwise_kernel , data_format ) return conv_out filter_dilation=dilation_rate , input_shape=None , input_depth = depthwise_kernel_shape [ 1 ] conv_out = T.nnet.conv2d ( conv_out , pointwise_kernel , input_shape=image_shape , # Will only work if ` depthwise_kernel ` is a shared variable . subsample=strides , # Raises filter_shape=depthwise_kernel_shape , x = _preprocess_conv2d_input ( x , data_format ) if hasattr ( x , '_keras_shape ' ) : if hasattr ( pointwise_kernel , '_keras_shape ' ) : `` `` '' 2D convolution with separable filters . depthwise_kernel_shape = depthwise_kernel._keras_shape strides , data_format ) conv_out = T.nnet.conv2d ( x , depthwise_kernel , else : dilation_rate : tuple of integers , # Returns pointwise_kernel_shape = pointwise_kernel._keras_shape th_padding = _preprocess_padding ( padding ) strides : strides tuple ( length 2 ) . image_shape = None output_depth = depthwise_kernel_shape [ 0 ]","['keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py', 'tests/keras/layers/convolutional_test.py']",Add ` separable_conv2d ` for Theano ( # 10003 )
475,b09ec1c9bf827622a5eebb94ba0af5bf202ca359,2018-04-23 12:43:48-07:00,"inconditional , or conditional on inputs to this model If it imports without error it is installed ortherwise you can find detaild If it imports without error it is installed otherwise you can find detailed unconditional , or conditional on inputs to this model # The chunking of layer names array should have happened . # The chunking of layer names array should have happend .","['docs/templates/getting-started/faq.md', 'keras/engine/topology.py', 'tests/test_model_saving.py']",fixing typos ( # 10016 )
476,b26fa3b63870b6160340bfeeb384b8d812dbbd15,2018-04-22 19:47:37-07:00,"will normalize each channel with respect to the ImageNet dataset . mode : One of `` caffe '' , `` tf '' . torch : will scale pixels between 0 and 1 and then mode : One of `` caffe '' , `` tf '' or `` torch '' .",['keras/applications/imagenet_utils.py'],Sync docstring of preprocess_input with _preprocess_numpy_input and _preprocess_symbolic_input in imagenet_utils.py ( # 10006 )
477,d673afd5979a4e541266763f65bbd65fabf20b0b,2018-04-17 12:56:32-07:00,"y = func ( x , w , args [ 2 ] , args [ 3 ] ) if x.ndim == 3 : import scipy.signal as signal if x.ndim == 3 : from __future__ import division return wrapper y.append ( _y ) for ( m , m1 ) in zip ( range ( pool_size [ 2 ] ) , range ( -pool_size [ 2 ] , 0 ) ) : w = np.transpose ( w , ( 2 , 3 , 0 , 1 ) ) if data_format == 'channels_last ' : y = np.transpose ( y , ( 0 , 2 , 1 ) ) _y.append ( np.sum ( np.stack ( __y , axis=-1 ) , axis=-1 ) ) if mask is not None : if y.ndim == 3 : _y = [ ] if w_h is not None : x = args [ 0 ] # indexing trick h.append ( h_t + h_t1 ) if np_mask is not None : elif pool_mode == 'max ' : x2 = ref_depthwise_conv ( x , w1 , padding , data_format ) h_t1 [ np_mask [ : , t ] == 0 ] = prev [ np_mask [ : , t ] == 0 ] w = args [ 1 ] x = np.transpose ( x , ( 0 , 2 , 1 ) ) pad = [ ( 0 , 0 ) , ( 0 , 0 ) ] + [ ( s // 2 , s // 2 ) for s in pool_size ] if padding == 'same ' : return wrapper for ( i , t ) in enumerate ( t_list ) : w = np.flip ( np.fliplr ( np.flipud ( w ) ) , axis=2 ) for i in range ( x.shape [ 0 ] ) : y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) else : __y = [ ] o_t = np.dot ( o_t , w_o ) y = np.max ( y , axis=-1 ) h = [ ] for ( k , k1 ) in zip ( range ( pool_size [ 0 ] ) , range ( -pool_size [ 0 ] , 0 ) ) : y = [ ] for ( k , k1 ) in zip ( range ( pool_size [ 0 ] ) , range ( -pool_size [ 0 ] , 0 ) ) ] __y.append ( signal.convolve ( x [ i , j ] , w [ j , k ] , mode=padding ) ) y = [ x [ : , : , k : k1 : strides [ 0 ] ] w = args [ 1 ] np_mask = None return conv ( x2 , w2 , padding , data_format ) if y.ndim == 3 : for k in range ( w.shape [ 0 ] ) : x2 = depthwise_conv ( x , w1 , padding , data_format ) for j in range ( w.shape [ 0 ] ) : def conv ( x , w , padding , data_format ) : y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) _y.append ( np.stack ( __y , axis=0 ) ) for ( k , k1 ) in zip ( range ( pool_size [ 0 ] ) , range ( -pool_size [ 0 ] , 0 ) ) ] if go_backwards : y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) if padding == 'same ' : _y.append ( np.sum ( np.stack ( __y , axis=-1 ) , axis=-1 ) ) o_t = np.dot ( o_t , w_o ) t_list = range ( x.shape [ 1 ] - 1 , -1 , -1 ) y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) return y import scipy.signal as signal y = func ( x , w , args [ 2 ] , args [ 3 ] ) y1 = ref_conv ( x , w , padding , data_format ) return o [ -1 ] , np.stack ( o , axis=1 ) , np.stack ( h , axis=1 ) x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) y = np.array ( y ) for k in range ( w.shape [ 0 ] ) : def ref_depthwise_conv ( x , w , padding , data_format ) : __y = [ ] for ( k , k1 ) in zip ( range ( pool_size [ 0 ] ) , range ( -pool_size [ 0 ] , 0 ) ) : y1 = reference_operations.depthwise_conv ( x , w , padding , data_format ) def pool ( x , pool_size , strides , padding , data_format , pool_mode ) : h_t1 = np.dot ( prev , w_h ) x = np.transpose ( x , ( 0 , 2 , 1 ) ) for ( i , t ) in enumerate ( t_list ) : def wrapper ( * args ) : y1 = ref_pool ( x , pool_size , strides , padding , data_format , pool_mode ) elif x.ndim == 4 : from keras import backend as K y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) w = np.transpose ( w , ( 2 , 3 , 0 , 1 ) ) y = np.transpose ( y , ( 0 , 2 , 1 ) ) from __future__ import print_function for k in range ( w.shape [ 1 ] ) : x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) go_backwards=False , mask=None ) prev = h [ i - 1 ] if i > 0 else init for j in range ( w.shape [ 0 ] ) : y = np.stack ( y , axis=-1 ) h = [ ] if data_format == 'channels_last ' : for k in range ( w.shape [ 1 ] ) : _y.append ( np.stack ( __y , axis=0 ) ) w = np.fliplr ( np.flipud ( w ) ) w = np.flipud ( w ) y.append ( _y ) y1 = ref_depthwise_conv ( x , w , padding , data_format ) y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) `` `` '' Utilities for backend functionality checks . '' '' '' return y def depthwise_conv ( x , w , padding , data_format ) : if np_mask is not None : for ( m , m1 ) in zip ( range ( pool_size [ 2 ] ) , range ( -pool_size [ 2 ] , 0 ) ) : x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) import reference_operations y1 = reference_operations.conv ( x , w , padding , data_format ) y.append ( x [ : , : , k : k1 : strides [ 0 ] , l : l1 : strides [ 1 ] ] ) _y = [ ] if np_mask is not None : elif y.ndim == 4 : if y.ndim == 3 : y = np.mean ( np.ma.masked_invalid ( y ) , axis=-1 ) .data elif pool_mode == 'max ' : def wrapper ( * args ) : else : else : return ref_conv ( x2 , w2 , padding , data_format ) if args [ 3 ] == 'channels_last ' : else : w = np.transpose ( w , ( 3 , 4 , 0 , 1 , 2 ) ) if x.ndim == 3 : elif y.ndim == 4 : def normalize_conv ( func ) : x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) y = np.transpose ( y , ( 0 , 2 , 1 ) ) from __future__ import absolute_import h_t1 = 0 x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) if np_mask is not None : x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) pad = [ ( 0 , 0 ) , ( 0 , 0 ) ] + [ ( s // 2 , s // 2 ) for s in pool_size ] y = [ x [ : , : , k : k1 : strides [ 0 ] ] prev = h [ i - 1 ] if i > 0 else init y.append ( x [ : , : , k : k1 : strides [ 0 ] , l : l1 : strides [ 1 ] ] ) w = np.flip ( np.fliplr ( np.flipud ( w ) ) , axis=2 ) np_mask = K.eval ( mask ) y = np.stack ( y , axis=-1 ) import numpy as np if pool_mode == 'avg ' : for j in range ( w.shape [ 1 ] ) : w_i , w_h , w_o = w if w_h is not None : t_list = range ( x.shape [ 1 ] - 1 , -1 , -1 ) o.append ( o_t ) y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) def separable_conv ( x , w1 , w2 , padding , data_format ) : return y elif x.ndim == 4 : if y.ndim == 3 : y = np.transpose ( y , ( 0 , 2 , 1 ) ) def ref_pool ( x , pool_size , strides , padding , data_format , pool_mode ) : for i in range ( x.shape [ 0 ] ) : x = np.pad ( x , [ ( 0 , 0 ) , ( 0 , 0 ) ] + [ ( 0 , 1 ) for _ in pool_size ] , def ref_rnn ( x , w , init , go_backwards=False , mask=None , unroll=False , input_length=None ) : y = [ ] np_mask = None o_t = h_t + h_t1 elif y.ndim == 4 : h_t = np.dot ( x [ : , t ] , w_i ) if args [ 3 ] == 'channels_last ' : x = np.transpose ( x , ( 0 , 2 , 1 ) ) def ref_separable_conv ( x , w1 , w2 , padding , data_format ) : @ normalize_ref_conv y1 = ref_separable_conv ( x , depthwise , pointwise , padding , data_format ) h_t1 [ np_mask [ : , t ] == 0 ] = prev [ np_mask [ : , t ] == 0 ] w = np.transpose ( w , ( 3 , 4 , 0 , 1 , 2 ) ) np_mask = K.eval ( mask ) __y.append ( signal.convolve ( x [ i , k ] , w [ k , j ] , mode=padding ) ) return y for ( l , l1 ) in zip ( range ( pool_size [ 1 ] ) , range ( -pool_size [ 1 ] , 0 ) ) : w = np.transpose ( w , ( 1 , 2 , 0 ) ) def ref_conv ( x , w , padding , data_format ) : else : y = np.mean ( np.ma.masked_invalid ( y ) , axis=-1 ) .data elif y.ndim == 4 : if x.ndim == 3 : __y.append ( signal.convolve ( x [ i , k ] , w [ k , j ] , mode=padding ) ) y1 = reference_operations.separable_conv ( x , depthwise , pointwise , padding , data_format ) x = np.pad ( x , pad , 'constant ' , constant_values=-np.inf ) h_t1 = 0 t_list = range ( x.shape [ 1 ] ) o = [ ] def normalize_ref_conv ( func ) : y.append ( np.concatenate ( _y , axis=0 ) ) __y.append ( signal.convolve ( x [ i , j ] , w [ j , k ] , mode=padding ) ) h_t = h_t * np_mask [ : , t ] .reshape ( -1 , 1 ) t_list = range ( x.shape [ 1 ] ) w = np.transpose ( w , ( 1 , 2 , 0 ) ) h.append ( h_t + h_t1 ) last_y1 , y1 , h1 = reference_operations.rnn ( x , [ wi , wh , None ] , h0 , * * kwargs ) h_t = np.dot ( x [ : , t ] , w_i ) y.append ( x [ : , : , k : k1 : strides [ 0 ] , l : l1 : strides [ 1 ] , m : m1 : strides [ 2 ] ] ) y = np.max ( y , axis=-1 ) elif x.ndim == 4 : w_i , w_h , w_o = w if pool_mode == 'avg ' : def rnn ( x , w , init , go_backwards=False , mask=None , unroll=False , input_length=None ) : else : x = np.transpose ( x , ( 0 , 2 , 1 ) ) last_y1 , y1 , h1 = reference_operations.rnn ( x , [ wi , None , None ] , None , y = [ ] # indexing trick 'constant ' , constant_values=0 ) last_y1 , y1 , h1 = ref_rnn ( x , [ wi , None , None ] , None , if args [ 3 ] == 'channels_last ' : o = [ ] elif x.ndim == 4 : y.append ( np.concatenate ( _y , axis=0 ) ) go_backwards=False , mask=None ) w = np.fliplr ( np.flipud ( w ) ) x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) o.append ( o_t ) x = np.pad ( x , [ ( 0 , 0 ) , ( 0 , 0 ) ] + [ ( 0 , 1 ) for _ in pool_size ] , x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) for ( l , l1 ) in zip ( range ( pool_size [ 1 ] ) , range ( -pool_size [ 1 ] , 0 ) ) : if go_backwards : if mask is not None : if args [ 3 ] == 'channels_last ' : y = [ ] return o [ -1 ] , np.stack ( o , axis=1 ) , np.stack ( h , axis=1 ) 'constant ' , constant_values=0 ) x = np.pad ( x , pad , 'constant ' , constant_values=-np.inf ) y1 = reference_operations.pool ( x , pool_size , strides , padding , data_format , pool_mode ) w = np.flipud ( w ) x = args [ 0 ] y = np.array ( y ) if w_o is not None : y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) if w_o is not None : h_t1 = np.dot ( prev , w_h ) o_t = h_t + h_t1 for j in range ( w.shape [ 1 ] ) : y.append ( x [ : , : , k : k1 : strides [ 0 ] , l : l1 : strides [ 1 ] , m : m1 : strides [ 2 ] ] ) last_y1 , y1 , h1 = ref_rnn ( x , [ wi , wh , None ] , h0 , * * kwargs ) h_t = h_t * np_mask [ : , t ] .reshape ( -1 , 1 )","['tests/keras/backend/backend_test.py', 'tests/keras/backend/reference_operations.py']",Immigrate reference operations to a separate module ( # 9948 )
478,a8ca67f281bdc5d751204e3b2bf6e98731513a27,2018-04-17 10:19:47-07:00,"# False = test , True = train out = model.predict ( input_4 ) _LEARNING_PHASE_PLACEHOLDER.value = np.asarray ( value ) _LEARNING_PHASE_PLACEHOLDER.value = np.asarray ( 1.0 ) input_4 = np.expand_dims ( np.arange ( 10 . ) , axis=1 ) _LEARNING_PHASE = -1 K.set_learning_phase ( 1 ) model = Model ( input , x ) np.array ( [ bn_mean ] ) , np.array ( [ bn_std * * 2 ] ) ] ) _LEARNING_PHASE.value = np.asarray ( value ) # static learning phase flag , if it is not 0 or 1 , we will go with dynamic learning phase tensor . if K.backend ( ) == 'tensorflow ' or K.backend ( ) == 'cntk ' : def test_batchnorm_trainable ( ) : return _LEARNING_PHASE result = C.element_select ( training , x , alt ) # LEARNING_PHASE_PLACEHOLDER is the placeholder for dynamic learning phase # If _LEARNING_PHASE is not 0 or 1 , return dynamic learning phase tensor `` `` '' else : model.set_weights ( [ np.array ( [ 1 . ] ) , np.array ( [ 0 . ] ) , if self.unrelated_updates is None and _LEARNING_PHASE.value == 1.0 : model = get_model ( bn_mean , bn_std ) def get_model ( bn_mean , bn_std ) : return model ( _LEARNING_PHASE_PLACEHOLDER.value == 1.0 or _LEARNING_PHASE == 1 ) ) : # if _LEARNING_PHASE is static _LEARNING_PHASE = C.constant ( shape= ( ) , dtype=np.float32 , value=1.0 , name='_keras_learning_phase ' ) `` `` '' Reset learning phase flag for cntk backend . _LEARNING_PHASE_PLACEHOLDER = C.constant ( shape= ( ) , dtype=np.float32 , value=1.0 , name='_keras_learning_phase ' ) if tensor == _LEARNING_PHASE_PLACEHOLDER : assert_allclose ( ( input_4 - np.mean ( input_4 ) ) / np.std ( input_4 ) , out , atol=1e-3 ) global _LEARNING_PHASE model.compile ( loss='mse ' , optimizer='rmsprop ' ) return _LEARNING_PHASE if _LEARNING_PHASE in { 0 , 1 } else _LEARNING_PHASE_PLACEHOLDER if tensor == _LEARNING_PHASE : if K.backend ( ) == 'tensorflow ' : input = Input ( shape= ( 1 , ) ) _LEARNING_PHASE = -1 result = x if training == 1 or training is True else alt global _LEARNING_PHASE_PLACEHOLDER if isinstance ( training , int ) or isinstance ( training , bool ) : result = C.element_select ( training , x , alt ) # Simulates training-mode with trainable layer . Should use mini-batch statistics . v = np.asarray ( value ) _LEARNING_PHASE.value = v global _LEARNING_PHASE_PLACEHOLDER bn_mean = 0.5 _LEARNING_PHASE = value if ( self.unrelated_updates is None and def clear_session ( ) : x = normalization.BatchNormalization ( ) ( input ) bn_std = 10 .","['keras/backend/cntk_backend.py', 'keras/utils/test_utils.py', 'tests/keras/layers/normalization_test.py']",Chenta/cntk bn ( # 9952 )
479,3578599b6499eac4dc1f320ec5e7d96433777ee8,2018-04-15 18:41:38-07:00,"x = K.variable ( np.random.random ( input_shape ) ) K.pool2d ( x , pool_size=pool_size , padding='twice ' ) with pytest.raises ( ValueError ) : K.variable ( np.ones ( ( 2 , 2 , 2 , 3 , 4 ) ) ) , k.variable ( np.ones ( ( 1 , 1 , 12 , 7 ) ) ) , data_format='channels_middle ' ) data_format='channels_middle ' ) k.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , k.pool2d ( x , pool_size=pool_size , padding='twice ' ) if len ( pool_size ) == 2 : K.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , x = k.variable ( np.random.random ( input_shape ) ) k.variable ( np.ones ( ( 2 , 2 , 2 , 3 , 4 ) ) ) , data_format='channels_middle ' ) K.variable ( np.ones ( ( 1 , 1 , 12 , 7 ) ) ) , k.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , if K.backend ( ) ! = 'theano ' : K.pool2d ( x , pool_size=pool_size , data_format='channels_middle ' ) k.conv3d ( k.variable ( np.ones ( ( 2 , 3 , 4 , 5 , 4 ) ) ) , data_format='channels_middle ' ) k.depthwise_conv2d ( k.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , K.depthwise_conv2d ( K.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , k.pool2d ( x , pool_size=pool_size , data_format='channels_middle ' ) k.variable ( np.ones ( ( 3 , 2 , 3 ) ) ) , data_format='channels_middle ' ) k.pool2d ( x , pool_size=pool_size , pool_mode='median ' ) k.pool3d ( x , pool_size=pool_size , padding='twice ' ) else : K.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , K.variable ( np.ones ( ( 3 , 2 , 3 ) ) ) , with pytest.raises ( ValueError ) : with pytest.raises ( ValueError ) : k.pool3d ( x , pool_size=pool_size , pool_mode='median ' ) with pytest.raises ( ValueError ) : k.separable_conv2d ( k.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , K.conv2d ( K.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , K.conv1d ( K.variable ( np.ones ( ( 4 , 8 , 2 ) ) ) , k.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , K.pool2d ( x , pool_size=pool_size , pool_mode='median ' ) K.pool3d ( x , pool_size=pool_size , data_format='channels_middle ' ) data_format='channels_middle ' ) else : K.conv3d ( K.variable ( np.ones ( ( 2 , 3 , 4 , 5 , 4 ) ) ) , K.separable_conv2d ( K.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , if k ! = KTH : with pytest.raises ( ValueError ) : for k in BACKENDS : for k in BACKENDS : K.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , k.conv2d ( k.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , K.pool3d ( x , pool_size=pool_size , pool_mode='median ' ) k.pool3d ( x , pool_size=pool_size , data_format='channels_middle ' ) if len ( pool_size ) == 2 : k.conv1d ( k.variable ( np.ones ( ( 4 , 8 , 2 ) ) ) , K.pool3d ( x , pool_size=pool_size , padding='twice ' )",['tests/keras/backend/backend_test.py'],Make conv_invalid_use and pooling_invalid_use efficient ( # 9944 )
480,ce13af5785bbb7a2be7d42699cfe25117177ddc7,2018-04-14 12:14:42-07:00,"weights == 'imagenet ' ) : if ( isinstance ( input_shape , tuple ) and None in input_shape and raise ValueError ( 'When specifying the input shape of a NASNet ' ' and loading ` ImageNet ` weights , ' 'the input_shape argument must be static ' ' ( no None entries ) . Got : ` input_shape= ' str ( input_shape ) + ' ` . ' )",['keras/applications/nasnet.py'],Added an error message for undefined shape on NASNet . ( # 9891 )
481,5422fdd38baad36730cb6aeb946e17eeae6a551c,2018-04-14 12:13:23-07:00,"# is not a multiple of ` batch_size ` . actual_batches = len ( y ) reverse=False , lengths , actual_sequences = sum ( len ( _y ) for _y in y ) x = np.array ( [ [ i ] for i in range ( 10 ) ] ) shuffles ) : % ( self.start_index , self.end_index ) ) 'would be left to be used as current step . ' for stride , length , batch_size , shuffle in zip ( strides , from numpy.testing import assert_allclose strides = ( 1 , 1 , 5 , 7 , 3 , 5 , 3 ) assert expected_sequences == actual_sequences for length in range ( 3 , 10 ) : length=length , with assert_raises ( ValueError ) as context : shuffles = ( False , True , True , False , False , False , False ) # last batch will be different if ` ( samples - length ) / stride ` ( self.end_index - self.start_index ) / from numpy.testing import assert_allclose , assert_raises self.stride , self.end_index + 1 ) , self.stride ) expected_sequences = ceil ( ( 23 - length ) / float ( stride ) ) ( self.end_index - self.start_index + 1 ) / assert len ( data_gen ) == 6 else : expected_batches = ceil ( expected_sequences / float ( batch_size ) ) TimeseriesGenerator ( data , targets , length=50 ) batch_size=1 ) sampling_rate=1 , y = [ g [ ix ] [ 1 ] for ix in range ( len ( g ) ) ] start_index=0 , error = str ( context.exception ) self.start_index , self.end_index + 1 , size=self.batch_size ) def test_TimeSeriesGenerator_doesnt_miss_any_sample ( ) : lengths = ( 3 , 3 , 4 , 3 , 1 , 3 , 7 ) stride=stride , assert_allclose ( y , expected ) 'is disallowed , as no part of the sequence ' if self.start_index > self.end_index : assert len ( data_gen ) == 5 assert ' ` start_index+length=50 > end_index=49 ` is disallowed ' in error assert expected_batches == actual_batches # all batches have the same size when shuffle is True . expected = max ( 0 , len ( x ) - length ) if shuffle : self.start_index , self.end_index , size=self.batch_size ) ( 23 - length ) / float ( batch_size * stride ) ) * batch_size batch_sizes , y = np.concatenate ( [ g [ ix ] [ 1 ] for ix in range ( len ( g ) ) ] , axis=0 ) actual = len ( g ) x = np.array ( [ [ i ] for i in range ( 23 ) ] ) self.stride , self.end_index ) , self.stride ) batch_size=batch_size ) batch_sizes = ( 6 , 6 , 6 , 5 , 6 , 6 , 6 ) if len ( g ) > 0 : g = TimeseriesGenerator ( x , x , from math import ceil assert expected == actual expected = np.arange ( length , 10 ) .reshape ( -1 , 1 ) expected_sequences = ceil ( shuffle=shuffle , # All elements in range ( length , 10 ) should be used as current step raise ValueError ( ' ` start_index+length= % i > end_index= % i ` ' end_index=None ,","['keras/preprocessing/sequence.py', 'tests/keras/preprocessing/sequence_test.py']",fix TimeSeriesGenerator glitch ( # 9899 )
482,6171b3656ebd9b6038f709ba83f7475de284ba4e,2018-04-14 11:53:05-07:00,"MNIST dataset with TensorFlow 's Dataset API . dataset = Dataset.from_tensor_slices ( ( x_train , y_train ) ) Note that from TensorFlow 1.4 , tf.contrib.data is deprecated from tensorflow.contrib.data import Dataset dataset = tf.data.Dataset.from_tensor_slices ( ( x_train , y_train ) ) and tf.data is preferred . See the release notes for details . [ mnist_dataset_api.py ] ( mnist_dataset_api.py )","['examples/README.md', 'examples/mnist_dataset_api.py']",Updated for TF 1.7 ( # 9937 )
483,083a41cc6be7b1796e3817df198d1557bb8557b8,2018-04-13 17:29:43-07:00,"import h5py If it imports without error it is installed ortherwise you can find detaild a dependency of Keras and should be installed by default . On Debian-based sudo apt-get install libhdf5-serial-dev in the FAQ for instructions on how to install ` h5py ` . `` `` '' Saves the weights of a model . Note : Please also see [ How can I install HDF5 or h5py to save my models in Keras ? ] ( /getting-started/faq/ # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) in the FAQ for instructions on how to install ` h5py ` . ` keras.callbacks.ModelCheckpoint ` , Keras uses the h5py Python package . It is ` model.load_weights ( filepath , by_name=False ) ` : loads the weights of the model from a HDF5 file ( created by ` save_weights ` ) . By default , the architecture is expected to be unchanged . To load weights into a different architecture ( with some layers in common ) , use ` by_name=True ` to load only those layers with the same name . Note that you will first need to install HDF5 and the Python library h5py , which do not come bundled with Keras . Note : Please also see If you are unsure if h5py is installed you can open a Python shell and load the installation instructions here : http : //docs.h5py.org/en/latest/build.html In order to save your Keras models as HDF5 files , e.g . via `` `` '' `` ` Note : Please also see # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) module via Please also see [ How can I install HDF5 or h5py to save my models in Keras ? ] ( # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras ) for instructions on how to install ` h5py ` . distributions , you will have to additionally install ` libhdf5 ` : ` model.load_weights ( filepath , by_name=False ) ` : loads the weights of the model from a HDF5 file ( created by ` save_weights ` ) . By default , the architecture is expected to be unchanged . To load weights into a different architecture ( with some layers in common ) , use ` by_name=True ` to load only those layers with the same name . # # # How can I install HDF5 or h5py to save my models in Keras ? /getting-started/faq/ in the FAQ for instructions on how to install ` h5py ` . [ How can I install HDF5 or h5py to save my models in Keras ? ] ( [ How can I install HDF5 or h5py to save my models in Keras ? ] ( [ How can I install HDF5 or h5py to save my models in Keras ? ] ( # how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras )","['docs/templates/getting-started/faq.md', 'docs/templates/models/about-keras-models.md', 'keras/models.py']",Added note to manually install h5py where needed ( # 9830 )
484,adc321b4d7a4e22f6bdb00b404dfe5e23d4887aa,2018-04-13 15:36:29-07:00,"m.reset_states ( ) metrics= { 'out ' : [ 'acc ' , metric_fn ] } ) loss='binary_crossentropy ' , def test_stateful_metrics ( ) : if metrics_mode == 'list ' : outputs = keras.layers.Dense ( 1 , activation='sigmoid ' , name='out ' ) ( inputs ) metrics= [ 'acc ' , metric_fn ] ) model.compile ( optimizer='sgd ' , m.reset_states ( ) for m in self.metrics : metrics= [ 'acc ' , metric_fn ] ) for m in self.stateful_metric_functions : def test_stateful_metrics ( metrics_mode ) : elif metrics_mode == 'dict ' : for m in self.metrics : m.reset_states ( ) self.stateful_metric_functions = [ ] outputs = keras.layers.Dense ( 1 , activation='sigmoid ' ) ( inputs ) if isinstance ( m , Layer ) and m.stateful : m.reset_states ( ) for i , m in enumerate ( self.metrics ) : if isinstance ( m , Layer ) and m.stateful : model.compile ( optimizer='sgd ' , loss='binary_crossentropy ' , self.stateful_metric_functions.append ( metric_fn ) for m in self.stateful_metric_functions :","['keras/engine/training.py', 'tests/keras/metrics_test.py']",Fix stateful metrics when passing dict to compile ( # 9894 )
485,0bcd9061856d699d9054f9475b5e59c51348319e,2018-04-13 15:20:03-07:00,"result = sum ( x * y , axis=axes [ 0 ] , keepdims=True ) else : return sum ( x * transpose ( y ) , axis=axes [ 0 ] , keepdims=True ) return result if axes [ 0 ] == 1 else transpose ( result ) return sum ( x * y , axis=1 , keepdims=True ) if axes [ 0 ] == axes [ 1 ] :",['keras/backend/cntk_backend.py'],Fix ` batch_dot ` of CNTK when ` axes=None ` ( # 9921 )
486,5caec3330b5ecbeb138026bd2be9b1adf6a4e2cc,2018-04-13 15:19:37-07:00,"axes [ 0 ] = x.ndim - 1 if axes [ 0 ] == 0 : if axes [ 1 ] == 0 : axes = list ( axes ) # which contains the batch axis ( 0 ) x = transpose ( x ) y = transpose ( y ) axes [ 1 ] = y.ndim - 1 # workaround because theano does n't accept axes if isinstance ( axes , tuple ) :",['keras/backend/theano_backend.py'],Fix ` batch_dot ` of Theano when ` axes=0 ` ( # 9920 )
487,388a9ab4cd47d76e6feb592a24d76503a692a286,2018-04-13 15:14:56-07:00,"from .. models import clone_model with tf.device ( '/cpu:0 ' if cpu_merge else '/gpu : % d ' % target_gpu_ids [ 0 ] ) : model = multi_gpu_model ( model , cpu_relocation=True ) `` ` python def multi_gpu_model ( model , gpus=None , cpu_merge=True , cpu_relocation=False ) : model = Xception ( weights=None , .. ) try : if cpu_relocation : cpu_merge : A boolean value to identify whether to force model.compile ( .. ) def multi_gpu_model ( model , gpus=None ) : # Example with tf.device ( '/cpu:0 ' ) : # Merge outputs on CPU . .. with tf.device ( '/cpu:0 ' ) : print ( `` Training using single GPU or CPU .. '' ) If the model is not defined under any preceding device model = clone_model ( model ) # Example 2 - Training models with weights merge on CPU using cpu_relocation except : scope , you can still rescue it by activating this option . merging model weights under the scope of the CPU or not . `` ` # Example 3 - Training models with weights merge on GPU ( recommended for NV-link ) # Relocate the model definition under CPU device scope if needed # Merge outputs under expected scope . # Not needed to change the device scope for model definition : model = multi_gpu_model ( model , cpu_merge=False ) # Example 1 - Training models with weights merge on CPU create the model 's weights under the scope of the CPU . print ( `` Training using multiple GPUs .. '' ) cpu_relocation : A boolean value to identify whether to",['keras/utils/multi_gpu_utils.py'],multi_gpu_model supporting legacy/fullCPU/fullGPU ( # 9638 )
488,10533e08e78c1675ef30f9b38b974735247a4507,2018-04-12 09:34:18-04:00,"( epoch + 1 , self.monitor ) ) ( epoch + 1 , self.monitor , self.best ) ) print ( '\nEpoch % 05d : % s did not improve from % 0.5f ' % print ( '\nEpoch % 05d : % s did not improve ' %",['keras/callbacks.py'],ModelCheckpoint : print previous best ( # 9911 )
489,a053416a310868971a5ec743e714961e28d78c79,2018-04-11 13:57:39-07:00,"classes : optional list of class subdirectories ( e.g . ` [ 'dogs ' , 'cats ' ] ` ) . to input images ( mainly used to work with autoencoders ) . Default : `` categorical '' . Determines the type of label arrays that are `` sparse '' will be 1D integer labels , `` input '' will be images identical to input images ( mainly used to work with autoencoders ) . `` binary '' will be 1D binary labels , `` sparse '' will be 1D integer labels , `` input '' will be images identical If not provided , the list of classes will be automatically ` y ` is a numpy array of corresponding labels . inferred from the subdirectory names/structure under ` directory ` , class_mode : one of `` categorical '' , `` binary '' , `` sparse '' , `` input '' or None . Default : `` categorical '' . classes : optional list of class subdirectories ( e.g . ` [ 'dogs ' , 'cats ' ] ` ) . Default : None . class_mode : one of `` categorical '' , `` binary '' , `` sparse '' , `` input '' or None . Determines the type of label arrays that are returned : `` categorical '' will be 2D one-hot encoded labels , A DirectoryIterator yielding tuples of ` ( x , y ) ` where ` x ` is a numpy array of image data and A DirectoryIterator yielding tuples of ` ( x , y ) ` where ` x ` is a numpy array containing a batch of images with shape ` ( batch_size , * target_size , channels ) ` and ` y ` is a numpy array of corresponding labels . returned : `` categorical '' will be 2D one-hot encoded labels , `` binary '' will be 1D binary labels , be automatically inferred from the subdirectory names/structure under ` directory ` , Default : None . If not provided , the list of classes will",['keras/preprocessing/image.py'],Fix documentation of flow_from_directory ( ) ( # 9910 )
490,946a3b371567af143e13c159f00051353f161c56,2018-04-11 13:55:14-07:00,"# ` pydot ` is an optional dependency , except OSError : except ImportError : ' ` pydot ` failed to call GraphViz . ' import pydot_ng as pydot .pytest_cache except ImportError : # see ` extras_require ` in ` setup.py ` . 'visualize ' : [ 'pydot > =1.2.4 ' ] , # pydot-ng is a fork of pydot that is better maintained . # pydot raises a generic Exception here , 'Failed to import ` pydot ` . ' ' and graphviz for ` pydotprint ` to work . ' ) raise OSError ( try : import pydot raise ImportError ( 'visualize ' : [ 'pydot > =1.2.0 ' ] , if pydot is None : 'For example with ` pip install pydot ` . ' ) `` `` '' Raise errors if ` pydot ` or GraphViz unavailable . '' '' '' except Exception : pydot = None # so no specific class can be caught . # Fall back on pydot if necessary . 'and ensure that its executables are in the $ PATH . ' ) try : import pydotplus as pydot raise ImportError ( 'Failed to import pydot . You must install pydot ' pydot = None # pydotplus is an improved version of pydot 'Please install GraphViz ( https : //www.graphviz.org/ ) ' import pydot 'Please install ` pydot ` . '","['.gitignore', 'keras/utils/vis_utils.py', 'setup.py']","import ` pydot ` , improve error messages about ` pydot ` and GraphViz , bump to ` pydot > = 1.2.4 ` ( # 9904 )"
491,12cf6523226bd22c02e66ebbc441ed7769ca28f0,2018-04-11 13:52:02-07:00,"# cntk will init type based on the value type assert K.dtype ( K.variable ( 1 , dtype='float16 ' ) ) == 'float16 ' dtype = 'float32 ' if 'int ' in str ( dtype ) else dtype def test_dtype ( self ) : if K.backend ( ) == 'cntk ' : # TODO : remove the conversion when cntk supports int32 , int64 with pytest.raises ( ValueError ) : K.variable ( 1 , dtype='float16 ' ) assert K.dtype ( K.variable ( 1 , dtype='float64 ' ) ) == 'float64 ' else : assert K.dtype ( K.variable ( 1 , dtype='float32 ' ) ) == 'float32 ' dtype=dtype , # https : //docs.microsoft.com/en-us/python/api/cntk.variables.parameter","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",Fix dtype designation for ` variable ` of CNTK and Add its tests ( # 9903 )
492,c6332819c7fb1b01730817a2d46e4fff5a60436b,2018-04-11 13:50:44-07:00,"check_two_tensor_operation ( 'in_train_phase ' , ( 2 , 3 ) , ( 2 , 3 ) , BACKENDS , check_two_tensor_operation ( 'in_test_phase ' , ( 3 , 3 ) , ( 2 , 2 ) , [ KTH , KTF ] , x = x ( ) def test_in_test_phase ( self ) : alt = alt ( ) check_two_tensor_operation ( 'in_test_phase ' , ( 2 , 3 ) , ( 2 , 3 ) , BACKENDS , training=training ) if callable ( alt ) and isinstance ( alt , C.cntk_py.Function ) is False : def in_test_phase ( x , alt , training=None ) : def in_test_phase ( x , alt ) : for training in [ True , False ] : global _LEARNING_PHASE return in_train_phase ( alt , x , training=training ) # Similar as in_train_phase , use element_select as workaround . return C.element_select ( learning_phase ( ) , x , alt ) if callable ( x ) and isinstance ( x , C.cntk_py.Function ) is False :","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py']",Fix ` in_test_phase ` of CNTK and Add its tests ( # 9902 )
493,e73199d413395d7ede0dd86c06f7a9ad2d1fee17,2018-04-11 14:21:51+09:00,"K.ones_like ( prev_output ) , if K.backend ( ) == 'cntk ' : K.ones_like ( h_tm1 ) , # Will update workaround once CNTK supports it . K.ones_like ( inputs ) , else : _generate_dropout_ones ( inputs , K.shape ( inputs ) [ -1 ] ) , ones = K.ones_like ( K.reshape ( inputs [ : , 0 ] , ( -1 , 1 ) ) ) # Currently , CNTK ca n't instantiate ` ones ` with symbolic shapes . return K.ones ( ( K.shape ( inputs ) [ 0 ] , dims ) ) return K.tile ( ones , ( 1 , dims ) ) _generate_dropout_ones ( inputs , self.units ) , def _generate_dropout_ones ( inputs , dims ) : K.ones_like ( states [ 0 ] ) ,",['keras/layers/recurrent.py'],Removed generate dropout ones from recurrent . ( # 9892 )
494,af804d0a5db9a8f20fbb083b48655b2687ce89d9,2018-04-08 11:44:41-07:00,"input_shape : Optional shape tuple , only to be specified if ` include_top ` is False ( otherwise the input shape has to be ` ( 331 , 331 , 3 ) ` for NASNetLarge or require_flatten=include_top or weights , require_flatten=False , ` ( 224 , 224 , 3 ) ` for NASNetMobile input_shape : Optional shape tuple , the input shape ` ( 224 , 224 , 3 ) ` for NASNetMobile . is by default ` ( 331 , 331 , 3 ) ` for NASNetLarge and",['keras/applications/nasnet.py'],Fixed the NASNet issue . ( # 9865 )
495,be112e1f48bbdf24d0a714b9cafc3fc9f43c7738,2018-04-07 18:16:17-07:00,"with codecs.open ( self.bigram_file , mode='rt ' , encoding='utf-8 ' ) as f : with codecs.open ( self.monogram_file , mode= ' r ' , encoding='utf-8 ' ) as f : with codecs.open ( self.bigram_file , mode= ' r ' , encoding='utf-8 ' ) as f : with codecs.open ( self.monogram_file , mode='rt ' , encoding='utf-8 ' ) as f :",['examples/image_ocr.py'],Fix image_ocr.py example ValueError ( # 9869 )
496,2c8d1d03599cc03243bce8f07ed9c4a3d5f384f9,2018-04-04 10:13:51-07:00,"out = model.fit_generator ( generator , epochs=1 , if isinstance ( x , list ) : if isinstance ( x , list ) : elif isinstance ( x , list ) : generator = data_tensors_generator ( ) def data_tensors_generator ( ) : elif isinstance ( x , list ) : while True : steps_per_epoch=3 ) yield ( None , None ) # Handle data tensors support when no input given # step-size = 1 for data tensors out = model.evaluate_generator ( generator , steps=3 ) if x is None or len ( x ) == 0 : # test evaluate_generator for framework-native data tensors # Handle data tensors support when no input given if x is None or len ( x ) == 0 : batch_size = 1 # step-size = 1 for data tensors # test fit_generator for framework-native data tensors batch_size = 1 # define a generator to produce x=None and y=None","['keras/engine/training.py', 'tests/keras/engine/test_training.py']",fit/evaluate_generator supporting native tensors ( # 9816 )
497,c08ef613af27da896cee168daeee5c6fad1980b6,2018-04-03 16:54:55-07:00,"dilation_rate=1 , incompatible with specifying any ` strides ` value ! = 1 . Can be a single integer to specify the same value for incompatible with specifying any stride value ! = 1 . Currently , specifying any ` dilation_rate ` value ! = 1 is all spatial dimensions . dilation_rate : an integer or tuple/list of n integers , specifying dilation_rate : An integer or tuple/list of 2 integers , specifying dilation_rate : An integer or tuple/list of a single integer , specifying dilation_rate= ( 1 , 1 ) , dilation_rate=dilation_rate , the dilation rate to use for dilated convolution .",['keras/layers/convolutional.py'],# 9642 Add kwarg and documentation for dilation_rate to SeparableConvs ( # 9844 )
498,b13326498e7cf7338a397f93a75248509256adc3,2018-04-03 15:30:09-07:00,"X = np.zeros ( ( samples , ) + input_shape ) categorical = np.zeros ( ( n , num_classes ) , dtype=np.float32 ) y = np.zeros ( ( samples , ) + output_shape ) categorical = np.zeros ( ( n , num_classes ) ) X = np.zeros ( ( samples , ) + input_shape , dtype=np.float32 ) y = np.zeros ( ( samples , ) + output_shape , dtype=np.float32 )","['keras/utils/np_utils.py', 'keras/utils/test_utils.py']",Improve tests by designating dtype of sample data ( # 9834 )
499,ef13db05731bfd53fa0a877637c99c1734be933b,2018-04-02 17:41:49-07:00,"subset : Subset of data ( ` `` training '' ` or ` `` validation '' ` ) if supported . If PIL version 3.4.0 or newer is installed , ` `` box '' ` and ` validation_split ` is set in ` ImageDataGenerator ` . Supported methods are ` `` nearest '' ` , ` `` bilinear '' ` , and ` `` bicubic '' ` . ` validation_split ` is set in ` ImageDataGenerator ` . If PIL version 1.1.3 or newer is installed , ` `` lanczos '' ` is also subset : Subset of data ( ` `` training '' ` or ` `` validation '' ` ) if interpolation : Interpolation method used to resample the image if the target size is different from that of the loaded image . ` `` hamming '' ` are also supported . By default , ` `` nearest '' ` is used .",['keras/preprocessing/image.py'],Add documentation for 'subset ' and interpolation ' arguments ( ImageDataGenerator ) ( # 9817 )
500,886c021e00c9a12e719371e234bd6df4e01a703f,2018-04-02 11:30:23-07:00,"preprocessing.image.ImageDataGenerator , for cls in classes : for method in methods : # theme_dir : theme if docstring : # 4 ) Choose which methods to document ( methods listed as qualified names ) : for _ , method in inspect.getmembers ( cls , predicate=inspect.isroutine ) : signature = get_function_signature ( function , method=method ) render_function ( method , subblocks , method=True ) return methods signature = signature.replace ( function.__module__ + ' . ' , `` ) element = ( element , [ ] ) subblocks.append ( ' # # # ' + cls.__name__ + ' methods\n ' ) subblocks.append ( ' # # # ' + cls.__name__ + '\n ' ) def collect_class_methods ( cls , methods ) : methods = [ ] for element in classes : if methods : if method.__name__ [ 0 ] == ' _ ' or method.__name__ in EXCLUDE : docstring = function.__doc__ blocks.append ( '\n\n'.join ( subblocks ) ) continue if element [ 1 ] : else : # [ classA , ( classB , [ `` method1 '' , `` method2 '' , ... ] ) , ... ] level = 4 if method else 3 blocks.append ( '\n\n'.join ( subblocks ) ) # 3 ) Choose which methods to document ( methods listed as strings ) : subblocks = [ ] # 1 ) Document only the class : [ classA , classB , ... ] subblocks = [ ] cls = element [ 0 ] subblocks.append ( ' # # # ' + cls.__name__ + '\n ' ) # 2 ) Document all its methods : [ classA , ( classB , `` * '' ) ] methods.append ( method ) if not isinstance ( element , ( list , tuple ) ) : signature = signature.replace ( function.__module__ + ' . ' , `` ) ( preprocessing.image.ImageDataGenerator , `` * '' ) def render_function ( function , blocks , method=True ) : # [ classA , ( classB , [ module.classB.method1 , module.classB.method2 , ... ] ) , ... ] return [ getattr ( cls , m ) if isinstance ( m , str ) else m for m in methods ] subblocks.append ( process_docstring ( docstring ) ) subblocks.append ( ' # # ' + cls.__name__ + ' class\n ' ) subblocks.append ( code_snippet ( signature ) ) # For each class to document , it is possible to : subblocks.append ( ' # # # ' + function.__name__ + '\n ' ) subblocks.append ( process_docstring ( docstring ) ) methods = collect_class_methods ( cls , element [ 1 ] ) subblocks.append ( ' # ' * level + function.__name__ + '\n ' ) if docstring : docstring = function.__doc__ signature = get_function_signature ( function , method=False ) render_function ( function , blocks , method=False ) if isinstance ( methods , ( list , tuple ) ) : subblocks.append ( code_snippet ( signature ) )","['docs/autogen.py', 'docs/mkdocs.yml']",Add support for class methods documentation ( # 9751 )
501,416783156c1b07f28131c493a55a93936b5fe163,2018-04-01 11:34:11-07:00,"use_multiprocessing=False , verbose : verbosity mode , 0 or 1 . progbar = Progbar ( target=steps ) progbar.update ( steps_done ) if verbose == 1 : verbose=0 ) : use_multiprocessing=False ) : # enable verbose for evaluate_generator if verbose == 1 : out = model.evaluate_generator ( gen_data ( 4 ) , steps=3 , verbose=1 )","['keras/engine/training.py', 'tests/keras/engine/test_training.py']",Add missing verbose opt for evaluate_generator ( # 9811 )
502,aedad3986200b825d94f847d52bd6b81f0419a06,2018-03-31 20:20:46-07:00,"layer_test ( layers.Flatten , inputs = K.permute_dimensions ( inputs , permutation ) data_format : A string , one of ` channels_last ` ( default ) or ` channels_first ` . The ordering of the dimensions in the inputs . def test_4d ( ) : expected_output=np_output_cl ) np_output_cl = layer_test ( layers.Flatten , kwargs= { } , np_inp_channels_first = np.transpose ( np_inp_channels_last , ( 1 , 4 , 3 ) ) from .. utils import conv_utils [ 0 , 3 , 1 , 2 ] ) ( 1 , 5 , 4 , 3 , 2 ) ) if isinstance ( pattern , list ) : current_layout = [ i for i in range ( dims ) ] # Arguments config = { 'data_format ' : self.data_format } 'channels_last ' } , np_inp_channels_last = np.arange ( 12 , dtype='float32 ' ) .reshape ( return dict ( list ( base_config.items ( ) ) + list ( config.items ( ) ) ) [ 0 , 2 , 1 ] ) np_inp_channels_last = np.arange ( 24 , dtype='float32 ' ) .reshape ( current_layout = tuple ( [ i for i in range ( dims ) ] ) base_config = super ( Flatten , self ) .get_config ( ) current_layout = tuple ( [ i for i in range ( dims ) ] ) test_4d ( ) self.data_format = conv_utils.normalize_data_format ( data_format ) inputs with shape ` ( batch , channels , ... ) ` . kwargs= { 'data_format ' : input_data=np_inp_channels_first , permutation.extend ( [ i for i in if self.data_format == 'channels_first ' : test_5d ( ) [ 0 , 4 , 1 , 2 , 3 ] ) permutation = [ 0 ] ` ( batch , ... , channels ) ` while ` channels_first ` corresponds to def get_config ( self ) : def __init__ ( self , * * kwargs ) : 'channels_first ' } , def test_3d ( ) : def test_5d ( ) : ( 1 , 4 , 3 , 2 ) ) else : np_output_cf = layer_test ( layers.Flatten , input_shape= ( 3 , 2 , 4 ) ) ` channels_last ` corresponds to inputs with shape # Ensure works for any dim test_3d ( ) np_inp_channels_last = np.arange ( 120 , dtype='float32 ' ) .reshape ( permutation.append ( 1 ) input_data=np_inp_channels_last ) range ( 2 , K.ndim ( inputs ) ) ] ) def __init__ ( self , data_format='channels_last ' , * * kwargs ) :","['keras/backend/cntk_backend.py', 'keras/layers/core.py', 'tests/keras/layers/core_test.py']",Added ` data_format ` to flatten layer . ( # 9696 )
503,1ee31ee45cc3386cb94bc1f5014f2687da1f63f6,2018-03-31 11:42:41-07:00,"with h5py.File ( 'does not matter ' , driver='core ' , filepath : one of the following : raw_file.write ( binary_data ) f = h5py.File ( filepath , mode= ' w ' ) # If file exists and should not be overwritten . # Load the manually-saved binary data , and make sure the model is intact . if not proceed : try : if not overwrite and os.path.isfile ( filepath ) : metrics= [ metrics.categorical_accuracy ] ) # Make sure the binary data is correct by saving it to a file manually return model if opened_new_file : # If file exists and should not be overwritten . out = model.predict ( x ) y = np.random.random ( ( 1 , 3 ) ) outputs = Dense ( 3 ) ( x ) opened_new_file = not isinstance ( filepath , h5py.File ) f.close ( ) f = h5py.File ( filepath , mode= ' r ' ) string , path where to save the model , or def test_model_saving_to_pre_created_h5py_file ( ) : optimizer=optimizers.Adam ( ) , with h5py.File ( fname , mode= ' r+ ' ) as h5file : h5py.File object from which to load the model # Save the model to an in-memory-only h5 file . with h5py.File ( fname , mode= ' r ' ) as h5file : model.train_on_batch ( x , y ) save_model ( model , h5file ) out2 = loaded_model.predict ( x ) model = Model ( inputs , outputs ) x = Dense ( 2 ) ( inputs ) finally : filepath : String , path where to save the model . f = filepath if opened_new_file : return with h5py.File ( filepath , mode= ' r ' ) as f : backing_store=False ) as h5file : return model assert_allclose ( out , out2 , atol=1e-05 ) if not isinstance ( filepath , h5py.File ) : return proceed = ask_to_proceed_with_overwrite ( filepath ) x = np.random.random ( ( 1 , 3 ) ) loaded_model = load_model ( h5file ) h5py.File object where to save the model model.compile ( loss=losses.MSE , _ , fname = tempfile.mkstemp ( '.h5 ' ) with open ( fname , 'wb ' ) as raw_file : if not overwrite and os.path.isfile ( filepath ) : inputs = Input ( shape= ( 3 , ) ) opened_new_file = True string , path to the saved model , or h5file.flush ( ) # Very important ! Otherwise you get all zeroes below . binary_data = h5file.fid.get_file_image ( ) filepath : String , path to the saved model . else : # test non-default options in h5 opened_new_file = False proceed = ask_to_proceed_with_overwrite ( filepath ) def test_model_saving_to_binary_stream ( ) : # and then loading it the usual way . with h5py.File ( filepath , mode= ' w ' ) as f : if not proceed :","['keras/models.py', 'tests/test_model_saving.py']",Allow saving models directly to binary stream ( # 9789 )
504,e2a10a5e6e156a45e946c4d08db7133f997c1f9a,2018-03-31 11:40:49-07:00,"k_inputs = K.variable ( inputs , dtype= '' float32 '' ) k_input_lens = K.variable ( input_lens , dtype= '' int32 '' ) assert_allclose ( res [ 0 , : ] if K.backend ( ) == 'theano ' else res [ : , 0 ] , ref , atol=1e-05 ) label_lens = np.expand_dims ( np.asarray ( [ 5 ] ) , 1 ) [ [ [ 0.633766 , 0.221185 , 0.0917319 , 0.0129757 , 0.0142857 , 0.0260553 ] , inputs = np.asarray ( labels = np.asarray ( [ [ 0 , 1 , 2 , 1 , 0 ] ] ) input_lens = np.expand_dims ( np.asarray ( [ 5 ] ) , 1 ) [ 0.111121 , 0.588392 , 0.278779 , 0.0055756 , 0.00569609 , 0.010436 ] , else : k_label_lens = K.variable ( label_lens , dtype= '' int32 '' ) [ 0.0357786 , 0.633813 , 0.321418 , 0.00249248 , 0.00272882 , 0.0037688 ] , [ 0.0663296 , 0.643849 , 0.280111 , 0.00283995 , 0.0035545 , 0.00331533 ] , if K.backend ( ) == 'theano ' : label_length = tf.to_int32 ( tf.squeeze ( label_length , axis=-1 ) ) # test when batch_size = 1 , that is , one sample only dtype=np.float32 ) res = K.eval ( K.ctc_batch_cost ( k_labels , k_inputs , k_input_lens , k_label_lens ) ) label_length = tf.to_int32 ( tf.squeeze ( label_length ) ) input_length = tf.to_int32 ( tf.squeeze ( input_length , axis=-1 ) ) ref = [ 3.34211 ] input_length = tf.to_int32 ( tf.squeeze ( input_length ) ) ref = [ 1.73308 ] [ 0.458235 , 0.396634 , 0.123377 , 0.00648837 , 0.00903441 , 0.00623107 ] ] ] , # get only first sample from above test case k_labels = K.variable ( labels , dtype= '' int32 '' )","['keras/backend/tensorflow_backend.py', 'tests/keras/backend/backend_test.py']",Fix ctc_batch_cost ( ) error when batch_size = 1 ( # 9775 )
505,ced81968b0e9d8b1389e6580721ac60d9cf3ca60,2018-03-30 13:27:39-04:00,"` ( samples , time , rows , cols , channels ) ` ` ( samples , time , channels , rows , cols ) ` ` ( samples , time , rows , cols , channels ) ` ` ( samples , time , channels , rows , cols ) `",['keras/layers/convolutional_recurrent.py'],Fix # 9802 ( # 9803 )
506,f7afc73780ffe89abd8dd08ebd815cbcf56720ee,2018-03-27 15:31:59-04:00,"Otherwise the serialized JSON will be send within a form classification=True , validation_data= ( X_test , y_test ) , callbacks=cbks , epochs=1 ) model.compile ( loss='categorical_crossentropy ' , field : String ; JSON field under which the data will be stored . The field is used only if the payload is sent requests.post ( self.root + self.path , headers=None , cbks = [ callbacks.RemoteMonitor ( send_as_json=True ) ] input_shape= ( input_dim , ) , num_classes=num_classes ) headers=None ) : field : String ; JSON field under which the data will be stored . self.send_as_json = send_as_json ( X_train , y_train ) , ( X_test , y_test ) = get_test_data ( num_train=train_samples , requests.post ( self.root + self.path , If send_as_json is set to True , the content type of the request will be application/json . if self.send_as_json : else : within a form ( i.e . send_as_json is set to False ) . y_test = np_utils.to_categorical ( y_test ) with patch ( 'requests.post ' ) : { self.field : json.dumps ( send ) } , num_test=test_samples , requests.post ( self.root + self.path , json=send , headers=self.headers ) send_as_json=False ) : model.add ( Dense ( num_classes , activation='softmax ' ) ) headers=self.headers ) optimizer='rmsprop ' , model.add ( Dense ( num_hidden , input_dim=input_dim , activation='relu ' ) ) { self.field : json.dumps ( send ) } , headers=self.headers ) y_train = np_utils.to_categorical ( y_train ) def tests_RemoteMonitorWithJsonPayload ( ) : send_as_json : Boolean ; whether the request should be send as application/json . model.fit ( X_train , y_train , batch_size=batch_size , metrics= [ 'accuracy ' ] ) model = Sequential ( )","['keras/callbacks.py', 'tests/keras/test_callbacks.py']",# 9733 : Extend RemoteMonitor to send data as application/json ( # 9734 )
507,709f791af201caaab4aa180bda259989087cfe47,2018-03-24 15:59:16-04:00,"reduce_on_plateau.on_epoch_end ( epoch , logs= { 'val_loss ' : losses [ epoch ] } ) self.monitor_op = lambda a , b : np.greater ( a , b + self.min_delta ) cbks = [ callbacks.ReduceLROnPlateau ( monitor='val_loss ' , factor=0.1 , min_delta=10 , patience=1 , cooldown=5 ) ] class DummyOptimizer ( object ) : def __init__ ( self ) : cbks = [ callbacks.ReduceLROnPlateau ( monitor='val_loss ' , factor=0.1 , epsilon=0 , patience=1 , cooldown=5 ) ] if 'epsilon ' in kwargs : self.wait += 1 self.monitor_op = lambda a , b : np.less ( a , b - self.epsilon ) import warnings assert not hasattr ( reduce_on_plateau , 'epsilon ' ) self.monitor_op = lambda a , b : np.greater ( a , b + self.epsilon ) min_delta = kwargs.pop ( 'epsilon ' ) min_delta : threshold for measuring the new optimum , assert reduce_on_plateau.min_delta == 1e-13 * * kwargs ) : self.lr = K.variable ( 1.0 ) def test_ReduceLROnPlateau_backwards_compatibility ( ) : self.optimizer = DummyOptimizer ( ) lrs = [ ] with warnings.catch_warnings ( record=True ) as ws : epsilon : threshold for measuring the new optimum , cbks = [ callbacks.ReduceLROnPlateau ( monitor='val_loss ' , factor=0.1 , min_delta=0 , patience=1 , cooldown=5 ) ] assert all ( [ lr == 1.0 for lr in lrs [ : -1 ] ] ) and lrs [ -1 ] < 1.0 reduce_on_plateau = callbacks.ReduceLROnPlateau ( epsilon=1e-13 ) for epoch in range ( len ( losses ) ) : lrs.append ( K.get_value ( reduce_on_plateau.model.optimizer.lr ) ) # Check if warnings are disabled 'will be removed , use ` min_delta ` insted . ' ) assert `` ` epsilon ` argument is deprecated '' in str ( ws [ 0 ] .message ) # The learning rates should be 1.0 except the last one warnings.warn ( ' ` epsilon ` argument is deprecated and ' patience=2 ) self.wait += 1 self.min_delta = min_delta def test_ReduceLROnPlateau_patience ( ) : reduce_on_plateau.model = DummyModel ( ) if os.environ.get ( `` PYTHONWARNINGS '' ) ! = `` ignore '' : self.monitor_op = lambda a , b : np.less ( a , b - self.min_delta ) cbks = [ callbacks.ReduceLROnPlateau ( monitor='val_loss ' , factor=0.1 , epsilon=10 , patience=1 , cooldown=5 ) ] verbose=0 , mode='auto ' , epsilon=1e-4 , cooldown=0 , min_lr=0 ) : verbose=0 , mode='auto ' , min_delta=1e-4 , cooldown=0 , min_lr=0 , losses = [ 0.0860 , 0.1096 , 0.1040 ] class DummyModel ( object ) : self.epsilon = epsilon assert hasattr ( reduce_on_plateau , 'min_delta ' ) reduce_on_plateau = callbacks.ReduceLROnPlateau ( monitor='val_loss ' ,","['keras/callbacks.py', 'tests/keras/test_callbacks.py']",Fixed inconsistencies regarding ReduceLROnPlateau ( # 9723 )
508,2f4685c8195597d5c150b8fadc1ec4d8827d95cc,2018-03-22 11:21:56-07:00,"gen = [ ( np.array ( [ x0 ] ) , np.array ( [ y0 ] ) ) for x0 , y0 in zip ( x , y ) ] if isinstance ( m , Layer ) and m.stateful : val_preds = model.predict_generator ( iter ( val_gen ) , steps=val_samples ) for m in self.metrics : weights=batch_sizes ) if hasattr ( self , 'metrics ' ) : # means `` take an average from a single value '' but keeps the stateful_metric_indices = [ outs = [ outs ] averages.append ( np.average ( [ out [ i ] for out in all_outs ] , if isinstance ( metric_fn , Layer ) and metric_fn.stateful : return averages outs = model.evaluate_generator ( iter ( gen ) , steps=samples ) if str ( name ) in self.stateful_metric_names ] for i in range ( len ( outs ) ) : val_gen = [ ( np.array ( [ x0 ] ) , np.array ( [ y0 ] ) ) for x0 , y0 in zip ( val_x , val_y ) ] # Test fit , evaluate for i in range ( len ( outs ) ) : val_outs = model.evaluate_generator ( iter ( val_gen ) , steps=val_samples ) if isinstance ( m , Layer ) : if not isinstance ( outs , list ) : stateful_metric_indices = [ ] # Test correctness of the validation metric computation for i , m in enumerate ( self.metrics ) : threshold : Float , lower limit on prediction value that counts as a self.stateful = True val_samples = 10 # Test correctness of the metric re ref_true_pos ( ) history = model.fit ( x , y , validation_data= ( val_x , val_y ) , epochs=1 , batch_size=10 ) else : stateful_metric_indices = [ ] averages.append ( float ( outs_per_batch [ -1 ] [ i ] ) ) outs [ i ] = batch_out outs_per_batch.append ( outs ) else : if len ( averages ) == 1 : positive class prediction . averages = [ ] history = model.fit_generator ( iter ( gen ) , epochs=1 , steps_per_epoch=samples , # numeric formatting . i for i , name in enumerate ( self.metrics_names ) if isinstance ( m , Layer ) and m.stateful : # Test with generators validation_data=iter ( val_gen ) , validation_steps=val_samples ) model.fit ( x , y , epochs=1 , batch_size=10 ) self._values [ k ] = v m.reset_states ( ) outs_per_batch = [ ] if i not in stateful_metric_indices : np.testing.assert_allclose ( val_outs [ 2 ] , history.history [ 'val_true_positives ' ] [ -1 ] , atol=1e-5 ) # Test fit and evaluate outs [ i ] = float ( batch_out ) np.testing.assert_allclose ( val_outs [ 2 ] , ref_true_pos ( val_y , val_preds ) , atol=1e-5 ) np.testing.assert_allclose ( outs [ 2 ] , ref_true_pos ( y , preds ) , atol=1e-5 ) val_y = np.random.randint ( 2 , size= ( val_samples , 1 ) ) if isinstance ( metric_fn , Layer ) : m.reset_states ( ) return np.average ( np.asarray ( all_outs ) , else : all_outs.append ( outs ) # Stateful metrics output a numeric value . This representation val_preds = model.predict ( val_x ) if not isinstance ( outs , list ) : preds = model.predict_generator ( iter ( gen ) , steps=samples ) averages.append ( np.average ( [ out [ i ] for out in outs_per_batch ] , averages = [ ] all_outs = [ ] val_x = np.random.random ( ( val_samples , 2 ) ) return averages self._values [ k ] = [ v , 1 ] val_outs = model.evaluate ( val_x , val_y , batch_size=10 ) return averages [ 0 ]","['keras/engine/training.py', 'keras/utils/generic_utils.py', 'tests/keras/metrics_test.py']",General stateful metrics fixes ( # 9446 )
509,a69008527787c387be7d7d4f6de91f9f08396dbc,2018-03-20 08:57:32-07:00,"Note that 'hash ' is not a stable hashing function , so __Return__ : List of integers in [ 1 , n ] . Each integer encodes a word ( unicity non-guaranteed ) . filters : Sequence of characters to filter out . __sequences__ : list of sequences to train on . keras.preprocessing.text.hashing_trick ( text , basic punctuation , tabs , and newlines . __texts__ : list of texts to train on . lower : Whether to convert the input to lowercase . Converts a text to a sequence of indices in a fixed-size hashing space lower=True , 'functions ' : [ __word_counts__ : dictionary mapping words ( str ) to the number of times they appeared on during fit . Only set after fit_on_texts was called . char_level=False , __mode__ : one of `` binary '' , `` count '' , `` tfidf '' , `` freq '' ( default : `` binary '' ) . modules = [ 'keras.layers ' , 'keras.models ' , 'keras ' , 'keras.backend.tensorflow_backend ' , 'keras.preprocessing.image ' , __Arguments__ : __word_docs__ : dictionary mapping words ( str ) to the number of documents/texts they appeared on during fit . Only set after fit_on_texts was called . n : int . Size of vocabulary . preprocessing.text.hashing_trick , __fit_on_texts ( texts ) __ : preprocessing.text.Tokenizer , __Return__ : list of sequences ( one per text input ) . split : str . Separator for word splitting . __word_index__ : dictionary mapping words ( str ) to their rank/index ( int ) . Only set after fit_on_texts was called . filters= ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , ] , __Arguments__ : __fit_on_sequences ( sequences ) __ : __texts_to_sequences ( texts ) __ filters= ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , filters : list ( or concatenation ) of characters to filter out , such as split= ' ' ) __char_level__ : if True , every character will be treated as a token . # # # Text Preprocessing split : Sentence split marker ( string ) . split= '' `` ) { { autogenerated } } Note that ` hash ` is not a stable hashing function , so keras.preprocessing.text.one_hot ( text , Class for vectorizing texts , or/and turning texts into sequences ( =list of word indexes , where the word of rank i in the dataset ( starting at 1 ) has index i ) . __Return__ : yield one sequence per input text . it is not consistent across different runs , while 'md5 ' __texts_to_matrix ( texts ) __ : hash_function : defaults to python ` hash ` function , can be 'md5 ' or oov_token=None ) lower : boolean . Whether to convert the input to lowercase . `` ` python 'keras.preprocessing.text ' ] n , preprocessing.text.one_hot , lower=True , __document_count__ : int . Number of documents ( texts/sequences ) the tokenizer was trained on . Only set after fit_on_texts or fit_on_sequences was called . `` ` # # # Tokenizer __hash_function__ : defaults to python ` hash ` function , can be 'md5 ' or Split a sentence into a list of words . n : Dimension of the hashing space . } , ] punctuation . Default : ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , includes __lower__ : boolean . Whether to set the text to lowercase . __Methods__ : filters= ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , __sequences_to_matrix ( sequences ) __ : __texts_to_sequences_generator ( texts ) __ : generator version of the above . lower=True , modules = [ 'keras.layers ' , 'keras.models ' , 'keras ' , 'keras.backend.tensorflow_backend ' , 'keras.preprocessing.image ' ] __Return__ : is a stable hashing function . __Return__ : numpy array of shape ` ( len ( sequences ) , num_words ) ` . __texts__ : list of texts to turn to sequences . hash_function : if ` None ` uses python ` hash ` function , can be 'md5 ' or filters= ' ! '' # $ % & ( ) * + , -./ : ; < = > ? @ [ \\ ] ^_ ` { | } ~\t\n ' , lower=True , __num_words__ : None or int . Maximum number of words to work with ( if set , tokenization will be restricted to the top num_words most common words in the dataset ) . __filters__ : list ( or concatenation ) of characters to filter out , such as A list of integer word indices ( unicity non-guaranteed ) . __text__ : str . __n__ : Dimension of the hashing space . 'page ' : 'preprocessing/text.md ' , n , split= '' `` , __sequences__ : list of sequences to vectorize . List of integers in [ 1 , n ] . Each integer encodes a word ( unicity non-guaranteed ) . lower : boolean . Whether to set the text to lowercase . __Arguments__ : Same as ` text_to_word_sequence ` above . hash_function=None , # # # hashing_trick # # # one_hot __texts__ : list of texts to vectorize . preprocessing.text.text_to_word_sequence , __Return__ : numpy array of shape ` ( len ( texts ) , num_words ) ` . __oov_token__ : None or str . If given , it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls . # # # text_to_word_sequence basic punctuation , tabs , and newlines . 'classes ' : [ { __n__ : int . Size of vocabulary . One-hot encodes a text into a list of word indexes in a vocabulary of size n . Note that 'hash ' is not a stable hashing function , so split : character or string to use for token splitting . This is a wrapper to the ` hashing_trick ` function using ` hash ` as the hashing function . keras.preprocessing.text.Tokenizer ( num_words=None , any function that takes in input a string and returns a int . split= '' `` ) __Attributes__ : keras.preprocessing.text.text_to_word_sequence ( text , __Arguments__ : __split__ : str . Separator for word splitting . __Return__ : List of words ( str ) .","['docs/autogen.py', 'docs/templates/preprocessing/text.md', 'keras/preprocessing/text.py', 'tests/test_documentation.py']",Add text doc ( # 9705 )
510,685bda8db301269b0f8542925f35678ed398f6df,2018-03-16 19:16:11-07:00,chown keras $ CONDA_DIR -R & & \ chown $ NB_USER /src chown keras /src chown $ NB_USER $ CONDA_DIR -R & & \ USER keras USER $ NB_USER,['docker/Dockerfile'],changed hardcoded username with variable one ( # 9677 )
511,5216360a3c70b0f4d685966ef449645fa131ade8,2018-03-15 22:53:57-07:00,"height_shift_range : Float ( fraction of total height ) . Range for random vertical shifts . brightness_range : Tuple of floats ; brightness range . The generator loops indefinitely . 'constant ' : kkkkkkkk|abcd|kkkkkkkk ( cval=k ) featurewise_std_normalization=True , # Examples if doc is None : ValueError if ` brightness_range ` is n't a tuple . Only required if featurewise_center or featurewise_std_normalization or zca_whitening . target_size= ( 150 , 150 ) , datagen.fit ( x_train ) width_shift_range=0.1 , horizontal_flip=False , `` `` '' Example of using ` .flow ( x , y ) ` : datagen = ImageDataGenerator ( indices can be obtained via the attribute ` class_indices ` . __class_mode__ : one of `` categorical '' , `` binary '' , `` sparse '' , `` input '' or None . Default : `` categorical '' . Determines the type of label arrays that are returned : `` categorical '' will be 2D one-hot encoded labels , `` binary '' will be 1D binary labels , `` sparse '' will be 1D integer labels , `` input '' will be images identical to input images ( mainly used to work with autoencoders ) . If None , no labels are returned ( the generator will only yield batches of image data , which is useful to use ` model.predict_generator ( ) ` , ` model.evaluate_generator ( ) ` , etc. ) . Please note that in case of class_mode None , the data still needs to reside in a subdirectory of ` directory ` for it to work correctly . horizontal_flip=True ) and should output a Numpy tensor with the same shape . shear_range : Float . Shear Intensity ( Shear angle in counter-clockwise direction as radians ) An Iterator yielding tuples of ` ( x , y ) ` where ` x ` is a numpy array of image data and __shuffle__ : boolean ( default : True ) . ( x_train , y_train ) , ( x_test , y_test ) = cifar10.load_data ( ) follow_links : whether to follow symlinks inside class subdirectories ( default : False ) . ( x_train , y_train ) , ( x_test , y_test ) = cifar10.load_data ( ) __data_format__ : One of { `` channels_first '' , `` channels_last '' } . preprocessing.image.random_shear , __Arguments__ : the channels axis should have value 1 , and in case cval=0. , if is_accepted ( name , member ) : mask_datagen.fit ( masks , augment=True , seed=seed ) train_datagen = ImageDataGenerator ( preprocessing_function=None , otherwise we multiply the data by the value provided ( before applying steps_per_epoch=2000 , featurewise_center=True , zoom_range=0.2 ) validation_split : fraction of images reserved for validation ( strictly between 0 and 1 ) . channel_shift_range : shift range for each channel . horizontal_flip : whether to randomly flip images horizontally . `` ` Example of using ` .flow_from_directory ( directory ) ` : zoom_range : amount of zoom . if scalar z , zoom will be randomly picked __follow_links__ : whether to follow symlinks inside class subdirectories ( default : False ) . ` model.predict_generator ( ) ` , ` model.evaluate_generator ( ) ` , etc . ) . returned : `` categorical '' will be 2D one-hot encoded labels , `` binary '' will be 1D binary labels , validation_generator = test_datagen.flow_from_directory ( mask_datagen.fit ( masks , augment=True , seed=seed ) __Methods__ : __flow ( x , y ) __ : Takes numpy data & label arrays , and generates batches of augmented/normalized data . Yields batches indefinitely , in an infinite loop . __save_to_dir__ : None or str ( default : None ) . This allows you to optionally specify a directory to which to save the augmented pictures being generated ( useful for visualizing what you are doing ) . `` `` '' Takes numpy data & label arrays , and generates batches of # combine generators into one which yields image and masks __featurewise_std_normalization__ : Boolean . Divide inputs by std of the dataset , feature-wise . data_format=K.image_data_format ( ) ) zoom_range=0.2 ) fill_mode : points outside the boundaries are filled according to the rotation_range=90. , fill_mode='nearest ' , __x__ : data . Should have rank 4 . # Raises # Raises rotation_range=20 , featurewise_std_normalization=False , horizontal_flip=True ) __featurewise_center__ : Boolean . Set input mean to 0 over the dataset , feature-wise . * `` reflect '' : ` abcddcba|abcd|dcbaabcd ` __flow_from_directory ( directory ) __ : Takes the path to a directory , and generates batches of augmented/normalized data . Yields batches indefinitely , in an infinite loop . test_datagen = ImageDataGenerator ( rescale=1./255 ) __samplewise_center__ : Boolean . Set each sample mean to 0 . the channels axis should have value 1 , and in case the channels axis should have value 1 , and in case and zca_whitening . seed = 1 'data/masks ' , `` ` python model.fit_generator ( datagen.flow ( x_train , y_train , batch_size=32 ) , rotation_range=20 , rescale=1./255 , # Arguments The dimensions to which all images found will be resized . A DirectoryIterator yielding tuples of ` ( x , y ) ` where ` x ` is a numpy array of image data and validation_split : Float . Fraction of images reserved for validation ( strictly between 0 and 1 ) . 'classes ' : [ # We do n't need to check this one . rounds : If ` augment ` , __zoom_range__ : Float or [ lower , upper ] . Range for random zoom . If a float , ` [ lower , upper ] = [ 1-zoom_range , 1+zoom_range ] ` . brightness_range : the range of brightness to apply mask_generator = mask_datagen.flow_from_directory ( ` y ` is a numpy array of corresponding labels . seed : random seed . featurewise_std_normalization=True , It defaults to the ` image_data_format ` value found in your zca_whitening : Boolean . Apply ZCA whitening . rotation_range=0. , any other transformation ) . If None , no labels are returned ( the generator will only yield batches of image data , which is useful to use train_datagen = ImageDataGenerator ( batch_size=32 , 'wrap ' : abcdabcd|abcd|abcdabcd image_generator = image_datagen.flow_from_directory ( mask_datagen = ImageDataGenerator ( * * data_gen_args ) zoom_range=0.2 , of RGB data , it should have value 3 . __batch_size__ : int ( default : 32 ) . seed = 1 height_shift_range=0.1 , Example of transforming images and masks together . height_shift_range : fraction of total height , if < 1 , or pixels if > = 1 . of RGB data , it should have value 3 . model.fit_generator ( 'functions ' : [ image_datagen.fit ( images , augment=True , seed=seed ) # Arguments # # # Image Preprocessing batch_size=32 , modules = [ 'keras.layers ' , 'keras.models ' , 'keras ' , 'keras.backend.tensorflow_backend ' ] fill_mode : One of { `` constant '' , `` nearest '' , `` reflect '' or `` wrap '' } . Default is 'nearest ' . seed : int ( default : None ) . ( the depth ) is at index 1 , in 'channels_last ' mode it is at index 3 . y_train = np_utils.to_categorical ( y_train , num_classes ) # combine generators into one which yields image and masks keras.preprocessing.image.ImageDataGenerator ( featurewise_center=False , if batches > = len ( x_train ) / 32 : one image ( Numpy tensor with rank 3 ) , if batches > = len ( x_train ) / 32 : if is_accepted ( name , member ) or member_too_small ( member ) : __seed__ : optional random seed for shuffling and transformations . # ( std , mean , and principal components if ZCA whitening is applied ) data_gen_args = dict ( featurewise_center=True , __preprocessing_function__ : function that will be implied on each input . steps_per_epoch=2000 , y_test = np_utils.to_categorical ( y_test , num_classes ) zoom_range : Float or [ lower , upper ] . Range for random zoom . If a float , ` [ lower , upper ] = [ 1-zoom_range , 1+zoom_range ] ` . `` `` '' Generate batches of tensor image data with real-time data augmentation . samplewise_std_normalization=False , The dimensions to which all images found will be resized . __zca_epsilon__ : epsilon for ZCA whitening . Default is 1e-6 . zoom_range=0.2 , 'constant ' : kkkkkkkk|abcd|kkkkkkkk ( cval=k ) `` channels_last '' mode means that the images should have shape ` ( samples , height , width , channels ) ` , __vertical_flip__ : Boolean . Randomly flip inputs vertically . for x_batch , y_batch in datagen.flow ( x_train , y_train , batch_size=32 ) : vertical_flip : whether to randomly flip images vertically . ( only relevant if ` save_to_dir ` is set ) . image_datagen = ImageDataGenerator ( * * data_gen_args ) { { autogenerated } } Whether the images will be converted to have 1 or 3 color channels . __yields__ : Tuples of ` ( x , y ) ` where ` x ` is a numpy array of image data and ` y ` is a numpy array of corresponding labels . cval : Float or Int . Value used for points outside the boundaries when ` fill_mode = `` constant '' ` . seed : optional random seed for shuffling and transformations . samplewise_std_normalization : Boolean . Divide each input by its std . from . import sequence validation_steps=800 ) height_shift_range=0. , 'data/train ' , image_datagen.fit ( images , augment=True , seed=seed ) vertical_flip : Boolean . Randomly flip inputs vertically . train_generator = zip ( image_generator , mask_generator ) width_shift_range : Float ( fraction of total width ) . Range for random horizontal shifts . rescale=None , } , preprocessing.image.random_brightness , augment : Whether to fit on randomly augmented samples classes : optional list of class subdirectories ( e.g . ` [ 'dogs ' , 'cats ' ] ` ) . be automatically inferred from the subdirectory names/structure under ` directory ` , featurewise_center : set input mean to 0 over the dataset . zoom_range=0. , __seed__ : int ( default : None ) . Random seed . `` `` '' Perform a random brightness shift . print ( 'Epoch ' , e ) to select this range . width_shift_range=0. , the data still needs to reside in a subdirectory of ` directory ` for it to work correctly . class_mode='binary ' ) 'data/validation ' , model.fit ( x_batch , y_batch ) zca_whitening=False , is 'nearest ' . class_mode : one of `` categorical '' , `` binary '' , `` sparse '' , `` input '' or None . rescale : rescaling factor . If None or 0 , no rescaling is applied , __rescale__ : rescaling factor . Defaults to None . If None or 0 , no rescaling is applied , epochs=50 , ( and the order of the classes , which will map to the label indices , will be alphanumeric ) . Default : None . If not provided , the list of classes will # compute quantities required for featurewise normalization __cval__ : Float or Int . Value used for points outside the boundaries when ` fill_mode = `` constant '' ` . __samplewise_std_normalization__ : Boolean . Divide each input by its std . # here 's a more `` manual '' example The function should take one argument : `` sparse '' will be 1D integer labels , `` input '' will be images identical to input images ( mainly used to work with autoencoders ) . print ( 'Epoch ' , e ) vertical_flip=False , shuffle : boolean ( default : True ) . Required for featurewise_center , featurewise_std_normalization featurewise_std_normalization=True , validation_steps=800 ) preprocessing.image.ImageDataGenerator , # # # ImageDataGenerator * `` constant '' : ` kkkkkkkk|abcd|kkkkkkkk ` ( ` cval=k ` ) The dictionary containing the mapping from class names to class shear_range=0.2 , batches = 0 samplewise_center=False , data_gen_args = dict ( featurewise_center=True , x : Numpy array , the data to fit on . Should have rank 4 . 'data/masks ' , preprocessing.image.random_rotation , model.fit_generator ( datagen.flow ( x_train , y_train , batch_size=32 ) , break Example of transforming images and masks together . Only required if featurewise_center or featurewise_std_normalization or zca_whitening . `` `` '' Perform a random channel shift . y_train = np_utils.to_categorical ( y_train , num_classes ) rotation_range=90. , featurewise_std_normalization=True , __x__ : sample data . Should have rank 4 . __zca_whitening__ : Boolean . Apply ZCA whitening . batches += 1 __channel_shift_range__ : Float . Range for random channel shifts . __save_prefix__ : str ( default : ` `` ` ) . Prefix to use for filenames of saved pictures ( only relevant if ` save_to_dir ` is set ) . `` `` '' Compute the internal data stats related to the data-dependent transformations , based on an array of sample data . __Arguments__ : Any PNG , JPG , BMP , PPM or TIF images inside each of the subdirectories directory tree will be included in the generator . save_prefix : str . Prefix to use for filenames of saved pictures ( only relevant if ` save_to_dir ` is set ) . datagen = ImageDataGenerator ( `` `` '' Fits internal statistics to some sample data . samplewise_std_normalization : divide each input by its std . # the generator loops indefinitely `` channels_first '' mode means that the images should have shape ` ( samples , channels , height , width ) ` . any other transformation ) . # the generator loops indefinitely * `` wrap '' : ` abcdabcd|abcd|abcdabcd ` Example of using ` .flow_from_directory ( directory ) ` : for x_batch , y_batch in datagen.flow ( x_train , y_train , batch_size=32 ) : shear_range : shear intensity ( shear angle in degrees ) . width_shift_range=0.1 , # we create two instances with the same arguments given mode ( 'constant ' , 'nearest ' , 'reflect ' or 'wrap ' ) . Default mask_generator = mask_datagen.flow_from_directory ( model.fit_generator ( featurewise_std_normalization : divide inputs by std of the dataset . See [ this script ] ( https : //gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d ) for more details . horizontal_flip : Boolean . Randomly flip inputs horizontally . This allows you to optionally specify a directory save_to_dir : None or str ( default : None ) . This allows you to optionally specify a directory to which to save # we need to break the loop by hand because Numpy image tensor . 'data/images ' , __shear_range__ : Float . Shear Intensity ( Shear angle in counter-clockwise direction as radians ) Points outside the boundaries of the input are filled according to the given mode : __Examples__ : __shuffle__ : whether to shuffle the data ( default : True ) rotation_range : Int . Degree range for random rotations . validation_data=validation_generator , for e in range ( epochs ) : width_shift_range : fraction of total width , if < 1 , or pixels if > = 1 . horizontal_flip=True ) ] epochs=50 ) 'constant ' . Default is 0 . target_size= ( 150 , 150 ) , rotation_range : degrees ( 0 to 180 ) . train_generator , break featurewise_std_normalization : Boolean . Divide inputs by std of the dataset , feature-wise . train_generator = train_datagen.flow_from_directory ( steps_per_epoch=2000 , 'reflect ' : abcddcba|abcd|dcbaabcd width_shift_range=0.2 , rescale : rescaling factor . Defaults to None . If None or 0 , no rescaling is applied , steps_per_epoch=2000 , for e in range ( epochs ) : validation_data=validation_generator , of RGB data , it should have value 3 . __height_shift_range__ : Float ( fraction of total height ) . Range for random vertical shifts . ( useful for visualizing what you are doing ) . to which to save the augmented pictures being generated height_shift_range=0.2 , 'data/train ' , `` channels_last '' mode means that the images should have shape ` ( samples , height , width , channels ) ` , Default : `` categorical '' . Determines the type of label arrays that are otherwise we multiply the data by the value provided ( before applying 'data/images ' , one image ( Numpy tensor with rank 3 ) , If you never set it , then it will be `` channels_last '' . train_generator , 'reflect ' : abcddcba|abcd|dcbaabcd Any PNG , JPG , BMP , PPM or TIF images inside each of the subdirectories directory tree will be included in the generator . featurewise_center : Boolean . Set input mean to 0 over the dataset , feature-wise . In case of grayscale data , __target_size__ : tuple of integers ` ( height , width ) ` , default : ` ( 256 , 256 ) ` . 'nearest ' : aaaaaaaa|abcd|dddddddd # fits the model on batches with real-time data augmentation : samplewise_center : set each sample mean to 0 . from . import image image_datagen = ImageDataGenerator ( * * data_gen_args ) Example of using ` .flow ( x , y ) ` : save_format : one of `` png '' , `` jpeg '' ( only relevant if ` save_to_dir ` is set ) . Default : `` png '' . { cval : value used for points outside the boundaries when fill_mode is where each subdirectory will be treated as a different class See [ this script ] ( https : //gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d ) for more details . x : Input tensor . Must be 3D . 'page ' : 'preprocessing/image.md ' , x : data . Should have rank 4 . Keras config file at ` ~/.keras/keras.json ` . train_generator , height_shift_range=0.1 , __color_mode__ : one of `` grayscale '' , `` rbg '' . Default : `` rgb '' . Whether the images will be converted to have 1 or 3 color channels . It should contain one subdirectory per class . shear_range=0.2 , y_test = np_utils.to_categorical ( y_test , num_classes ) class_mode=None , class_mode=None , directory : path to the target directory . __rotation_range__ : Int . Degree range for random rotations . Points outside the boundaries of the input are filled according to the given mode : preprocessing.image.random_shift , the augmented pictures being generated ( useful for visualizing what you are doing ) . if doc is None and not member_too_small ( member ) : __directory__ : path to the target directory . It should contain one subdirectory per class . seed=seed ) `` `` '' __save_format__ : one of `` png '' , `` jpeg '' ( only relevant if ` save_to_dir ` is set ) . Default : `` png '' . # Provide the same seed and keyword arguments to the fit and flow methods `` `` '' Takes the path to a directory , and generates batches of augmented/normalized data . model.fit ( x_batch , y_batch ) zca_whitening : apply ZCA whitening . epochs=50 , modules = [ 'keras.layers ' , 'keras.models ' , 'keras ' , 'keras.backend.tensorflow_backend ' , 'keras.preprocessing.image ' ] seed=seed ) from . import text applied after the ` preprocessing_function ` ( if any provided ) __augment__ : Boolean ( default : False ) . Whether to fit on randomly augmented samples . steps_per_epoch=len ( x_train ) / 32 , epochs=epochs ) # fits the model on batches with real-time data augmentation : intensity : Transformation intensity . batches += 1 batches = 0 save_prefix : str ( default : ` `` ` ) . Prefix to use for filenames of saved pictures save_to_dir : None or str ( default : None ) . ] , __rounds__ : int ( default : 1 ) . If augment , how many augmentation passes over the data to use . preprocessing.image.random_zoom , steps_per_epoch=len ( x_train ) / 32 , epochs=epochs ) The data will be looped over ( in batches ) . how many augmentation passes to do over the data y : labels . # we create two instances with the same arguments channel_shift_range=0. , epochs=50 ) shuffle : whether to shuffle the data ( default : True ) 'nearest ' : aaaaaaaa|abcd|dddddddd batch_size : size of the batches of data ( default : 32 ) . image_generator = image_datagen.flow_from_directory ( __width_shift_range__ : Float ( fraction of total width ) . Range for random horizontal shifts . Generate batches of tensor image data with real-time data augmentation . The data will be looped over ( in batches ) indefinitely . `` `` '' Generate minibatches of image data with real-time data augmentation . horizontal_flip=True ) `` ` python augment : Boolean ( default : False ) . Whether to fit on randomly augmented samples . __classes__ : optional list of class subdirectories ( e.g . ` [ 'dogs ' , 'cats ' ] ` ) . Default : None . If not provided , the list of classes will be automatically inferred from the subdirectory names/structure under ` directory ` , where each subdirectory will be treated as a different class ( and the order of the classes , which will map to the label indices , will be alphanumeric ) . The dictionary containing the mapping from class names to class indices can be obtained via the attribute ` class_indices ` . shear_range=0. , channel_shift_range : Float . Range for random channel shifts . `` channels_first '' mode means that the images should have shape ` ( samples , channels , height , width ) ` . # ( std , mean , and principal components if ZCA whitening is applied ) but before any other transformation . `` ` width_shift_range=0.2 , __save_prefix__ : str . Prefix to use for filenames of saved pictures ( only relevant if ` save_to_dir ` is set ) . # here 's a more `` manual '' example target_size : tuple of integers ` ( height , width ) ` , default : ` ( 256 , 256 ) ` . color_mode : one of `` grayscale '' , `` rbg '' . Default : `` rgb '' . mask_datagen = ImageDataGenerator ( * * data_gen_args ) In case of grayscale data , data_format : One of { `` channels_first '' , `` channels_last '' } . # compute quantities required for featurewise normalization otherwise we multiply the data by the value provided . This is 'data/validation ' , data_format : 'channels_first ' or 'channels_last ' . In 'channels_first ' mode , the channels dimension * `` nearest '' : ` aaaaaaaa|abcd|dddddddd ` rounds : int ( default : 1 ) . If augment , how many augmentation passes over the data to use . `` `` '' in the range [ 1-z , 1+z ] . A sequence of two can be passed instead height_shift_range=0.2 , 'wrap ' : abcdabcd|abcd|abcdabcd validation_generator = test_datagen.flow_from_directory ( and should output a Numpy tensor with the same shape . __horizontal_flip__ : Boolean . Randomly flip inputs horizontally . featurewise_center=True , # Returns train_generator = zip ( image_generator , mask_generator ) __fill_mode__ : One of { `` constant '' , `` nearest '' , `` reflect '' or `` wrap '' } . Points outside the boundaries of the input are filled according to the given mode : batch_size : int ( default : 32 ) . # Returns Please note that in case of class_mode None , __seed__ : int ( default : None ) . In case of grayscale data , class_mode='binary ' ) test_datagen = ImageDataGenerator ( rescale=1./255 ) preprocessing.image.random_channel_shift , zca_epsilon=1e-6 , ValueError : in case of invalid input ` x ` . __y__ : labels . __batch_size__ : size of the batches of data ( default : 32 ) . datagen.fit ( x_train ) # Provide the same seed and keyword arguments to the fit and flow methods augmented/normalized data . train_generator , The function should take one argument : x : sample data . Should have rank 4 . # we need to break the loop by hand because samplewise_center : Boolean . Set each sample mean to 0 . The function will run before any other modification on it . channel_axis : Index of axis for channels in the input tensor . train_generator = train_datagen.flow_from_directory ( ` y ` is a numpy array of corresponding labels . '' '' '' __fit ( x ) __ : Compute the internal data stats related to the data-dependent transformations , based on an array of sample data . seed : int ( default : None ) . Random seed . `` `` '' The function will run after the image is resized and augmented . rescale=1./255 ,","['docs/autogen.py', 'docs/templates/preprocessing/image.md', 'keras/preprocessing/__init__.py', 'keras/preprocessing/image.py', 'tests/test_documentation.py']",Add preprocessing.image to autodocs ( # 9656 )
512,bc54e152666bbd7ed741ec812ea2fc24c894ed11,2018-03-15 18:17:07-07:00,"x = BatchNormalization ( axis=channel_axis , name='conv_pw_ % d_bn ' % block_id ) ( x ) axis=channel_axis , name='conv_dw_ % d_bn ' % block_id ) ( x ) ' Input shape provided = % s ' % ( input_shape , ) ) 'input must have a static square shape ( one of ' rows = 224 ' ( 128,128 ) , ( 160,160 ) , ( 192,192 ) , or ( 224 , 224 ) ) . ' raise ValueError ( 'If imagenet weights are being loaded , ' else : ' Input shape provided = % s ' % ( input_shape , ) ) ' Weights for input shape ( 224 , 224 ) will be loaded . ' ) ' ( 128 , 128 ) , ( 160 , 160 ) , ( 192 , 192 ) , or ( 224 , 224 ) ) . ' if rows is None : axis=channel_axis , name='conv_pw_ % d_bn ' % block_id ) ( x ) x = BatchNormalization ( axis=channel_axis , name='conv_dw_ % d_bn ' % block_id ) ( x ) raise ValueError ( 'If imagenet weights are being loaded , ' 'input must have a static square shape ( one of ' warnings.warn ( 'MobileNet shape is undefined . ' x = BatchNormalization (",['keras/applications/mobilenet.py'],Use MobileNet model with undefined shape ( # 9623 )
513,068a680480ebf27ecbe57f1406d60157b7d2df38,2018-03-14 15:31:38-07:00,"`` `` '' Note : This is made to enforce backward compatibility . `` `` '' This will fail if not provided by a Numpy array . generator = image.ImageDataGenerator ( ) def preprocessing_function ( x ) : x = self.preprocessing_function ( x ) assert type ( x ) is np.ndarray assert x.shape == ( 26 , 26 , 3 ) x = self.image_data_generator.preprocessing_function ( x ) generator = image.ImageDataGenerator ( preprocessing_function=preprocessing_function ) assert ( x1 == 0 ) .all ( ) return np.zeros_like ( x )","['keras/preprocessing/image.py', 'tests/keras/preprocessing/image_test.py']",Add a test for preprocessing function for flow_from_directory ( # 9639 )
514,e3e15c699139a11c6124e13bacc5da4fb51d9a96,2018-03-14 15:28:34-07:00,"for i in range ( len ( new_list ) - ngram_range + 1 ) : [ [ 1 , 3 , 4 , 5 , 1337 ] , [ 1 , 3 , 7 , 9 , 2 , 1337 , 2018 ] ] for ngram_value in range ( 2 , ngram_range + 1 ) : for i in range ( len ( new_list ) - ngram_value + 1 ) : for ngram_value in range ( 2 , ngram_range + 1 ) : [ [ 1 , 3 , 4 , 5 , 1337 , 2017 ] , [ 1 , 3 , 7 , 9 , 2 , 1337 , 42 , 2018 ] ]",['examples/imdb_fasttext.py'],Fix to add_ngram function and docstring ( # 9657 )
515,5d8063359d5d4223a8b892d186118c500890c932,2018-03-12 18:02:42-07:00,"width_height_tuple = ( self.target_size [ 1 ] , self.target_size [ 0 ] ) def preprocess_test ( img ) : 'Invalid interpolation method { } specified . Supported ' if self.target_size is not None : if img.size ! = width_height_tuple : generator = image.ImageDataGenerator ( preprocessing_function=preprocess_test ) self.interpolation , return img.resize ( ( 1 , 1 ) ) if self.interpolation not in _PIL_INTERPOLATION_METHODS : batch_size=1 , test_x1 = image.load_img ( os.path.join ( dir_seq.directory , dir_seq.filenames [ 1 ] ) , grayscale=False ) x = self.image_data_generator.preprocessing_function ( x ) test_x1 = preprocess_test ( test_x1 ) target_size=None , assert gen_x1.shape [ 1 : ] == test_x1.shape x = self.image_data_generator.preprocessing_function ( x ) 'methods are { } '.format ( target_size= ( 26 , 26 ) , img = self.image_data_generator.preprocessing_function ( img ) color_mode='rgb ' , resample = _PIL_INTERPOLATION_METHODS [ self.interpolation ] img = img.resize ( width_height_tuple , resample ) # Test Preprocessing before resize class_mode='categorical ' ) test_x1 = image.img_to_array ( test_x1 ) `` , `` .join ( _PIL_INTERPOLATION_METHODS.keys ( ) ) ) ) if self.preprocessing_function : raise ValueError ( test_x1 = test_x1.resize ( ( 26 , 26 ) ) test_x1 = dir_seq.image_data_generator.random_transform ( test_x1 ) gen_x1 , gen_y1 = dir_seq [ 1 ] dir_seq = generator.flow_from_directory ( str ( tmpdir ) , test_x1 = dir_seq.image_data_generator.standardize ( test_x1 ) target_size=self.target_size , if self.image_data_generator.preprocessing_function :","['keras/preprocessing/image.py', 'tests/keras/preprocessing/image_test.py']",Revert 9273 ( # 9625 )
516,96f71e70dc54282df4a693c54efdbfd7d52429af,2018-03-10 13:13:38-08:00,"If tuple of 3 tuples of 2 ints : If tuple of 2 ints : padding : int , or tuple of 3 ints , or tuple of 3 tuples of 2 ints . If tuple of 3 ints : If tuple of 2 tuples of 2 ints : padding : int , or tuple of 2 ints , or tuple of 2 tuples of 2 ints .",['keras/layers/convolutional.py'],Fix the number of arguments ( # 9611 )
517,853774ada4aa191ec8ad2f5a3484fae46920bf7c,2018-03-09 11:07:15-08:00,"assert o2._keras_shape == ( None , 3 , 2 , 1 ) assert out.shape == ( 4 , 3 , 2 , 1 ) assert o1._keras_shape == ( None , 3 , 2 , 1 ) i2 = layers.Input ( shape= ( 64 , 64 , 3 ) ) assert o2._keras_shape == ( None , 64 , 64 , 3 ) assert out1.shape == ( 4 , 64 , 64 , 3 ) assert o [ 0 ] ._keras_shape == ( None , 3 , 2 , 1 ) assert o1._keras_shape == ( None , 64 , 64 , 3 ) x = np.random.random ( ( 4 , 3 , 2 , 1 ) ) i2 = layers.Input ( shape= ( 3 , 2 , 1 ) ) i = layers.Input ( shape= ( 64 , 64 , 3 ) ) assert out1.shape == ( 4 , 3 , 2 , 1 ) assert o [ 1 ] ._keras_shape == ( None , 64 , 64 , 3 ) assert out.shape == ( 4 , 64 , 64 , 3 ) assert out2.shape == ( 4 , 64 , 64 , 3 ) i = layers.Input ( shape= ( 3 , 2 , 1 ) ) assert o [ 0 ] ._keras_shape == ( None , 64 , 64 , 3 ) assert out2.shape == ( 4 , 3 , 2 , 1 ) assert o [ 1 ] ._keras_shape == ( None , 3 , 2 , 1 ) x = np.random.random ( ( 4 , 64 , 64 , 3 ) )",['tests/keras/layers/core_test.py'],Lambda test speed up ( # 9601 )
518,6249bd5e0fbd1968d9d57eea1bd4547dec8c19c9,2018-03-09 10:59:17-08:00,"axis : The dimension softmax would be performed on . The default is -1 which indicates the last dimension . check_single_tensor_operation ( 'softmax ' , ( 4 , 10 ) , BACKENDS ) check_single_tensor_operation ( 'softmax ' , ( 4 , 5 , 3 , 10 ) , BACKENDS , axis=2 ) return T.exp ( x ) / T.exp ( x ) .sum ( axis=axis , keepdims=True ) return C.softmax ( x ) return C.softmax ( x , axis=axis ) check_single_tensor_operation ( 'softmax ' , ( 4 , 10 ) , BACKENDS ) def softmax ( x , axis=-1 ) : if axis == -1 or axis == x.ndim - 1 : return tf.nn.softmax ( x , axis=axis ) return T.nnet.softmax ( x ) def softmax ( x ) : return tf.nn.softmax ( x ) return T.nnet.softmax ( x )","['keras/backend/cntk_backend.py', 'keras/backend/tensorflow_backend.py', 'keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py']",WIP : Add axis=-1 as an argument to keras.backend.softmax ( # 9600 )
519,839d3dd0b598ecf6f2e93b9f9068c2e21fa1c97b,2018-03-08 17:22:33-08:00,"computed_masks ) ) output_masks = _to_list ( layer.compute_mask ( computed_tensor , output_shape=output_shape ) ( i ) assert o [ 0 ] ._keras_shape == ( None , 64 , 64 , 3 ) out = model2.predict ( x ) i = layers.Input ( shape= ( 64 , 64 , 3 ) ) output_masks = layer.compute_mask ( computed_tensor , test_multiple_outputs_no_mask ( ) x = np.random.random ( ( 4 , 64 , 64 , 3 ) ) ' ' + str ( len ( output_masks ) ) + ' output masks . ' ) if len ( output_masks ) ! = len ( output_tensors ) : return [ x * 0.2 , x * 0.3 ] assert o [ 1 ] ._keras_shape == ( None , 64 , 64 , 3 ) output_masks = layer.compute_mask ( computed_tensors , else : o = layers.add ( o ) if output_masks is None : computed_mask ) model = Model ( i , o ) # explicit mask assert out.shape == ( 4 , 64 , 64 , 3 ) o = layers.Lambda ( function=func , raise Exception ( 'Layers should have equal number of output tensors ' output_masks = [ None for _ in output_tensors ] output_masks = _to_list ( output_masks ) 'and output masks . Layer ' + str ( layer.name ) + ' has ' ' ' + str ( len ( output_tensors ) ) + ' output tensors and ' computed_mask ) ) output_masks = _to_list ( layer.compute_mask ( computed_tensors , computed_masks ) def output_shape ( input_shape ) : # test layer with multiple outputs and no def test_multiple_outputs_no_mask ( ) : i2 = layers.Input ( shape= ( 64 , 64 , 3 ) ) o2 = model ( i2 ) def func ( x ) : return [ input_shape , input_shape ] model2 = Model ( i2 , o2 ) assert_allclose ( out , x * 0.2 + x * 0.3 , atol=1e-4 )","['keras/engine/topology.py', 'tests/keras/layers/core_test.py']",bug fix - run_internal_graph ( ) ( # 9599 )
520,0121ba067e19c023b725f9bee581633a276db890,2018-03-08 11:29:17-08:00,"if x.ndim == 3 : _ , x = parse_shape_or_val ( input_shape ) y = np.transpose ( y , ( 0 , 2 , 1 ) ) ( 'pool3d ' , ( 2 , 8 , 9 , 5 , 3 ) , ( 3 , 2 , 3 ) , ( 1 , 1 , 1 ) , 'valid ' , 'channels_last ' , 'avg ' ) , def test_pool ( self , op , input_shape , pool_size , strides , padding , data_format , pool_mode ) : if data_format == 'channels_last ' : x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) k = K.backend ( ) def test_pool2d ( self ) : ( 'pool2d ' , ( 3 , 6 , 7 , 3 ) , ( 3 , 3 ) , ( 1 , 1 ) , 'same ' , 'channels_last ' , 'max ' ) , for ( l , l1 ) in zip ( range ( pool_size [ 1 ] ) , range ( -pool_size [ 1 ] , 0 ) ) : op , x , [ KTH if k == 'theano ' else KC if k == 'cntk ' else KTF ] , ( 'pool2d ' , ( 2 , 9 , 5 , 3 ) , ( 3 , 2 ) , ( 1 , 1 ) , 'valid ' , 'channels_last ' , 'avg ' ) , def legacy_test_pool3d ( self ) : y.append ( x [ : , : , k : k1 : strides [ 0 ] , l : l1 : strides [ 1 ] ] ) elif pool_mode == 'max ' : y2 = check_single_tensor_operation ( pad = [ ( 0 , 0 ) , ( 0 , 0 ) ] + [ ( s // 2 , s // 2 ) for s in pool_size ] x = np.transpose ( x , ( 0 , 2 , 1 ) ) elif x.ndim == 4 : if pool_mode == 'avg ' : ( 'pool3d ' , ( 3 , 5 , 6 , 7 , 3 ) , ( 3 , 3 , 3 ) , ( 1 , 1 , 1 ) , 'same ' , 'channels_last ' , 'max ' ) , ] ) else : 'constant ' , constant_values=0 ) # indexing trick y = np.stack ( y , axis=-1 ) y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) elif x.ndim == 4 : y = np.max ( y , axis=-1 ) return y padding=padding , data_format=data_format , pool_mode=pool_mode , x = np.pad ( x , [ ( 0 , 0 ) , ( 0 , 0 ) ] + [ ( 0 , 1 ) for _ in pool_size ] , for ( k , k1 ) in zip ( range ( pool_size [ 0 ] ) , range ( -pool_size [ 0 ] , 0 ) ) : def legacy_test_pool2d ( self ) : y = [ ] for ( m , m1 ) in zip ( range ( pool_size [ 2 ] ) , range ( -pool_size [ 2 ] , 0 ) ) : x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) for ( k , k1 ) in zip ( range ( pool_size [ 0 ] ) , range ( -pool_size [ 0 ] , 0 ) ) ] y = [ x [ : , : , k : k1 : strides [ 0 ] ] x = np.pad ( x , pad , 'constant ' , constant_values=-np.inf ) ( 'pool2d ' , ( 3 , 3 , 8 , 5 ) , ( 2 , 3 ) , ( 1 , 1 ) , 'valid ' , 'channels_first ' , 'max ' ) , if y.ndim == 3 : y = np.mean ( np.ma.masked_invalid ( y ) , axis=-1 ) .data def test_pool3d ( self ) : cntk_dynamicity=True , return_results=True ) else : y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) ( 'pool2d ' , ( 2 , 3 , 7 , 7 ) , ( 3 , 3 ) , ( 1 , 1 ) , 'same ' , 'channels_first ' , 'avg ' ) , y1 = ref_pool ( x , pool_size , strides , padding , data_format , pool_mode ) assert_allclose ( y1 , y2 , atol=1e-05 ) if x.ndim == 3 : if padding == 'same ' : elif y.ndim == 4 : ( 'pool3d ' , ( 3 , 3 , 8 , 5 , 9 ) , ( 2 , 3 , 2 ) , ( 1 , 1 , 1 ) , 'valid ' , 'channels_first ' , 'max ' ) , def ref_pool ( x , pool_size , strides , padding , data_format , pool_mode ) : ( 'pool3d ' , ( 2 , 3 , 7 , 7 , 7 ) , ( 3 , 3 , 3 ) , ( 1 , 1 , 1 ) , 'same ' , 'channels_first ' , 'avg ' ) , y.append ( x [ : , : , k : k1 : strides [ 0 ] , l : l1 : strides [ 1 ] , m : m1 : strides [ 2 ] ] ) pool_size=pool_size , strides=strides ,",['tests/keras/backend/backend_test.py'],Make pool backend tests efficient ( # 9580 )
521,70ad0d6e4a569701ef106058397ad0540ec08340,2018-03-07 15:52:44-08:00,"text = text.replace ( c , split ) text = text.translate ( translate_map ) text = text.translate ( translate_map ) translate_map = dict ( ( ord ( c ) , unicode ( split ) ) for c in filters ) text = u'ali ! stopveli ? stopkırkstopdokuzstopelli ' def test_text_to_word_sequence_unicode_multichar_split ( ) : translate_map = maketrans ( translate_dict ) assert text_to_word_sequence ( text , split='stop ' ) == [ 'hello ' , 'world ' ] else : assert text_to_word_sequence ( text , split='stop ' ) == [ u'ali ' , u'veli ' , u'kırk ' , u'dokuz ' , u'elli ' ] translate_map = maketrans ( filters , split * len ( filters ) ) translate_dict = dict ( ( c , split ) for c in filters ) if sys.version_info < ( 3 , ) : elif len ( split ) == 1 : def test_text_to_word_sequence_multichar_split ( ) : translate_map = dict ( ( ord ( c ) , unicode ( split ) ) for c in filters ) if sys.version_info < ( 3 , ) and isinstance ( text , unicode ) : for c in filters : if isinstance ( text , unicode ) : translate_map = maketrans ( filters , split * len ( filters ) ) text = 'hello ! stop ? world ! ' text = text.translate ( translate_map )","['keras/preprocessing/text.py', 'tests/keras/preprocessing/text_test.py']",Correct tokenization with multi-character ` split ` ( # 9585 )
522,1c9a49781da2101507db23e2014e4e5d16bd2e52,2018-03-06 22:17:33-08:00,"pad=padding , pad = ( w_pad , h_pad , d_pad ) pad=pad , pad = ( 0 , 0 , 0 ) padding = ( 0 , 0 , 0 ) padding = ( w_pad , h_pad , d_pad )",['keras/backend/theano_backend.py'],Fix ` pool3d ` padding of Theano ( # 9578 )
523,830c3633176bd7a22955a0264b79242c41f862b5,2018-03-06 12:21:55-08:00,HDF5 and h5py ( required if you plan on saving Keras models to disk ) . HDF5 and [ h5py ] ( http : //docs.h5py.org/en/latest/build.html ) ( required if you plan on saving Keras models to disk ) . [ cuDNN ] ( https : //docs.nvidia.com/deeplearning/sdk/cudnn-install/ ) ( recommended if you plan on running Keras on GPU ) . graphviz and pydot ( used by [ visualization utilities ] ( https : //keras.io/visualization/ ) to plot model graphs ) . [ graphviz ] ( https : //graphviz.gitlab.io/download/ ) and [ pydot ] ( https : //github.com/erocarrera/pydot ) ( used by [ visualization utilities ] ( https : //keras.io/visualization/ ) to plot model graphs ) . cuDNN ( recommended if you plan on running Keras on GPU ) .,['README.md'],Add optional dependencies ' links to download pages in README ( # 9563 )
524,62c395e7b6c7eb405614969035407560c82f5096,2018-03-05 22:25:33-08:00,"_ , x = parse_shape_or_val ( input_shape ) ( 'separable_conv2d ' , ( 2 , 3 , 5 , 6 ) , ( 4 , 3 ) , 2 , 'valid ' , 'channels_first ' ) , ( 'separable_conv2d ' , ( 2 , 3 , 4 , 5 ) , ( 3 , 3 ) , 1 , 'same ' , 'channels_first ' ) , return ref_conv ( x2 , w2 , padding , data_format ) if K.backend ( ) == 'cntk ' : y2 = K.eval ( getattr ( K , op ) ( padding=padding , data_format=data_format ) ) ( 'separable_conv2d ' , ( 1 , 7 , 6 , 3 ) , ( 3 , 3 ) , 2 , 'same ' , 'channels_last ' ) , y2 = cntk_func_three_tensor ( K.variable ( x ) , ( ( 2 , 3 , 5 , 6 ) , 'channels_first ' ) , z_tf = KTF.eval ( KTF.separable_conv2d ( x_tf , KTF.variable ( depthwise_val ) , input_depth = input_shape [ 1 ] if data_format == 'channels_first ' else input_shape [ -1 ] assert_allclose ( z_tf , z_c , 1e-3 ) for ( input_shape , data_format ) in [ def test_separable_conv2d ( self ) : K.variable ( depthwise ) , K.variable ( pointwise ) , op , input_shape , ( ( 2 , 3 , 4 , 5 ) , 'channels_first ' ) , x2 = ref_depthwise_conv ( x , w1 , padding , data_format ) ] ) ( 'separable_conv2d ' , ( 1 , 6 , 5 , 3 ) , ( 3 , 4 ) , 1 , 'valid ' , 'channels_last ' ) , _ , pointwise = parse_shape_or_val ( ( 1 , 1 ) + ( input_depth * depth_multiplier , 7 ) ) for kernel_shape in [ ( 2 , 2 ) , ( 4 , 3 ) ] : else : def ref_separable_conv ( x , w1 , w2 , padding , data_format ) : _ , depthwise_val = parse_shape_or_val ( kernel_shape + ( input_depth , depth_multiplier ) ) y1 = ref_separable_conv ( x , depthwise , pointwise , padding , data_format ) def test_separable_conv2d ( self , op , input_shape , kernel_shape , depth_multiplier , padding , data_format ) : depthwise , pointwise , _ , x_val = parse_shape_or_val ( input_shape ) _ , depthwise = parse_shape_or_val ( kernel_shape + ( input_depth , depth_multiplier ) ) pointwise_val , x_tf = KTF.variable ( x_val ) input_depth = input_shape [ 1 ] if data_format == 'channels_first ' else input_shape [ -1 ] data_format=data_format ) ( [ x_val ] ) [ 0 ] assert_allclose ( y1 , y2 , atol=1e-05 ) for depth_multiplier in [ 1 , 2 ] : depthwise_val , KTF.variable ( pointwise_val ) , padding=padding , data_format=data_format ) ( [ x ] ) [ 0 ] data_format=data_format ) ) _ , pointwise_val = parse_shape_or_val ( ( 1 , 1 ) + ( input_depth * depth_multiplier , 7 ) ) z_c = cntk_func_three_tensor ( 'separable_conv2d ' , input_shape , ( ( 1 , 6 , 5 , 3 ) , 'channels_last ' ) ] :",['tests/keras/backend/backend_test.py'],Make separable conv backend tests efficient ( # 9570 )
525,16fba547a15958225077303635b816c68263e73d,2018-03-05 14:05:11-08:00,"if x.ndim == 3 : kernel_val = np.arange ( np.prod ( kernel_shape ) ) .reshape ( kernel_shape ) return wrapper w = np.transpose ( w , ( 2 , 3 , 0 , 1 ) ) k = K.backend ( ) z = k.eval ( k.depthwise_conv2d ( k.variable ( x_val ) , k.variable ( kernel_val ) , def ref_depthwise_conv ( x , w , padding , data_format ) : w = np.transpose ( w , ( 1 , 2 , 0 ) ) w = args [ 1 ] w = np.flip ( np.fliplr ( np.flipud ( w ) ) , axis=2 ) if data_format == 'channels_first ' : for i in range ( x.shape [ 0 ] ) : op , x , w , [ KTH if k == 'theano ' else KC if k == 'cntk ' else KTF ] , else : __y = [ ] def normalize_ref_conv ( func ) : y = [ ] input_shape = ( 2 , ) + x_shape + ( 3 , ) for j in range ( w.shape [ 0 ] ) : _ , w = parse_shape_or_val ( kernel_shape ) _y.append ( np.stack ( __y , axis=0 ) ) for z_i in np.split ( z , 6 , axis=1 if data_format == 'channels_first ' else -1 ) : y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) padding=padding , data_format=data_format , y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) _ , x = parse_shape_or_val ( input_shape ) return y y = func ( x , w , args [ 2 ] , args [ 3 ] ) y = np.array ( y ) input_shape = ( 2 , 3 ) + x_shape ] ) for k in range ( w.shape [ 1 ] ) : x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) w = np.fliplr ( np.flipud ( w ) ) if data_format == 'channels_last ' : return y y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) def test_depthwise_conv ( self , op , input_shape , kernel_shape , padding , data_format ) : _y = [ ] if y.ndim == 3 : elif y.ndim == 4 : def wrapper ( * args ) : else : @ pytest.mark.parametrize ( ' k ' , [ KTF ] , ids= [ 'TensorFlow ' ] ) else : w = np.transpose ( w , ( 3 , 4 , 0 , 1 , 2 ) ) x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) y1 = ref_depthwise_conv ( x , w , padding , data_format ) y = np.transpose ( y , ( 0 , 2 , 1 ) ) x_val = np.ones ( input_shape ) ( 'depthwise_conv2d ' , ( 1 , 6 , 5 , 3 ) , ( 3 , 4 , 3 , 2 ) , 'valid ' , 'channels_last ' ) , ( 'depthwise_conv2d ' , ( 2 , 3 , 5 , 6 ) , ( 4 , 3 , 3 , 4 ) , 'valid ' , 'channels_first ' ) , x_shape = ( 4 , 4 ) ( 'depthwise_conv2d ' , ( 2 , 3 , 4 , 5 ) , ( 3 , 3 , 3 , 2 ) , 'same ' , 'channels_first ' ) , for data_format in [ 'channels_first ' , 'channels_last ' ] : y2 = check_two_tensor_operation ( def test_depthwise_conv_2d ( self ) : elif x.ndim == 4 : if y.ndim == 3 : x = np.transpose ( x , ( 0 , 2 , 1 ) ) w = np.flip ( np.fliplr ( np.flipud ( w ) ) , axis=2 ) ( 'depthwise_conv2d ' , ( 1 , 7 , 6 , 3 ) , ( 3 , 3 , 3 , 4 ) , 'same ' , 'channels_last ' ) , w = np.transpose ( w , ( 2 , 3 , 0 , 1 ) ) assert_allclose ( z_i , z_i [ 0 ] * np.ones_like ( z_i ) ) elif y.ndim == 4 : if x.ndim == 3 : data_format=data_format ) ) __y.append ( signal.convolve ( x [ i , j ] , w [ j , k ] , mode=padding ) ) w = np.transpose ( w , ( 1 , 2 , 0 ) ) kernel_shape = ( 3 , 3 , 3 , 2 ) if data_format == 'channels_last ' : else : w = np.transpose ( w , ( 3 , 4 , 0 , 1 , 2 ) ) x = np.transpose ( x , ( 0 , 2 , 1 ) ) if args [ 3 ] == 'channels_last ' : elif x.ndim == 4 : y.append ( np.concatenate ( _y , axis=0 ) ) x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) if args [ 3 ] == 'channels_last ' : def test_depthwise_conv_2d ( self , k ) : def legacy_test_depthwise_conv_2d ( self ) : w = np.fliplr ( np.flipud ( w ) ) w = np.flipud ( w ) x = args [ 0 ] cntk_dynamicity=True , return_results=True ) assert_allclose ( y1 , y2 , atol=1e-05 ) elif data_format == 'channels_last ' : w = np.flipud ( w ) y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) y = np.transpose ( y , ( 0 , 2 , 1 ) )",['tests/keras/backend/backend_test.py'],Make depthwise conv backend tests efficient ( # 9558 )
526,ae72ea0bfe7f7041d6f7580427fa1f69158bab7e,2018-03-05 14:02:05-08:00,"{ 'go_backwards ' : True , 'mask ' : None } , def legacy_test_rnn ( self ) : go_backwards=False , mask=None ) h2 = h2 * np.expand_dims ( mask [ : , -1 ] , -1 ) assert len ( h_k ) == 0 if w_h is not None : assert len ( h2 ) == 0 kwargs_list = [ h.append ( h_t + h_t1 ) for ( i , t ) in enumerate ( t_list ) : for ( i , kwargs ) in enumerate ( kwargs_list ) : return y_k , [ y_k ] def test_rnn_no_states ( self ) : last_y1 = last_y1 * np.expand_dims ( mask [ : , -1 ] , -1 ) y_k = K.dot ( x_k , wi_k ) + K.dot ( h_k [ 0 ] , wh_k ) else : # implement a simple RNN o_t = np.dot ( o_t , w_o ) h = [ ] y_k = K.dot ( x_k , wi_k ) _ , x = parse_shape_or_val ( ( num_samples , timesteps , input_dim ) ) input_dim = 8 if kwargs [ 'mask ' ] is not None : if go_backwards : mask = np.random.randint ( 2 , size= ( num_samples , timesteps ) ) t_list = range ( x.shape [ 1 ] - 1 , -1 , -1 ) output_dim = 4 last_output_list.append ( last_y2 ) return o [ -1 ] , np.stack ( o , axis=1 ) , np.stack ( h , axis=1 ) timesteps = 5 h_t1 = np.dot ( prev , w_h ) h0_k = [ K.variable ( h0 ) ] { 'go_backwards ' : True , 'mask ' : None , 'unroll ' : True , 'input_length ' : timesteps } , { 'go_backwards ' : False , 'mask ' : None , 'unroll ' : True , 'input_length ' : timesteps } , outputs_list.append ( y2 ) if i % 2 == 0 : prev = h [ i - 1 ] if i > 0 else init last_y1 , y1 , h1 = ref_rnn ( x , [ wi , wh , None ] , h0 , * * kwargs ) input_dim = 5 return y_k , [ ] def ref_rnn ( x , w , init , go_backwards=False , mask=None , unroll=False , input_length=None ) : assert len ( h_k ) == 1 if np_mask is not None : y2 = y2 * np.expand_dims ( mask , -1 ) last_output_list = [ ] last_y2 , y2 , h2 = K.rnn ( rnn_fn , x_k , [ ] , else : last_y2 , y2 , h2 = K.rnn ( rnn_fn , x_k , h0_k , * * kwargs ) ] assert_allclose ( outputs_list [ i - 1 ] , outputs_list [ i ] , atol=1e-05 ) last_y2 = K.eval ( last_y2 ) assert len ( h2 ) == 1 state_list.append ( h2 ) def test_rnn_no_states ( self ) : def rnn_fn ( x_k , h_k ) : x_k = K.variable ( x ) assert_allclose ( last_y1 , last_y2 , atol=1e-05 ) h_t1 = 0 if np_mask is not None : h1 = h1 * np.expand_dims ( mask [ : , -1 ] , -1 ) np_mask = K.eval ( mask ) last_y2 = last_y2 * np.expand_dims ( mask [ : , -1 ] , -1 ) assert_allclose ( h1 , h2 , atol=1e-05 ) outputs_list = [ ] y2 = K.eval ( y2 ) h1 = h1 [ : , -1 ] _ , wh = parse_shape_or_val ( ( output_dim , output_dim ) ) np_mask = None o_t = h_t + h_t1 go_backwards=False , mask=None ) assert_allclose ( last_output_list [ i - 1 ] , last_output_list [ i ] , atol=1e-05 ) h_t1 [ np_mask [ : , t ] == 0 ] = prev [ np_mask [ : , t ] == 0 ] _ , x = parse_shape_or_val ( ( 32 , timesteps , input_dim ) ) { 'go_backwards ' : False , 'mask ' : None } , state_list = [ ] timesteps = 6 last_y1 , y1 , h1 = ref_rnn ( x , [ wi , None , None ] , None , assert_allclose ( state_list [ i - 1 ] , state_list [ i ] , atol=1e-05 ) t_list = range ( x.shape [ 1 ] ) mask_k = K.variable ( mask ) o = [ ] assert_allclose ( last_y1 , last_y2 , atol=1e-05 ) h_t = np.dot ( x [ : , t ] , w_i ) { 'go_backwards ' : False , 'mask ' : mask_k , 'unroll ' : True , 'input_length ' : timesteps } , w_i , w_h , w_o = w # test default setup else : y1 = y1 * np.expand_dims ( mask , -1 ) wi_k = K.variable ( wi ) wh_k = K.variable ( wh ) last_y2 = K.eval ( last_y2 ) o.append ( o_t ) h2 = K.eval ( h2 [ 0 ] ) output_dim = 3 if mask is not None : def legacy_test_rnn_no_states ( self ) : y2 = K.eval ( y2 ) { 'go_backwards ' : False , 'mask ' : mask_k } , _ , h0 = parse_shape_or_val ( ( num_samples , output_dim ) ) num_samples = 4 _ , wi = parse_shape_or_val ( ( input_dim , output_dim ) ) # implement a simple RNN without states assert_allclose ( y1 , y2 , atol=1e-05 ) assert_allclose ( y1 , y2 , atol=1e-05 ) if w_o is not None : h_t = h_t * np_mask [ : , t ] .reshape ( -1 , 1 )",['tests/keras/backend/backend_test.py'],Make RNN backend tests efficient ( # 9557 )
527,7a1b9005c220afb9271d20a73e168cf18d0cd631,2018-03-04 13:23:28-08:00,"return samples , targets indices = range ( rows [ j ] - self.length , rows [ j ] , self.sampling_rate ) length=10 , sampling_rate=2 , shuffle=True , data = np.array ( [ np.random.random_sample ( ( 1 , 2 , 3 , 4 ) ) for i in range ( 50 ) ] ) length=10 , sampling_rate=2 , batch_size=2 ) A ` Sequence ` instance . shuffle : whether to shuffle training samples , or draw them in np.array ( [ [ [ 10 ] , [ 12 ] , [ 14 ] , [ 16 ] , [ 18 ] ] , training/validation . assert ( np.allclose ( data_gen [ 0 ] [ 0 ] , # Examples self.targets = targets to be used for batch generation . Useful to reserve part of def test_TimeseriesGenerator ( ) : length=10 , sampling_rate=2 , Should have same length as ` data ` . self.reverse = reverse data_gen = TimeseriesGenerator ( data , targets , np.array ( [ [ [ 0 ] , [ 2 ] , [ 4 ] , [ 6 ] , [ 8 ] ] , equal intervals , along with time series parameters such as def __len__ ( self ) : if self.reverse : targets_shape = [ num_rows ] ( self.end_index - self.start_index ) / be centered around ` data [ i ] ` , ` data [ i+s ] ` , ` data [ i+2 * s ] ` , ... This class takes in a sequence of data-points gathered at assert ( np.allclose ( batch [ 0 ] , targets = np.array ( [ [ i ] for i in range ( 50 ) ] ) end_index=None , for j , row in enumerate ( rows ) : assert ( np.allclose ( data_gen [ 0 ] [ 1 ] , np.array ( [ [ [ 2 ] , [ 4 ] , [ 6 ] , [ 8 ] , [ 10 ] ] , if self.shuffle : assert np.allclose ( data_gen [ 0 ] [ 0 ] , np.array ( self.batch_size = batch_size return np.empty ( samples_shape ) , np.empty ( targets_shape ) return samples [ : , : :-1 , ... ] , targets samples , targets = self._empty_batch ( len ( rows ) ) start_index=10 , end_index=30 , def __getitem__ ( self , index ) : targets = np.array ( [ [ i ] for i in range ( 50 ) ] ) self.stride = stride if end_index is None : np.array ( [ [ 12 ] , [ 13 ] ] ) ) ) np.array ( [ [ [ 8 ] , [ 6 ] , [ 4 ] , [ 2 ] , [ 0 ] ] , targets [ j ] = self.targets [ rows [ j ] ] self.sampling_rate = sampling_rate [ r - 6 ] , assert len ( data_gen ) == 20 ` data [ i-length : i+1 ] ` is used to prepare the input . reverse=False , [ [ 9 ] , [ 7 ] , [ 5 ] , [ 3 ] , [ 1 ] ] ] ) ) ) r = batch [ 1 ] [ 0 ] [ 0 ] length=10 , sampling_rate=2 , stride=2 , np.array ( [ [ 10 ] , [ 11 ] ] ) ) ) `` ` rows = np.arange ( i , min ( i + self.batch_size * [ np.array ( data [ 10:19:2 ] ) , np.array ( data [ 11:20:2 ] ) ] ) ) data = np.array ( [ [ i ] for i in range ( 50 ) ] ) np.array ( [ [ 14 ] , [ 16 ] ] ) ) ) batch_size=1 ) self.stride , self.end_index ) , self.stride ) end_index = len ( data ) - 1 assert ( np.allclose ( batch [ 1 ] , np.array ( [ [ r ] , ] ) ) ) assert ( np.allclose ( data_gen [ 1 ] [ 1 ] , batch_size=2 ) self.length = length [ r - 4 ] , targets = np.array ( [ np.random.random_sample ( ( 3 , 2 , 1 ) ) for i in range ( 50 ) ] ) [ [ 6 ] , [ 8 ] , [ 10 ] , [ 12 ] , [ 14 ] ] ] ) ) ) assert ( np.array_equal ( data_gen [ 0 ] [ 0 ] , [ r - 2 ] ] ] ) ) ) np.array ( [ [ 10 ] , [ 11 ] ] ) ) ) `` `` '' self.shuffle = shuffle i = self.start_index + self.batch_size * self.stride * index assert len ( data_gen ) == 20 containing consecutive data points . assert len ( data_gen ) == 10 from keras.preprocessing.sequence import TimeseriesGenerator def __init__ ( self , data , targets , sampling_rate=1 , [ [ 1 ] , [ 3 ] , [ 5 ] , [ 7 ] , [ 9 ] ] ] ) ) ) data : indexable generator ( such as list or Numpy array ) chronological order . assert ( np.array_equal ( data_gen [ 0 ] [ 1 ] , `` ` data = np.array ( [ [ i ] for i in range ( 50 ) ] ) samples [ j ] = self.data [ indices ] start_index=0 , # Returns rows = np.random.randint ( self.start_index , self.end_index , size=self.batch_size ) `` `` '' Utility class for generating batches of temporal data . np.array ( [ [ [ 0 ] , [ 2 ] , [ 4 ] , [ 6 ] , [ 8 ] ] , from .. utils.data_utils import Sequence data_gen = TimeseriesGenerator ( data , targets , batch = data_gen [ 0 ] [ r - 8 ] , np.array ( [ [ 20 ] , [ 21 ] ] ) ) ) ( self.batch_size * self.stride ) ) ) stride , length of history etc. , to produce batches for np.array ( [ [ [ 4 ] , [ 6 ] , [ 8 ] , [ 10 ] , [ 12 ] ] , shuffle=False , samples_shape.extend ( self.data.shape [ 1 : ] ) [ [ 11 ] , [ 13 ] , [ 15 ] , [ 17 ] , [ 19 ] ] ] ) ) ) batch_size : number of timeseries samples in each batch np.array ( [ targets [ 20 ] , targets [ 21 ] ] ) ) ) ` data [ i ] ` , ` data [ i-r ] ` , ... ` data [ i - length ] ` are used as input . batch_size=128 ) : class TimeseriesGenerator ( Sequence ) : samples_shape = [ num_rows , self.length // self.sampling_rate ] # Arguments training . For stride ` s ` , consecutive training samples would reverse : if ` true ` , input points are provided in reverse order . self.end_index = end_index else : length : length of history to consider for prediction . targets_shape.extend ( self.targets.shape [ 1 : ] ) [ [ 1 ] , [ 3 ] , [ 5 ] , [ 7 ] , [ 9 ] ] ] ) ) ) length=10 , sampling_rate=2 , reverse=True , stride=1 , stride : period between successive sub-sequences used for points . For rate ` r ` , points length , def _empty_batch ( self , num_rows ) : np.array ( [ [ [ r - 10 ] , assert ( np.allclose ( data_gen [ 1 ] [ 0 ] , data for validation . assert len ( data_gen ) == 5 return int ( np.ceil ( self.start_index = start_index + length sampling_rate : period between successive individual data targets : targets corresponding to instances in ` data ` . self.data = data [ [ 3 ] , [ 5 ] , [ 7 ] , [ 9 ] , [ 11 ] ] ] ) ) ) start_index , end_index : delimiters for sub-sets of ` data ` & ` targets ` ( except maybe last one )","['keras/preprocessing/sequence.py', 'tests/keras/preprocessing/sequence_test.py']",TimeSeriesSequence to generate time-series data efficiently ( # 9450 )
528,f47e362a18dee71f31f350a660ed2883699df05a,2018-03-04 10:55:14-08:00,"def ref_conv ( x , w , padding , data_format ) : w = np.transpose ( w , ( 3 , 4 , 0 , 1 , 2 ) ) _ , x = parse_shape_or_val ( input_shape ) y = np.transpose ( y , ( 0 , 2 , 1 ) ) import scipy.signal as signal y.append ( _y ) if data_format == 'channels_last ' : x = np.transpose ( x , ( 0 , 3 , 1 , 2 ) ) y = np.array ( y ) if return_results : _y.append ( np.sum ( np.stack ( __y , axis=-1 ) , axis=-1 ) ) k = K.backend ( ) def test_conv ( self , op , input_shape , kernel_shape , padding , data_format ) : w = np.fliplr ( np.flipud ( w ) ) ( 'conv2d ' , ( 1 , 6 , 5 , 3 ) , ( 3 , 4 , 3 , 2 ) , 'valid ' , 'channels_last ' ) , if len ( z_list ) > 1 : ( 'conv3d ' , ( 1 , 2 , 2 , 2 , 1 ) , ( 2 , 2 , 2 , 1 , 1 ) , 'valid ' , 'channels_last ' ) , x = np.transpose ( x , ( 0 , 2 , 1 ) ) elif x.ndim == 4 : w = np.transpose ( w , ( 1 , 2 , 0 ) ) ( 'conv2d ' , ( 1 , 7 , 6 , 3 ) , ( 3 , 3 , 3 , 4 ) , 'same ' , 'channels_last ' ) , ( 'conv3d ' , ( 1 , 3 , 5 , 4 , 2 ) , ( 3 , 3 , 3 , 2 , 3 ) , 'same ' , 'channels_last ' ) , ( 'conv2d ' , ( 2 , 3 , 4 , 5 ) , ( 3 , 3 , 3 , 2 ) , 'same ' , 'channels_first ' ) , ] ) def test_conv2d ( self ) : else : for i in range ( x.shape [ 0 ] ) : op , x , w , [ KTH if k == 'theano ' else KC if k == 'cntk ' else KTF ] , y = np.transpose ( y , ( 0 , 2 , 3 , 1 ) ) ( 'conv3d ' , ( 2 , 3 , 4 , 5 , 4 ) , ( 3 , 3 , 3 , 3 , 4 ) , 'same ' , 'channels_first ' ) , def legacy_test_conv2d ( self ) : __y = [ ] y2 = check_two_tensor_operation ( return y return z_list y1 = ref_conv ( x , w , padding , data_format ) x = np.transpose ( x , ( 0 , 4 , 1 , 2 , 3 ) ) y = [ ] _y = [ ] w = np.transpose ( w , ( 2 , 3 , 0 , 1 ) ) w = np.flip ( np.fliplr ( np.flipud ( w ) ) , axis=2 ) w = np.flipud ( w ) def test_conv1d ( self ) : if y.ndim == 3 : for k in range ( w.shape [ 0 ] ) : ( 'conv3d ' , ( 2 , 3 , 5 , 4 , 6 ) , ( 3 , 2 , 4 , 3 , 4 ) , 'valid ' , 'channels_first ' ) , ( 'conv1d ' , ( 2 , 8 , 2 ) , ( 3 , 2 , 3 ) , 'same ' , 'channels_last ' ) , ( 'conv1d ' , ( 1 , 8 , 2 ) , ( 3 , 2 , 3 ) , 'valid ' , 'channels_last ' ) , cntk_dynamicity=True , return_results=True ) else : y = np.transpose ( y , ( 0 , 2 , 3 , 4 , 1 ) ) _ , w = parse_shape_or_val ( kernel_shape ) def legacy_test_conv1d ( self ) : return_results = kwargs.pop ( 'return_results ' , False ) return z_list [ 0 ] if x.ndim == 3 : ( 'conv2d ' , ( 2 , 3 , 5 , 6 ) , ( 4 , 3 , 3 , 4 ) , 'valid ' , 'channels_first ' ) , assert_allclose ( y1 , y2 , atol=1e-05 ) if data_format == 'channels_last ' : padding=padding , data_format=data_format , __y.append ( signal.convolve ( x [ i , k ] , w [ k , j ] , mode=padding ) ) elif y.ndim == 4 : def legacy_test_conv3d ( self ) : for j in range ( w.shape [ 1 ] ) : def test_conv3d ( self ) :",['tests/keras/backend/backend_test.py'],Make conv backend tests efficient ( # 9555 )
529,9118ea65f40874e915dd1299efd1cc3a7ca2c333,2018-03-03 14:19:52-08:00,"validation_generator = validation_data for cbk in callbacks : val_data += [ 0 . ] assert 3 < = gen_counters [ 0 ] < = 5 wait_time=wait_time ) val_sample_weight = None val_x , val_y , val_sample_weight ) max_queue_size=max_queue_size ) use_multiprocessing=use_multiprocessing , 'or ` ( val_x , val_y ) ` . Found : ' val_x , val_y = validation_data val_enqueuer = GeneratorEnqueuer ( str ( validation_data ) ) ' ` ( val_x , val_y , val_sample_weight ) ` ' if do_validation and not val_gen : cbk.validation_data = val_data str ( validation_data ) ) ' ` ( val_x , val_y , val_sample_weight ) ` ' cbk.validation_data = val_data if val_gen : raise ValueError ( ' ` validation_data ` should be a tuple ' if validation_steps is None : if self.uses_learning_phase and not isinstance ( K.learning_phase ( ) , int ) : if self.uses_learning_phase and not isinstance ( K.learning_phase ( ) , int ) : use_multiprocessing=use_multiprocessing , validation_generator , val_x , val_y = validation_data elif len ( validation_data ) == 3 : validation_data , raise ValueError ( ' ` validation_data ` should be a tuple ' val_x , val_y , val_sample_weight = validation_data val_enqueuer.start ( workers=workers , max_queue_size=max_queue_size ) if do_validation : assert 3 < = gen_counters [ 0 ] < = 12 val_x , val_y , val_sample_weights = self._standardize_user_data ( val_x , val_y , val_sample_weight = validation_data validation_generator = iter ( validation_data ) val_data += [ 0 . ] # Prepare data for validation val_enqueuer = OrderedEnqueuer ( 'or ` ( val_x , val_y ) ` . Found : ' validation_steps = len ( validation_data ) if workers > 0 : validation_generator = val_enqueuer.get ( ) val_x , val_y , val_sample_weights = self._standardize_user_data ( if isinstance ( validation_data , Sequence ) : workers=workers , use_multiprocessing=use_multiprocessing ) else : for cbk in callbacks : val_data = val_x + val_y + val_sample_weights val_x , val_y , val_sample_weight ) if len ( validation_data ) == 2 : if len ( validation_data ) == 2 : workers=0 ) val_sample_weight = None else : elif len ( validation_data ) == 3 : # 12 = ( epoch * workers * validation steps * max_queue_size ) assert 3 < = gen_counters [ 1 ] < = 12 val_data = val_x + val_y + val_sample_weights validation_data , assert 3 < = gen_counters [ 1 ] < = 5","['keras/engine/training.py', 'tests/keras/engine/test_training.py']",Cleanup fit_genenerator for validation ( # 9537 )
530,ca27e445a15fba61401b2ee23765349515d3f28d,2018-03-02 15:05:18-08:00,"print ( ' Q ' , q [ : :-1 ] if INVERT else q , end= ' ' ) if INVERT : Four digits reversed : Five digits reversed : REVERSE = True Four digits inverted : Three digits reversed : Two digits reversed : Input may optionally be reversed , shown to increase performance in many tasks in : print ( ' Q ' , q [ : :-1 ] if REVERSE else q , end= ' ' ) Two digits inverted : Three digits inverted : if REVERSE : INVERT = True Input may optionally be inverted , shown to increase performance in many tasks in : Five digits inverted :",['examples/addition_rnn.py'],Changed `` invert the input '' to `` reverse the input '' ( # 9542 )
531,500401bac7dfbb922408de79a1e217856cc7c627,2018-03-01 11:36:20-08:00,"return y_true * K.square ( K.relu ( 1 - margin - y_pred ) ) + lamb * ( return K.sum ( y_true * K.square ( K.relu ( 1 - margin - y_pred ) ) + lamb * ( 1 - y_true ) * K.square ( K.relu ( y_pred - margin ) ) , axis=-1 ) 1 - y_true ) * K.square ( K.relu ( y_pred - margin ) )",['examples/cifar10_cnn_capsule.py'],bug-fix cifar10_cnn_capsule.py missing K.sum ( ) ( # 9520 )
532,d79200a51710a23cc47b4ca5ad3f8df7c32f4eae,2018-02-28 10:43:03-08:00,"k_input_lens = k.variable ( input_lens , dtype= '' int32 '' ) def test_batch_dot_shape ( self ) : assert_allclose ( K.eval ( xy_batch_dot ) , np.ones ( ( 20 , 1 ) ) * 32 , atol=1e-05 ) xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes= ( 1 , 0 ) ) ( KTH , [ 1.73308 , 3.81351 ] ) , k_inputs = K.variable ( inputs , dtype= '' float32 '' ) k_input_lens = K.variable ( input_lens , dtype= '' int32 '' ) k_label_lens = k.variable ( label_lens , dtype= '' int32 '' ) @ pytest.mark.parametrize ( ' k ' , [ KTH , KTF ] , ids= [ 'Theano ' , 'TensorFlow ' ] ) y_batch = K.ones ( shape= ( 20 , 32 ) ) assert_allclose ( res [ 0 , : ] if K.backend ( ) == 'theano ' else res [ : , 0 ] , ref , atol=1e-05 ) assert_allclose ( res [ : , 0 ] if k == KTF else res [ 0 , : ] , ref , atol=1e-05 ) kx = k.eval ( k.foldl ( lambda a , b : a + b , k.variable ( x ) ) ) res = k.eval ( k.ctc_batch_cost ( k_labels , k_inputs , k_input_lens , k_label_lens ) ) kx2 = K.eval ( K.map_fn ( ref = [ 3.34211 , 5.42262 ] kx = K.eval ( K.foldl ( lambda a , b : a + b , K.variable ( x ) ) ) p1 = K.eval ( K.foldl ( lambda a , b : a * b , vx ) ) lambda i : k.sum ( vx [ i ] ) , dtype=k.floatx ( ) def test_ctc ( self , k , ref ) : def test_map ( self ) : kx2 = k.eval ( k.map_fn ( xy_batch_dot = k.batch_dot ( x_batch , y_batch , axes=0 ) x_batch = K.ones ( shape= ( 32 , 20 ) ) xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes=0 ) xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes= ( 0 , 1 ) ) ] , ids= [ 'TensorFlow ' , 'Theano ' ] ) else : assert_allclose ( k.eval ( xy_batch_dot ) , np.ones ( ( 20 , 1 ) ) * 32 , atol=1e-05 ) def test_ctc ( self ) : k_label_lens = K.variable ( label_lens , dtype= '' int32 '' ) ref = [ 1.73308 , 3.81351 ] def test_map ( self , k ) : ( KTF , [ 3.34211 , 5.42262 ] ) , K.arange ( 10 ) , assert_allclose ( K.eval ( xy_batch_dot ) , np.ones ( ( 32 , 1 ) ) * 20 , atol=1e-05 ) y_batch = K.ones ( shape= ( 32 , 20 ) ) def test_foldr ( self ) : vx = k.variable ( x ) p1 = k.eval ( k.foldl ( lambda a , b : a * b , vx ) ) @ pytest.mark.parametrize ( ' k , ref ' , [ if K.backend ( ) == 'theano ' : x_batch = k.ones ( shape= ( 32 , 20 ) ) xy_batch_dot = k.batch_dot ( x_batch , y_batch , axes= ( 1 , 0 ) ) k_labels = k.variable ( labels , dtype= '' int32 '' ) xy_batch_dot = k.batch_dot ( x_batch , y_batch , axes=1 ) def test_foldr ( self , k ) : y_batch = k.ones ( shape= ( 20 , 32 ) ) def test_batch_dot_shape ( self , k ) : res = K.eval ( K.ctc_batch_cost ( k_labels , k_inputs , k_input_lens , k_label_lens ) ) p2 = K.eval ( K.foldr ( lambda a , b : a * b , vx ) ) assert_allclose ( k.eval ( xy_batch_dot ) , np.ones ( ( 32 , 1 ) ) * 20 , atol=1e-05 ) kx = K.eval ( K.map_fn ( K.sum , vx ) ) kx = k.eval ( k.map_fn ( k.sum , vx ) ) k.arange ( 10 ) , dtype=K.floatx ( ) xy_batch_dot = k.batch_dot ( x_batch , y_batch , axes= ( 0 , 1 ) ) vx = K.variable ( x ) k_inputs = k.variable ( inputs , dtype= '' float32 '' ) @ pytest.mark.parametrize ( ' k ' , [ KTF ] , ids= [ 'TensorFlow ' ] ) p2 = k.eval ( k.foldr ( lambda a , b : a * b , vx ) ) y_batch = k.ones ( shape= ( 32 , 20 ) ) xy_batch_dot = K.batch_dot ( x_batch , y_batch , axes=1 ) def test_foldl ( self , k ) : k_labels = K.variable ( labels , dtype= '' int32 '' ) def test_foldl ( self ) : lambda i : K.sum ( vx [ i ] ) ,",['tests/keras/backend/backend_test.py'],Replace ` parametrize ` with ` skipif ` ( # 9511 )
533,4b74fc5418944c9f449eb88ed4b40ada280fa5ca,2018-02-28 10:42:14-08:00,"if is_sequence : output_generator = generator validation_generator = validation_data use_multiprocessing=False ) use_multiprocessing=True ) output_generator = generator else : yield item steps_per_epoch=STEPS_PER_EPOCH , while True : from keras.utils import Sequence validation_data=custom_generator ( True ) , model.fit_generator ( DummySequence ( ) , for item in ( self [ i ] for i in range ( len ( self ) ) ) : class DummySequence ( Sequence ) : `` `` '' Create an infinite generator that iterate over the Sequence . '' '' '' def __getitem__ ( self , idx ) : if isinstance ( validation_data , Sequence ) : # - For Sequence def __iter__ ( self ) : validation_generator = iter ( validation_data ) workers=0 , validation_generator = validation_data return 10 def __len__ ( self ) : max_queue_size=10 , output_generator = iter ( generator ) return np.zeros ( [ 10 , 2 ] ) , np.ones ( [ 10 ] ) validation_steps=1 , else :","['keras/engine/training.py', 'keras/utils/data_utils.py', 'tests/test_multiprocessing.py']",Fix sequence bug ( # 9513 )
534,07c77c65b5ea9c246690d2eb116c7c8570dc5a7e,2018-02-25 18:20:57-08:00,"mode='average_exc_pad ' ) th_avg_pool_mode = 'average_inc_pad ' pool_size= ( 3 , 3 ) , strides= ( 1 , 1 ) , BACKENDS , cntk_dynamicity=True , check_single_tensor_operation ( 'pool2d ' , ( 2 , 7 , 7 , 5 ) , th_avg_pool_mode = 'average_exc_pad ' elif padding == 'valid ' : if padding == 'same ' : padding='same ' , pool_mode='avg ' ) mode=th_avg_pool_mode )","['keras/backend/theano_backend.py', 'tests/keras/backend/backend_test.py']",Fix ` pool_2d ` of Theano for backend compatibility ( # 9479 )
535,5e797436c3defd2d863ac1ffab11c48dbd42588e,2018-02-25 10:17:03-08:00,"k.variable ( pointwise_val ) , k.conv2d ( k.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , with pytest.raises ( ValueError ) : xval = np.random.random ( input_shape ) if k ! = KTH : k.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , data_format='channels_middle ' ) for k in [ KTF , KC ] : for k in BACKENDS : k.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , ( ( 2 , 3 , 5 , 6 ) , 'channels_first ' ) , kernel_shape = ( 3 , 3 , 3 , 2 ) BACKENDS , cntk_dynamicity=True , input_shape = ( 1 , 2 , 2 , 2 , 1 ) ( ( 2 , 3 , 4 , 5 , 4 ) , ( 2 , 2 , 2 , 3 , 4 ) , 'channels_first ' ) , ( ( 2 , 3 , 4 , 5 ) , 'channels_first ' ) , ( ( 1 , 2 , 2 , 2 , 1 ) , ( 2 , 2 , 2 , 1 , 1 ) , 'channels_last ' ) ] : input_shape = ( 1 , 6 , 5 , 3 ) ( ( 2 , 3 , 5 , 4 , 6 ) , ( 3 , 2 , 4 , 3 , 4 ) , 'channels_first ' ) , check_two_tensor_operation ( 'conv3d ' , input_shape , kernel_shape , ( ( 2 , 3 , 5 , 6 ) , 'channels_first ' ) , for ( input_shape , kernel_shape , data_format ) in [ k.conv1d ( k.variable ( xval ) , k.variable ( kernel_val ) , data_format='channels_middle ' ) k.variable ( np.ones ( ( 2 , 2 , 2 , 3 , 4 ) ) ) , kernel_shape = ( 2 , 2 , 2 , 1 , 1 ) ( ( 1 , 6 , 5 , 3 ) , 'channels_last ' ) ] : k.separable_conv2d ( k.variable ( x_val ) , k.variable ( depthwise_val ) , k.conv3d ( k.variable ( np.ones ( ( 2 , 3 , 4 , 5 , 4 ) ) ) , data_format='channels_last ' ) kernel_val = np.random.random ( kernel_shape ) - 0.5 ( ( 2 , 3 , 4 , 5 ) , ( 2 , 2 , 3 , 4 ) , 'channels_first ' ) , for kernel_shape in [ ( 2 , 2 , 2 , 3 , 4 ) , ( 3 , 2 , 4 , 3 , 4 ) ] : k.conv1d ( k.variable ( np.ones ( ( 4 , 8 , 2 ) ) ) , data_format='channels_middle ' ) k.depthwise_conv2d ( k.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , k.depthwise_conv2d ( k.variable ( np.ones ( input_shape ) ) , k.variable ( np.ones ( kernel_shape ) ) , with pytest.raises ( ValueError ) : for input_shape in [ ( 2 , 3 , 4 , 5 , 4 ) , ( 2 , 3 , 5 , 4 , 6 ) ] : # test in data_format = channels_last with pytest.raises ( ValueError ) : k.conv3d ( k.variable ( xval ) , k.variable ( kernel_val ) , data_format='channels_middle ' ) check_two_tensor_operation ( 'conv2d ' , input_shape , kernel_shape , ( ( 2 , 3 , 5 , 6 ) , ( 4 , 3 , 3 , 4 ) , 'channels_first ' ) , k.conv2d ( k.variable ( xval ) , k.variable ( kernel_val ) , data_format='channels_middle ' ) for ( input_shape , data_format ) in [ ( ( 2 , 3 , 4 , 5 ) , 'channels_first ' ) , data_format='channels_middle ' ) BACKENDS , cntk_dynamicity=True , data_format=data_format ) k.variable ( np.ones ( ( 3 , 2 , 3 ) ) ) , k.variable ( np.ones ( ( 2 , 2 , 3 , 4 ) ) ) , for kernel_shape in [ ( 2 , 2 , 3 , 4 ) , ( 4 , 3 , 3 , 4 ) ] : BACKENDS , cntk_dynamicity=True , check_two_tensor_operation ( 'conv3d ' , input_shape , kernel_shape , def test_conv_invalid_use ( self ) : for k in BACKENDS : ( ( 1 , 6 , 5 , 3 ) , 'channels_last ' ) ] : data_format='channels_middle ' ) k.separable_conv2d ( k.variable ( np.ones ( ( 2 , 3 , 4 , 5 ) ) ) , # test in data_format = channels_first ( ( 1 , 6 , 5 , 3 ) , ( 3 , 3 , 3 , 2 ) , 'channels_last ' ) ] : for input_shape in [ ( 2 , 3 , 4 , 5 ) , ( 2 , 3 , 5 , 6 ) ] : for ( input_shape , data_format ) in [ check_two_tensor_operation ( 'conv2d ' , input_shape , kernel_shape , k.variable ( np.ones ( ( 1 , 1 , 12 , 7 ) ) ) , # Test invalid use cases data_format='channels_first ' ) check_two_tensor_operation ( 'conv3d ' , input_shape , kernel_shape , check_two_tensor_operation ( 'conv2d ' , input_shape , kernel_shape ,",['tests/keras/backend/backend_test.py'],Clean up conv backend tests ( # 9478 )
536,c2a8450279d10796e8b4ca79d375e80d7b8e055c,2018-02-24 10:46:01-08:00,"x = ZeroPadding2D ( padding= ( 1 , 1 ) , name='conv1_pad ' ) ( inputs ) x = ZeroPadding2D ( padding= ( 1 , 1 ) , name='conv_pad_ % d ' % block_id ) ( inputs ) x = Conv2D ( name='conv_dw_ % d ' % block_id ) ( x ) name='conv1 ' ) ( x ) x = ZeroPadding2D ( padding= ( 3 , 3 ) , name='conv1_pad ' ) ( img_input ) x = Conv2D ( 64 , ( 7 , 7 ) , strides= ( 2 , 2 ) , padding='valid ' , name='conv1 ' ) ( x ) padding='same ' , from .. layers import ZeroPadding2D name='conv_dw_ % d ' % block_id ) ( inputs ) padding='valid ' , name='conv1 ' ) ( inputs ) 64 , ( 7 , 7 ) , strides= ( 2 , 2 ) , padding='same ' , name='conv1 ' ) ( img_input ) padding='valid ' , padding='same ' ,","['keras/applications/mobilenet.py', 'keras/applications/resnet50.py']",Fix different results over three backends for ResNet50 and MobileNet ( # 9473 )
537,cdbe1542d7b694d96e428fcba340fa82bc4cc8e5,2018-02-23 10:03:13-08:00,"with pytest.raises ( ValueError ) : depthwise_kernel = reshape ( depthwise_kernel , depthwise_kernel_shape ) raise NotImplementedError out_filters = input_shape [ 3 ] * self.depth_multiplier depthwise_kernel = depthwise_kernel.dimshuffle ( ( 1 , 0 , 2 , 3 ) ) num_groups=input_depth ) border_mode=th_padding , for strides in [ ( 1 , 1 ) , ( 2 , 2 ) ] : def call ( self , inputs , training=None ) : raise RuntimeError ( 'Only TensorFlow backend is currently supported , ' width and height of the 2D convolution window . padding=self.padding , padding : string , ` `` same '' ` or ` `` valid '' ` . reason='MobileNets are supported only on TensorFlow ' ) self.bias , raise ValueError ( 'Inputs to ` DepthwiseConv2D ` should have rank 4 . ' data_format=self.data_format ) regularizer=self.bias_regularizer , rows = input_shape [ 1 ] config [ 'depthwise_initializer ' ] = initializers.serialize ( self.depthwise_initializer ) ` custom_objects ` parameter . raise ValueError ( 'CNTK Backend : non-square dilation_rate is ' conv_out = T.nnet.conv2d ( x , depthwise_kernel , dilation_rate=self.dilation_rate , num_col = 6 'activation ' : None , depthwise_initializer : Initializer for the depthwise kernel matrix raise ValueError ( 'Invalid strides for dilated convolution ' ) data_format='channels_middle ' ) depth_multiplier : The number of depthwise convolution output channels return ( input_shape [ 0 ] , rows , cols , out_filters ) It defaults to the ` image_data_format ` value found in your * * kwargs ) : padding : one of ` 'valid ' ` or ` 'same ' ` ( case-insensitive ) . Depthwise Separable convolutions consists in performing return self.activation ( outputs ) raise ValueError ( 'Unknown data_format ' + str ( data_format ) ) * * kwargs ) : activity_regularizer=None , # Set input spec . 'depth_multiplier ' : multiplier } , super ( DepthwiseConv2D , self ) .__init__ ( activation : Activation function to use if len ( input_shape ) < 4 : self.input_spec = InputSpec ( ndim=4 , axes= { channel_axis : input_dim } ) padding=padding , 4D tensor with shape : bias_constraint=bias_constraint , subsample=strides , bias_initializer='zeros ' , ( which acts on each input channel separately ) . else : 'strides ' : strides , name='depthwise_kernel ' , ( ( 1 , 6 , 5 , 3 ) , ( 3 , 3 , 3 , 2 ) , 'channels_last ' ) ] : strides : strides tuple ( length 2 ) . # Test invalid use cases if strides ! = ( 1 , 1 ) : input_dim , The total number of depthwise convolution output data_format=data_format , 4D tensor with shape : config.pop ( 'filters ' ) any ` dilation_rate ` value ! = 1 . Output tensor . filters=None , ` ( batch , height , width , channels ) ` while ` channels_first ` outputs = K.bias_add ( bias_regularizer : Regularizer function applied to the bias vector self.bias , return conv_out def compute_output_shape ( self , input_shape ) : padding='valid ' , If you never set it , then it will be 'channels_last ' . num_row = 7 activation=None , outputs , kernel_size : An integer or tuple/list of 2 integers , specifying the channels will be equal to ` filters_in * depth_multiplier ` . # Raises `` `` '' Depthwise separable 2D convolution . channel_axis = 1 ( ie . 'linear ' activation : ` a ( x ) = x ` ) . ` [ batch , rows , cols , channels ] ` if data_format='channels_last ' . self.depthwise_kernel , activation=activation , depthwise_kernel_shape = depthwise_kernel._keras_shape if input_shape [ channel_axis ] is None : regularizer=self.depthwise_regularizer , self.kernel_size [ 1 ] , ` [ batch , filters , new_rows , new_cols ] ` if data_format='channels_first ' objects ` relu6 ` and ` DepthwiseConv2D ` and pass them to the filters=None , if K.backend ( ) ! = 'tensorflow ' : or 4D tensor with shape : depthwise_kernel : convolution kernel for the depthwise convolution . constraint=self.depthwise_constraint ) * * kwargs ) kwargs= { 'kernel_size ' : ( 3 , 3 ) , self.depthwise_constraint = constraints.get ( depthwise_constraint ) # Set input spec . kernel_size , data_format = image_data_format ( ) # Arguments self.bias = self.add_weight ( shape= ( input_dim * self.depth_multiplier , ) , for ( input_shape , kernel_shape , data_format ) in [ rows = conv_utils.conv_output_length ( rows , self.kernel_size [ 0 ] , name='bias ' , config [ 'depthwise_constraint ' ] = constraints.serialize ( self.depthwise_constraint ) self.depth_multiplier = depth_multiplier else : 'DepthwiseConv2D ' : mobilenet.DepthwiseConv2D } ) ` ( batch , channels , height , width ) ` . layer_test ( convolutional.DepthwiseConv2D , 'should be defined . Found ` None ` . ' ) ` [ batch , rows , cols , channels ] ` if data_format='channels_last ' . filter_shape=depthwise_kernel_shape , input_dim , cols = conv_utils.conv_output_length ( cols , self.kernel_size [ 1 ] , raise ValueError ( 'Inputs to ` DepthwiseConv2D ` should have rank 4 . ' If you never set it , then it will be 'channels_last ' . padding=padding , all spatial dimensions . return ( input_shape [ 0 ] , out_filters , rows , cols ) # Test invalid use cases strides : An integer or tuple/list of 2 integers , activity_regularizer=None , raise ValueError ( 'The channel dimension of the inputs to ' outputs , depthwise_kernel_shape = ( input_depth * output_depth , 1 ) + depthwise_kernel_shape [ 2 : ] raise ValueError ( 'Unknown data_format ' , data_format ) the output of the layer ( its 'activation ' ) . depthwise_constraint=None , bias_constraint=None , config.pop ( 'kernel_initializer ' ) width and height of the 2D convolution window . self.input_spec = InputSpec ( ndim=4 , axes= { channel_axis : input_dim } ) # Output shape x = C.convolution ( depthwise_kernel , x , dilation rates for the separable convolution . use_bias : Boolean , whether the layer uses a bias vector . depthwise_kernel = _preprocess_conv2d_kernel ( depthwise_kernel , data_format ) # Input shape return _postprocess_conv2d_output ( x , data_format ) activity_regularizer=activity_regularizer , stack_size = 3 'data_format ' : 'channels_first ' , outputs = K.depthwise_conv2d ( ( ie . 'linear ' activation : ` a ( x ) = x ` ) . strides=self.strides , def test_depthwise_conv_2d ( ) : padding = _preprocess_border_mode ( padding ) padding='valid ' , for padding in _convolution_paddings : depth_multiplier=1 , ( see [ initializers ] ( .. /initializers.md ) ) . just the first step in a depthwise spatial convolution for each input channel . # Will only work if ` kernel ` is a shared variable . 'padding ' : padding , layer_test ( convolutional.DepthwiseConv2D , constraint=self.bias_constraint ) from .. layers import DepthwiseConv2D ( see [ constraints ] ( .. /constraints.md ) ) . self.kernel_size [ 1 ] , The ordering of the dimensions in the inputs . elif self.data_format == 'channels_last ' : image_shape = None The ordering of the dimensions in the inputs . if dilation_rate [ 0 ] ! = dilation_rate [ 1 ] : depthwise_regularizer : Regularizer function applied to dilation_rate=self.dilation_rate , self.bias = self.add_weight ( shape= ( input_dim * self.depth_multiplier , ) , image_shape = _preprocess_conv2d_image_shape ( int_shape ( x ) , data_format ) any ` dilation_rate ` value ! = 1 . ( see [ regularizer ] ( .. /regularizers.md ) ) . input_shape= ( num_samples , outputs = K.bias_add ( strides=dilation_rate [ 0 ] , depthwise_kernel_shape = ( self.kernel_size [ 0 ] , config.pop ( 'kernel_constraint ' ) constraint=self.depthwise_constraint ) def get_config ( self ) : If you do n't specify anything , no activation is applied self.depth_multiplier = depth_multiplier config [ 'depth_multiplier ' ] = self.depth_multiplier activity_regularizer=activity_regularizer , cols = input_shape [ 2 ] kernel_size : An integer or tuple/list of 2 integers , specifying the at ` ~/.keras/keras.json ` . if self.activation is not None : else : rows = input_shape [ 2 ] if self.bias : padding : one of ` 'valid ' ` or ` 'same ' ` ( case-insensitive ) . one of ` channels_last ` ( default ) or ` channels_first ` . bias_regularizer=None , channel_axis = 1 self.strides [ 0 ] ) stack_size ) ) kernel_size , use_bias=use_bias , outputs = K.depthwise_conv2d ( self.depthwise_regularizer = regularizers.get ( depthwise_regularizer ) 'Received input shape : ' , str ( input_shape ) ) def build ( self , input_shape ) : strides= ( 1 , 1 ) , use_bias=use_bias , 'activity_regularizer ' : 'l2 ' , strides : An integer or tuple/list of 2 integers , ` ( batch , channels , height , width ) ` . self.depthwise_initializer = initializers.get ( depthwise_initializer ) cols = conv_utils.conv_output_length ( cols , self.kernel_size [ 1 ] , `` `` '' 'not supported . ' ) The total number of depthwise convolution output 'depthwise convolution . ' ) self.strides [ 0 ] ) ` rows ` and ` cols ` values might have changed due to padding . It defaults to the ` image_data_format ` value found in your for multiplier in [ 1 , 2 ] : ( see [ activations ] ( .. /activations.md ) ) . data_format : A string , use_bias=True , bias_regularizer=bias_regularizer , rows = conv_utils.conv_output_length ( rows , self.kernel_size [ 0 ] , depthwise_constraint : Constraint function applied to initializer=self.depthwise_initializer , ` channels_last ` corresponds to inputs with shape return config ValueError : if ` data_format ` is neither ` `` channels_last '' ` or ` `` channels_first '' ` . depthwise_kernel_shape = depthwise_kernel.eval ( ) .shape 'as other backends do not support ' k.depthwise_conv2d ( k.variable ( x_val ) , k.variable ( kernel_val ) , data_format='channels_middle ' ) class DepthwiseConv2D ( Conv2D ) : ` ( batch , height , width , channels ) ` while ` channels_first ` filter_dilation=dilation_rate , strides=strides , data_format=self.data_format ) self.depth_multiplier ) auto_padding= [ False , padding , padding ] , initializer=self.bias_initializer , def test_depthwise_conv_2d ( self ) : depthwise_regularizer : Regularizer function applied to if self.use_bias : data_format=data_format ) def get_config ( self ) : `` `` '' Depthwise separable 2D convolution . the output of the layer ( its 'activation ' ) . data_format=None , config = super ( DepthwiseConv2D , self ) .get_config ( ) config [ 'depthwise_regularizer ' ] = regularizers.serialize ( self.depthwise_regularizer ) The ` depth_multiplier ` argument controls how many bias_constraint : Constraint function applied to the bias vector if self.activation is not None : 'depthwise_constraint ' : 'unit_norm ' , return ( input_shape [ 0 ] , rows , cols , out_filters ) ' ` DepthwiseConv2D ` ' depthwise_constraint=None , depth_multiplier=1 , data_format=self.data_format ) if dilation_rate == ( 1 , 1 ) : out_filters = input_shape [ 1 ] * self.depth_multiplier if data_format not in { 'channels_first ' , 'channels_last ' } : elif self.data_format == 'channels_last ' : return self.activation ( outputs ) 'Received input shape : ' , str ( input_shape ) ) 'relu6 ' : mobilenet.relu6 } ) bias_regularizer : Regularizer function applied to the bias vector config.pop ( 'kernel_regularizer ' ) input_shape=image_shape , with pytest.raises ( ValueError ) : corresponds to inputs with shape shape=depthwise_kernel_shape , x = _preprocess_conv2d_input ( x , data_format ) if hasattr ( x , '_keras_shape ' ) : batch_input_shape= ( None , None , 5 , None ) ) ] ) config [ 'depthwise_constraint ' ] = constraints.serialize ( self.depthwise_constraint ) if padding == 'same ' and strides ! = ( 1 , 1 ) : kwargs= { 'kernel_size ' : 3 , kernel_size=3 , dilation_rate : tuple of integers , check_two_tensor_operation ( 'depthwise_conv2d ' , super ( DepthwiseConv2D , self ) .__init__ ( 'depth_multiplier ' : multiplier } , class DepthwiseConv2D ( Conv2D ) : ( see [ activations ] ( .. /activations.md ) ) . # channels_first input shape : ( n , input_depth , rows , cols ) ` rows ` and ` cols ` values might have changed due to padding . kernel_size=kernel_size , return ( input_shape [ 0 ] , out_filters , rows , cols ) depthwise_initializer='glorot_uniform ' , Depthwise Separable convolutions consists in performing return outputs depth_multiplier : The number of depthwise convolution output channels if data_format is None : Keras config file at ` ~/.keras/keras.json ` . self.depthwise_constraint = constraints.get ( depthwise_constraint ) # Output shape groups=x.shape [ 0 ] ) The ` depth_multiplier ` argument controls how many def call ( self , inputs , training=None ) : self.bias = None for each input channel . ` channels_last ` corresponds to inputs with shape self.bias_initializer = initializers.get ( bias_initializer ) ( see [ initializers ] ( .. /initializers.md ) ) . ` [ batch , new_rows , new_cols , filters ] ` if data_format='channels_last ' . conv_out = _postprocess_conv2d_output ( conv_out , x , padding , for k in BACKENDS : bias_regularizer=bias_regularizer , depthwise_regularizer=None , shape=depthwise_kernel_shape , 'padding ' : padding , the depthwise kernel matrix # Arguments return config if input_shape [ channel_axis ] is None : self.padding , Can be a single integer to specify the same value for self.built = True objects ` relu6 ` and pass them to the ` custom_objects ` parameter . ( ( 2 , 3 , 5 , 6 ) , ( 4 , 3 , 3 , 4 ) , 'channels_first ' ) , activation=None , bias_constraint : Constraint function applied to the bias vector bias_initializer : Initializer for the bias vector 'bias_regularizer ' : 'l2 ' , BACKENDS , cntk_dynamicity=True , if self.data_format == 'channels_first ' : th_padding = _preprocess_padding ( padding ) ( see [ regularizer ] ( .. /regularizers.md ) ) . ( -1 , 1 ) + depthwise_kernel.shape [ 2 : ] ) depthwise_constraint : Constraint function applied to 'depthwise_regularizer ' : 'l2 ' , x : input tensor config [ 'depthwise_regularizer ' ] = regularizers.serialize ( self.depthwise_regularizer ) initializer=self.depthwise_initializer , activity_regularizer : Regularizer function applied to ` [ batch , channels , rows , cols ] ` if data_format='channels_first ' depthwise_initializer : Initializer for the depthwise kernel matrix raise ValueError ( 'The channel dimension of the inputs to ' self.depthwise_regularizer = regularizers.get ( depthwise_regularizer ) depthwise_kernel_shape = _preprocess_conv2d_filter_shape ( depthwise_kernel_shape , data_format ) just the first step in a depthwise spatial convolution ` image_data_format='channels_last ' ` in your Keras config 'relu6 ' : mobilenet.relu6 , ( ( 2 , 3 , 4 , 5 ) , ( 2 , 2 , 3 , 4 ) , 'channels_first ' ) , initializer=self.bias_initializer , self.padding , if self.data_format == 'channels_first ' : output channels are generated per input channel in the depthwise step . num_samples = 2 return outputs bias_initializer : Initializer for the bias vector ` [ batch , channels , rows , cols ] ` if data_format='channels_first ' bias_regularizer=None , config [ 'depthwise_initializer ' ] = initializers.serialize ( self.depthwise_initializer ) depthwise_kernel = C.reshape ( C.transpose ( depthwise_kernel , ( 1 , 0 , 2 , 3 ) ) , use_bias : Boolean , whether the layer uses a bias vector . `` `` '' 2D convolution with separable filters . continue cols = input_shape [ 2 ] def compute_output_shape ( self , input_shape ) : config.pop ( 'kernel_initializer ' ) config.pop ( 'kernel_regularizer ' ) depthwise_kernel_shape , strides , data_format ) channels will be equal to ` filters_in * depth_multiplier ` . input_shape , kernel_shape , bias_initializer='zeros ' , activation=activation , input_dim = int ( input_shape [ channel_axis ] ) # Test invalid use case ` [ batch , new_rows , new_cols , filters ] ` if data_format='channels_last ' . out_filters = input_shape [ 1 ] * self.depth_multiplier input_depth = depthwise_kernel_shape [ 1 ] regularizer=self.bias_regularizer , data_format : A string , config.pop ( 'kernel_constraint ' ) k.depthwise_conv2d ( k.variable ( np.ones ( input_shape ) ) , ' ` DepthwiseConv2D ` ' depthwise_initializer='glorot_uniform ' , 'should be defined . Found ` None ` . ' ) k.variable ( np.ones ( kernel_shape ) ) , output channels are generated per input channel in the depthwise step . name='bias ' , specifying the strides of the convolution along the width and height . kernel_size=kernel_size , config = super ( DepthwiseConv2D , self ) .get_config ( ) activation : Activation function to use @ pytest.mark.skipif ( ( K.backend ( ) ! = 'tensorflow ' ) , regularizer=self.depthwise_regularizer , num_col , # Input shape depthwise_regularizer=None , activity_regularizer : Regularizer function applied to depthwise_kernel_shape = ( self.kernel_size [ 0 ] , or 4D tensor with shape : self.depthwise_kernel = self.add_weight ( strides = ( 1 , ) + strides padding=self.padding , specifying the strides of the convolution along the width and height . Specifying any stride value ! = 1 is incompatible with specifying ( see [ constraints ] ( .. /constraints.md ) ) . If you do n't specify anything , no activation is applied all spatial dimensions . input_shape= ( num_samples , stack_size , num_row , num_col ) ) self.built = True self.strides [ 1 ] ) def __init__ ( self , self.depthwise_initializer = initializers.get ( depthwise_initializer ) inputs , data_format=None , self.strides [ 1 ] ) inputs , ( which acts on each input channel separately ) . use_bias=True , 'strides ' : strides , self.depth_multiplier ) # TF kernel shape : ( rows , cols , input_depth , depth_multiplier ) cols = input_shape [ 3 ] self.depthwise_kernel , out_filters = input_shape [ 3 ] * self.depth_multiplier constraint=self.bias_constraint ) strides= ( 1 , 1 ) , strides=self.strides , `` `` '' strides=strides , Sequential ( [ convolutional.DepthwiseConv2D ( with pytest.raises ( ValueError ) : strides=strides , rows = input_shape [ 2 ] # Returns data_format=self.data_format ) if len ( input_shape ) < 4 : Specifying any stride value ! = 1 is incompatible with specifying def build ( self , input_shape ) : output_depth = depthwise_kernel_shape [ 0 ] depthwise_kernel = depthwise_kernel [ : , : , : :-1 , : :-1 ] corresponds to inputs with shape data_format : string , ` `` channels_last '' ` or ` `` channels_first '' ` . def __init__ ( self , config [ 'depth_multiplier ' ] = self.depth_multiplier one of ` channels_last ` ( default ) or ` channels_first ` . rows = input_shape [ 1 ] config.pop ( 'filters ' ) bias_constraint=None , ` [ batch , filters , new_rows , new_cols ] ` if data_format='channels_first ' * * kwargs ) channel_axis = 3 if hasattr ( depthwise_kernel , '_keras_shape ' ) : self.bias = None Note that only TensorFlow is supported for now , therefore it only works with the data format name='depthwise_kernel ' , the depthwise kernel matrix self.depthwise_kernel = self.add_weight ( channel_axis = 3 cols = input_shape [ 3 ] input_dim = int ( input_shape [ channel_axis ] ) Can be a single integer to specify the same value for num_row , Keras config file at ` ~/.keras/keras.json ` . data_format=data_format , if self.bias : self.bias_initializer = initializers.get ( bias_initializer ) if self.use_bias : bias_constraint=bias_constraint ,","['keras/applications/mobilenet.py', 'keras/backend/cntk_backend.py', 'keras/backend/theano_backend.py', 'keras/layers/convolutional.py', 'tests/keras/applications/applications_test.py', 'tests/keras/backend/backend_test.py', 'tests/keras/layers/convolutional_test.py']",Add depthwise conv2d for Theano and CNTK ( # 9457 )
538,06eaeebecfb73c23bfd531013ca172ee3bf5069c,2018-02-22 11:09:18-08:00,"if strides ! = ( 1 , 1 ) : raise ValueError ( 'Invalid strides for dilated convolution ' ) pointwise_val , data_format='channels_middle ' ) _ , pointwise_val = parse_shape_or_val ( ( 1 , 1 ) + ( input_depth * depth_multiplier , 7 ) ) x_tf = KTF.variable ( x_val ) pointwise_kernel = _preprocess_conv2d_kernel ( pointwise_kernel , data_format ) data_format=data_format ) ) z_c = cntk_func_three_tensor ( 'separable_conv2d ' , input_shape , raise NotImplementedError z_tf = KTF.eval ( KTF.separable_conv2d ( x_tf , KTF.variable ( depthwise_val ) , data_format=data_format ) ( [ x_val ] ) [ 0 ] auto_padding= [ False ] ) for ( input_shape , data_format ) in [ ( ( 2 , 3 , 4 , 5 ) , 'channels_first ' ) , ( -1 , 1 ) + depthwise_kernel.shape [ 2 : ] ) data_format = image_data_format ( ) def test_separable_conv2d ( self ) : def cntk_func_three_tensor ( function_name , x_shape , y , z , * * kwargs ) : ( ( 1 , 6 , 5 , 3 ) , 'channels_last ' ) ] : for depth_multiplier in [ 1 , 2 ] : if dilation_rate == ( 1 , 1 ) : raise ValueError ( 'Unknown data_format ' + str ( data_format ) ) strides=dilation_rate [ 0 ] , if data_format not in { 'channels_first ' , 'channels_last ' } : continue return KC.function ( [ xc ] , [ output_cntk ] ) depthwise_val , strides = ( 1 , ) + strides x = C.convolution ( depthwise_kernel , x , k.variable ( pointwise_val ) , if data_format is None : depthwise_kernel = _preprocess_conv2d_kernel ( depthwise_kernel , data_format ) return _postprocess_conv2d_output ( x , data_format ) KTF.variable ( pointwise_val ) , strides=strides , auto_padding= [ False , padding , padding ] , groups=x.shape [ 0 ] ) for k in [ KTF , KC ] : _ , depthwise_val = parse_shape_or_val ( kernel_shape + ( input_depth , depth_multiplier ) ) with pytest.raises ( ValueError ) : 'not supported . ' ) strides= ( 1 , 1 , 1 ) , padding = _preprocess_border_mode ( padding ) assert_allclose ( z_tf , z_c , 1e-3 ) k.variable ( depthwise_val ) , _ , x_val = parse_shape_or_val ( input_shape ) ( ( 2 , 3 , 5 , 6 ) , 'channels_first ' ) , x = _preprocess_conv2d_input ( x , data_format ) output_cntk = getattr ( KC , function_name ) ( xc , KC.variable ( y ) , KC.variable ( z ) , * * kwargs ) xc = KC.placeholder ( x_shape ) raise ValueError ( 'CNTK Backend : non-square dilation_rate is ' x = C.convolution ( pointwise_kernel , x , depthwise_kernel = C.reshape ( C.transpose ( depthwise_kernel , ( 1 , 0 , 2 , 3 ) ) , else : auto_padding= [ False , padding , padding ] ) for kernel_shape in [ ( 2 , 2 ) , ( 4 , 3 ) ] : k.separable_conv2d ( k.variable ( x_val ) , input_depth = input_shape [ 1 ] if data_format == 'channels_first ' else input_shape [ -1 ] if dilation_rate [ 0 ] ! = dilation_rate [ 1 ] : # Test invalid use cases @ pytest.mark.skipif ( K.backend ( ) ! = 'tensorflow ' , reason='Requires TF backend ' ) if dilation_rate ! = ( 1 , 1 ) and K.backend ( ) == 'cntk ' :","['keras/backend/cntk_backend.py', 'tests/keras/backend/backend_test.py', 'tests/keras/layers/convolutional_test.py']",Add separable conv2d for CNTK ( # 9442 )
539,4f2e65c385d60fa87bb143c6c506cbe428895f44,2018-02-22 11:06:24-08:00,"# the data , split between train and test sets # the data , shuffled and split between train and test sets # the data , shuffled and split between train and test sets # The data , shuffled and split between train and test sets : # the data , split between train and test sets # The data , shuffled and split between train and test sets . # The data , split between train and test sets : # The data , split between train and test sets .","['examples/antirectifier.py', 'examples/cifar10_cnn.py', 'examples/cifar10_cnn_tfaugment2d.py', 'examples/mnist_cnn.py', 'examples/mnist_hierarchical_rnn.py', 'examples/mnist_irnn.py', 'examples/mnist_mlp.py', 'examples/mnist_siamese.py', 'examples/mnist_swwae.py', 'examples/mnist_transfer_cnn.py']",Remove word “ shuffled ” from comments in examples ( # 9453 )
540,8c2944e00121b6182ca7e0ee7d9e6537eb3e56e5,2018-02-21 16:38:45-05:00,"x = img_to_array ( x ) x = random_brightness ( x , self.brightness_range ) from PIL import ImageEnhance 'Received arg : ' , brightness_range ) if self.brightness_range is not None : return x if len ( brightness_range ) ! = 2 : u = np.random.uniform ( brightness_range [ 0 ] , brightness_range [ 1 ] ) self.brightness_range = brightness_range raise ValueError ( ' ` brightness_range should be tuple or list of two floats . ' x = array_to_img ( x ) x = imgenhancer_Brightness = ImageEnhance.Brightness ( x ) brightness_range=None , brightness_range= ( 1 , 5 ) , brightness_range : the range of brightness to apply x = imgenhancer_Brightness.enhance ( u ) def random_brightness ( x , brightness_range ) :","['keras/preprocessing/image.py', 'tests/keras/preprocessing/image_test.py']",Add random brightness to Image Preprocessing ( Code Cleanup ) ( # 9390 )
541,6b329a17e194e6e945bda87e0707ccb97e481746,2018-02-21 11:00:27-08:00,if tf.get_default_session ( ) is not None : session = default_session session = tf.get_default_session ( ) if default_session is not None : default_session = tf.get_default_session ( ),['keras/backend/tensorflow_backend.py'],Misc : Slight optimisation ( # 9445 )
542,ff0690ab533b7b882301f8b836ccfda8fd4f7f21,2018-02-21 10:52:01-08:00,"return wrapper with closing ( self.executor_fn ( _SHARED_SEQUENCES ) ) as executor : `` `` '' Decorator to test both Unix ( fork ) and Windows ( spawn ) '' '' '' with closing ( self.executor_fn ( ) ) as executor : def use_spawn ( func ) : _SHARED_SEQUENCES = seqs def init_pool ( seqs ) : mp.set_start_method ( 'spawn ' , force=True ) # We do not need the init since it 's threads . self.executor_fn = lambda seqs : multiprocessing.Pool ( workers , import six initializer=init_pool , out = func ( * args , * * kwargs ) mp.set_start_method ( 'fork ' , force=True ) self.executor_fn = lambda _ : ThreadPool ( workers ) import multiprocessing as mp initargs= ( seqs , ) ) func ( * args , * * kwargs ) def wrapper ( * args , * * kwargs ) : self.executor_fn = lambda : multiprocessing.Pool ( workers ) global _SHARED_SEQUENCES if sys.version_info > ( 3 , 4 ) : return out self.executor_fn = lambda : ThreadPool ( workers )","['keras/utils/data_utils.py', 'tests/keras/utils/data_utils_test.py']",Force update of Sequences for Windows ( # 9436 )
543,2ba2076275fab79cf4b8c281ccce506b257cab21,2018-02-21 10:06:42-08:00,"raise ValueError ( ' x ( images tensor ) and y ( labels ) ' `` `` '' Deserialize a layer , then call it on appropriate inputs . `` `` '' Adds losses to the layer . raise ValueError ( ' ` x ` ( images tensor ) and ` y ` ( labels ) ' `` `` '' Add losses to the layer . `` `` '' Counts the total number of scalars composing the weights . use_multiprocessing=use_multiprocessing , `` `` '' Count the total number of scalars composing the weights . `` `` '' Deserializes a layer , then call it on appropriate inputs . `` `` '' Checks if conversion on kernel matrices is required during weight loading . `` `` '' Save the model to a single HDF5 file . use_multiprocessing=use_multiprocessing ) `` `` '' Add updates to the layer . `` `` '' Retrieve the model 's losses . val_enqueuer = GeneratorEnqueuer ( `` `` '' Adds updates to the layer . `` `` '' Check if conversion on kernel matrices is required during weight loading . wait_time=wait_time ) use_multiprocessing=use_multiprocessing , `` `` '' Calls the model on new inputs . val_enqueuer = OrderedEnqueuer ( validation_data , `` `` '' Retrieve the model 's updates . raise ValueError ( 'device_type should be either `` CPU '' or `` GPU '' . ' ) raise ValueError ( ' ` device_type ` should be either `` CPU '' or `` GPU '' . ' ) val_enqueuer = GeneratorEnqueuer ( validation_data , `` `` '' Retrieves the model 's updates . wait_time=wait_time ) `` `` '' Saves the model to a single HDF5 file . use_multiprocessing=use_multiprocessing ) val_enqueuer = OrderedEnqueuer ( `` `` '' Retrieves the model 's losses . validation_data , `` `` '' Call the model on new inputs .","['keras/backend/tensorflow_backend.py', 'keras/engine/topology.py', 'keras/engine/training.py', 'keras/preprocessing/image.py']",Style fixes ( # 9441 )
544,531a8eabe222c3831cc5b55bafc81d2aa36f0672,2018-02-20 10:43:14-08:00,raise ValueError ( 'Length of the specified weight list ( ' ' ) does not match the number of weights ' if len ( params ) ! = len ( weights ) : str ( len ( weights ) ) 'of the optimizer ( ' + str ( len ( params ) ) + ' ) ' ),['keras/optimizers.py'],Optimizer - set_weights : check weights length ( # 9435 )
545,ba204bc43847a62de8eb8a7941da24f1fe6ac2a6,2018-02-20 10:40:46-08:00,"assert conv_utils.conv_output_length ( 32 , 5 , 'full ' , 2 ) == 18 conv_utils.normalize_tuple ( None , 2 , 'kernel_size ' ) assert conv_utils.conv_output_length ( 224 , 7 , 'same ' , 2 ) == 112 with pytest.raises ( AssertionError ) : assert conv_utils.normalize_tuple ( 5 , 2 , 'kernel_size ' ) == ( 5 , 5 ) if __name__ == '__main__ ' : assert conv_utils.deconv_length ( 32 , 1 , 5 , 'valid ' ) == 36 assert conv_utils.deconv_length ( 224 , 1 , 7 , 'same ' ) == 224 assert conv_utils.deconv_length ( 32 , 1 , 5 , 'full ' ) == 28 def test_invalid_data_format ( ) : conv_utils.normalize_data_format ( 'channels_middle ' ) import numpy as np assert conv_utils.conv_output_length ( None , 7 , 'same ' , 1 ) is None assert conv_utils.conv_input_length ( 36 , 5 , 'full ' , 1 ) == 32 assert conv_utils.deconv_length ( 224 , 2 , 7 , 'same ' ) == 448 conv_utils.normalize_padding ( 'diagonal ' ) assert conv_utils.conv_input_length ( None , 7 , 'same ' , 1 ) is None assert conv_utils.deconv_length ( 32 , 2 , 5 , 'full ' ) == 59 conv_utils.convert_kernel ( np.zeros ( ( 10 , 20 ) ) ) conv_utils.normalize_tuple ( [ 2 , 3 , 4 ] , 2 , 'kernel_size ' ) assert conv_utils.conv_input_length ( 14 , 5 , 'valid ' , 2 ) == 31 def test_deconv_length ( ) : conv_utils.normalize_tuple ( [ 'str ' , 'impossible ' ] , 2 , 'kernel_size ' ) def test_invalid_convert_kernel ( ) : pytest.main ( [ __file__ ] ) assert conv_utils.conv_input_length ( 112 , 7 , 'same ' , 1 ) == 112 assert conv_utils.conv_input_length ( 28 , 5 , 'valid ' , 1 ) == 32 assert conv_utils.conv_output_length ( 32 , 5 , 'causal ' , 2 ) == 16 def test_conv_input_length ( ) : assert conv_utils.conv_output_length ( 32 , 5 , 'full ' , 1 ) == 36 assert conv_utils.deconv_length ( None , 1 , 7 , 'same ' ) is None assert conv_utils.deconv_length ( 32 , 2 , 5 , 'valid ' ) == 67 with pytest.raises ( ValueError ) : from keras.utils import conv_utils assert conv_utils.conv_output_length ( 32 , 5 , 'valid ' , 1 ) == 28 assert conv_utils.conv_output_length ( 224 , 7 , 'same ' , 1 ) == 224 assert conv_utils.conv_output_length ( 32 , 5 , 'causal ' , 1 ) == 32 conv_utils.conv_output_length ( 18 , 5 , 'diagonal ' , 2 ) import pytest def test_invalid_padding ( ) : assert conv_utils.conv_input_length ( 18 , 5 , 'full ' , 2 ) == 31 assert conv_utils.normalize_tuple ( [ 7 , 9 ] , 2 , 'kernel_size ' ) == ( 7 , 9 ) def test_normalize_tuple ( ) : conv_utils.conv_output_length ( 32 , 5 , 'diagonal ' , 2 ) def test_conv_output_length ( ) : assert conv_utils.conv_output_length ( 32 , 5 , 'valid ' , 2 ) == 14 assert conv_utils.conv_input_length ( 112 , 7 , 'same ' , 2 ) == 223",['tests/keras/utils/conv_utils_test.py'],Add ` conv_utils_test ` ( # 9429 )
546,84f755259cf64cf38395ce5e5bafa36df9f979e8,2018-02-20 10:39:31-08:00,"if K.backend ( ) ! = 'cntk ' : # CNTK does not support dynamic padding . _test_application_variable_input_channels ( app , last_dim ) _test_application_variable_input_channels ( app , last_dim )",['tests/keras/applications/applications_test.py'],Enable ` variable_input_channels ` test for ` InceptionV3 ` ( # 9425 )
547,4eab0556d29f11ff41758d80c15d6457263f6a93,2018-02-17 13:29:35-08:00,"HDF5_OBJECT_HEADER_LIMIT = 64512 chunk_id += 1 n_weight_names_arrays = len ( [ attr for attr in h5file [ 'model_weights ' ] [ 'nested_model ' ] .attrs # possible . g.attrs [ 'weight_names ' ] = weight_names # because in that case even chunking the array would not make the saving # Expecting this to never be true . out2 = model.predict ( x ) f = nested_model ( x ) model.train_on_batch ( x , y ) bad_attributes = [ x for x in data if len ( x ) > HDF5_OBJECT_HEADER_LIMIT ] # This will never loop forever thanks to the test above . chunked_data = np.array_split ( data_npy , num_chunks ) # amout of memory for every item , it increases the memory # out of proportion . Note that it fits into the internal HDF5 chunked_data = np.array_split ( data_npy , num_chunks ) # the list of layer names into numpy array , which uses the same x = Input ( shape= ( 2 , ) , name='input_ ' + ( ' x ' * ( 2 * * 15 ) ) ) ' , '.join ( [ x for x in bad_attributes ] ) ) ) # of layer names . # Check that no item in ` data ` is larger than ` HDF5_OBJECT_HEADER_LIMIT ` def _load_attributes_from_hdf5_group ( group , name ) : # attribute memory limit on its own but because h5py converts os.remove ( fname ) group : A pointer to a HDF5 group . 'because they are larger than % d bytes : % s ' num_chunks += 1 assert n_layer_names_arrays > 0 f = Dense ( 2 , name='nested_model_dense_ % d ' % ( i , ) ) ( f ) data = [ n.decode ( 'utf8 ' ) for n in group.attrs [ name ] ] save_model ( model , fname ) name : A name of the attributes to load . name : A name of the attributes to save . f.attrs [ 'layer_names ' ] = [ layer.name.encode ( 'utf8 ' ) for layer in layers ] # requirements substantially . model = Model ( inputs= [ x ] , outputs= [ f ] ) `` `` '' Loads attributes of the specified name from the HDF5 group . This method for i in range ( 4 ) : data larger than HDF5_OBJECT_HEADER_LIMIT bytes . layer_names = _load_attributes_from_hdf5_group ( f , 'layer_names ' ) weight_names = [ n.decode ( 'utf8 ' ) for n in g.attrs [ 'weight_names ' ] ] data : Attributes data . from numpy.testing import assert_allclose `` `` '' Saves attributes ( data ) of the specified name into the HDF5 group . num_chunks = 1 data : Attributes data to store . y = np.random.random ( ( 1 , 2 ) ) from numpy.testing import assert_raises if len ( bad_attributes ) > 0 : def test_saving_model_with_long_weights_names ( ) : else : f = x chunk_id = 0 This method deals with an inherent problem of HDF5 file which is not # This layer name will make the ` weights_name ` for chunk_id , chunk_data in enumerate ( chunked_data ) : x = np.random.random ( ( 1 , 2 ) ) # of weight names . data.extend ( [ n.decode ( 'utf8 ' ) for n in group.attrs [ ' % s % d ' % ( name , chunk_id ) ] ] ) while ( ' % s % d ' % ( name , chunk_id ) ) in group.attrs : x = Input ( shape= ( 2 , ) , name='nested_model_input ' ) n_layer_names_arrays = len ( [ attr for attr in h5file [ 'model_weights ' ] .attrs _save_attributes_to_hdf5_group ( f , 'layer_names ' , [ layer.name.encode ( 'utf8 ' ) for layer in layers ] ) model = load_model ( fname ) with h5py.File ( fname , ' r ' ) as h5file : # This layer name will make the ` layers_name ` HDF5 attribute blow `` `` '' # The chunking of layer names array should have happend . raise RuntimeError ( 'the following attributes can not be saved to HDF5 file ' assert_allclose ( out , out2 , atol=1e-05 ) x = Input ( shape= ( 2 , ) , name='outer_model_input ' ) % ( HDF5_OBJECT_HEADER_LIMIT , def test_saving_model_with_long_layer_names ( ) : data_npy = np.asarray ( data ) from numpy.testing import assert_allclose , assert_raises if name in group.attrs : def _save_attributes_to_hdf5_group ( group , name , data ) : nested_model = Model ( inputs= [ x ] , outputs= [ f ] , name='nested_model ' ) layer_names = [ n.decode ( 'utf8 ' ) for n in f.attrs [ 'layer_names ' ] ] # Returns # HDF5 attribute blow out of proportion . if attr.startswith ( 'layer_names ' ) ] ) import h5py model.compile ( loss='mse ' , optimizer='adam ' , metrics= [ 'acc ' ] ) out = model.predict ( x ) able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes . f = Dense ( 2 , name='outer_model_output ' ) ( f ) if attr.startswith ( 'weight_names ' ) ] ) f = Dense ( 2 , name='dense_ % d ' % ( i , ) ) ( f ) # Arguments group.attrs [ name ] = data while any ( map ( lambda x : x.nbytes > HDF5_OBJECT_HEADER_LIMIT , chunked_data ) ) : deals with an inherent problem of HDF5 file which is not able to store _save_attributes_to_hdf5_group ( g , 'weight_names ' , weight_names ) f = Dense ( 2 , name='nested_model_output ' + ( ' x ' * ( 2 * * 15 ) ) ) ( f ) if num_chunks > 1 : return data data = [ ] group.attrs [ ' % s % d ' % ( name , chunk_id ) ] = chunk_data _ , fname = tempfile.mkstemp ( '.h5 ' ) # Check that the HDF5 files contains chunked array weight_names = _load_attributes_from_hdf5_group ( g , 'weight_names ' ) assert n_weight_names_arrays > 0","['keras/engine/topology.py', 'tests/test_model_saving.py']",Allow saving weights of a very deep model into a HDF5 file . ( # 9398 )
548,173a1a545954bae38e40f4fb0bde228765a9b059,2018-02-17 13:27:18-08:00,"self.lower , tokenizer.fit_on_texts ( word_sequences ) tokenizer = Tokenizer ( ) seq = text if self.char_level or isinstance ( text , list ) : self.document_count += len ( sequences ) texts = [ 'The cat sat on the mat . ' , `` `` '' Transforms each text in ` texts ` in a sequence of integers . `` `` '' Transforms each text in texts in a sequence of integers . tokenizer.fit_on_texts ( texts ) [ 'The ' , 'cat ' , 'is ' , 'sitting ' ] , a generator of strings ( for memory-efficiency ) , self.index_docs = { } tokenizer.texts_to_matrix ( word_sequences ) self.split ) self.document_count = len ( sequences ) In the case where texts contains lists , we assume each entry of the lists self.document_count = 0 or a list of list of strings . to be a token . [ 'The ' , 'dog ' , 'is ' , 'standing ' ] self.filters , self.filters , assert tokenizer.document_count == 5 tokenizer.texts_to_matrix ( texts ) 'The dog sat on the log . ' , or a generator of strings ( for memory-efficiency ) def test_sequential_fit ( ) : word_sequences = [ else : Each item in texts can also be a list , in which case we assume each item of that list 'Dogs and cats living together . ' ] self.split ) self.lower , seq = text_to_word_sequence ( text , ] self.index_docs = { } seq = text if self.char_level else text_to_word_sequence ( text ,","['keras/preprocessing/text.py', 'tests/keras/preprocessing/text_test.py']",Fit and transform sequences of words ( # 9384 )
549,f1df4297c27f4a2dab675d253be67eb0372817af,2018-02-17 13:25:59-08:00,"if isinstance ( m , Layer ) : if hasattr ( self , 'metrics ' ) : m.reset_states ( ) progbar = Progbar ( target=num_samples ) progbar = Progbar ( target=num_samples , for m in self.metrics : stateful_metrics=self.stateful_metric_names ) progbar = Progbar ( target=steps ) progbar = Progbar ( target=steps ,",['keras/engine/training.py'],remove metrics for predict ( # 9411 )
550,5a7906ba6156f57025520fadbff52331eec4a12d,2018-02-15 10:08:27-08:00,"conda create -q -n test-environment python= $ TRAVIS_PYTHON_VERSION numpy nose scipy matplotlib pandas pytest h5py conda create -q -n test-environment python= $ TRAVIS_PYTHON_VERSION pytest pandas pip install theano pip install -- only-binary=numpy , scipy numpy nose scipy matplotlib h5py theano",['.travis.yml'],Speed up Travis tests ( # 9386 )
551,dc698c5486117780b643eda0a2f60a8753625b8a,2018-02-12 19:29:21-08:00,"Copyright ( c ) 2017 - 2018 , Microsoft , Inc . Copyright ( c ) 2017 , Microsoft , Inc . Copyright ( c ) 2015 - 2018 , Google , Inc . Copyright ( c ) 2015 - 2018 , François Chollet . Copyright ( c ) 2015 - 2018 , the respective contributors . Copyright ( c ) 2015 , Google , Inc . Copyright ( c ) 2015 , François Chollet . Copyright ( c ) 2015 - 2017 , the respective contributors .",['LICENSE'],Corrected copyright years ( # 9375 )
